{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datasets import load_metric, Dataset\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from datasets import load_dataset\n",
    "from lexicalrichness import LexicalRichness\n",
    "import functools\n",
    "import string\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import operator\n",
    "import spacy\n",
    "import textstat\n",
    "import difflib as dl\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "np.random.seed(seed)\n",
    "run_name = \"mild-jazz-178\"\n",
    "wandb_run_path = \"uts_nlp/travis_attack/36206pkc\"  # get this from wandb overview section \n",
    "\n",
    "# Init W&B run object\n",
    "api = wandb.Api()\n",
    "run = api.run(wandb_run_path)\n",
    "config = run.config\n",
    "\n",
    "# Init spacy model and other global nlp things\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "# Load CSV files\n",
    "path_results = f\"../model_checkpoints/travis_attack/{run_name}/\"\n",
    "train            = pd.read_csv(path_results + \"train.csv\")\n",
    "valid            = pd.read_csv(path_results + \"valid.csv\")\n",
    "test             = pd.read_csv(path_results + \"test.csv\")\n",
    "training_step    = pd.read_csv(path_results + \"training_step.csv\")\n",
    "training_summary = pd.read_csv(path_results + \"training_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            with pd.option_context(\"max_colwidth\", 480):\n",
    "                display(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General run information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Summary \n",
       "\n",
       "**Dataset**: rotten_tomatoes  \n",
       "**Paraphrase model**: `eugenesiow/bart-paraphrase`  \n",
       "**Victim model**: `textattack/distilbert-base-uncased-rotten-tomatoes`  \n",
       "**Semantic Textual Similarity model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`  \n",
       "**Number of epochs**: 30  \n",
       "**Reward function**: `[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]`  \n",
       "**Using the full dataset**: True   \n",
       "We freeze all layers of the paraphrase model except the last **2** layers.  \n",
       "We eval every **1** epochs.   \n",
       "\n",
       "### Paraphrase parameters \n",
       "**Decoding strategy**: greedy  \n",
       "**Number of beams**: 1  \n",
       "**Number of return sequences**: 1  \n",
       "**Max length**: 64  \n",
       "**Min length**: 5  \n",
       "**Temperature**: 1.5  \n",
       "**Length penalty**: 1  \n",
       "**Number of beam groups**: 1  \n",
       "**Diversity penalty**: 0  \n",
       "\n",
       "  \n",
       "### Run parameters\n",
       "**Seed**: 420  \n",
       "**Learning rate**: 3e-05  \n",
       "**Batch sizes:** Train: 32, Eval: 128  \n",
       "**Max number of tokens in input**: 64  \n",
       "**Remove initially misclassified examples**: True  \n",
       "**Input bucketed by length:** True  \n",
       "**Shuffle training data:** False  \n",
       "**Pad input data to multiple of**: 8  \n",
       "**Pad embedding matrices**: True  \n",
       "**Normalise rewards?**: False  \n",
       "**Gradient accumulation?**: False  \n",
       "\n",
       "### Low-level parameters \n",
       "**Use fp16 for training?**: True  \n",
       "**Use memory pinning with dataloaders?**: True  \n",
       "**Initilise gradients with `None` when running `zero_grad()`**: False  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below \n",
    "Markdown(f\"\"\"\n",
    "### Summary \n",
    "\n",
    "**Dataset**: {config['dataset_name']}  \n",
    "**Paraphrase model**: `{config['pp_name']}`  \n",
    "**Victim model**: `{config['vm_name']}`  \n",
    "**Semantic Textual Similarity model**: `{config['sts_name']}`  \n",
    "**Number of epochs**: {config['n_train_epochs']}  \n",
    "**Reward function**: `{config['reward_strategy']}`  \n",
    "**Using the full dataset**: {True if not config['use_small_ds'] else f\"False: n_shards set to {config['shard_params']['n_shards']} shards with shard_contiguous set to {config['shard_params']['shard_contiguous']}\"}   \n",
    "We freeze all layers of the paraphrase model except the last **{config['n_layers_frozen']}** layers.  \n",
    "We eval every **{config['eval_freq']}** epochs.   \n",
    "\n",
    "### Paraphrase parameters \n",
    "**Decoding strategy**: {config['sampling_strategy']}  \n",
    "**Number of beams**: {config['pp_model_params']['num_beams']}  \n",
    "**Number of return sequences**: {config['pp_model_params']['num_return_sequences']}  \n",
    "**Max length**: {config['pp_model_params']['max_length']}  \n",
    "**Min length**: {config['pp_model_params']['min_length']}  \n",
    "**Temperature**: {config['pp_model_params']['temperature']}  \n",
    "**Length penalty**: {config['pp_model_params']['length_penalty']}  \n",
    "**Number of beam groups**: {config['pp_model_params']['num_beam_groups']}  \n",
    "**Diversity penalty**: {config['pp_model_params']['diversity_penalty']}  \n",
    "\n",
    "  \n",
    "### Run parameters\n",
    "**Seed**: {config['seed']}  \n",
    "**Learning rate**: {config['lr']}  \n",
    "**Batch sizes:** Train: {config['batch_size_train']}, Eval: {config['batch_size_eval']}  \n",
    "**Max number of tokens in input**: {config['max_length']}  \n",
    "**Remove initially misclassified examples**: {config['remove_misclassified_examples']}  \n",
    "**Input bucketed by length:** {config['bucket_by_length']}  \n",
    "**Shuffle training data:** {config['shuffle_train']}  \n",
    "**Pad input data to multiple of**: {config['padding_multiple']}  \n",
    "**Pad embedding matrices**: {config['pad_token_embeddings']}  \n",
    "**Normalise rewards?**: {config['normalise_rewards']}  \n",
    "**Gradient accumulation?**: {False if config['accumulation_steps'] == 1  else f\"Every {config['accumulation_steps']} steps\"}  \n",
    "\n",
    "### Low-level parameters \n",
    "**Use fp16 for training?**: {config['fp16']}  \n",
    "**Use memory pinning with dataloaders?**: {config['pin_memory']}  \n",
    "**Initilise gradients with `None` when running `zero_grad()`**: {config['zero_grad_with_none']}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For this one go to the [dashboard](https://wandb.ai/uts_nlp/travis_attack/36206pkc) and have a look at it manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below\n",
    "Markdown(f\"\"\"For this one go to the [dashboard](https://wandb.ai/{wandb_run_path}) and have a look at it manually\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_step  # can set to train, valid or test (and maybe training_step)\n",
    "df = df.sort_values(by=['idx', \"epoch\"], axis=0)\n",
    "df = df.query(\"idx <= 300\")  # just for testing purposes\n",
    "# Getting weird behaviour with group_by's so binning some of the numeric values\n",
    "df['sts_score'] = df['sts_score'].round(4) \n",
    "df['vm_score'] = df['vm_score'].round(4) \n",
    "df['reward'] = df['reward'].round(4) \n",
    "df['pp_truelabel_probs'] = df['pp_truelabel_probs'].round(4) \n",
    "\n",
    "#display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     17,
     35
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of examples with more than one paraphrase tried: 0.623\n",
      "Fraction of examples with a label flip at some point: 0.09799999999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_number_of_unique_pps_per_idx(df): \n",
    "    df_grp = df.groupby(\"idx\").agg({\"pp_l\":\"nunique\"})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_l\":\"idx_n_unique_pp\"})\n",
    "    # plot histogram\n",
    "    fig = sns.countplot(x='idx_n_unique_pp', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of unique paraphrases per original example\")\n",
    "    plt.xlabel(\"Unique paraphrases\")\n",
    "    plt.ylabel(\"Count\") \n",
    "    # Add column to orig df and return \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df\n",
    "\n",
    "df = add_number_of_unique_pps_per_idx(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','idx_n_unique_pp']].drop_duplicates()['idx_n_unique_pp'].value_counts()\n",
    "print(f\"Fraction of examples with more than one paraphrase tried: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n",
    "\n",
    "def add_number_of_pp_changes_per_idx(df): \n",
    "    df['pp_changed'] = df.sort_values([\"idx\",\"epoch\"]).groupby('idx')['pp_l'].shift().ne(df['pp_l']).astype(int)\n",
    "    df_grp = df.groupby('idx').agg({'pp_changed': 'sum'})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_changed\":\"idx_n_pp_changes\"})\n",
    "    df_grp['idx_n_pp_changes'] -= 1  # The first paraphrase isn't a change\n",
    "    df = df.drop('pp_changed', 1) # don't need this anymore \n",
    "    \n",
    "    # Plot histogram\n",
    "    fig = sns.countplot(x='idx_n_pp_changes', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of paraphrase changes during training per original example\")\n",
    "    plt.xlabel(\"Paraphrase changes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Add column to orig df before returning \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df \n",
    "df = add_number_of_pp_changes_per_idx(df)\n",
    "\n",
    "def add_epoch_of_first_label_flip(df): \n",
    "    rownum_of_first_flip = df.groupby('idx')[['epoch','label_flip']].idxmax()['label_flip']\n",
    "    ## idxmax returns first max\n",
    "    df_grp = df[['idx','epoch']].loc[rownum_of_first_flip]\n",
    "    df_grp= df_grp.rename(columns = {\"epoch\":\"epoch_of_first_label_flip\"})\n",
    "    fig = sns.countplot(x='epoch_of_first_label_flip', data=df_grp)\n",
    "    plt.title(\"Distribution of the epoch a label flip first occurs for each original example\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    df = df.merge(df_grp, left_on='idx', right_on='idx', how='left')\n",
    "    return df\n",
    "df = add_epoch_of_first_label_flip(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','epoch_of_first_label_flip']].drop_duplicates()['epoch_of_first_label_flip'].value_counts()\n",
    "print(f\"Fraction of examples with a label flip at some point: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sampling some low sts score examples \n",
    "#display_all(df.query('sts_score < 0.7').sort_values('sts_score').sample(10)[['orig_l','pp_l','sts_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     28
    ]
   },
   "outputs": [],
   "source": [
    "def get_added_end_of_sentence_phrase(orig, pp): \n",
    "    if len(pp) > len(orig): \n",
    "        rep = pp.replace(orig, \"\")\n",
    "        if rep != pp: return rep \n",
    "    return \"\"\n",
    "\n",
    "def get_removals_insertions_unchanged_phrases(orig, pp): \n",
    "    orig_t = [token.text for token in nlp(orig)]\n",
    "    pp_t = [token.text for token in nlp(pp)]\n",
    "    diff = [x for x in dl.ndiff(orig_t, pp_t)]\n",
    "    d = defaultdict(lambda: [])\n",
    "    \n",
    "    def get_subsequences(sign): \n",
    "        if   sign == \"insertions\": op = \"+\"\n",
    "        elif sign == \"removals\":   op = \"-\"\n",
    "        elif sign == \"unchanged\":  op = \" \"\n",
    "        else: raise Exception(\"shouldn't get here\")\n",
    "        idx,words = [],[]\n",
    "        for i, o in enumerate(diff): \n",
    "            if o[0] == op: \n",
    "                idx.append(i)\n",
    "                words.append(o[2:])\n",
    "        # Group words that go together        \n",
    "        word_groups = []\n",
    "        # bit of a mystery this bit but seems to work. just need 1-1 mapping between data and words \n",
    "        for k, g in groupby(zip(enumerate(idx), words), lambda ix: ix[0][0] - ix[0][1]):\n",
    "            word_groups.append(list(map(itemgetter(1), g)))\n",
    "            \n",
    "        def join_punctuation(seq, characters=set(string.punctuation)):\n",
    "            \"handle \"\n",
    "            seq = iter(seq)\n",
    "            current = next(seq)\n",
    "            for nxt in seq:\n",
    "                if nxt in characters:\n",
    "                    current += nxt\n",
    "                else:\n",
    "                    yield current\n",
    "                    current = nxt\n",
    "            yield current\n",
    "\n",
    "        phrases = [' '.join(join_punctuation(l)) for l in word_groups]\n",
    "        return idx, words, word_groups, phrases\n",
    "    \n",
    "    insertions_idx,_,_,insertions = get_subsequences(\"insertions\")\n",
    "    removals_idx,_,_,removals   = get_subsequences(\"removals\")\n",
    "    unchanged_idx,_,_,unchanged = get_subsequences(\"unchanged\")\n",
    "    def is_truncation(unchanged_idx, removals_idx):\n",
    "        if len(removals_idx) == 0 or len(unchanged_idx) == 0 : return False \n",
    "        if max(unchanged_idx) < max(removals_idx): return True \n",
    "        else: return False\n",
    "    \n",
    "    return {'removals_idx': removals_idx, \n",
    "            'removals': removals,\n",
    "            'is_truncation': is_truncation(unchanged_idx, removals_idx),\n",
    "            'insertions_idx': insertions_idx,\n",
    "            'insertions': insertions, \n",
    "            'unchanged_idx': unchanged_idx,\n",
    "            'unchanged': unchanged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_text_metrics(text):\n",
    "    def get_chartype_count(text, strset=string.ascii_letters):\n",
    "        return len(list(filter(functools.partial(operator.contains, strset), text)))     \n",
    "    d = defaultdict(lambda: 0)\n",
    "    ### Spacy stuff\n",
    "    doc = nlp(text)\n",
    "    # Which tags to keep\n",
    "    # see  https://universaldependencies.org/docs/u/pos/\n",
    "    UPOS_tags = ['ADJ','ADP','ADV','AUX','CONJ','DET','INTJ','NOUN',\n",
    "     'NUM','PART','PRON','PROPN','PUNCT','SCONJ',\n",
    "     'SYM','VERB','X']\n",
    "    d_POS = defaultdict(lambda: 0)  # d_POS holds parts of speech\n",
    "    d['n_nonzero_synsets'] = 0  # fix something weird with datasets.map code?\n",
    "    for token in doc:\n",
    "        n_synsets = len(token._.wordnet.synsets())\n",
    "        n_lemmas = len(token._.wordnet.lemmas())\n",
    "        d['n_total_synsets'] += n_synsets; \n",
    "        d['n_total_lemmas'] += n_lemmas\n",
    "        if n_synsets > 0: d['n_nonzero_synsets'] += 1 \n",
    "        d_POS[token.pos_] += 1\n",
    "\n",
    "    d['n_tokens'] = max(len(doc), 1)  # handle empty string \n",
    "    d['avg_synsets'] = d['n_total_synsets'] / d['n_tokens']\n",
    "    d['avg_lemmas']  = d['n_total_lemmas']  / d['n_tokens']     \n",
    "    for tag in UPOS_tags: d['n_upos_tag_' + tag] = d_POS[tag]\n",
    "\n",
    "    ### Lexical stuff\n",
    "    lex = LexicalRichness(text)    \n",
    "    d['n_words'] = lex.words\n",
    "    d['n_stopwords'] = sum([token.is_stop for token in doc])\n",
    "    d['n_named_entities'] = len(doc.ents)\n",
    "    d['n_unique_words'] = lex.terms\n",
    "    d['n_punctuation'] = get_chartype_count(text, strset=string.punctuation)\n",
    "    d['n_digits'] = get_chartype_count(text, strset=string.digits)\n",
    "    d['n_letters'] = get_chartype_count(text, strset=string.ascii_letters)\n",
    "    d['MTLD'] = lex.mtld(threshold=0.72) if lex.words > 1 else 0.0 \n",
    "    d['HDD'] = lex.hdd(draws=min(lex.words, 30)) if lex.words > 1 else 0.0 \n",
    "    d['Maas'] = lex.Maas if lex.words > 1 else 0.0 \n",
    "\n",
    "    ### Textstat stuff\n",
    "    d['flesch_kincaid_ease']      = textstat.flesch_reading_ease(text)\n",
    "    d['SMOG']                     = textstat.smog_index(text)\n",
    "    d['gunning_fog']              = textstat.gunning_fog(text)\n",
    "    d['difficult_words']          = textstat.difficult_words(text)\n",
    "    d['dale_chall']               = textstat.dale_chall_readability_score(text)\n",
    "    d['ARI']                      = textstat.automated_readability_index(text)\n",
    "    d['coleman_liau']             = textstat.coleman_liau_index(text)\n",
    "    d['linsear_write']            = textstat.linsear_write_formula(text)\n",
    "    d['readability_consensus']    = textstat.text_standard(text, float_output=True)\n",
    "    #d['avg_sentence_length']      = textstat.avg_sentence_length(text)\n",
    "    return d\n",
    "\n",
    "def get_text_pair_metrics(orig, pp): \n",
    "    def get_rouge_score(ref, pred):\n",
    "        return rouge_metric.compute(rouge_types=[\"rougeL\"], predictions=[pred], references=[ref])['rougeL'].mid.fmeasure \n",
    "    d = dict()\n",
    "    d['rouge_score'] = get_rouge_score(ref=orig, pred=pp)\n",
    "    #d['added_eos_phrase'] = get_added_end_of_sentence_phrase(orig, pp)\n",
    "    d1 = get_removals_insertions_unchanged_phrases(orig, pp)\n",
    "    d = {**d, **d1}  # merge two dicts\n",
    "    return d\n",
    "\n",
    "def get_text_metrics_for_ds(ds, colname, suffix, num_proc):\n",
    "    \"\"\"returns a df\"\"\"\n",
    "    # num_proc=8 seems pretty good - diminishing returns and we may as well leave some CPU for others \n",
    "    x = ds.map(get_text_metrics, input_columns = [colname], batched=False, \n",
    "               num_proc = num_proc )\n",
    "    # rename columns\n",
    "    colnames_mapping = dict()\n",
    "    for k in x.column_names: colnames_mapping[k] = k + f\"_{suffix}\" if k != colname else k\n",
    "    x = x.rename_columns(colnames_mapping)\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "def get_text_pair_metrics_for_ds(ds, num_proc): \n",
    "    x = ds.map(get_text_pair_metrics, input_columns = [\"orig_l\", \"pp_l\"], batched=False, \n",
    "               num_proc = num_proc )\n",
    "    return pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Calculating text statistics for the original examples. ####\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4a1bce3cc94132b2f16e616572480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/276 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Calculating text statistics for paraphrases. ####\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8840e00353df4e499d643648d7f94320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/815 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Calculating text pair statistics ####\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908f76d56d494397951296eafd2a5793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/815 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_text_stats(df):\n",
    "    # Go through all original examples, calculate stats, then join back to main df  \n",
    "    #num_proc = min(8, psutil.cpu_count())\n",
    "    num_proc=1  # for testing\n",
    "    ds_orig = Dataset.from_pandas(df['orig_l'].drop_duplicates().to_frame())\n",
    "    print(\"\\n#### Calculating text statistics for the original examples. ####\\n\")\n",
    "    df_orig = get_text_metrics_for_ds(ds_orig, colname=\"orig_l\", suffix=\"orig\", num_proc=num_proc)    \n",
    "    df = pd.merge(df, df_orig, how='left', on=['orig_l'])\n",
    "    \n",
    "    # Go through all paraphrases, calculate stats, then join back to main df   \n",
    "    ds_pp = Dataset.from_pandas(df['pp_l'].drop_duplicates().to_frame())\n",
    "    print(\"\\n#### Calculating text statistics for paraphrases. ####\\n\")\n",
    "    df_pp = get_text_metrics_for_ds(ds_pp, colname=\"pp_l\", suffix=\"pp\", num_proc=num_proc)    \n",
    "    df = pd.merge(df, df_pp, how='left', on=['pp_l'])\n",
    "    \n",
    "    ## Calculate differences in stats\n",
    "    keys = get_text_metrics(\"some text here\").keys()\n",
    "    for k in keys:  df[f\"{k}_diff\"] = df[f\"{k}_orig\"] - df[f\"{k}_pp\"]\n",
    "    \n",
    "    ## Calc stats for (orig, pp) unique pairs\n",
    "    ds_pairs = Dataset.from_pandas(df[['orig_l','pp_l']].drop_duplicates())\n",
    "    print(\"\\n#### Calculating text pair statistics ####\\n\")\n",
    "    df_pairs = get_text_pair_metrics_for_ds(ds_pairs, num_proc=num_proc)\n",
    "    df = pd.merge(df, df_pairs, how='left', on=['orig_l','pp_l'])    \n",
    "    return df \n",
    "\n",
    "\n",
    "df = add_text_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_l</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>removals</th>\n",
       "      <th>insertions</th>\n",
       "      <th>unchanged</th>\n",
       "      <th>is_truncation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>for a good chunk of its running time , trapped is an effective and claustrophobic thriller .</td>\n",
       "      <td>For a good chunk of its running time, trapped is an effective and claustrophobic thriller.</td>\n",
       "      <td>[for]</td>\n",
       "      <td>[For]</td>\n",
       "      <td>[a good chunk of its running time, trapped is an effective and claustrophobic thriller.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>a cop story that understands the medium amazingly well .</td>\n",
       "      <td>A cop story that understands the medium amazingly well.</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[cop story that understands the medium amazingly well.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>the passions aroused by the discord between old and new cultures are set against the strange , stark beauty of the mideast desert , so lovingly and perceptively filmed that you can almost taste the desiccated air .</td>\n",
       "      <td>The passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[The]</td>\n",
       "      <td>[passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art .</td>\n",
       "      <td>behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[behan himself knew how to spin a tale and one ca n't help but think he 'd appreciate this attempt to turn his life into art.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>an enthralling , playful film that constantly frustrates our desire to know the 'truth' about this man , while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to .</td>\n",
       "      <td>An enthralling, playful film that constantly frustrates our desire to know the 'truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.</td>\n",
       "      <td>[an]</td>\n",
       "      <td>[An]</td>\n",
       "      <td>[enthralling, playful film that constantly frustrates our desire to know the' truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>[drumline] is entertaining for what it does , and admirable for what it doesn't do .</td>\n",
       "      <td>[drumline] is entertaining for what it does, and admirable for whatit doesn't do.</td>\n",
       "      <td>[what, it]</td>\n",
       "      <td>[whatit]</td>\n",
       "      <td>[[ drumline] is entertaining for what it does, and admirable for, does n't do.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>thankfully , the film , which skirts that rapidly deteriorating line between fantasy and reality . . . takes a tongue-in-cheek attitude even as it pushes the croc hunter agenda .</td>\n",
       "      <td>thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.</td>\n",
       "      <td>[.. takes a tongue- in- cheek attitude even as it pushes the croc hunter agenda.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>part of the charm of satin rouge is that it avoids the obvious with humour and lightness .</td>\n",
       "      <td>Part of the charm of satin rouge is that it avoids the obvious with humour and lightness.</td>\n",
       "      <td>[part]</td>\n",
       "      <td>[Part]</td>\n",
       "      <td>[of the charm of satin rouge is that it avoids the obvious with humour and lightness.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .</td>\n",
       "      <td>sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self-absorbed martha as her heart begins to open.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self- absorbed martha as her heart begins to open.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>intriguing and beautiful film , but those of you who read the book are likely to be disappointed .</td>\n",
       "      <td>intriguing and beautiful film, but those of you who read the book are likely to be disappointed.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[intriguing and beautiful film, but those of you who read the book are likely to be disappointed.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                              orig_l  \\\n",
       "4613                                                                                                                                    for a good chunk of its running time , trapped is an effective and claustrophobic thriller .   \n",
       "5013                                                                                                                                                                        a cop story that understands the medium amazingly well .   \n",
       "6495          the passions aroused by the discord between old and new cultures are set against the strange , stark beauty of the mideast desert , so lovingly and perceptively filmed that you can almost taste the desiccated air .   \n",
       "6606                                                                                                     behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art .   \n",
       "3278  an enthralling , playful film that constantly frustrates our desire to know the 'truth' about this man , while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to .   \n",
       "5744                                                                                                                                            [drumline] is entertaining for what it does , and admirable for what it doesn't do .   \n",
       "7555                                              thankfully , the film , which skirts that rapidly deteriorating line between fantasy and reality . . . takes a tongue-in-cheek attitude even as it pushes the croc hunter agenda .   \n",
       "1800                                                                                                                                      part of the charm of satin rouge is that it avoids the obvious with humour and lightness .   \n",
       "1408                                                                                 sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .   \n",
       "8058                                                                                                                              intriguing and beautiful film , but those of you who read the book are likely to be disappointed .   \n",
       "\n",
       "                                                                                                                                                                                                                             pp_l  \\\n",
       "4613                                                                                                                                   For a good chunk of its running time, trapped is an effective and claustrophobic thriller.   \n",
       "5013                                                                                                                                                                      A cop story that understands the medium amazingly well.   \n",
       "6495          The passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.   \n",
       "6606                                                                                                   behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art.   \n",
       "3278  An enthralling, playful film that constantly frustrates our desire to know the 'truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.   \n",
       "5744                                                                                                                                            [drumline] is entertaining for what it does, and admirable for whatit doesn't do.   \n",
       "7555                                                                                                                              thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.   \n",
       "1800                                                                                                                                    Part of the charm of satin rouge is that it avoids the obvious with humour and lightness.   \n",
       "1408                                                                                 sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self-absorbed martha as her heart begins to open.   \n",
       "8058                                                                                                                             intriguing and beautiful film, but those of you who read the book are likely to be disappointed.   \n",
       "\n",
       "                                                                               removals  \\\n",
       "4613                                                                              [for]   \n",
       "5013                                                                                [a]   \n",
       "6495                                                                              [the]   \n",
       "6606                                                                                 []   \n",
       "3278                                                                               [an]   \n",
       "5744                                                                         [what, it]   \n",
       "7555  [.. takes a tongue- in- cheek attitude even as it pushes the croc hunter agenda.]   \n",
       "1800                                                                             [part]   \n",
       "1408                                                                                 []   \n",
       "8058                                                                                 []   \n",
       "\n",
       "     insertions  \\\n",
       "4613      [For]   \n",
       "5013        [A]   \n",
       "6495      [The]   \n",
       "6606         []   \n",
       "3278       [An]   \n",
       "5744   [whatit]   \n",
       "7555         []   \n",
       "1800     [Part]   \n",
       "1408         []   \n",
       "8058         []   \n",
       "\n",
       "                                                                                                                                                                                                                       unchanged  \\\n",
       "4613                                                                                                                                    [a good chunk of its running time, trapped is an effective and claustrophobic thriller.]   \n",
       "5013                                                                                                                                                                     [cop story that understands the medium amazingly well.]   \n",
       "6495           [passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.]   \n",
       "6606                                                                                              [behan himself knew how to spin a tale and one ca n't help but think he 'd appreciate this attempt to turn his life into art.]   \n",
       "3278  [enthralling, playful film that constantly frustrates our desire to know the' truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.]   \n",
       "5744                                                                                                                                             [[ drumline] is entertaining for what it does, and admirable for, does n't do.]   \n",
       "7555                                                                                                                           [thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.]   \n",
       "1800                                                                                                                                      [of the charm of satin rouge is that it avoids the obvious with humour and lightness.]   \n",
       "1408                                                                             [sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self- absorbed martha as her heart begins to open.]   \n",
       "8058                                                                                                                          [intriguing and beautiful film, but those of you who read the book are likely to be disappointed.]   \n",
       "\n",
       "      is_truncation  \n",
       "4613          False  \n",
       "5013          False  \n",
       "6495          False  \n",
       "6606          False  \n",
       "3278          False  \n",
       "5744          False  \n",
       "7555           True  \n",
       "1800          False  \n",
       "1408          False  \n",
       "8058          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df[['orig_l', 'pp_l','removals','insertions','unchanged',  'is_truncation']].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interesting_idx(df, n):\n",
    "    def get_idx_with_top_column_values(cname, n=5, ascending=False):\n",
    "        return df[['idx',cname]].\\\n",
    "            drop_duplicates().\\\n",
    "            sort_values(cname, ascending=ascending)\\\n",
    "            ['idx'][0:n].values.tolist()\n",
    "    \n",
    "    def sample_idx_with_label_flips(n=5): \n",
    "        return df[['idx','epoch_of_first_label_flip']].\\\n",
    "            query(\"epoch_of_first_label_flip!=0\").\\\n",
    "            drop_duplicates()\\\n",
    "            ['idx'].sample(n).values.tolist()\n",
    "    \n",
    "    idx_d = dict(\n",
    "        random           = df.idx.drop_duplicates().sample(n).tolist(),\n",
    "        label_flips = sample_idx_with_label_flips(n=n),\n",
    "        idx_n_unique_pp  = get_idx_with_top_column_values('idx_n_unique_pp',n=n,ascending=False),\n",
    "       # idx_n_pp_changes = get_idx_with_top_column_values('idx_n_pp_changes',n=n,ascending=False),\n",
    "        low_sts = get_idx_with_top_column_values('sts_score',n=n,ascending=True)\n",
    "      #  orig_long = None, \n",
    "      #  orig_short = None,\n",
    "    )\n",
    "    return idx_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "idx_d = get_interesting_idx(df, n)\n",
    "#idx_d\n",
    "\n",
    "def print_stats(key,i):\n",
    "    print(\"\\n###############\\n\")\n",
    "    print(key, i, \"\\n\")\n",
    "    idx = idx_d[key][i]\n",
    "    # Setup \n",
    "    df1 = df.query('idx==@idx')\n",
    "    orig = pd.unique(df1['orig_l'])[0]\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Original label\", pd.unique(df1['truelabel'])[0] )\n",
    "    pp_all = list(df1['pp_l'])\n",
    "    #print(\"All paraphrases\", pp_all)\n",
    "    pp_unique = list(pd.unique(df1['pp_l']))\n",
    "    n_pp_unique = len(pp_unique)\n",
    "\n",
    "    # showing a \"timeline\" of how the paraphrases change over the epochs\n",
    "    g_fields = [\"pp_l\",\"pp_truelabel_probs\",\"vm_score\",\"sts_score\",\"reward\",\"label_flip\"]\n",
    "    #g_fields = [\"pp_l\",\"vm_score\"]\n",
    "    g = df1.groupby(g_fields).agg({'epoch' : lambda x: list(x)})\n",
    "    g = g.sort_values(by='epoch', key = lambda col: col.map(lambda x: np.min(x)))\n",
    "    print(\"Unique paraphrases:\", n_pp_unique)\n",
    "    print(\"How the paraphrases change:\")\n",
    "    display_all(g)\n",
    "\n",
    "    # Showing a dataframe of the few best paraphrases\n",
    "    best_pps = df1.sort_values('pp_truelabel_probs').iloc[0]\n",
    "    print(\"Best Paraphrase\")\n",
    "    display_all(best_pps.to_frame().T)\n",
    "\n",
    "    #print(\"Everything\")\n",
    "    #display_all(df1)\n",
    "  \n",
    "for key in idx_d.keys():\n",
    "    for i in range(n): \n",
    "        print_stats(key,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics (to send to w&b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
