{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datasets import load_metric\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from datasets import load_dataset\n",
    "from lexicalrichness import LexicalRichness\n",
    "import functools\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import operator\n",
    "import spacy\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "np.random.seed(seed)\n",
    "run_name = \"mild-jazz-178\"\n",
    "wandb_run_path = \"uts_nlp/travis_attack/36206pkc\"  # get this from wandb overview section \n",
    "\n",
    "# Init W&B run object\n",
    "api = wandb.Api()\n",
    "run = api.run(wandb_run_path)\n",
    "config = run.config\n",
    "\n",
    "# Init spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "\n",
    "# Load CSV files\n",
    "path_results = f\"../model_checkpoints/travis_attack/{run_name}/\"\n",
    "train            = pd.read_csv(path_results + \"train.csv\")\n",
    "valid            = pd.read_csv(path_results + \"valid.csv\")\n",
    "test             = pd.read_csv(path_results + \"test.csv\")\n",
    "training_step    = pd.read_csv(path_results + \"training_step.csv\")\n",
    "training_summary = pd.read_csv(path_results + \"training_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            with pd.option_context(\"max_colwidth\", 480):\n",
    "                display(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## General run information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Summary \n",
       "\n",
       "**Dataset**: rotten_tomatoes  \n",
       "**Paraphrase model**: `eugenesiow/bart-paraphrase`  \n",
       "**Victim model**: `textattack/distilbert-base-uncased-rotten-tomatoes`  \n",
       "**Semantic Textual Similarity model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`  \n",
       "**Number of epochs**: 30  \n",
       "**Reward function**: `[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]`  \n",
       "**Using the full dataset**: True   \n",
       "We freeze all layers of the paraphrase model except the last **2** layers.  \n",
       "We eval every **1** epochs.   \n",
       "\n",
       "### Paraphrase parameters \n",
       "**Decoding strategy**: greedy  \n",
       "**Number of beams**: 1  \n",
       "**Number of return sequences**: 1  \n",
       "**Max length**: 64  \n",
       "**Min length**: 5  \n",
       "**Temperature**: 1.5  \n",
       "**Length penalty**: 1  \n",
       "**Number of beam groups**: 1  \n",
       "**Diversity penalty**: 0  \n",
       "\n",
       "  \n",
       "### Run parameters\n",
       "**Seed**: 420  \n",
       "**Learning rate**: 3e-05  \n",
       "**Batch sizes:** Train: 32, Eval: 128  \n",
       "**Max number of tokens in input**: 64  \n",
       "**Remove initially misclassified examples**: True  \n",
       "**Input bucketed by length:** True  \n",
       "**Shuffle training data:** False  \n",
       "**Pad input data to multiple of**: 8  \n",
       "**Pad embedding matrices**: True  \n",
       "**Normalise rewards?**: False  \n",
       "**Gradient accumulation?**: False  \n",
       "\n",
       "### Low-level parameters \n",
       "**Use fp16 for training?**: True  \n",
       "**Use memory pinning with dataloaders?**: True  \n",
       "**Initilise gradients with `None` when running `zero_grad()`**: False  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below \n",
    "Markdown(f\"\"\"\n",
    "### Summary \n",
    "\n",
    "**Dataset**: {config['dataset_name']}  \n",
    "**Paraphrase model**: `{config['pp_name']}`  \n",
    "**Victim model**: `{config['vm_name']}`  \n",
    "**Semantic Textual Similarity model**: `{config['sts_name']}`  \n",
    "**Number of epochs**: {config['n_train_epochs']}  \n",
    "**Reward function**: `{config['reward_strategy']}`  \n",
    "**Using the full dataset**: {True if not config['use_small_ds'] else f\"False: n_shards set to {config['shard_params']['n_shards']} shards with shard_contiguous set to {config['shard_params']['shard_contiguous']}\"}   \n",
    "We freeze all layers of the paraphrase model except the last **{config['n_layers_frozen']}** layers.  \n",
    "We eval every **{config['eval_freq']}** epochs.   \n",
    "\n",
    "### Paraphrase parameters \n",
    "**Decoding strategy**: {config['sampling_strategy']}  \n",
    "**Number of beams**: {config['pp_model_params']['num_beams']}  \n",
    "**Number of return sequences**: {config['pp_model_params']['num_return_sequences']}  \n",
    "**Max length**: {config['pp_model_params']['max_length']}  \n",
    "**Min length**: {config['pp_model_params']['min_length']}  \n",
    "**Temperature**: {config['pp_model_params']['temperature']}  \n",
    "**Length penalty**: {config['pp_model_params']['length_penalty']}  \n",
    "**Number of beam groups**: {config['pp_model_params']['num_beam_groups']}  \n",
    "**Diversity penalty**: {config['pp_model_params']['diversity_penalty']}  \n",
    "\n",
    "  \n",
    "### Run parameters\n",
    "**Seed**: {config['seed']}  \n",
    "**Learning rate**: {config['lr']}  \n",
    "**Batch sizes:** Train: {config['batch_size_train']}, Eval: {config['batch_size_eval']}  \n",
    "**Max number of tokens in input**: {config['max_length']}  \n",
    "**Remove initially misclassified examples**: {config['remove_misclassified_examples']}  \n",
    "**Input bucketed by length:** {config['bucket_by_length']}  \n",
    "**Shuffle training data:** {config['shuffle_train']}  \n",
    "**Pad input data to multiple of**: {config['padding_multiple']}  \n",
    "**Pad embedding matrices**: {config['pad_token_embeddings']}  \n",
    "**Normalise rewards?**: {config['normalise_rewards']}  \n",
    "**Gradient accumulation?**: {False if config['accumulation_steps'] == 1  else f\"Every {config['accumulation_steps']} steps\"}  \n",
    "\n",
    "### Low-level parameters \n",
    "**Use fp16 for training?**: {config['fp16']}  \n",
    "**Use memory pinning with dataloaders?**: {config['pin_memory']}  \n",
    "**Initilise gradients with `None` when running `zero_grad()`**: {config['zero_grad_with_none']}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For this one go to the [dashboard](https://wandb.ai/uts_nlp/travis_attack/36206pkc) and have a look at it manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below\n",
    "Markdown(f\"\"\"For this one go to the [dashboard](https://wandb.ai/{wandb_run_path}) and have a look at it manually\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_step  # can set to train, valid or test (and maybe training_step)\n",
    "df = df.sort_values(by=['idx', \"epoch\"], axis=0)\n",
    "df = df.query(\"idx <= 50\")  # just for testing purposes\n",
    "# Getting weird behaviour with group_by's so binning some of the numeric values\n",
    "df['sts_score'] = df['sts_score'].round(4) \n",
    "df['vm_score'] = df['vm_score'].round(4) \n",
    "df['reward'] = df['reward'].round(4) \n",
    "df['pp_truelabel_probs'] = df['pp_truelabel_probs'].round(4) \n",
    "\n",
    "#display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     0,
     17,
     35
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of examples with more than one paraphrase tried: 0.5\n",
      "Fraction of examples with a label flip at some point: 0.11399999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_number_of_unique_pps_per_idx(df): \n",
    "    df_grp = df.groupby(\"idx\").agg({\"pp_l\":\"nunique\"})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_l\":\"idx_n_unique_pp\"})\n",
    "    # plot histogram\n",
    "    fig = sns.countplot(x='idx_n_unique_pp', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of unique paraphrases per original example\")\n",
    "    plt.xlabel(\"Unique paraphrases\")\n",
    "    plt.ylabel(\"Count\") \n",
    "    # Add column to orig df and return \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df\n",
    "\n",
    "df = add_number_of_unique_pps_per_idx(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','idx_n_unique_pp']].drop_duplicates()['idx_n_unique_pp'].value_counts()\n",
    "print(f\"Fraction of examples with more than one paraphrase tried: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n",
    "\n",
    "def add_number_of_pp_changes_per_idx(df): \n",
    "    df['pp_changed'] = df.sort_values([\"idx\",\"epoch\"]).groupby('idx')['pp_l'].shift().ne(df['pp_l']).astype(int)\n",
    "    df_grp = df.groupby('idx').agg({'pp_changed': 'sum'})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_changed\":\"idx_n_pp_changes\"})\n",
    "    df_grp['idx_n_pp_changes'] -= 1  # The first paraphrase isn't a change\n",
    "    df = df.drop('pp_changed', 1) # don't need this anymore \n",
    "    \n",
    "    # Plot histogram\n",
    "    fig = sns.countplot(x='idx_n_pp_changes', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of paraphrase changes during training per original example\")\n",
    "    plt.xlabel(\"Paraphrase changes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Add column to orig df before returning \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df \n",
    "df = add_number_of_pp_changes_per_idx(df)\n",
    "\n",
    "def add_epoch_of_first_label_flip(df): \n",
    "    rownum_of_first_flip = df.groupby('idx')[['epoch','label_flip']].idxmax()['label_flip']\n",
    "    ## idxmax returns first max\n",
    "    df_grp = df[['idx','epoch']].loc[rownum_of_first_flip]\n",
    "    df_grp= df_grp.rename(columns = {\"epoch\":\"epoch_of_first_label_flip\"})\n",
    "    fig = sns.countplot(x='epoch_of_first_label_flip', data=df_grp)\n",
    "    plt.title(\"Distribution of the epoch a label flip first occurs for each original example\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    df = df.merge(df_grp, left_on='idx', right_on='idx', how='left')\n",
    "    return df\n",
    "df = add_epoch_of_first_label_flip(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','epoch_of_first_label_flip']].drop_duplicates()['epoch_of_first_label_flip'].value_counts()\n",
    "print(f\"Fraction of examples with a label flip at some point: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sampling some low sts score examples \n",
    "#display_all(df.query('sts_score < 0.7').sort_values('sts_score').sample(10)[['orig_l','pp_l','sts_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_metrics(text): \n",
    "    def get_chartype_count(text, strset=string.ascii_letters):\n",
    "        return len(list(filter(functools.partial(operator.contains, strset), text)))     \n",
    "    d = defaultdict(lambda: 0)\n",
    "    ### Spacy stuff\n",
    "    doc = nlp(text)\n",
    "    # Which tags to keep\n",
    "    # see  https://universaldependencies.org/docs/u/pos/\n",
    "    UPOS_tags = ['ADJ','ADP','ADV','AUX','CONJ','DET','INTJ','NOUN',\n",
    "     'NUM','PART','PRON','PROPN','PUNCT','SCONJ',\n",
    "     'SYM','VERB','X']\n",
    "    d_POS = defaultdict(lambda: 0)  # d_POS holds parts of speech\n",
    "    for token in doc:\n",
    "        n_synsets = len(token._.wordnet.synsets())\n",
    "        n_lemmas = len(token._.wordnet.lemmas())\n",
    "        d['n_total_synsets'] += n_synsets; \n",
    "        d['n_total_lemmas'] += n_lemmas\n",
    "        if n_synsets > 0: d['n_nonzero_synsets'] += 1 \n",
    "        d_POS[token.pos_] += 1\n",
    "\n",
    "    d['n_tokens'] = len(doc)\n",
    "    d['avg_synsets'] = d['n_total_synsets'] / d['n_tokens']\n",
    "    d['avg_lemmas']  = d['n_total_lemmas']  / d['n_tokens']     \n",
    "    for tag in UPOS_tags: d['n_upos_tag_' + tag] = d_POS[tag]\n",
    "\n",
    "    ### Lexical stuff\n",
    "    lex = LexicalRichness(orig)    \n",
    "    d['n_words'] = lex.words\n",
    "    d['n_stopwords'] = sum([token.is_stop for token in doc])\n",
    "    d['n_named_entities'] = len(doc.ents)\n",
    "    d['n_unique_words'] = lex.terms\n",
    "    d['n_punctuation'] = get_chartype_count(text, strset=string.punctuation)\n",
    "    d['n_digits'] = get_chartype_count(text, strset=string.digits)\n",
    "    d['n_letters'] = get_chartype_count(text, strset=string.ascii_letters)\n",
    "    d['MTLD'] = lex.mtld(threshold=0.72)\n",
    "    d['HDD'] = lex.hdd(draws=min(lex.words, 30))\n",
    "    d['Maas'] = lex.Maas\n",
    "\n",
    "    ### Textstat stuff\n",
    "    d['flesch_kincaid_ease']      = textstat.flesch_reading_ease(text)\n",
    "    d['SMOG']                     = textstat.smog_index(text)\n",
    "    d['gunning_fog']              = textstat.gunning_fog(text)\n",
    "    d['difficult_words']          = textstat.difficult_words(text)\n",
    "    d['dale_chall']               = textstat.dale_chall_readability_score(text)\n",
    "    d['ARI']                      = textstat.automated_readability_index(text)\n",
    "    d['coleman_liau']             = textstat.coleman_liau_index(text)\n",
    "    d['linsear_write']            = textstat.linsear_write_formula(text)\n",
    "    d['readability_consensus']    = textstat.text_standard(text, float_output=True)\n",
    "    #d['avg_sentence_length']      = textstat.avg_sentence_length(text)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Calculating text statistics for the original examples. ####\n",
      "\n",
      "Done 0 out of 44\n",
      "Done 5 out of 44\n",
      "Done 10 out of 44\n",
      "Done 15 out of 44\n",
      "Done 20 out of 44\n",
      "Done 25 out of 44\n",
      "Done 30 out of 44\n",
      "Done 35 out of 44\n",
      "Done 40 out of 44\n",
      "\n",
      "#### Calculating text statistics for paraphrases and for (orig, pp) pairs. ####\n",
      "\n",
      "Done 0 out of 125\n",
      "Done 5 out of 125\n",
      "Done 10 out of 125\n",
      "Done 15 out of 125\n",
      "Done 20 out of 125\n",
      "Done 25 out of 125\n",
      "Done 30 out of 125\n",
      "Done 35 out of 125\n",
      "Done 40 out of 125\n",
      "Done 45 out of 125\n",
      "Done 50 out of 125\n",
      "Done 55 out of 125\n",
      "Done 60 out of 125\n",
      "Done 65 out of 125\n",
      "Done 70 out of 125\n",
      "Done 75 out of 125\n",
      "Done 80 out of 125\n",
      "Done 85 out of 125\n",
      "Done 90 out of 125\n",
      "Done 95 out of 125\n",
      "Done 100 out of 125\n",
      "Done 105 out of 125\n",
      "Done 110 out of 125\n",
      "Done 115 out of 125\n",
      "Done 120 out of 125\n",
      "\n",
      "#### Calculating differences in metrics between orig and pp. ####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_text_stats(df):\n",
    "    ### Definitions \n",
    "    def is_truncation(orig, pp): return pp in orig\n",
    "    \n",
    "    def get_rouge_score(ref, pred):\n",
    "        return rouge_metric.compute(rouge_types=[\"rougeL\"], predictions=[pred], references=[ref])['rougeL'].mid.fmeasure \n",
    "    \n",
    "    # Prepare df and other setup \n",
    "    df_tmp = df[['orig_l','pp_l']].drop_duplicates()\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    \n",
    "    # Loop through all original examples and calculate stats \n",
    "    orig_l = df['orig_l'].drop_duplicates().tolist()\n",
    "    results_l = []\n",
    "    print(\"\\n#### Calculating text statistics for the original examples. ####\\n\")\n",
    "    for i, orig in enumerate(orig_l):\n",
    "        if i % 5 == 0 : print(f\"Done {i} out of {len(orig_l)}\") \n",
    "        d = dict()\n",
    "        d_metrics = get_text_metrics(orig)\n",
    "        for k in d_metrics.keys(): d[f\"{k}_orig\"] = d_metrics[k]\n",
    "        results_l.append(d)\n",
    "        \n",
    "    # Join back to orignal df    \n",
    "    df_orig_stats = pd.DataFrame(results_l)\n",
    "    df_orig_stats['orig_l'] = orig_l\n",
    "    df = pd.merge(df, df_orig_stats, how='left', on=['orig_l'])\n",
    "    \n",
    "    \n",
    "    ## Loop for (orig, pp) unique pairs and calc stats \n",
    "    results_l = [] \n",
    "    orig_l,pp_l = df_tmp['orig_l'].tolist(),df_tmp['pp_l'].tolist()\n",
    "    print(\"\\n#### Calculating text statistics for paraphrases and for (orig, pp) pairs. ####\\n\")\n",
    "    for i, (orig, pp) in enumerate(zip(orig_l, pp_l)):\n",
    "        if i % 5 == 0 : print(f\"Done {i} out of {len(orig_l)}\") \n",
    "        d = dict() \n",
    "        ### Paraphrase text metrics \n",
    "        d_metrics = get_text_metrics(pp)\n",
    "        for k in d_metrics.keys(): d[f\"{k}_pp\"] = d_metrics[k] \n",
    "            \n",
    "        # Orig + pp comparison metrics   \n",
    "        d['rouge_score'] = get_rouge_score(ref=orig,pred=pp)\n",
    "        d['is_truncation'] = is_truncation(orig,pp)\n",
    "        results_l.append(d)\n",
    "        \n",
    "    df_orig_pp_stats = pd.DataFrame(results_l)\n",
    "    df_orig_pp_stats['orig_l'] = orig_l\n",
    "    df_orig_pp_stats['pp_l'] = pp_l\n",
    "    df = pd.merge(df, df_orig_pp_stats, how='left', on=['orig_l', 'pp_l'] )\n",
    "    \n",
    "    ## Calculate differences \n",
    "    for k in d_metrics.keys():  df[f\"{k}_diff\"] = df[f\"{k}_orig\"] - df[f\"{k}_pp\"]\n",
    "    return df \n",
    "\n",
    "\n",
    "df = add_text_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>orig_l</th>\n",
       "      <th>truelabel</th>\n",
       "      <th>orig_truelabel_probs</th>\n",
       "      <th>epoch</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>pp_truelabel_probs</th>\n",
       "      <th>pp_predclass</th>\n",
       "      <th>pp_predclass_probs</th>\n",
       "      <th>loss</th>\n",
       "      <th>pp_logp</th>\n",
       "      <th>reward</th>\n",
       "      <th>vm_score</th>\n",
       "      <th>sts_score</th>\n",
       "      <th>label_flip</th>\n",
       "      <th>idx_n_unique_pp</th>\n",
       "      <th>idx_n_pp_changes</th>\n",
       "      <th>epoch_of_first_label_flip</th>\n",
       "      <th>n_total_synsets_orig</th>\n",
       "      <th>n_total_lemmas_orig</th>\n",
       "      <th>n_nonzero_synsets_orig</th>\n",
       "      <th>n_tokens_orig</th>\n",
       "      <th>avg_synsets_orig</th>\n",
       "      <th>avg_lemmas_orig</th>\n",
       "      <th>n_upos_tag_ADJ_orig</th>\n",
       "      <th>n_upos_tag_ADP_orig</th>\n",
       "      <th>n_upos_tag_ADV_orig</th>\n",
       "      <th>n_upos_tag_AUX_orig</th>\n",
       "      <th>n_upos_tag_CONJ_orig</th>\n",
       "      <th>n_upos_tag_DET_orig</th>\n",
       "      <th>n_upos_tag_INTJ_orig</th>\n",
       "      <th>n_upos_tag_NOUN_orig</th>\n",
       "      <th>n_upos_tag_NUM_orig</th>\n",
       "      <th>n_upos_tag_PART_orig</th>\n",
       "      <th>n_upos_tag_PRON_orig</th>\n",
       "      <th>n_upos_tag_PROPN_orig</th>\n",
       "      <th>n_upos_tag_PUNCT_orig</th>\n",
       "      <th>n_upos_tag_SCONJ_orig</th>\n",
       "      <th>n_upos_tag_SYM_orig</th>\n",
       "      <th>n_upos_tag_VERB_orig</th>\n",
       "      <th>n_upos_tag_X_orig</th>\n",
       "      <th>n_words_orig</th>\n",
       "      <th>n_stopwords_orig</th>\n",
       "      <th>n_named_entities_orig</th>\n",
       "      <th>n_unique_words_orig</th>\n",
       "      <th>n_punctuation_orig</th>\n",
       "      <th>n_digits_orig</th>\n",
       "      <th>n_letters_orig</th>\n",
       "      <th>MTLD_orig</th>\n",
       "      <th>HDD_orig</th>\n",
       "      <th>Maas_orig</th>\n",
       "      <th>flesch_kincaid_ease_orig</th>\n",
       "      <th>SMOG_orig</th>\n",
       "      <th>gunning_fog_orig</th>\n",
       "      <th>difficult_words_orig</th>\n",
       "      <th>dale_chall_orig</th>\n",
       "      <th>ARI_orig</th>\n",
       "      <th>coleman_liau_orig</th>\n",
       "      <th>linsear_write_orig</th>\n",
       "      <th>readability_consensus_orig</th>\n",
       "      <th>avg_sentence_length_orig</th>\n",
       "      <th>n_total_synsets_pp</th>\n",
       "      <th>n_total_lemmas_pp</th>\n",
       "      <th>n_nonzero_synsets_pp</th>\n",
       "      <th>n_tokens_pp</th>\n",
       "      <th>avg_synsets_pp</th>\n",
       "      <th>avg_lemmas_pp</th>\n",
       "      <th>n_upos_tag_ADJ_pp</th>\n",
       "      <th>n_upos_tag_ADP_pp</th>\n",
       "      <th>n_upos_tag_ADV_pp</th>\n",
       "      <th>n_upos_tag_AUX_pp</th>\n",
       "      <th>n_upos_tag_CONJ_pp</th>\n",
       "      <th>n_upos_tag_DET_pp</th>\n",
       "      <th>n_upos_tag_INTJ_pp</th>\n",
       "      <th>n_upos_tag_NOUN_pp</th>\n",
       "      <th>n_upos_tag_NUM_pp</th>\n",
       "      <th>n_upos_tag_PART_pp</th>\n",
       "      <th>n_upos_tag_PRON_pp</th>\n",
       "      <th>n_upos_tag_PROPN_pp</th>\n",
       "      <th>n_upos_tag_PUNCT_pp</th>\n",
       "      <th>n_upos_tag_SCONJ_pp</th>\n",
       "      <th>n_upos_tag_SYM_pp</th>\n",
       "      <th>n_upos_tag_VERB_pp</th>\n",
       "      <th>n_upos_tag_X_pp</th>\n",
       "      <th>n_words_pp</th>\n",
       "      <th>n_stopwords_pp</th>\n",
       "      <th>n_named_entities_pp</th>\n",
       "      <th>n_unique_words_pp</th>\n",
       "      <th>n_punctuation_pp</th>\n",
       "      <th>n_digits_pp</th>\n",
       "      <th>n_letters_pp</th>\n",
       "      <th>MTLD_pp</th>\n",
       "      <th>HDD_pp</th>\n",
       "      <th>Maas_pp</th>\n",
       "      <th>flesch_kincaid_ease_pp</th>\n",
       "      <th>SMOG_pp</th>\n",
       "      <th>gunning_fog_pp</th>\n",
       "      <th>difficult_words_pp</th>\n",
       "      <th>dale_chall_pp</th>\n",
       "      <th>ARI_pp</th>\n",
       "      <th>coleman_liau_pp</th>\n",
       "      <th>linsear_write_pp</th>\n",
       "      <th>readability_consensus_pp</th>\n",
       "      <th>avg_sentence_length_pp</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>is_truncation</th>\n",
       "      <th>n_total_synsets_diff</th>\n",
       "      <th>n_total_lemmas_diff</th>\n",
       "      <th>n_nonzero_synsets_diff</th>\n",
       "      <th>n_tokens_diff</th>\n",
       "      <th>avg_synsets_diff</th>\n",
       "      <th>avg_lemmas_diff</th>\n",
       "      <th>n_upos_tag_ADJ_diff</th>\n",
       "      <th>n_upos_tag_ADP_diff</th>\n",
       "      <th>n_upos_tag_ADV_diff</th>\n",
       "      <th>n_upos_tag_AUX_diff</th>\n",
       "      <th>n_upos_tag_CONJ_diff</th>\n",
       "      <th>n_upos_tag_DET_diff</th>\n",
       "      <th>n_upos_tag_INTJ_diff</th>\n",
       "      <th>n_upos_tag_NOUN_diff</th>\n",
       "      <th>n_upos_tag_NUM_diff</th>\n",
       "      <th>n_upos_tag_PART_diff</th>\n",
       "      <th>n_upos_tag_PRON_diff</th>\n",
       "      <th>n_upos_tag_PROPN_diff</th>\n",
       "      <th>n_upos_tag_PUNCT_diff</th>\n",
       "      <th>n_upos_tag_SCONJ_diff</th>\n",
       "      <th>n_upos_tag_SYM_diff</th>\n",
       "      <th>n_upos_tag_VERB_diff</th>\n",
       "      <th>n_upos_tag_X_diff</th>\n",
       "      <th>n_words_diff</th>\n",
       "      <th>n_stopwords_diff</th>\n",
       "      <th>n_named_entities_diff</th>\n",
       "      <th>n_unique_words_diff</th>\n",
       "      <th>n_punctuation_diff</th>\n",
       "      <th>n_digits_diff</th>\n",
       "      <th>n_letters_diff</th>\n",
       "      <th>MTLD_diff</th>\n",
       "      <th>HDD_diff</th>\n",
       "      <th>Maas_diff</th>\n",
       "      <th>flesch_kincaid_ease_diff</th>\n",
       "      <th>SMOG_diff</th>\n",
       "      <th>gunning_fog_diff</th>\n",
       "      <th>difficult_words_diff</th>\n",
       "      <th>dale_chall_diff</th>\n",
       "      <th>ARI_diff</th>\n",
       "      <th>coleman_liau_diff</th>\n",
       "      <th>linsear_write_diff</th>\n",
       "      <th>readability_consensus_diff</th>\n",
       "      <th>avg_sentence_length_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0</td>\n",
       "      <td>The rock is destined to be the new \" Conan '' and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.</td>\n",
       "      <td>0.8068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806813</td>\n",
       "      <td>2.474591</td>\n",
       "      <td>-4.545957</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>162</td>\n",
       "      <td>418</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>4.628571</td>\n",
       "      <td>11.942857</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>68.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>4</td>\n",
       "      <td>7.28</td>\n",
       "      <td>14.8</td>\n",
       "      <td>8.88</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.286466</td>\n",
       "      <td>-0.732331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>1</td>\n",
       "      <td>The rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.053502</td>\n",
       "      <td>-0.107004</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>66.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  \\\n",
       "0    0   \n",
       "1    0   \n",
       "\n",
       "                                                                                                                                                                              orig_l  \\\n",
       "0  the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .   \n",
       "1  the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .   \n",
       "\n",
       "   truelabel  orig_truelabel_probs  epoch  \\\n",
       "0          1              0.855556      0   \n",
       "1          1              0.855556      1   \n",
       "\n",
       "                                                                                                                                                                              pp_l  \\\n",
       "0                The rock is destined to be the new \" Conan '' and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.   \n",
       "1  The rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.   \n",
       "\n",
       "   pp_truelabel_probs  pp_predclass  pp_predclass_probs      loss   pp_logp  \\\n",
       "0              0.8068             1            0.806813  2.474591 -4.545957   \n",
       "1              0.8556             1            0.855556  0.053502 -0.107004   \n",
       "\n",
       "   reward  vm_score  sts_score  label_flip  idx_n_unique_pp  idx_n_pp_changes  \\\n",
       "0  0.5443    0.0487     0.9099           0                3                 3   \n",
       "1  0.5000   -0.0000     0.9997           0                3                 3   \n",
       "\n",
       "   epoch_of_first_label_flip  n_total_synsets_orig  n_total_lemmas_orig  \\\n",
       "0                          0                   165                  426   \n",
       "1                          0                   165                  426   \n",
       "\n",
       "   n_nonzero_synsets_orig  n_tokens_orig  avg_synsets_orig  avg_lemmas_orig  \\\n",
       "0                      18             38          4.342105        11.210526   \n",
       "1                      18             38          4.342105        11.210526   \n",
       "\n",
       "   n_upos_tag_ADJ_orig  n_upos_tag_ADP_orig  n_upos_tag_ADV_orig  \\\n",
       "0                    4                    0                    1   \n",
       "1                    4                    0                    1   \n",
       "\n",
       "   n_upos_tag_AUX_orig  n_upos_tag_CONJ_orig  n_upos_tag_DET_orig  \\\n",
       "0                    3                     0                    3   \n",
       "1                    3                     0                    3   \n",
       "\n",
       "   n_upos_tag_INTJ_orig  n_upos_tag_NOUN_orig  n_upos_tag_NUM_orig  \\\n",
       "0                     0                     5                    0   \n",
       "1                     0                     5                    0   \n",
       "\n",
       "   n_upos_tag_PART_orig  n_upos_tag_PRON_orig  n_upos_tag_PROPN_orig  \\\n",
       "0                     3                     1                      6   \n",
       "1                     3                     1                      6   \n",
       "\n",
       "   n_upos_tag_PUNCT_orig  n_upos_tag_SCONJ_orig  n_upos_tag_SYM_orig  \\\n",
       "0                      5                      2                    0   \n",
       "1                      5                      2                    0   \n",
       "\n",
       "   n_upos_tag_VERB_orig  n_upos_tag_X_orig  n_words_orig  n_stopwords_orig  \\\n",
       "0                     3                  0            19                16   \n",
       "1                     3                  0            19                16   \n",
       "\n",
       "   n_named_entities_orig  n_unique_words_orig  n_punctuation_orig  \\\n",
       "0                      5                   16                   7   \n",
       "1                      5                   16                   7   \n",
       "\n",
       "   n_digits_orig  n_letters_orig  MTLD_orig  HDD_orig  Maas_orig  \\\n",
       "0              2             135  33.693333  0.842105   0.019822   \n",
       "1              2             135  33.693333  0.842105   0.019822   \n",
       "\n",
       "   flesch_kincaid_ease_orig  SMOG_orig  gunning_fog_orig  \\\n",
       "0                     57.95        0.0             13.33   \n",
       "1                     57.95        0.0             13.33   \n",
       "\n",
       "   difficult_words_orig  dale_chall_orig  ARI_orig  coleman_liau_orig  \\\n",
       "0                     5             7.76      16.2               9.82   \n",
       "1                     5             7.76      16.2               9.82   \n",
       "\n",
       "   linsear_write_orig  readability_consensus_orig  avg_sentence_length_orig  \\\n",
       "0                18.0                        13.0                      30.0   \n",
       "1                18.0                        13.0                      30.0   \n",
       "\n",
       "   n_total_synsets_pp  n_total_lemmas_pp  n_nonzero_synsets_pp  n_tokens_pp  \\\n",
       "0                 162                418                    16           35   \n",
       "1                 165                426                    18           38   \n",
       "\n",
       "   avg_synsets_pp  avg_lemmas_pp  n_upos_tag_ADJ_pp  n_upos_tag_ADP_pp  \\\n",
       "0        4.628571      11.942857                  3                  0   \n",
       "1        4.342105      11.210526                  4                  0   \n",
       "\n",
       "   n_upos_tag_ADV_pp  n_upos_tag_AUX_pp  n_upos_tag_CONJ_pp  \\\n",
       "0                  1                  3                   0   \n",
       "1                  1                  3                   0   \n",
       "\n",
       "   n_upos_tag_DET_pp  n_upos_tag_INTJ_pp  n_upos_tag_NOUN_pp  \\\n",
       "0                  3                   0                   3   \n",
       "1                  3                   0                   5   \n",
       "\n",
       "   n_upos_tag_NUM_pp  n_upos_tag_PART_pp  n_upos_tag_PRON_pp  \\\n",
       "0                  0                   2                   1   \n",
       "1                  0                   3                   1   \n",
       "\n",
       "   n_upos_tag_PROPN_pp  n_upos_tag_PUNCT_pp  n_upos_tag_SCONJ_pp  \\\n",
       "0                    7                    5                    2   \n",
       "1                    6                    5                    2   \n",
       "\n",
       "   n_upos_tag_SYM_pp  n_upos_tag_VERB_pp  n_upos_tag_X_pp  n_words_pp  \\\n",
       "0                  0                   3                0          19   \n",
       "1                  0                   3                0          19   \n",
       "\n",
       "   n_stopwords_pp  n_named_entities_pp  n_unique_words_pp  n_punctuation_pp  \\\n",
       "0              15                    5                 16                 7   \n",
       "1              16                    5                 16                 7   \n",
       "\n",
       "   n_digits_pp  n_letters_pp    MTLD_pp    HDD_pp   Maas_pp  \\\n",
       "0            0           125  33.693333  0.842105  0.019822   \n",
       "1            2           135  33.693333  0.842105  0.019822   \n",
       "\n",
       "   flesch_kincaid_ease_pp  SMOG_pp  gunning_fog_pp  difficult_words_pp  \\\n",
       "0                   68.44      0.0           11.20                   4   \n",
       "1                   66.41      0.0           13.33                   5   \n",
       "\n",
       "   dale_chall_pp  ARI_pp  coleman_liau_pp  linsear_write_pp  \\\n",
       "0           7.28    14.8             8.88              15.0   \n",
       "1           7.76    16.2             9.82              17.0   \n",
       "\n",
       "   readability_consensus_pp  avg_sentence_length_pp  rouge_score  \\\n",
       "0                      15.0                    28.0     0.952381   \n",
       "1                       8.0                    30.0     1.000000   \n",
       "\n",
       "   is_truncation  n_total_synsets_diff  n_total_lemmas_diff  \\\n",
       "0          False                     3                    8   \n",
       "1          False                     0                    0   \n",
       "\n",
       "   n_nonzero_synsets_diff  n_tokens_diff  avg_synsets_diff  avg_lemmas_diff  \\\n",
       "0                       2              3         -0.286466        -0.732331   \n",
       "1                       0              0          0.000000         0.000000   \n",
       "\n",
       "   n_upos_tag_ADJ_diff  n_upos_tag_ADP_diff  n_upos_tag_ADV_diff  \\\n",
       "0                    1                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "\n",
       "   n_upos_tag_AUX_diff  n_upos_tag_CONJ_diff  n_upos_tag_DET_diff  \\\n",
       "0                    0                     0                    0   \n",
       "1                    0                     0                    0   \n",
       "\n",
       "   n_upos_tag_INTJ_diff  n_upos_tag_NOUN_diff  n_upos_tag_NUM_diff  \\\n",
       "0                     0                     2                    0   \n",
       "1                     0                     0                    0   \n",
       "\n",
       "   n_upos_tag_PART_diff  n_upos_tag_PRON_diff  n_upos_tag_PROPN_diff  \\\n",
       "0                     1                     0                     -1   \n",
       "1                     0                     0                      0   \n",
       "\n",
       "   n_upos_tag_PUNCT_diff  n_upos_tag_SCONJ_diff  n_upos_tag_SYM_diff  \\\n",
       "0                      0                      0                    0   \n",
       "1                      0                      0                    0   \n",
       "\n",
       "   n_upos_tag_VERB_diff  n_upos_tag_X_diff  n_words_diff  n_stopwords_diff  \\\n",
       "0                     0                  0             0                 1   \n",
       "1                     0                  0             0                 0   \n",
       "\n",
       "   n_named_entities_diff  n_unique_words_diff  n_punctuation_diff  \\\n",
       "0                      0                    0                   0   \n",
       "1                      0                    0                   0   \n",
       "\n",
       "   n_digits_diff  n_letters_diff  MTLD_diff  HDD_diff  Maas_diff  \\\n",
       "0              2              10        0.0       0.0        0.0   \n",
       "1              0               0        0.0       0.0        0.0   \n",
       "\n",
       "   flesch_kincaid_ease_diff  SMOG_diff  gunning_fog_diff  \\\n",
       "0                    -10.49        0.0              2.13   \n",
       "1                     -8.46        0.0              0.00   \n",
       "\n",
       "   difficult_words_diff  dale_chall_diff  ARI_diff  coleman_liau_diff  \\\n",
       "0                     1             0.48       1.4               0.94   \n",
       "1                     0             0.00       0.0               0.00   \n",
       "\n",
       "   linsear_write_diff  readability_consensus_diff  avg_sentence_length_diff  \n",
       "0                 3.0                        -2.0                       2.0  \n",
       "1                 1.0                         5.0                       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1320, 149)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interesting_idx(df, n):\n",
    "    def get_idx_with_top_column_values(cname, n=5, ascending=False):\n",
    "        return df[['idx',cname]].\\\n",
    "            drop_duplicates().\\\n",
    "            sort_values(cname, ascending=ascending)\\\n",
    "            ['idx'][0:n].values.tolist()\n",
    "    \n",
    "    def sample_idx_with_label_flips(n=5): \n",
    "        return df[['idx','epoch_of_first_label_flip']].\\\n",
    "            query(\"epoch_of_first_label_flip!=0\").\\\n",
    "            drop_duplicates()\\\n",
    "            ['idx'].sample(n).values.tolist()\n",
    "    \n",
    "    idx_d = dict(\n",
    "        random           = df.idx.drop_duplicates().sample(n).tolist(),\n",
    "        label_flips = sample_idx_with_label_flips(n=n),\n",
    "        idx_n_unique_pp  = get_idx_with_top_column_values('idx_n_unique_pp',n=n,ascending=False),\n",
    "       # idx_n_pp_changes = get_idx_with_top_column_values('idx_n_pp_changes',n=n,ascending=False),\n",
    "        low_sts = get_idx_with_top_column_values('sts_score',n=n,ascending=True)\n",
    "      #  orig_long = None, \n",
    "      #  orig_short = None,\n",
    "    )\n",
    "    return idx_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "idx_d = get_interesting_idx(df, n)\n",
    "#idx_d\n",
    "\n",
    "def print_stats(key,i):\n",
    "    print(\"\\n###############\\n\")\n",
    "    print(key, i, \"\\n\")\n",
    "    idx = idx_d[key][i]\n",
    "    # Setup \n",
    "    df1 = df.query('idx==@idx')\n",
    "    orig = pd.unique(df1['orig_l'])[0]\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Original label\", pd.unique(df1['truelabel'])[0] )\n",
    "    pp_all = list(df1['pp_l'])\n",
    "    #print(\"All paraphrases\", pp_all)\n",
    "    pp_unique = list(pd.unique(df1['pp_l']))\n",
    "    n_pp_unique = len(pp_unique)\n",
    "\n",
    "    # showing a \"timeline\" of how the paraphrases change over the epochs\n",
    "    g_fields = [\"pp_l\",\"pp_truelabel_probs\",\"vm_score\",\"sts_score\",\"reward\",\"label_flip\"]\n",
    "    #g_fields = [\"pp_l\",\"vm_score\"]\n",
    "    g = df1.groupby(g_fields).agg({'epoch' : lambda x: list(x)})\n",
    "    g = g.sort_values(by='epoch', key = lambda col: col.map(lambda x: np.min(x)))\n",
    "    print(\"Unique paraphrases:\", n_pp_unique)\n",
    "    print(\"How the paraphrases change:\")\n",
    "    display_all(g)\n",
    "\n",
    "    # Showing a dataframe of the few best paraphrases\n",
    "    best_pps = df1.sort_values('pp_truelabel_probs').iloc[0]\n",
    "    print(\"Best Paraphrase\")\n",
    "    display_all(best_pps.to_frame().T)\n",
    "\n",
    "    #print(\"Everything\")\n",
    "    #display_all(df1)\n",
    "  \n",
    "for key in idx_d.keys():\n",
    "    for i in range(n): \n",
    "        print_stats(key,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics (to send to w&b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
