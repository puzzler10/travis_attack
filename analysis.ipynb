{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datasets import load_metric\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from datasets import load_dataset\n",
    "from lexicalrichness import LexicalRichness\n",
    "import functools\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import operator\n",
    "import spacy\n",
    "import textstat\n",
    "import difflib as dl\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "np.random.seed(seed)\n",
    "run_name = \"mild-jazz-178\"\n",
    "wandb_run_path = \"uts_nlp/travis_attack/36206pkc\"  # get this from wandb overview section \n",
    "\n",
    "# Init W&B run object\n",
    "api = wandb.Api()\n",
    "run = api.run(wandb_run_path)\n",
    "config = run.config\n",
    "\n",
    "# Init spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "\n",
    "# Load CSV files\n",
    "path_results = f\"../model_checkpoints/travis_attack/{run_name}/\"\n",
    "train            = pd.read_csv(path_results + \"train.csv\")\n",
    "valid            = pd.read_csv(path_results + \"valid.csv\")\n",
    "test             = pd.read_csv(path_results + \"test.csv\")\n",
    "training_step    = pd.read_csv(path_results + \"training_step.csv\")\n",
    "training_summary = pd.read_csv(path_results + \"training_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            with pd.option_context(\"max_colwidth\", 480):\n",
    "                display(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## General run information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Summary \n",
       "\n",
       "**Dataset**: rotten_tomatoes  \n",
       "**Paraphrase model**: `eugenesiow/bart-paraphrase`  \n",
       "**Victim model**: `textattack/distilbert-base-uncased-rotten-tomatoes`  \n",
       "**Semantic Textual Similarity model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`  \n",
       "**Number of epochs**: 30  \n",
       "**Reward function**: `[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]`  \n",
       "**Using the full dataset**: True   \n",
       "We freeze all layers of the paraphrase model except the last **2** layers.  \n",
       "We eval every **1** epochs.   \n",
       "\n",
       "### Paraphrase parameters \n",
       "**Decoding strategy**: greedy  \n",
       "**Number of beams**: 1  \n",
       "**Number of return sequences**: 1  \n",
       "**Max length**: 64  \n",
       "**Min length**: 5  \n",
       "**Temperature**: 1.5  \n",
       "**Length penalty**: 1  \n",
       "**Number of beam groups**: 1  \n",
       "**Diversity penalty**: 0  \n",
       "\n",
       "  \n",
       "### Run parameters\n",
       "**Seed**: 420  \n",
       "**Learning rate**: 3e-05  \n",
       "**Batch sizes:** Train: 32, Eval: 128  \n",
       "**Max number of tokens in input**: 64  \n",
       "**Remove initially misclassified examples**: True  \n",
       "**Input bucketed by length:** True  \n",
       "**Shuffle training data:** False  \n",
       "**Pad input data to multiple of**: 8  \n",
       "**Pad embedding matrices**: True  \n",
       "**Normalise rewards?**: False  \n",
       "**Gradient accumulation?**: False  \n",
       "\n",
       "### Low-level parameters \n",
       "**Use fp16 for training?**: True  \n",
       "**Use memory pinning with dataloaders?**: True  \n",
       "**Initilise gradients with `None` when running `zero_grad()`**: False  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below \n",
    "Markdown(f\"\"\"\n",
    "### Summary \n",
    "\n",
    "**Dataset**: {config['dataset_name']}  \n",
    "**Paraphrase model**: `{config['pp_name']}`  \n",
    "**Victim model**: `{config['vm_name']}`  \n",
    "**Semantic Textual Similarity model**: `{config['sts_name']}`  \n",
    "**Number of epochs**: {config['n_train_epochs']}  \n",
    "**Reward function**: `{config['reward_strategy']}`  \n",
    "**Using the full dataset**: {True if not config['use_small_ds'] else f\"False: n_shards set to {config['shard_params']['n_shards']} shards with shard_contiguous set to {config['shard_params']['shard_contiguous']}\"}   \n",
    "We freeze all layers of the paraphrase model except the last **{config['n_layers_frozen']}** layers.  \n",
    "We eval every **{config['eval_freq']}** epochs.   \n",
    "\n",
    "### Paraphrase parameters \n",
    "**Decoding strategy**: {config['sampling_strategy']}  \n",
    "**Number of beams**: {config['pp_model_params']['num_beams']}  \n",
    "**Number of return sequences**: {config['pp_model_params']['num_return_sequences']}  \n",
    "**Max length**: {config['pp_model_params']['max_length']}  \n",
    "**Min length**: {config['pp_model_params']['min_length']}  \n",
    "**Temperature**: {config['pp_model_params']['temperature']}  \n",
    "**Length penalty**: {config['pp_model_params']['length_penalty']}  \n",
    "**Number of beam groups**: {config['pp_model_params']['num_beam_groups']}  \n",
    "**Diversity penalty**: {config['pp_model_params']['diversity_penalty']}  \n",
    "\n",
    "  \n",
    "### Run parameters\n",
    "**Seed**: {config['seed']}  \n",
    "**Learning rate**: {config['lr']}  \n",
    "**Batch sizes:** Train: {config['batch_size_train']}, Eval: {config['batch_size_eval']}  \n",
    "**Max number of tokens in input**: {config['max_length']}  \n",
    "**Remove initially misclassified examples**: {config['remove_misclassified_examples']}  \n",
    "**Input bucketed by length:** {config['bucket_by_length']}  \n",
    "**Shuffle training data:** {config['shuffle_train']}  \n",
    "**Pad input data to multiple of**: {config['padding_multiple']}  \n",
    "**Pad embedding matrices**: {config['pad_token_embeddings']}  \n",
    "**Normalise rewards?**: {config['normalise_rewards']}  \n",
    "**Gradient accumulation?**: {False if config['accumulation_steps'] == 1  else f\"Every {config['accumulation_steps']} steps\"}  \n",
    "\n",
    "### Low-level parameters \n",
    "**Use fp16 for training?**: {config['fp16']}  \n",
    "**Use memory pinning with dataloaders?**: {config['pin_memory']}  \n",
    "**Initilise gradients with `None` when running `zero_grad()`**: {config['zero_grad_with_none']}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For this one go to the [dashboard](https://wandb.ai/uts_nlp/travis_attack/36206pkc) and have a look at it manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some markdown below\n",
    "Markdown(f\"\"\"For this one go to the [dashboard](https://wandb.ai/{wandb_run_path}) and have a look at it manually\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_step  # can set to train, valid or test (and maybe training_step)\n",
    "df = df.sort_values(by=['idx', \"epoch\"], axis=0)\n",
    "df = df.query(\"idx <= 300\")  # just for testing purposes\n",
    "# Getting weird behaviour with group_by's so binning some of the numeric values\n",
    "df['sts_score'] = df['sts_score'].round(4) \n",
    "df['vm_score'] = df['vm_score'].round(4) \n",
    "df['reward'] = df['reward'].round(4) \n",
    "df['pp_truelabel_probs'] = df['pp_truelabel_probs'].round(4) \n",
    "\n",
    "#display_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     0,
     17,
     35
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of examples with more than one paraphrase tried: 0.5\n",
      "Fraction of examples with a label flip at some point: 0.11399999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_number_of_unique_pps_per_idx(df): \n",
    "    df_grp = df.groupby(\"idx\").agg({\"pp_l\":\"nunique\"})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_l\":\"idx_n_unique_pp\"})\n",
    "    # plot histogram\n",
    "    fig = sns.countplot(x='idx_n_unique_pp', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of unique paraphrases per original example\")\n",
    "    plt.xlabel(\"Unique paraphrases\")\n",
    "    plt.ylabel(\"Count\") \n",
    "    # Add column to orig df and return \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df\n",
    "\n",
    "df = add_number_of_unique_pps_per_idx(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','idx_n_unique_pp']].drop_duplicates()['idx_n_unique_pp'].value_counts()\n",
    "print(f\"Fraction of examples with more than one paraphrase tried: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n",
    "\n",
    "def add_number_of_pp_changes_per_idx(df): \n",
    "    df['pp_changed'] = df.sort_values([\"idx\",\"epoch\"]).groupby('idx')['pp_l'].shift().ne(df['pp_l']).astype(int)\n",
    "    df_grp = df.groupby('idx').agg({'pp_changed': 'sum'})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_changed\":\"idx_n_pp_changes\"})\n",
    "    df_grp['idx_n_pp_changes'] -= 1  # The first paraphrase isn't a change\n",
    "    df = df.drop('pp_changed', 1) # don't need this anymore \n",
    "    \n",
    "    # Plot histogram\n",
    "    fig = sns.countplot(x='idx_n_pp_changes', data=df_grp) \n",
    "    plt.title(\"Distribution of the number of paraphrase changes during training per original example\")\n",
    "    plt.xlabel(\"Paraphrase changes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Add column to orig df before returning \n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df \n",
    "df = add_number_of_pp_changes_per_idx(df)\n",
    "\n",
    "def add_epoch_of_first_label_flip(df): \n",
    "    rownum_of_first_flip = df.groupby('idx')[['epoch','label_flip']].idxmax()['label_flip']\n",
    "    ## idxmax returns first max\n",
    "    df_grp = df[['idx','epoch']].loc[rownum_of_first_flip]\n",
    "    df_grp= df_grp.rename(columns = {\"epoch\":\"epoch_of_first_label_flip\"})\n",
    "    fig = sns.countplot(x='epoch_of_first_label_flip', data=df_grp)\n",
    "    plt.title(\"Distribution of the epoch a label flip first occurs for each original example\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    df = df.merge(df_grp, left_on='idx', right_on='idx', how='left')\n",
    "    return df\n",
    "df = add_epoch_of_first_label_flip(df)\n",
    "# some summary statistics \n",
    "counts = df[['idx','epoch_of_first_label_flip']].drop_duplicates()['epoch_of_first_label_flip'].value_counts()\n",
    "print(f\"Fraction of examples with a label flip at some point: {1 - (counts.iloc[0]/sum(counts)).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sampling some low sts score examples \n",
    "#display_all(df.query('sts_score < 0.7').sort_values('sts_score').sample(10)[['orig_l','pp_l','sts_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_added_end_of_sentence_phrase(orig, pp): \n",
    "    if len(pp) > len(orig): \n",
    "        rep = pp.replace(orig, \"\")\n",
    "        if rep != pp: return rep \n",
    "    return \"\"\n",
    "\n",
    "def get_removals_and_insertions(orig, pp): \n",
    "    orig_t = [token.text for token in nlp(orig)]\n",
    "    pp_t = [token.text for token in nlp(pp)]\n",
    "    diff = [x for x in dl.ndiff(orig_t, pp_t)]\n",
    "    d = defaultdict(lambda: [])\n",
    "    \n",
    "    def get_subsequences(sign): \n",
    "        if   sign == \"insertions\": op = \"+\"\n",
    "        elif sign == \"removals\":   op = \"-\"\n",
    "        else: raise Exception(\"shouldn't get here\")\n",
    "        idx,words = [],[]\n",
    "        for i, o in enumerate(diff): \n",
    "            if o[0] == op: \n",
    "                idx.append(i)\n",
    "                words.append(o[2:])\n",
    "        # Group words that go together        \n",
    "        word_groups = []\n",
    "        # bit of a mystery this bit but seems to work. just need 1-1 mapping between data and words \n",
    "        for k, g in groupby(zip(enumerate(idx), words), lambda ix: ix[0][0] - ix[0][1]):\n",
    "            word_groups.append(list(map(itemgetter(1), g)))\n",
    "        phrases = [' '.join(l) for l in word_groups]\n",
    "        return idx, words, word_groups, phrases\n",
    "    \n",
    "    _,_,_,insertions = get_subsequences(\"insertions\")\n",
    "    _,_,_,removals   = get_subsequences(\"removals\")\n",
    "    return removals, insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_text_metrics(text):\n",
    "    def get_chartype_count(text, strset=string.ascii_letters):\n",
    "        return len(list(filter(functools.partial(operator.contains, strset), text)))     \n",
    "    d = defaultdict(lambda: 0)\n",
    "    ### Spacy stuff\n",
    "    doc = nlp(text)\n",
    "    # Which tags to keep\n",
    "    # see  https://universaldependencies.org/docs/u/pos/\n",
    "    UPOS_tags = ['ADJ','ADP','ADV','AUX','CONJ','DET','INTJ','NOUN',\n",
    "     'NUM','PART','PRON','PROPN','PUNCT','SCONJ',\n",
    "     'SYM','VERB','X']\n",
    "    d_POS = defaultdict(lambda: 0)  # d_POS holds parts of speech\n",
    "    d['n_nonzero_synsets'] = 0  # fix something weird with datasets.map code?\n",
    "    for token in doc:\n",
    "        n_synsets = len(token._.wordnet.synsets())\n",
    "        n_lemmas = len(token._.wordnet.lemmas())\n",
    "        d['n_total_synsets'] += n_synsets; \n",
    "        d['n_total_lemmas'] += n_lemmas\n",
    "        if n_synsets > 0: d['n_nonzero_synsets'] += 1 \n",
    "        d_POS[token.pos_] += 1\n",
    "\n",
    "    d['n_tokens'] = max(len(doc), 1)  # handle empty string \n",
    "    d['avg_synsets'] = d['n_total_synsets'] / d['n_tokens']\n",
    "    d['avg_lemmas']  = d['n_total_lemmas']  / d['n_tokens']     \n",
    "    for tag in UPOS_tags: d['n_upos_tag_' + tag] = d_POS[tag]\n",
    "\n",
    "    ### Lexical stuff\n",
    "    lex = LexicalRichness(text)    \n",
    "    d['n_words'] = lex.words\n",
    "    d['n_stopwords'] = sum([token.is_stop for token in doc])\n",
    "    d['n_named_entities'] = len(doc.ents)\n",
    "    d['n_unique_words'] = lex.terms\n",
    "    d['n_punctuation'] = get_chartype_count(text, strset=string.punctuation)\n",
    "    d['n_digits'] = get_chartype_count(text, strset=string.digits)\n",
    "    d['n_letters'] = get_chartype_count(text, strset=string.ascii_letters)\n",
    "    d['MTLD'] = lex.mtld(threshold=0.72) if lex.words > 1 else 0.0 \n",
    "    d['HDD'] = lex.hdd(draws=min(lex.words, 30)) if lex.words > 1 else 0.0 \n",
    "    d['Maas'] = lex.Maas if lex.words > 1 else 0.0 \n",
    "\n",
    "    ### Textstat stuff\n",
    "    d['flesch_kincaid_ease']      = textstat.flesch_reading_ease(text)\n",
    "    d['SMOG']                     = textstat.smog_index(text)\n",
    "    d['gunning_fog']              = textstat.gunning_fog(text)\n",
    "    d['difficult_words']          = textstat.difficult_words(text)\n",
    "    d['dale_chall']               = textstat.dale_chall_readability_score(text)\n",
    "    d['ARI']                      = textstat.automated_readability_index(text)\n",
    "    d['coleman_liau']             = textstat.coleman_liau_index(text)\n",
    "    d['linsear_write']            = textstat.linsear_write_formula(text)\n",
    "    d['readability_consensus']    = textstat.text_standard(text, float_output=True)\n",
    "    #d['avg_sentence_length']      = textstat.avg_sentence_length(text)\n",
    "    return d\n",
    "\n",
    "def get_text_pair_metrics(orig, pp): \n",
    "    def is_truncation(orig, pp): return pp in orig\n",
    "    def get_rouge_score(ref, pred):\n",
    "        return rouge_metric.compute(rouge_types=[\"rougeL\"], predictions=[pred], references=[ref])['rougeL'].mid.fmeasure \n",
    "    d = dict()\n",
    "    d['rouge_score'] = get_rouge_score(ref=orig, pred=pp)\n",
    "    d['is_truncation'] = is_truncation(orig,pp)\n",
    "    d['added_eos_phrase'] = get_added_end_of_sentence_phrase(orig, pp)\n",
    "    d['removals'],d['insertions'] = get_removals_and_insertions(orig, pp)\n",
    "    return d\n",
    "\n",
    "def get_text_metrics_for_ds(ds, colname, suffix, num_proc):\n",
    "    \"\"\"returns a df\"\"\"\n",
    "    # num_proc=8 seems pretty good - diminishing returns and we may as well leave some CPU for others \n",
    "    x = ds.map(get_text_metrics, input_columns = [colname], batched=False, \n",
    "               num_proc = num_proc )\n",
    "    # rename columns\n",
    "    colnames_mapping = dict()\n",
    "    for k in x.column_names: colnames_mapping[k] = k + f\"_{suffix}\" if k != colname else k\n",
    "    x = x.rename_columns(colnames_mapping)\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "def get_text_pair_metrics_for_ds(ds, num_proc): \n",
    "    x = ds.map(get_text_pair_metrics, input_columns = [\"orig_l\", \"pp_l\"], batched=False, \n",
    "               num_proc = num_proc )\n",
    "    return pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Calculating text statistics for the original examples. ####\n",
      "\n",
      "\n",
      "#### Calculating text statistics for paraphrases. ####\n",
      "\n",
      "\n",
      "#### Calculating text pair statistics ####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_text_stats(df):\n",
    "    # Go through all original examples, calculate stats, then join back to main df  \n",
    "    num_proc = min(8, psutil.cpu_count())\n",
    "    #num_proc=1  # for testing\n",
    "    ds_orig = Dataset.from_pandas(df['orig_l'].drop_duplicates().to_frame())\n",
    "    print(\"\\n#### Calculating text statistics for the original examples. ####\\n\")\n",
    "    df_orig = get_text_metrics_for_ds(ds_orig, colname=\"orig_l\", suffix=\"orig\", num_proc=num_proc)    \n",
    "    df = pd.merge(df, df_orig, how='left', on=['orig_l'])\n",
    "    \n",
    "    # Go through all paraphrases, calculate stats, then join back to main df   \n",
    "    ds_pp = Dataset.from_pandas(df['pp_l'].drop_duplicates().to_frame())\n",
    "    print(\"\\n#### Calculating text statistics for paraphrases. ####\\n\")\n",
    "    df_pp = get_text_metrics_for_ds(ds_pp, colname=\"pp_l\", suffix=\"pp\", num_proc=num_proc)    \n",
    "    df = pd.merge(df, df_pp, how='left', on=['pp_l'])\n",
    "    \n",
    "    ## Calculate differences in stats\n",
    "    keys = get_text_metrics(\"some text here\").keys()\n",
    "    for k in keys:  df[f\"{k}_diff\"] = df[f\"{k}_orig\"] - df[f\"{k}_pp\"]\n",
    "    \n",
    "    ## Calc stats for (orig, pp) unique pairs\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    ds_pairs = Dataset.from_pandas(df[['orig_l','pp_l']].drop_duplicates())\n",
    "    print(\"\\n#### Calculating text pair statistics ####\\n\")\n",
    "    df_pairs = get_text_pair_metrics_for_ds(ds_pairs, num_proc=num_proc)\n",
    "    df = pd.merge(df, df_pairs, how='left', on=['orig_l','pp_l'])    \n",
    "    return df \n",
    "\n",
    "\n",
    "df = add_text_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_l</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>removals</th>\n",
       "      <th>insertions</th>\n",
       "      <th>added_eos_phrase</th>\n",
       "      <th>is_truncation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>for a good chunk of its running time , trapped is an effective and claustrophobic thriller .</td>\n",
       "      <td>For a good chunk of its running time, trapped is an effective and claustrophobic thriller.</td>\n",
       "      <td>[for]</td>\n",
       "      <td>[For]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>a cop story that understands the medium amazingly well .</td>\n",
       "      <td>A cop story that understands the medium amazingly well.</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[A]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>the passions aroused by the discord between old and new cultures are set against the strange , stark beauty of the mideast desert , so lovingly and perceptively filmed that you can almost taste the desiccated air .</td>\n",
       "      <td>The passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[The]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art .</td>\n",
       "      <td>behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>an enthralling , playful film that constantly frustrates our desire to know the 'truth' about this man , while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to .</td>\n",
       "      <td>An enthralling, playful film that constantly frustrates our desire to know the 'truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.</td>\n",
       "      <td>[an]</td>\n",
       "      <td>[An]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>[drumline] is entertaining for what it does , and admirable for what it doesn't do .</td>\n",
       "      <td>[drumline] is entertaining for what it does, and admirable for whatit doesn't do.</td>\n",
       "      <td>[what, it]</td>\n",
       "      <td>[whatit]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>thankfully , the film , which skirts that rapidly deteriorating line between fantasy and reality . . . takes a tongue-in-cheek attitude even as it pushes the croc hunter agenda .</td>\n",
       "      <td>thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.</td>\n",
       "      <td>[. . takes a tongue - in - cheek attitude even as it pushes the croc hunter agenda .]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>part of the charm of satin rouge is that it avoids the obvious with humour and lightness .</td>\n",
       "      <td>Part of the charm of satin rouge is that it avoids the obvious with humour and lightness.</td>\n",
       "      <td>[part]</td>\n",
       "      <td>[Part]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .</td>\n",
       "      <td>sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self-absorbed martha as her heart begins to open.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>intriguing and beautiful film , but those of you who read the book are likely to be disappointed .</td>\n",
       "      <td>intriguing and beautiful film, but those of you who read the book are likely to be disappointed.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                              orig_l  \\\n",
       "4613                                                                                                                                    for a good chunk of its running time , trapped is an effective and claustrophobic thriller .   \n",
       "5013                                                                                                                                                                        a cop story that understands the medium amazingly well .   \n",
       "6495          the passions aroused by the discord between old and new cultures are set against the strange , stark beauty of the mideast desert , so lovingly and perceptively filmed that you can almost taste the desiccated air .   \n",
       "6606                                                                                                     behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art .   \n",
       "3278  an enthralling , playful film that constantly frustrates our desire to know the 'truth' about this man , while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to .   \n",
       "5744                                                                                                                                            [drumline] is entertaining for what it does , and admirable for what it doesn't do .   \n",
       "7555                                              thankfully , the film , which skirts that rapidly deteriorating line between fantasy and reality . . . takes a tongue-in-cheek attitude even as it pushes the croc hunter agenda .   \n",
       "1800                                                                                                                                      part of the charm of satin rouge is that it avoids the obvious with humour and lightness .   \n",
       "1408                                                                                 sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .   \n",
       "8058                                                                                                                              intriguing and beautiful film , but those of you who read the book are likely to be disappointed .   \n",
       "\n",
       "                                                                                                                                                                                                                             pp_l  \\\n",
       "4613                                                                                                                                   For a good chunk of its running time, trapped is an effective and claustrophobic thriller.   \n",
       "5013                                                                                                                                                                      A cop story that understands the medium amazingly well.   \n",
       "6495          The passions aroused by the discord between old and new cultures are set against the strange, stark beauty of the mideast desert, so lovingly and perceptively filmed that you can almost taste the desiccated air.   \n",
       "6606                                                                                                   behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art.   \n",
       "3278  An enthralling, playful film that constantly frustrates our desire to know the 'truth' about this man, while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to.   \n",
       "5744                                                                                                                                            [drumline] is entertaining for what it does, and admirable for whatit doesn't do.   \n",
       "7555                                                                                                                              thankfully, the film, which skirts that rapidly deteriorating line between fantasy and reality.   \n",
       "1800                                                                                                                                    Part of the charm of satin rouge is that it avoids the obvious with humour and lightness.   \n",
       "1408                                                                                 sandra nettelbeck beautifully orchestrates the transformation of the chilly, neurotic, and self-absorbed martha as her heart begins to open.   \n",
       "8058                                                                                                                             intriguing and beautiful film, but those of you who read the book are likely to be disappointed.   \n",
       "\n",
       "                                                                                   removals  \\\n",
       "4613                                                                                  [for]   \n",
       "5013                                                                                    [a]   \n",
       "6495                                                                                  [the]   \n",
       "6606                                                                                     []   \n",
       "3278                                                                                   [an]   \n",
       "5744                                                                             [what, it]   \n",
       "7555  [. . takes a tongue - in - cheek attitude even as it pushes the croc hunter agenda .]   \n",
       "1800                                                                                 [part]   \n",
       "1408                                                                                     []   \n",
       "8058                                                                                     []   \n",
       "\n",
       "     insertions added_eos_phrase  is_truncation  \n",
       "4613      [For]                           False  \n",
       "5013        [A]                           False  \n",
       "6495      [The]                           False  \n",
       "6606         []                           False  \n",
       "3278       [An]                           False  \n",
       "5744   [whatit]                           False  \n",
       "7555         []                           False  \n",
       "1800     [Part]                           False  \n",
       "1408         []                           False  \n",
       "8058         []                           False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df[['orig_l', 'pp_l','removals','insertions', 'added_eos_phrase', 'is_truncation']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(psutil.cpu_count(), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [o + \"_orig\" if o != \"orig_l\" else o for o in x.column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-fa96841434da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "x.column_names = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['orig_l', 'n_nonzero_synsets_orig', 'n_total_synsets_orig', 'n_total_lemmas_orig', 'n_tokens_orig', 'avg_synsets_orig', 'avg_lemmas_orig', 'n_upos_tag_ADJ_orig', 'n_upos_tag_ADP_orig', 'n_upos_tag_ADV_orig', 'n_upos_tag_AUX_orig', 'n_upos_tag_CONJ_orig', 'n_upos_tag_DET_orig', 'n_upos_tag_INTJ_orig', 'n_upos_tag_NOUN_orig', 'n_upos_tag_NUM_orig', 'n_upos_tag_PART_orig', 'n_upos_tag_PRON_orig', 'n_upos_tag_PROPN_orig', 'n_upos_tag_PUNCT_orig', 'n_upos_tag_SCONJ_orig', 'n_upos_tag_SYM_orig', 'n_upos_tag_VERB_orig', 'n_upos_tag_X_orig', 'n_words_orig', 'n_stopwords_orig', 'n_named_entities_orig', 'n_unique_words_orig', 'n_punctuation_orig', 'n_digits_orig', 'n_letters_orig', 'MTLD_orig', 'HDD_orig', 'Maas_orig', 'flesch_kincaid_ease_orig', 'SMOG_orig', 'gunning_fog_orig', 'difficult_words_orig', 'dale_chall_orig', 'ARI_orig', 'coleman_liau_orig', 'linsear_write_orig', 'readability_consensus_orig'],\n",
       "    num_rows: 2623\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.rename_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_l</th>\n",
       "      <th>n_nonzero_synsets</th>\n",
       "      <th>n_total_synsets</th>\n",
       "      <th>n_total_lemmas</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>avg_synsets</th>\n",
       "      <th>avg_lemmas</th>\n",
       "      <th>n_upos_tag_ADJ</th>\n",
       "      <th>n_upos_tag_ADP</th>\n",
       "      <th>n_upos_tag_ADV</th>\n",
       "      <th>...</th>\n",
       "      <th>Maas</th>\n",
       "      <th>flesch_kincaid_ease</th>\n",
       "      <th>SMOG</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>ARI</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>linsear_write</th>\n",
       "      <th>readability_consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>18</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>28</td>\n",
       "      <td>141</td>\n",
       "      <td>376</td>\n",
       "      <td>48</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.68</td>\n",
       "      <td>11</td>\n",
       "      <td>10.54</td>\n",
       "      <td>21.9</td>\n",
       "      <td>14.40</td>\n",
       "      <td>26.5</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>12</td>\n",
       "      <td>133</td>\n",
       "      <td>367</td>\n",
       "      <td>21</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>17.476190</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>86.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>272</td>\n",
       "      <td>23</td>\n",
       "      <td>4.695652</td>\n",
       "      <td>11.826087</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>69.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>3</td>\n",
       "      <td>7.07</td>\n",
       "      <td>10.6</td>\n",
       "      <td>9.28</td>\n",
       "      <td>10.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the film provides some great insight into the ...</td>\n",
       "      <td>16</td>\n",
       "      <td>95</td>\n",
       "      <td>273</td>\n",
       "      <td>26</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>64.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.93</td>\n",
       "      <td>5</td>\n",
       "      <td>8.12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>9.58</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>beautifully reclaiming the story of carmen and...</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>88</td>\n",
       "      <td>15</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>40.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.17</td>\n",
       "      <td>5</td>\n",
       "      <td>9.97</td>\n",
       "      <td>9.4</td>\n",
       "      <td>11.13</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>the camera soars above the globe in dazzling p...</td>\n",
       "      <td>31</td>\n",
       "      <td>215</td>\n",
       "      <td>510</td>\n",
       "      <td>44</td>\n",
       "      <td>4.886364</td>\n",
       "      <td>11.590909</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017105</td>\n",
       "      <td>40.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.68</td>\n",
       "      <td>10</td>\n",
       "      <td>9.62</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>a flawed but engrossing thriller .</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.04</td>\n",
       "      <td>8.4</td>\n",
       "      <td>10.76</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>demonstrates the unusual power of thoughtful ,...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.20</td>\n",
       "      <td>5</td>\n",
       "      <td>13.90</td>\n",
       "      <td>18.5</td>\n",
       "      <td>23.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>expect no major discoveries , nor any stylish ...</td>\n",
       "      <td>19</td>\n",
       "      <td>107</td>\n",
       "      <td>247</td>\n",
       "      <td>28</td>\n",
       "      <td>3.821429</td>\n",
       "      <td>8.821429</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>46.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>5</td>\n",
       "      <td>8.03</td>\n",
       "      <td>14.6</td>\n",
       "      <td>11.32</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2623 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 orig_l  n_nonzero_synsets  \\\n",
       "0     the rock is destined to be the 21st century's ...                 18   \n",
       "1     the gorgeously elaborate continuation of \" the...                 28   \n",
       "2     if you sometimes like to go to the movies to h...                 12   \n",
       "3     emerges as something rare , an issue movie tha...                 15   \n",
       "4     the film provides some great insight into the ...                 16   \n",
       "...                                                 ...                ...   \n",
       "2618  beautifully reclaiming the story of carmen and...                 10   \n",
       "2619  the camera soars above the globe in dazzling p...                 31   \n",
       "2620                 a flawed but engrossing thriller .                  5   \n",
       "2621  demonstrates the unusual power of thoughtful ,...                  5   \n",
       "2622  expect no major discoveries , nor any stylish ...                 19   \n",
       "\n",
       "      n_total_synsets  n_total_lemmas  n_tokens  avg_synsets  avg_lemmas  \\\n",
       "0                 165             426        38     4.342105   11.210526   \n",
       "1                 141             376        48     2.937500    7.833333   \n",
       "2                 133             367        21     6.333333   17.476190   \n",
       "3                 108             272        23     4.695652   11.826087   \n",
       "4                  95             273        26     3.653846   10.500000   \n",
       "...               ...             ...       ...          ...         ...   \n",
       "2618               31              88        15     2.066667    5.866667   \n",
       "2619              215             510        44     4.886364   11.590909   \n",
       "2620               12              38         6     2.000000    6.333333   \n",
       "2621               23              62        10     2.300000    6.200000   \n",
       "2622              107             247        28     3.821429    8.821429   \n",
       "\n",
       "      n_upos_tag_ADJ  n_upos_tag_ADP  n_upos_tag_ADV  ...      Maas  \\\n",
       "0                  4               0               1  ...  0.008196   \n",
       "1                  3               4               3  ...  0.016838   \n",
       "2                  1               1               1  ...  0.019822   \n",
       "3                  2               0               2  ...  0.005264   \n",
       "4                  3               3               1  ...  0.018052   \n",
       "...              ...             ...             ...  ...       ...   \n",
       "2618               1               2               1  ...  0.010641   \n",
       "2619               7              10               0  ...  0.017105   \n",
       "2620               1               0               0  ...  0.000000   \n",
       "2621               3               1               0  ...  0.000000   \n",
       "2622               4               3               0  ...  0.003940   \n",
       "\n",
       "      flesch_kincaid_ease  SMOG  gunning_fog  difficult_words  dale_chall  \\\n",
       "0                   57.95   0.0        13.33                5        7.76   \n",
       "1                    4.14   0.0        21.68               11       10.54   \n",
       "2                   86.03   0.0         7.60                0        0.94   \n",
       "3                   69.11   0.0         7.60                3        7.07   \n",
       "4                   64.04   0.0        12.93                5        8.12   \n",
       "...                   ...   ...          ...              ...         ...   \n",
       "2618                40.35   0.0        14.17                5        9.97   \n",
       "2619                40.35   0.0        18.68               10        9.62   \n",
       "2620                66.40   0.0        10.00                1        7.04   \n",
       "2621                -4.33   0.0        18.20                5       13.90   \n",
       "2622                46.10   0.0        14.80                5        8.03   \n",
       "\n",
       "       ARI  coleman_liau  linsear_write  readability_consensus  \n",
       "0     16.2          9.82           18.0                   13.0  \n",
       "1     21.9         14.40           26.5                   22.0  \n",
       "2      5.2          3.19           10.5                    6.0  \n",
       "3     10.6          9.28           10.5                    8.0  \n",
       "4     12.8          9.58           15.0                   13.0  \n",
       "...    ...           ...            ...                    ...  \n",
       "2618   9.4         11.13           10.5                   11.0  \n",
       "2619  21.0         10.98           24.0                   11.0  \n",
       "2620   8.4         10.76            3.0                    8.0  \n",
       "2621  18.5         23.16            7.0                   19.0  \n",
       "2622  14.6         11.32           17.0                   15.0  \n",
       "\n",
       "[2623 rows x 43 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.toF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3 s ± 236 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 3.44 s, total: 1min 41s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "l = []\n",
    "for text in ds_tmp['orig_l']: \n",
    "    l.append(get_text_metrics(text))\n",
    "x = Dataset.from_pandas(pd.DataFrame(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil \n",
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9a4567b284aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/datasets/table.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/datasets/table.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIndexedTableMixin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrecordbatch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecordbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "datasets.table.Table(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_metrics = get_text_metrics(pp)\n",
    "for k in d_metrics.keys(): d[f\"{k}_pp\"] = d_metrics[k]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['n_total_synsets', 'n_total_lemmas', 'n_nonzero_synsets', 'n_tokens', 'avg_synsets', 'avg_lemmas', 'n_upos_tag_ADJ', 'n_upos_tag_ADP', 'n_upos_tag_ADV', 'n_upos_tag_AUX', 'n_upos_tag_CONJ', 'n_upos_tag_DET', 'n_upos_tag_INTJ', 'n_upos_tag_NOUN', 'n_upos_tag_NUM', 'n_upos_tag_PART', 'n_upos_tag_PRON', 'n_upos_tag_PROPN', 'n_upos_tag_PUNCT', 'n_upos_tag_SCONJ', 'n_upos_tag_SYM', 'n_upos_tag_VERB', 'n_upos_tag_X', 'n_words', 'n_stopwords', 'n_named_entities', 'n_unique_words', 'n_punctuation', 'n_digits', 'n_letters', 'MTLD', 'HDD', 'Maas', 'flesch_kincaid_ease', 'SMOG', 'gunning_fog', 'difficult_words', 'dale_chall', 'ARI', 'coleman_liau', 'linsear_write', 'readability_consensus'],\n",
       "    num_rows: 44\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a7344f577875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    781\u001b[0m         mapping = {\n\u001b[1;32m    782\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         }\n\u001b[1;32m    785\u001b[0m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "Dataset.from_dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['orig_l', 'pp_l', '__index_level_0__'],\n",
       "    num_rows: 125\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.table.Table"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.table.Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_l</th>\n",
       "      <th>pp_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>The rock is destined to be the new \" Conan '' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>The rock is destined to be the 21st century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59701</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>The rock is destined to be the 21st century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>The gorgeously elaborate continuation of \" the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>The gorgeously elaborate continuation of \" the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45232</th>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60118</th>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89890</th>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112219</th>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "      <td>may be spoofing an easy target -- those old '5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6844</th>\n",
       "      <td>an engaging overview of johnson's eccentric ca...</td>\n",
       "      <td>An engaging overview of johnson's eccentric ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   orig_l  \\\n",
       "157     the rock is destined to be the 21st century's ...   \n",
       "7600    the rock is destined to be the 21st century's ...   \n",
       "59701   the rock is destined to be the 21st century's ...   \n",
       "87      the gorgeously elaborate continuation of \" the...   \n",
       "7530    the gorgeously elaborate continuation of \" the...   \n",
       "...                                                   ...   \n",
       "45232   may be spoofing an easy target -- those old '5...   \n",
       "60118   may be spoofing an easy target -- those old '5...   \n",
       "89890   may be spoofing an easy target -- those old '5...   \n",
       "112219  may be spoofing an easy target -- those old '5...   \n",
       "6844    an engaging overview of johnson's eccentric ca...   \n",
       "\n",
       "                                                     pp_l  \n",
       "157     The rock is destined to be the new \" Conan '' ...  \n",
       "7600    The rock is destined to be the 21st century's ...  \n",
       "59701   The rock is destined to be the 21st century's ...  \n",
       "87      The gorgeously elaborate continuation of \" the...  \n",
       "7530    The gorgeously elaborate continuation of \" the...  \n",
       "...                                                   ...  \n",
       "45232   may be spoofing an easy target -- those old '5...  \n",
       "60118   may be spoofing an easy target -- those old '5...  \n",
       "89890   may be spoofing an easy target -- those old '5...  \n",
       "112219  may be spoofing an easy target -- those old '5...  \n",
       "6844    An engaging overview of johnson's eccentric ca...  \n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>orig_l</th>\n",
       "      <th>truelabel</th>\n",
       "      <th>orig_truelabel_probs</th>\n",
       "      <th>epoch</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>pp_truelabel_probs</th>\n",
       "      <th>pp_predclass</th>\n",
       "      <th>pp_predclass_probs</th>\n",
       "      <th>loss</th>\n",
       "      <th>pp_logp</th>\n",
       "      <th>reward</th>\n",
       "      <th>vm_score</th>\n",
       "      <th>sts_score</th>\n",
       "      <th>label_flip</th>\n",
       "      <th>idx_n_unique_pp</th>\n",
       "      <th>idx_n_pp_changes</th>\n",
       "      <th>epoch_of_first_label_flip</th>\n",
       "      <th>n_total_synsets_orig</th>\n",
       "      <th>n_total_lemmas_orig</th>\n",
       "      <th>n_nonzero_synsets_orig</th>\n",
       "      <th>n_tokens_orig</th>\n",
       "      <th>avg_synsets_orig</th>\n",
       "      <th>avg_lemmas_orig</th>\n",
       "      <th>n_upos_tag_ADJ_orig</th>\n",
       "      <th>n_upos_tag_ADP_orig</th>\n",
       "      <th>n_upos_tag_ADV_orig</th>\n",
       "      <th>n_upos_tag_AUX_orig</th>\n",
       "      <th>n_upos_tag_CONJ_orig</th>\n",
       "      <th>n_upos_tag_DET_orig</th>\n",
       "      <th>n_upos_tag_INTJ_orig</th>\n",
       "      <th>n_upos_tag_NOUN_orig</th>\n",
       "      <th>n_upos_tag_NUM_orig</th>\n",
       "      <th>n_upos_tag_PART_orig</th>\n",
       "      <th>n_upos_tag_PRON_orig</th>\n",
       "      <th>n_upos_tag_PROPN_orig</th>\n",
       "      <th>n_upos_tag_PUNCT_orig</th>\n",
       "      <th>n_upos_tag_SCONJ_orig</th>\n",
       "      <th>n_upos_tag_SYM_orig</th>\n",
       "      <th>n_upos_tag_VERB_orig</th>\n",
       "      <th>n_upos_tag_X_orig</th>\n",
       "      <th>n_words_orig</th>\n",
       "      <th>n_stopwords_orig</th>\n",
       "      <th>n_named_entities_orig</th>\n",
       "      <th>n_unique_words_orig</th>\n",
       "      <th>n_punctuation_orig</th>\n",
       "      <th>n_digits_orig</th>\n",
       "      <th>n_letters_orig</th>\n",
       "      <th>MTLD_orig</th>\n",
       "      <th>HDD_orig</th>\n",
       "      <th>Maas_orig</th>\n",
       "      <th>flesch_kincaid_ease_orig</th>\n",
       "      <th>SMOG_orig</th>\n",
       "      <th>gunning_fog_orig</th>\n",
       "      <th>difficult_words_orig</th>\n",
       "      <th>dale_chall_orig</th>\n",
       "      <th>ARI_orig</th>\n",
       "      <th>coleman_liau_orig</th>\n",
       "      <th>linsear_write_orig</th>\n",
       "      <th>readability_consensus_orig</th>\n",
       "      <th>avg_sentence_length_orig</th>\n",
       "      <th>n_total_synsets_pp</th>\n",
       "      <th>n_total_lemmas_pp</th>\n",
       "      <th>n_nonzero_synsets_pp</th>\n",
       "      <th>n_tokens_pp</th>\n",
       "      <th>avg_synsets_pp</th>\n",
       "      <th>avg_lemmas_pp</th>\n",
       "      <th>n_upos_tag_ADJ_pp</th>\n",
       "      <th>n_upos_tag_ADP_pp</th>\n",
       "      <th>n_upos_tag_ADV_pp</th>\n",
       "      <th>n_upos_tag_AUX_pp</th>\n",
       "      <th>n_upos_tag_CONJ_pp</th>\n",
       "      <th>n_upos_tag_DET_pp</th>\n",
       "      <th>n_upos_tag_INTJ_pp</th>\n",
       "      <th>n_upos_tag_NOUN_pp</th>\n",
       "      <th>n_upos_tag_NUM_pp</th>\n",
       "      <th>n_upos_tag_PART_pp</th>\n",
       "      <th>n_upos_tag_PRON_pp</th>\n",
       "      <th>n_upos_tag_PROPN_pp</th>\n",
       "      <th>n_upos_tag_PUNCT_pp</th>\n",
       "      <th>n_upos_tag_SCONJ_pp</th>\n",
       "      <th>n_upos_tag_SYM_pp</th>\n",
       "      <th>n_upos_tag_VERB_pp</th>\n",
       "      <th>n_upos_tag_X_pp</th>\n",
       "      <th>n_words_pp</th>\n",
       "      <th>n_stopwords_pp</th>\n",
       "      <th>n_named_entities_pp</th>\n",
       "      <th>n_unique_words_pp</th>\n",
       "      <th>n_punctuation_pp</th>\n",
       "      <th>n_digits_pp</th>\n",
       "      <th>n_letters_pp</th>\n",
       "      <th>MTLD_pp</th>\n",
       "      <th>HDD_pp</th>\n",
       "      <th>Maas_pp</th>\n",
       "      <th>flesch_kincaid_ease_pp</th>\n",
       "      <th>SMOG_pp</th>\n",
       "      <th>gunning_fog_pp</th>\n",
       "      <th>difficult_words_pp</th>\n",
       "      <th>dale_chall_pp</th>\n",
       "      <th>ARI_pp</th>\n",
       "      <th>coleman_liau_pp</th>\n",
       "      <th>linsear_write_pp</th>\n",
       "      <th>readability_consensus_pp</th>\n",
       "      <th>avg_sentence_length_pp</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>is_truncation</th>\n",
       "      <th>n_total_synsets_diff</th>\n",
       "      <th>n_total_lemmas_diff</th>\n",
       "      <th>n_nonzero_synsets_diff</th>\n",
       "      <th>n_tokens_diff</th>\n",
       "      <th>avg_synsets_diff</th>\n",
       "      <th>avg_lemmas_diff</th>\n",
       "      <th>n_upos_tag_ADJ_diff</th>\n",
       "      <th>n_upos_tag_ADP_diff</th>\n",
       "      <th>n_upos_tag_ADV_diff</th>\n",
       "      <th>n_upos_tag_AUX_diff</th>\n",
       "      <th>n_upos_tag_CONJ_diff</th>\n",
       "      <th>n_upos_tag_DET_diff</th>\n",
       "      <th>n_upos_tag_INTJ_diff</th>\n",
       "      <th>n_upos_tag_NOUN_diff</th>\n",
       "      <th>n_upos_tag_NUM_diff</th>\n",
       "      <th>n_upos_tag_PART_diff</th>\n",
       "      <th>n_upos_tag_PRON_diff</th>\n",
       "      <th>n_upos_tag_PROPN_diff</th>\n",
       "      <th>n_upos_tag_PUNCT_diff</th>\n",
       "      <th>n_upos_tag_SCONJ_diff</th>\n",
       "      <th>n_upos_tag_SYM_diff</th>\n",
       "      <th>n_upos_tag_VERB_diff</th>\n",
       "      <th>n_upos_tag_X_diff</th>\n",
       "      <th>n_words_diff</th>\n",
       "      <th>n_stopwords_diff</th>\n",
       "      <th>n_named_entities_diff</th>\n",
       "      <th>n_unique_words_diff</th>\n",
       "      <th>n_punctuation_diff</th>\n",
       "      <th>n_digits_diff</th>\n",
       "      <th>n_letters_diff</th>\n",
       "      <th>MTLD_diff</th>\n",
       "      <th>HDD_diff</th>\n",
       "      <th>Maas_diff</th>\n",
       "      <th>flesch_kincaid_ease_diff</th>\n",
       "      <th>SMOG_diff</th>\n",
       "      <th>gunning_fog_diff</th>\n",
       "      <th>difficult_words_diff</th>\n",
       "      <th>dale_chall_diff</th>\n",
       "      <th>ARI_diff</th>\n",
       "      <th>coleman_liau_diff</th>\n",
       "      <th>linsear_write_diff</th>\n",
       "      <th>readability_consensus_diff</th>\n",
       "      <th>avg_sentence_length_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0</td>\n",
       "      <td>The rock is destined to be the new \" Conan '' and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.</td>\n",
       "      <td>0.8068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806813</td>\n",
       "      <td>2.474591</td>\n",
       "      <td>-4.545957</td>\n",
       "      <td>0.5443</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>162</td>\n",
       "      <td>418</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>4.628571</td>\n",
       "      <td>11.942857</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>68.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>4</td>\n",
       "      <td>7.28</td>\n",
       "      <td>14.8</td>\n",
       "      <td>8.88</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.286466</td>\n",
       "      <td>-0.732331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>1</td>\n",
       "      <td>The rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.053502</td>\n",
       "      <td>-0.107004</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>165</td>\n",
       "      <td>426</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>4.342105</td>\n",
       "      <td>11.210526</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>33.693333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>66.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>5</td>\n",
       "      <td>7.76</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  \\\n",
       "0    0   \n",
       "1    0   \n",
       "\n",
       "                                                                                                                                                                              orig_l  \\\n",
       "0  the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .   \n",
       "1  the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .   \n",
       "\n",
       "   truelabel  orig_truelabel_probs  epoch  \\\n",
       "0          1              0.855556      0   \n",
       "1          1              0.855556      1   \n",
       "\n",
       "                                                                                                                                                                              pp_l  \\\n",
       "0                The rock is destined to be the new \" Conan '' and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.   \n",
       "1  The rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.   \n",
       "\n",
       "   pp_truelabel_probs  pp_predclass  pp_predclass_probs      loss   pp_logp  \\\n",
       "0              0.8068             1            0.806813  2.474591 -4.545957   \n",
       "1              0.8556             1            0.855556  0.053502 -0.107004   \n",
       "\n",
       "   reward  vm_score  sts_score  label_flip  idx_n_unique_pp  idx_n_pp_changes  \\\n",
       "0  0.5443    0.0487     0.9099           0                3                 3   \n",
       "1  0.5000   -0.0000     0.9997           0                3                 3   \n",
       "\n",
       "   epoch_of_first_label_flip  n_total_synsets_orig  n_total_lemmas_orig  \\\n",
       "0                          0                   165                  426   \n",
       "1                          0                   165                  426   \n",
       "\n",
       "   n_nonzero_synsets_orig  n_tokens_orig  avg_synsets_orig  avg_lemmas_orig  \\\n",
       "0                      18             38          4.342105        11.210526   \n",
       "1                      18             38          4.342105        11.210526   \n",
       "\n",
       "   n_upos_tag_ADJ_orig  n_upos_tag_ADP_orig  n_upos_tag_ADV_orig  \\\n",
       "0                    4                    0                    1   \n",
       "1                    4                    0                    1   \n",
       "\n",
       "   n_upos_tag_AUX_orig  n_upos_tag_CONJ_orig  n_upos_tag_DET_orig  \\\n",
       "0                    3                     0                    3   \n",
       "1                    3                     0                    3   \n",
       "\n",
       "   n_upos_tag_INTJ_orig  n_upos_tag_NOUN_orig  n_upos_tag_NUM_orig  \\\n",
       "0                     0                     5                    0   \n",
       "1                     0                     5                    0   \n",
       "\n",
       "   n_upos_tag_PART_orig  n_upos_tag_PRON_orig  n_upos_tag_PROPN_orig  \\\n",
       "0                     3                     1                      6   \n",
       "1                     3                     1                      6   \n",
       "\n",
       "   n_upos_tag_PUNCT_orig  n_upos_tag_SCONJ_orig  n_upos_tag_SYM_orig  \\\n",
       "0                      5                      2                    0   \n",
       "1                      5                      2                    0   \n",
       "\n",
       "   n_upos_tag_VERB_orig  n_upos_tag_X_orig  n_words_orig  n_stopwords_orig  \\\n",
       "0                     3                  0            19                16   \n",
       "1                     3                  0            19                16   \n",
       "\n",
       "   n_named_entities_orig  n_unique_words_orig  n_punctuation_orig  \\\n",
       "0                      5                   16                   7   \n",
       "1                      5                   16                   7   \n",
       "\n",
       "   n_digits_orig  n_letters_orig  MTLD_orig  HDD_orig  Maas_orig  \\\n",
       "0              2             135  33.693333  0.842105   0.019822   \n",
       "1              2             135  33.693333  0.842105   0.019822   \n",
       "\n",
       "   flesch_kincaid_ease_orig  SMOG_orig  gunning_fog_orig  \\\n",
       "0                     57.95        0.0             13.33   \n",
       "1                     57.95        0.0             13.33   \n",
       "\n",
       "   difficult_words_orig  dale_chall_orig  ARI_orig  coleman_liau_orig  \\\n",
       "0                     5             7.76      16.2               9.82   \n",
       "1                     5             7.76      16.2               9.82   \n",
       "\n",
       "   linsear_write_orig  readability_consensus_orig  avg_sentence_length_orig  \\\n",
       "0                18.0                        13.0                      30.0   \n",
       "1                18.0                        13.0                      30.0   \n",
       "\n",
       "   n_total_synsets_pp  n_total_lemmas_pp  n_nonzero_synsets_pp  n_tokens_pp  \\\n",
       "0                 162                418                    16           35   \n",
       "1                 165                426                    18           38   \n",
       "\n",
       "   avg_synsets_pp  avg_lemmas_pp  n_upos_tag_ADJ_pp  n_upos_tag_ADP_pp  \\\n",
       "0        4.628571      11.942857                  3                  0   \n",
       "1        4.342105      11.210526                  4                  0   \n",
       "\n",
       "   n_upos_tag_ADV_pp  n_upos_tag_AUX_pp  n_upos_tag_CONJ_pp  \\\n",
       "0                  1                  3                   0   \n",
       "1                  1                  3                   0   \n",
       "\n",
       "   n_upos_tag_DET_pp  n_upos_tag_INTJ_pp  n_upos_tag_NOUN_pp  \\\n",
       "0                  3                   0                   3   \n",
       "1                  3                   0                   5   \n",
       "\n",
       "   n_upos_tag_NUM_pp  n_upos_tag_PART_pp  n_upos_tag_PRON_pp  \\\n",
       "0                  0                   2                   1   \n",
       "1                  0                   3                   1   \n",
       "\n",
       "   n_upos_tag_PROPN_pp  n_upos_tag_PUNCT_pp  n_upos_tag_SCONJ_pp  \\\n",
       "0                    7                    5                    2   \n",
       "1                    6                    5                    2   \n",
       "\n",
       "   n_upos_tag_SYM_pp  n_upos_tag_VERB_pp  n_upos_tag_X_pp  n_words_pp  \\\n",
       "0                  0                   3                0          19   \n",
       "1                  0                   3                0          19   \n",
       "\n",
       "   n_stopwords_pp  n_named_entities_pp  n_unique_words_pp  n_punctuation_pp  \\\n",
       "0              15                    5                 16                 7   \n",
       "1              16                    5                 16                 7   \n",
       "\n",
       "   n_digits_pp  n_letters_pp    MTLD_pp    HDD_pp   Maas_pp  \\\n",
       "0            0           125  33.693333  0.842105  0.019822   \n",
       "1            2           135  33.693333  0.842105  0.019822   \n",
       "\n",
       "   flesch_kincaid_ease_pp  SMOG_pp  gunning_fog_pp  difficult_words_pp  \\\n",
       "0                   68.44      0.0           11.20                   4   \n",
       "1                   66.41      0.0           13.33                   5   \n",
       "\n",
       "   dale_chall_pp  ARI_pp  coleman_liau_pp  linsear_write_pp  \\\n",
       "0           7.28    14.8             8.88              15.0   \n",
       "1           7.76    16.2             9.82              17.0   \n",
       "\n",
       "   readability_consensus_pp  avg_sentence_length_pp  rouge_score  \\\n",
       "0                      15.0                    28.0     0.952381   \n",
       "1                       8.0                    30.0     1.000000   \n",
       "\n",
       "   is_truncation  n_total_synsets_diff  n_total_lemmas_diff  \\\n",
       "0          False                     3                    8   \n",
       "1          False                     0                    0   \n",
       "\n",
       "   n_nonzero_synsets_diff  n_tokens_diff  avg_synsets_diff  avg_lemmas_diff  \\\n",
       "0                       2              3         -0.286466        -0.732331   \n",
       "1                       0              0          0.000000         0.000000   \n",
       "\n",
       "   n_upos_tag_ADJ_diff  n_upos_tag_ADP_diff  n_upos_tag_ADV_diff  \\\n",
       "0                    1                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "\n",
       "   n_upos_tag_AUX_diff  n_upos_tag_CONJ_diff  n_upos_tag_DET_diff  \\\n",
       "0                    0                     0                    0   \n",
       "1                    0                     0                    0   \n",
       "\n",
       "   n_upos_tag_INTJ_diff  n_upos_tag_NOUN_diff  n_upos_tag_NUM_diff  \\\n",
       "0                     0                     2                    0   \n",
       "1                     0                     0                    0   \n",
       "\n",
       "   n_upos_tag_PART_diff  n_upos_tag_PRON_diff  n_upos_tag_PROPN_diff  \\\n",
       "0                     1                     0                     -1   \n",
       "1                     0                     0                      0   \n",
       "\n",
       "   n_upos_tag_PUNCT_diff  n_upos_tag_SCONJ_diff  n_upos_tag_SYM_diff  \\\n",
       "0                      0                      0                    0   \n",
       "1                      0                      0                    0   \n",
       "\n",
       "   n_upos_tag_VERB_diff  n_upos_tag_X_diff  n_words_diff  n_stopwords_diff  \\\n",
       "0                     0                  0             0                 1   \n",
       "1                     0                  0             0                 0   \n",
       "\n",
       "   n_named_entities_diff  n_unique_words_diff  n_punctuation_diff  \\\n",
       "0                      0                    0                   0   \n",
       "1                      0                    0                   0   \n",
       "\n",
       "   n_digits_diff  n_letters_diff  MTLD_diff  HDD_diff  Maas_diff  \\\n",
       "0              2              10        0.0       0.0        0.0   \n",
       "1              0               0        0.0       0.0        0.0   \n",
       "\n",
       "   flesch_kincaid_ease_diff  SMOG_diff  gunning_fog_diff  \\\n",
       "0                    -10.49        0.0              2.13   \n",
       "1                     -8.46        0.0              0.00   \n",
       "\n",
       "   difficult_words_diff  dale_chall_diff  ARI_diff  coleman_liau_diff  \\\n",
       "0                     1             0.48       1.4               0.94   \n",
       "1                     0             0.00       0.0               0.00   \n",
       "\n",
       "   linsear_write_diff  readability_consensus_diff  avg_sentence_length_diff  \n",
       "0                 3.0                        -2.0                       2.0  \n",
       "1                 1.0                         5.0                       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1320, 149)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interesting_idx(df, n):\n",
    "    def get_idx_with_top_column_values(cname, n=5, ascending=False):\n",
    "        return df[['idx',cname]].\\\n",
    "            drop_duplicates().\\\n",
    "            sort_values(cname, ascending=ascending)\\\n",
    "            ['idx'][0:n].values.tolist()\n",
    "    \n",
    "    def sample_idx_with_label_flips(n=5): \n",
    "        return df[['idx','epoch_of_first_label_flip']].\\\n",
    "            query(\"epoch_of_first_label_flip!=0\").\\\n",
    "            drop_duplicates()\\\n",
    "            ['idx'].sample(n).values.tolist()\n",
    "    \n",
    "    idx_d = dict(\n",
    "        random           = df.idx.drop_duplicates().sample(n).tolist(),\n",
    "        label_flips = sample_idx_with_label_flips(n=n),\n",
    "        idx_n_unique_pp  = get_idx_with_top_column_values('idx_n_unique_pp',n=n,ascending=False),\n",
    "       # idx_n_pp_changes = get_idx_with_top_column_values('idx_n_pp_changes',n=n,ascending=False),\n",
    "        low_sts = get_idx_with_top_column_values('sts_score',n=n,ascending=True)\n",
    "      #  orig_long = None, \n",
    "      #  orig_short = None,\n",
    "    )\n",
    "    return idx_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "idx_d = get_interesting_idx(df, n)\n",
    "#idx_d\n",
    "\n",
    "def print_stats(key,i):\n",
    "    print(\"\\n###############\\n\")\n",
    "    print(key, i, \"\\n\")\n",
    "    idx = idx_d[key][i]\n",
    "    # Setup \n",
    "    df1 = df.query('idx==@idx')\n",
    "    orig = pd.unique(df1['orig_l'])[0]\n",
    "    print(\"Original:\", orig)\n",
    "    print(\"Original label\", pd.unique(df1['truelabel'])[0] )\n",
    "    pp_all = list(df1['pp_l'])\n",
    "    #print(\"All paraphrases\", pp_all)\n",
    "    pp_unique = list(pd.unique(df1['pp_l']))\n",
    "    n_pp_unique = len(pp_unique)\n",
    "\n",
    "    # showing a \"timeline\" of how the paraphrases change over the epochs\n",
    "    g_fields = [\"pp_l\",\"pp_truelabel_probs\",\"vm_score\",\"sts_score\",\"reward\",\"label_flip\"]\n",
    "    #g_fields = [\"pp_l\",\"vm_score\"]\n",
    "    g = df1.groupby(g_fields).agg({'epoch' : lambda x: list(x)})\n",
    "    g = g.sort_values(by='epoch', key = lambda col: col.map(lambda x: np.min(x)))\n",
    "    print(\"Unique paraphrases:\", n_pp_unique)\n",
    "    print(\"How the paraphrases change:\")\n",
    "    display_all(g)\n",
    "\n",
    "    # Showing a dataframe of the few best paraphrases\n",
    "    best_pps = df1.sort_values('pp_truelabel_probs').iloc[0]\n",
    "    print(\"Best Paraphrase\")\n",
    "    display_all(best_pps.to_frame().T)\n",
    "\n",
    "    #print(\"Everything\")\n",
    "    #display_all(df1)\n",
    "  \n",
    "for key in idx_d.keys():\n",
    "    for i in range(n): \n",
    "        print_stats(key,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text statistics (to send to w&b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
