{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, wandb, gc, numpy as np, pandas as pd,os\n",
    "from tqdm.auto import tqdm\n",
    "from travis_attack.utils import timecode, show_gpu\n",
    "from travis_attack.models import save_model\n",
    "from travis_attack.charts import plot_grad_flow, plot_wandb_charts\n",
    "from travis_attack.charts import plot_examples_charts \n",
    "from travis_attack.charts import plot_summary_charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b253756c445fb811\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be55855c6e9480eb32c39234349e650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c802946231f72062\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baa3faf37374389b0a1b1265c6555bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-43a49c5188c42e69\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908b00751cca4718a2b64dbba1963630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febda99604d24ed39041fedfe3e036b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815ba9e92cef46e0829ca92c2b5ec782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fea1c0c8314c29bd75a2ee03e05ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad0b07bfe404757b51a4a726ad4896f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b091fc2766654aa8b55039db81b9b2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcdf3e7e776482e894040f8925e9ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5049ad0f6a5748389021ecef1ecb5030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8267c5ab281a4db08a8bd8c072cf170f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058b879ca02240e3935eea78c276418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9120e230fa8c4dbc8fd0b672abcc77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc03591caba643529ea41152cae2fad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999a74ac9747478d893695495e77e799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32de81ae064472ea907d2e48bcf09f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38630e4d21b94b84882c3ab73139ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767a3287b9a64a398ec2f661147557b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6223f9f484404ac7a8af294cc12826e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0b0fd0b52d46fbbd0c91e54d8a39b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d5abdf71944ae8e054ec27b9a44bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from travis_attack.utils import set_seed, set_session_options, prepare_logger\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from travis_attack.trainer import get_optimizer\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "cfg = Config()\n",
    "cfg.device = accelerator.device\n",
    "cfg = cfg.adjust_config_for_simple_dataset()  # for testing, easier \n",
    "set_seed(cfg.seed)\n",
    "set_session_options()\n",
    "logger = prepare_logger()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)\n",
    "optimizer = get_optimizer(cfg, pp_model)\n",
    "ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model)\n",
    "vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'] = accelerator.prepare(vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'])\n",
    "cfg.n_train_steps = cfg.n_train_epochs * len(ds.dld_tkn['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_vm_probs(text, cfg, vm_tokenizer, vm_model, return_predclass=False): \n",
    "    \"\"\"Used in data cleaning and by the reward_fn to get vm_score\"\"\"\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tkns = vm_tokenizer(text, truncation=True, padding=True, pad_to_multiple_of=cfg.orig_padding_multiple,\n",
    "                            return_tensors=\"pt\").to(cfg.device)\n",
    "        logits = vm_model(**tkns).logits\n",
    "        probs = torch.softmax(logits,1)\n",
    "        if return_predclass:    return probs, torch.argmax(probs,1)\n",
    "        else:                   return probs\n",
    "\n",
    "def get_optimizer(cfg, pp_model):  return torch.optim.AdamW(pp_model.parameters(), lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Trainer: \n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, accelerator, ds,\n",
    "                logger): \n",
    "        store_attr()\n",
    "        self._cfg = self.cfg; del self.cfg;\n",
    "        self.accumulation_num,self.global_step = 0,0\n",
    "        self._setup_wandb_run()\n",
    "        self._setup_data_stores()\n",
    "        self._setup_wandb_examples_plots()\n",
    "    \n",
    "    def _setup_wandb_run(self): \n",
    "        \"\"\"Init wandb run, set up paths, create dir for model artifacts if needed, \"\"\"\n",
    "        self.run = wandb.init(project=self._cfg.wandb['project'], entity=self._cfg.wandb['project'], \n",
    "                              config=vars(self._cfg), mode=self._cfg.wandb['mode'],\n",
    "                              notes=self._cfg.wandb['run_notes'], save_code=True)\n",
    "        if self._cfg.wandb['log_grads']: \n",
    "            wandb.watch(self.pp_model, log='gradients', log_freq=self._cfg.wandb['log_grads_freq'])\n",
    "        self._cfg.run_name,self._cfg.run_id = self.run.name, self.run.id\n",
    "        self._cfg.path_run = f\"{self._cfg.path_checkpoints}{self.run.name}/\"\n",
    "        if not os.path.exists(self._cfg.path_run): os.makedirs(self._cfg.path_run, exist_ok=True)\n",
    "\n",
    "    \n",
    "    def _setup_data_stores(self): \n",
    "        \"\"\"Setup dict `self.data_d` to store observations. Setup column names for wandb tables.  \n",
    "         \"\"\"\n",
    "        # Raw observation data (lists of dicts, later becomes pandas df)\n",
    "        self.data_d = dict()\n",
    "        # These have to be in the keys of the output from eval_dl\n",
    "        self.table_columns = ['idx', 'orig_l',  'truelabel', 'orig_truelabel_probs', 'epoch', 'pp_l',\n",
    "                     'pp_truelabel_probs', \"pp_predclass\", \"pp_predclass_probs\"] + self._cfg.metrics\n",
    "        self.summary_table_columns = ['epoch','split'] + [f'{m}_avg' for m in self._cfg.metrics]\n",
    "        for key in self._cfg.splits + ['training_summary']:         self.data_d[key]             = [] \n",
    "        if self._cfg.wandb['log_training_step_table']:              self.data_d['training_step'] = []\n",
    "    \n",
    "    \n",
    "    def _setup_wandb_examples_plots(self): \n",
    "        \"\"\"If we plot a few examples this sets that up.\"\"\"\n",
    "        # Get indices for the examples plots\n",
    "        if self._cfg.wandb['plot_examples']:\n",
    "            def get_examples_plot_idxs(ds): \n",
    "                return np.random.choice(ds['idx'], size=self._cfg.wandb['n_examples_plot'], replace=False).tolist()\n",
    "            self.plt_idx_d = dict()\n",
    "            for split in self._cfg.splits:  self.plt_idx_d[split] = get_examples_plot_idxs(self.ds.dsd[split])\n",
    "    \n",
    "    \n",
    "    def training_function(self): \n",
    "        #self.logger.info(show_gpu(f' GPU memory usage after loading models:'))\n",
    "        progress_bar = tqdm(range(self._cfg.n_train_steps))\n",
    "        self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none) \n",
    "        for self.epoch in range(self.cfg_n_train_epochs): \n",
    "            self.logger.info(f\"Now on epoch {self.epoch} of {self._cfg.n_train_epochs}\")\n",
    "            if not self.pp_model.training: self.pp_model.train()\n",
    "            with timecode() as time_train_one_epoch:\n",
    "                for self.batch_num, (data, raw) in enumerate(zip(self.ds.dld_tkn['train'], self.ds.dld_raw['train'])): \n",
    "                    if self.batch_num % 10 == 0 :   self.logger.info(f\"Now processing batch {self.batch_num} out of {len(dld_tkn['train'])}\")\n",
    "                    self.training_step(data, raw) \n",
    "                    self.accumulation_num += 1  ; self.global_step += 1 ;  progress_bar.update(1) \n",
    "                    \n",
    "            wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                       'time/train_one_epoch_thoroughput': len(self.ds.dsd['train']) / time_train_one_epoch.t,\n",
    "                       'epoch': self.epoch}, commit=True)\n",
    "\n",
    "            if self._cfg.wandb_log_grads and self.epoch % self._cfg.wandb_log_grads_freq == 0: \n",
    "                plt = plot_grad_flow(self.pp_model.named_parameters())\n",
    "                wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "                del plt \n",
    "            #gc.collect() \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if self._cfg.save_model_while_training and (self.epoch + 1) % self._cfg.save_model_freq == 0:  save_model(epoch)\n",
    "\n",
    "            # Evaluation loop\n",
    "            if self.epoch % self._cfg.eval_freq == 0: \n",
    "                self.logger.info(f\"Now doing train eval\")\n",
    "                with timecode() as time_eval_train:\n",
    "                    train_set_preds = self.eval_dl(dl_tkn=self.ds.dld_tkn['train_eval'], \n",
    "                                                   dl_raw=self.ds.dld_raw['train_eval'])\n",
    "\n",
    "                self.logger.info(f\"Now doing valid eval\")\n",
    "                with timecode() as time_eval_valid:\n",
    "                    valid_set_preds = self.eval_dl(dl_tkn=self.ds.dld_tkn['valid'],\n",
    "                                                   dl_raw=self.ds.dld_raw['valid'])\n",
    "\n",
    "                # update the tables every epoch and log them\n",
    "                with timecode() as time_update_training_summary_table:\n",
    "                    self.update_training_summary_table(train_set_preds, split='train')\n",
    "                    self.update_training_summary_table(valid_set_preds, split='valid')\n",
    "                with timecode() as time_add_eval_preds_to_data_d:    \n",
    "                    self.add_preds_to_data_d(train_set_preds, split='train')\n",
    "                    self.add_preds_to_data_d(valid_set_preds, split='valid')\n",
    "                self.plot_wandb_charts()\n",
    "                del train_set_preds\n",
    "                del valid_set_preds\n",
    "                with timecode() as time_eval_gc_collect:\n",
    "                    gc.collect() \n",
    "                with timecode() as time_eval_empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                           'time/eval_train_thoroughput': len(self.ds.dsd['train']) / time_eval_train.t,\n",
    "                           'time/eval_valid_thoroughput': len(self.ds.dsd['valid']) / time_eval_valid.t,\n",
    "                           'time/eval_update_training_summary_table': time_update_training_summary_table.t, \n",
    "                           'time/eval_add_preds_to_data_d': time_add_eval_preds_to_data_d.t,\n",
    "                           'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                           'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "                   'epoch': self.epoch}, commit=True)\n",
    "\n",
    "        self.logger.info(f\"Now doing test eval\")        \n",
    "        # Eval on test set \n",
    "        test_set_preds = self.eval_dl(dl_tkn = self.ds.dld_tkn['test'], dl_raw=self.ds.dld_raw['test'])\n",
    "        self.add_preds_to_data_d(test_set_preds, split='test')\n",
    "\n",
    "        # Data -> df and save dfs to file \n",
    "        for key in data_d.keys():  # splits and sometimes 'training_step' too \n",
    "            self.data_d[key] = pd.DataFrame(self.data_d[key]) # dict of list of dict -> dict of dataframe\n",
    "            self.data_d[key].to_csv(f\"{path_run}{key}.csv\", index=False)\n",
    "        # Save training_summary table to csv too \n",
    "        pd.DataFrame(self.data_d['training_summary']).to_csv(f\"{path_run}training_summary.csv\", index=False)\n",
    "\n",
    "        # plot_wandb_charts()  # don't think I need this\n",
    "        add_wandb_run_summary_statistics(run)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, accelerator, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "trainer.test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def training_step(self, data, raw, accelerator): \n",
    "        \"\"\"With gradient accumulation\"\"\"\n",
    "        with timecode() as time_generate_pp:\n",
    "            pp_output, pp_l = self.self.pp_model_forward(data)\n",
    "\n",
    "        #logger.info(show_gpu(f'Batch {self.batch_num}, GPU memory usage after forward pass: '))\n",
    "\n",
    "        with self.accelerator.autocast():\n",
    "            with timecode() as time_loss_fn:\n",
    "                if self._cfg.wandb_log_training_step_table: \n",
    "                    results_d = self.loss_fn(data, raw, pp_output, pp_l, return_components=True)\n",
    "                    loss_batch = results_d['loss_batch']\n",
    "                else: \n",
    "                    loss_batch = self.loss_fn(data, raw, pp_output, pp_l, return_components=False)\n",
    "\n",
    "            loss_batch = loss_batch / self._cfg.accumulation_steps  # Normalize our loss for gradient accumulation\n",
    "\n",
    "        with timecode() as time_backwards:\n",
    "            self.accelerator.backward(loss_batch) \n",
    "\n",
    "        #logger.info(show_gpu(f'Batch {self.batch_num}, GPU memory usage after backwards pass: '))\n",
    "        if (self.accumulation_num + 1) % self._cfg.accumulation_steps == 0: \n",
    "            with timecode() as time_opt_step:\n",
    "                optimizer.step()\n",
    "\n",
    "            self.pp_model.zero_grad(set_to_none=zero_grad_with_none)\n",
    "        if wandb_log_training_step_table: \n",
    "            with timecode() as time_add_to_training_step_table:\n",
    "                results_d = process_results_d1(results_d, raw)\n",
    "                add_preds_to_data_d(results_d, split='training_step') \n",
    "\n",
    "    #     print(\"### INSIDE training_step ####\")    \n",
    "    #     print(\"self.epoch\", self.epoch)\n",
    "    #     print(\"self.batch_num\", self.batch_num)\n",
    "    #     print(\"self.global_step\", self.global_step)\n",
    "    #     print(\"model in training mode\", self.pp_model.training)\n",
    "    #     print(\"orig\", raw['text'])\n",
    "    #     print(\"pp_l\", pp_l)\n",
    "    #     print(\"pp_seq\", pp_output.sequences)\n",
    "    #     print(\"pp_length\", pp_output.sequences.shape, pp_length)\n",
    "    #     print(\"loss_fn_time\", time_loss_fn.t)\n",
    "    #     print(\"### INSIDE training_step ####\")\n",
    "\n",
    "        wandb.log({'time/generate_pp': time_generate_pp.t, 'time/loss_fn': time_loss_fn.t, \n",
    "                   'time/backwards_pass': time_backwards.t, 'time/optimizer_step': time_opt_step.t, \n",
    "                   'time/add_to_training_step_table': time_add_to_training_step_table.t, \n",
    "                   'self.epoch': self.epoch, 'self.global_step': self.global_step,'self.batch_num': self.batch_num,\n",
    "                   'orig_length': orig_length,'orig_batch_size': orig_batch_size,\n",
    "                  'pp_length': pp_length, 'pp_batch_size': pp_batch_size}\n",
    "                  ,commit=False)\n",
    "        \n",
    "        \n",
    "        def get_paraphrases(input_ids,attention_mask):\n",
    "        \"\"\"Wrapper for generating paraphrases (pp's). Most keywords are passed on to self.pp_model.generate function, \n",
    "        so see docs for that function. \"\"\"\n",
    "        # Only greedy search supported at the moment\n",
    "        pp_output = self.pp_model.generate_with_grad(input_ids=input_ids, \n",
    "                                                attention_mask=attention_mask, \n",
    "                                                 **self.pp_model_params,\n",
    "                                                 do_sample=False, \n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 output_scores=True,\n",
    "                                                    remove_invalid_values=False, \n",
    "                                                 pad_token_id = self.pp_tokenizer.pad_token_id,\n",
    "                                                 eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "        pp_l = self.pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "        if track_pp_sizes:  # DEV CODE (can delete later)\n",
    "            orig_max_l.append(batch['input_ids'].shape[1])\n",
    "            pp_max_l.append(pp_output.sequences.shape[1])\n",
    "        return pp_output, pp_l\n",
    "\n",
    "    def get_pp_logp(pp_output, log_times=True): \n",
    "        \"\"\"log(p(pp|orig)) basically.\n",
    "        works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "        ### TODO: this looks like logp to me, not plogp. Find out if this is right and if so rename, if not, fix\n",
    "        ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "        # and has no probability attached to it, so we remove it. \n",
    "        seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "        assert seq_without_first_tkn.shape == torch.Size([orig_batch_size, pp_length - 1])\n",
    "\n",
    "        ### Convert from tuple of scores to one big tensor of scores \n",
    "        scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "        ### TESTS \n",
    "        # We check shape and that there is no +inf or nan in scores. \n",
    "        # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "        assert scores_stacked.shape == torch.Size([orig_batch_size, (pp_length - 1), vocab_size])\n",
    "        assert torch.all(~torch.isnan(scores_stacked))\n",
    "        assert torch.all(~torch.isposinf(scores_stacked))\n",
    "        # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "        # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "        # shouldn't be set to -inf\n",
    "        # Not a 100% test but very likely to identify\n",
    "        idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "        assert len(idx_neginf[idx_neginf[:,2] == self.pp_tokenizer.eos_token_id, :]) == \\\n",
    "                  (self.pp_model_params[\"min_length\"] -1) * orig_batch_size  \n",
    "        del idx_neginf\n",
    "\n",
    "        ### Take log softmax of scores and then extract those that correspond \n",
    "        # to the generated sequences    \n",
    "        scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "        seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "        ### TESTS \n",
    "        # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "        assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "        assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "        check_scores_log_softmax_sums(scores_log_softmax)\n",
    "        # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "        assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "        # Check that the last token probability corresponds to a possible end token\n",
    "        # this has to be tested before the attention mask is multiplied with it because if the \n",
    "        # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "        output_end_ids = get_start_end_special_token_ids(self.pp_tokenizer)['output_end_id']\n",
    "        assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "        del output_end_ids\n",
    "        ## THIS ONE IS LONG - a test rather than assert \n",
    "        # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "        #                                             seq_token_log_probs) \n",
    "\n",
    "        ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "        # sequence probabilities so that we don't consider probability of padding tokens \n",
    "        # when getting sequence probabilities. \n",
    "        # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "        # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "        # since  -inf * 0 = nan \n",
    "        attention_mask = self.pp_model._prepare_attention_mask_for_generation(\n",
    "            seq_without_first_tkn, self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id\n",
    "        )\n",
    "        seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-10000)\n",
    "        seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "        ### TESTS\n",
    "        assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "        # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "        assert all(seq_without_first_tkn[attention_mask == 0] == self.pp_tokenizer.pad_token_id)\n",
    "        check_no_nans_or_infs(seq_token_log_probs)\n",
    "        # check that we aren't picking extrememly rare tokens\n",
    "        assert torch.all(seq_token_log_probs  > -10)  \n",
    "\n",
    "        ### Get sequence probabilities by summing up token log probabilities \n",
    "        seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "        ## TESTS \n",
    "        assert seq_log_prob.shape == torch.Size([pp_batch_size])\n",
    "        check_no_nans_or_infs(seq_log_prob)\n",
    "\n",
    "\n",
    "        if wandb_log_token_entropy:\n",
    "            with timecode() as time_log_entropy:\n",
    "                ent_d = self._get_entropy_metrics(scores_stacked, attention_mask)\n",
    "            ent_d['time/log_entropy'] = time_log_entropy.t\n",
    "            if log_times:   # need a better way to handle this. \n",
    "                wandb.log(ent_d, commit=False)\n",
    "\n",
    "\n",
    "        if wandb_log_token_probabilities: \n",
    "            with timecode() as time_log_token_probabilities:\n",
    "                token_prob_d = self._get_token_probability_metrics(scores_log_softmax, attention_mask, k=3)\n",
    "            token_prob_d['time/log_token_probabilities'] = time_log_token_probabilities.t\n",
    "            if log_times: \n",
    "                wandb.log(token_prob_d, commit=False)\n",
    "\n",
    "        return seq_log_prob\n",
    "\n",
    "    def reward_fn(data, raw, pp_l, return_components=False, log_times=True): \n",
    "        \"\"\"orig_l, pp_l are lists of original and paraphrase respectively\"\"\"\n",
    "        # Victim model probability differences between orig and pp\n",
    "        with timecode() as time_vm_scores:\n",
    "            pp_probs = get_vm_probs(pp_l) \n",
    "            pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "            pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "            pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "            label_flip = (pp_predclass != data['label']) * 1\n",
    "            vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "\n",
    "\n",
    "        # STS scores\n",
    "        with timecode() as time_sts_scores:\n",
    "            pp_embeddings   = self.sts_model.encode(pp_l,        batch_size=len(raw), convert_to_tensor=True, device=self._cfg.device)\n",
    "            # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "            sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "\n",
    "        # Reward calculation \n",
    "        rewards = torch.tensor([-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)],device=self._cfg.device)\n",
    "\n",
    "        if log_times:\n",
    "            wandb.log({'self.epoch': self.epoch, 'self.global_step': self.global_step, \n",
    "                       'time/vm_scores': time_vm_scores.t, 'time/sts_scores': time_sts_scores.t }, \n",
    "                       commit=False)\n",
    "\n",
    "        if return_components: \n",
    "            return {\n",
    "                \"orig_l\": raw['text'],\n",
    "                \"pp_l\": pp_l,  \n",
    "                \"truelabel\": data['label'],\n",
    "                \"orig_truelabel_probs\": data['orig_truelabel_probs'],\n",
    "                \"pp_truelabel_probs\":  pp_truelabel_probs,\n",
    "                \"pp_predclass\": pp_predclass,\n",
    "                \"pp_predclass_probs\": pp_predclass_probs,\n",
    "                \"vm_score\": vm_scores, \n",
    "                \"sts_score\": sts_scores,\n",
    "                \"reward\": rewards,\n",
    "                \"label_flip\": label_flip\n",
    "            }\n",
    "        else:  return {\"reward\": rewards}\n",
    "\n",
    "    def self.pp_model_forward(data): \n",
    "        global orig_batch_size,orig_length,pp_batch_size,pp_length\n",
    "        orig_batch_size     = data['input_ids'].shape[0]\n",
    "        orig_length         = data['input_ids'].shape[1]\n",
    "        pp_output, pp_l = get_paraphrases(data['input_ids'], data['attention_mask']) \n",
    "        pp_batch_size = pp_output.sequences.shape[0]\n",
    "        # for greedy search pp_length is equal to orig_batch_size but this won't be for beam search\n",
    "        pp_length     = pp_output.sequences.shape[1]  \n",
    "        return pp_output, pp_l\n",
    "\n",
    "    def loss_fn(data, raw, pp_output, pp_l, return_components=False, log_times=True): \n",
    "        with timecode() as time_reward_fn:\n",
    "            d = reward_fn(data, raw, pp_l, return_components=return_components, log_times=log_times)\n",
    "\n",
    "        if normalise_rewards: \n",
    "            d['orig_reward'] = copy.deepcopy(d['reward'])\n",
    "            d['reward'] = (d['reward']-torch.mean(d['reward']))/torch.std(d['reward'])\n",
    "\n",
    "        with timecode() as time_pp_logp:\n",
    "            d['pp_logp'] = get_pp_logp(pp_output,log_times=log_times)\n",
    "\n",
    "        with timecode() as time_loss_fn_loss_calc:\n",
    "            d['loss'] = -d['reward'] * d['pp_logp']\n",
    "            d['loss_batch'] = torch.mean(d['loss'])\n",
    "            if return_components ==  False: return d['loss_batch'] \n",
    "\n",
    "        # remove some items from compgraph\n",
    "        with timecode() as time_loss_fn_detach:\n",
    "            d['pp_logp'] = d['pp_logp'].detach()  \n",
    "            d['loss']    = d['loss'].detach()\n",
    "\n",
    "    #     # This was taking a lot of time so removed it. Add it in if needed. \n",
    "    #     #gc.collect() \n",
    "    #     print(\"\\t### INSIDE loss_fn### \")\n",
    "    #     print(\"\\tself.global_step\", self.global_step)\n",
    "    #     print(\"\\treward_fn_time\", time_reward_fn.t)\n",
    "    #     print(\"\\t######### \")\n",
    "\n",
    "        if log_times:   # true for training, not eval. \n",
    "            wandb.log({'self.epoch': self.epoch, 'self.global_step': self.global_step, \n",
    "                       'time/reward_fn': time_reward_fn.t, 'time/pp_logp': time_pp_logp.t, \n",
    "                      'time/loss_fn_loss_calc': time_loss_fn_loss_calc.t, \n",
    "                       'time/pp_logp_detach': time_loss_fn_detach.t, \n",
    "                      }, \n",
    "                     commit=False)\n",
    "        return d\n",
    "\n",
    "    def process_results_d1(results_d, raw): \n",
    "        \"\"\"REFACTOR THIS LATER\"\"\"\n",
    "        # wandb logging \n",
    "        results_d['self.epoch'] = self.epoch\n",
    "        results_d['idx'] = raw['idx']\n",
    "        for k,v in results_d.items(): \n",
    "            if torch.is_tensor(v): \n",
    "                results_d[k] = v.detach().cpu().tolist()\n",
    "            elif type(v) == int or type(v) == float: \n",
    "                # make into list repeated n times\n",
    "                results_d[k] = [v for o in range(batch_size_train)]\n",
    "        return results_d    \n",
    "        \n",
    "    def _get_entropy_metrics(self, scores_stacked, attention_mask): \n",
    "        ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "        assert ent.shape == attention_mask.shape == torch.Size([pp_batch_size, pp_length - 1])\n",
    "\n",
    "        ent = ent * attention_mask  # stop values after eos token from contributing to ent score \n",
    "        # first remove structure (otherwise we have ragged arrays)\n",
    "        # then remove corresponding attention mask values\n",
    "        # we can't just filter by ent[ent != 0] because we might have zero tokens during the sequence\n",
    "        att_flat= attention_mask.flatten()\n",
    "        indices = torch.nonzero(att_flat)\n",
    "        ent_flat = ent.flatten()[indices].flatten()\n",
    "        assert ent_flat.shape[0] == (torch.sum(att_flat)*1).item()\n",
    "        # check everything we filter out is zero \n",
    "        torch.isclose(ent.flatten()[torch.nonzero(~(att_flat > 0))].sum(), torch.tensor(0.), 1e-3)\n",
    "        ent_d = dict(\n",
    "            ent_min             = ent_flat.quantile(0).item(),\n",
    "            ent_lower_quartile  = ent_flat.quantile(0.25).item(), \n",
    "            ent_median          = ent_flat.median().item(), \n",
    "            ent_mean            = ent_flat.mean().item(), \n",
    "            ent_upper_quartile  = ent_flat.quantile(0.75).item(), \n",
    "            ent_max             = ent_flat.quantile(1).item(), \n",
    "            self.epoch=self.epoch, self.global_step=self.global_step\n",
    "        )\n",
    "        return ent_d\n",
    "\n",
    "    def _get_token_probability_metrics(self, scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, tkn_kmaxidx = torch.topk(scores_log_softmax,largest=True,  k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  # log these \n",
    "        assert tkn_kmaxprob.shape == torch.Size([pp_batch_size, pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [0.1, 0.01, 0.001, 0.0001, 0.00001]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/vocab_size)) / allprobs.shape[0]).item()\n",
    "\n",
    "        token_prob_d['self.epoch'] = self.epoch\n",
    "        token_prob_d['self.global_step'] = self.global_step\n",
    "        return token_prob_d\n",
    "    \n",
    "    \n",
    "    \n",
    "    def table2df(table):  return pd.DataFrame(data=table.data, columns=table.columns)  # wandb table to dataframe\n",
    "\n",
    "    def process_results_d_for_wandb(results_d): \n",
    "        # Flatten batches for each key, depending on datatype (e.g. lists of lists )\n",
    "        for k,v in results_d.items(): \n",
    "            # v[0] is arbitrary - we are just checking the first item in the list to see the type\n",
    "            if type(v) == float or type(v) == int: \n",
    "                next\n",
    "            elif  torch.is_tensor(v[0]): \n",
    "                # case where we have a list of scalars - the cat function doesn't work here \n",
    "                if  v[0].size() == torch.Size([]): x = torch.stack(v)\n",
    "                else:                              x = torch.cat(v)\n",
    "                results_d[k] = x.detach().cpu().squeeze().tolist()  # convert to list (squeeze is for single scalar list)\n",
    "            elif type(v[0]) == list:  # this is True for tensors also, so it has to go after the is_tensor check\n",
    "                results_d[k] = list(itertools.chain(*v)) \n",
    "            elif type(v) == list: \n",
    "                next\n",
    "            else: \n",
    "                raise Exception(\"shouldn't get here\")\n",
    "        return results_d\n",
    "\n",
    "    def eval_dl(dl_tkn, dl_raw): \n",
    "        \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "        # Put models in eval mode and do the forward pass \n",
    "        # Current logic: push all batches together into one big list.     \n",
    "        if pp_model.training: pp_model.eval()\n",
    "        if vm_model.training: vm_model.eval()\n",
    "        results_d = defaultdict(list)\n",
    "        with torch.no_grad(): \n",
    "            for eval_batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "              #  logger.info(show_gpu(f'EVAL, batch {i}, GPU memory usage after loading data: '))\n",
    "                for k, v in data.items(): \n",
    "                    if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "               # if data['input_ids'].device != self._cfg.device: data['input_ids'].to(self._cfg.device)\n",
    "\n",
    "                pp_output, pp_l = pp_model_forward(data)\n",
    "                d = loss_fn(data, raw, pp_output, pp_l, return_components=True, log_times=False)\n",
    "                #logger.info(show_gpu(f'EVAL, batch {eval_batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "                d['idx'] = raw['idx']\n",
    "\n",
    "                for k,v in d.items(): \n",
    "                    results_d[k].append(v) \n",
    "        del eval_batch_num, data, raw, pp_output, pp_l, d\n",
    "        results_d = process_results_d_for_wandb(results_d)\n",
    "\n",
    "        # Calculate additional metrics \n",
    "        results_d['epoch'] = epoch\n",
    "        return results_d\n",
    "\n",
    "    def add_preds_to_data_d(results_d, split):\n",
    "        if split not in data_d.keys() or split == \"training_summary\": # training summary table logic is elsewhere\n",
    "            raise Exception(\"split not in table keys or split == training_summary \") \n",
    "        #table = table_d[split]\n",
    "\n",
    "        # Need epoch to be repeated to the same length as the rest of the fields \n",
    "        # (this isn't the batch size because we concat a bunch of stuff)\n",
    "        # we don't want to change the `epoch` key because it screws up logging of the other metrics. \n",
    "        # So we make a new dict.\n",
    "        # d1 = copy.deepcopy(results_d)\n",
    "        d1 = results_d\n",
    "        d1['epoch'] = [epoch for o in range(len(d1['pp_l']))]\n",
    "        dcols = [d1[c] for c in table_columns]  # filter out loss_batch\n",
    "        assert len(set([len(o) for o in dcols])) == 1  # all lists should be of the same length \n",
    "\n",
    "        for row in zip(*dcols):\n",
    "            d2 = {k:v for k,v in zip(table_columns,row)}\n",
    "            data_d[split].append(d2)\n",
    "\n",
    "    def update_training_summary_table(results_d, split):\n",
    "        d = dict()\n",
    "        # key names here have to match those in summary_table_columns\n",
    "        d['epoch'] = epoch\n",
    "        d['split'] = split\n",
    "        for metric in metrics:\n",
    "            d[f'{metric}_avg'] = np.mean(results_d[metric])\n",
    "        #data_d['training_summary'].append(*[d[c] for c in summary_table_columns])\n",
    "        data_d['training_summary'].append(d)\n",
    "\n",
    "#     def log_wandb_tables(run): \n",
    "#         \"\"\"Log wandb tables to the UI\"\"\"\n",
    "#         d = dict()\n",
    "#         d[\"eval/training_summary_table\"] = table_d['training_summary']\n",
    "#       #  print(len(d[\"eval/training_summary_table\"].data))\n",
    "#         run.log(d)\n",
    "\n",
    "\n",
    "    def plot_wandb_charts(self): \n",
    "        if self._cfg.wandb_plot_examples: \n",
    "            # Examples charts \n",
    "            for split in ['train', 'valid']:\n",
    "                df = pd.DataFrame(data_d[split]) if type(data_d[split]) is list else data_d[split]\n",
    "                df = df.query(\"idx in @plt_idx_d[@split]\").sort_values(['idx', 'epoch'])\n",
    "                for metric in metrics: \n",
    "                    chart = plot_examples_chart(split, table=wandb.Table(dataframe=df), metric=metric)\n",
    "                    wandb.log({f\"individual_examples/{split}_{metric}_vs_epoch_examples\": chart}, commit=False)\n",
    "                    \n",
    "                    \n",
    "        ## Summary charts \n",
    "        for metric in metrics: \n",
    "            df = pd.DataFrame(data_d['training_summary'])\n",
    "            chart = plot_summary_charts(metric, table=wandb.Table(dataframe=df))\n",
    "            wandb.log({f\"summary_charts/avg_{metric}_vs_epoch\": chart}, \n",
    "                      commit=True if metric == metrics[len(metrics)-1] else False)\n",
    "\n",
    "            \n",
    "    def add_wandb_run_summary_statistics(run):\n",
    "        ## Training summary statistics \n",
    "        df_summary = pd.DataFrame(data_d['training_summary']) \n",
    "        # We calculate the best epoch according to the validation set\n",
    "        best_epoch_idx = df_summary.query(\"split=='valid'\")['loss_avg'].idxmin() \n",
    "        valid_row = df_summary.iloc[best_epoch_idx]\n",
    "        best_epoch = valid_row['epoch'].item()\n",
    "        run.summary['best_epoch'] = best_epoch\n",
    "        # iloc transforms 1row df to series (so it is same as  valid_row)\n",
    "        train_row = df_summary.query(\"split=='train' & epoch==@best_epoch\").iloc[0]  \n",
    "        for metric in metrics: \n",
    "            run.summary[f\"{metric}_avg_train\"] = train_row[f\"{metric}_avg\"].item()\n",
    "            run.summary[f\"{metric}_avg_valid\"] = valid_row[f\"{metric}_avg\"].item()\n",
    "\n",
    "        ## Summary statistics of the test set \n",
    "        # From the last epoch atm because we don't have early stopping \n",
    "        test_metrics = data_d['test'].filter(metrics, axis=1).mean()\n",
    "        for metric, val in zip(test_metrics.index, test_metrics): \n",
    "            run.summary[f\"{metric}_avg_test\"] = val\n",
    "    \n",
    "    \n",
    "    def get_start_end_special_token_ids(tokenizer): \n",
    "    \"\"\"The token id's that input/output sequences should start and end with\"\"\"\n",
    "    d = {}\n",
    "    if tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "        d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    elif tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "        d[\"input_start_id\"] =  None\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "        d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    else: \n",
    "        raise Exception(\"unrecognised tokenizer\")\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "    def check_no_nans_or_infs(x):\n",
    "        assert torch.all(~torch.isnan(x))\n",
    "        assert torch.all(~torch.isneginf(x))\n",
    "        assert torch.all(~torch.isposinf(x))\n",
    "\n",
    "    def assert_start_and_end_tokens_are_correct(tokenizer, orig_token_ids, pp_token_ids):\n",
    "        \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "        right special tokens (depends on tokenizer)\"\"\"\n",
    "        start_end_token_d = get_start_end_special_token_ids(pp_tokenizer)\n",
    "\n",
    "        # Input\n",
    "        if start_end_token_d['input_start_id'] is not None: \n",
    "            assert torch.all(orig_token_ids[:,0] == start_end_token_d['input_start_id'])\n",
    "        # can probs rewrite this to make it nicer but it's fine for now\n",
    "        assert torch.all(torch.logical_or(orig_token_ids[:,-1] == start_end_token_d['input_end_id'][0], \n",
    "                                          orig_token_ids[:,-1] == start_end_token_d['input_end_id'][1]))\n",
    "\n",
    "        # Output\n",
    "        assert torch.all(pp_token_ids[:,0] == start_end_token_d['output_start_id'])\n",
    "        assert torch.all(torch.logical_or(pp_token_ids[:,-1] == start_end_token_d['output_end_id'][0], \n",
    "                                          pp_token_ids[:,-1] == start_end_token_d['output_end_id'][1]))\n",
    "\n",
    "    def check_scores_log_softmax_sums(scores_log_softmax):\n",
    "        sums = scores_log_softmax.exp().sum(2)\n",
    "        # check that the axes is right\n",
    "        # we want to sum over token probabilities at each generation step, so we \n",
    "        # should end up with a shape [orig_batch_size, pp_length]\n",
    "        assert sums.shape[0] == orig_batch_size  \n",
    "        assert sums.shape[1] == pp_length - 1\n",
    "        # check that they sum to 1 along the pp_length axis\n",
    "        assert torch.allclose(sums, torch.ones(sums.size(), device=self._cfg.device), atol = 1e-4)\n",
    "\n",
    "    def check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "        \"\"\"Just enumerates and checks values\n",
    "        Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i_ex in range(orig_batch_size):\n",
    "            for i_step in range(pp_length - 1):\n",
    "                i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "                l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "        assert all(l)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
