{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, wandb, gc, numpy as np, pandas as pd,os\n",
    "from wandb.data_types import Histogram\n",
    "from tqdm.auto import tqdm\n",
    "from travis_attack.utils import (timecode, show_gpu, merge_dicts, unpack_nested_lists_in_df, \n",
    "                                 display_all, append_df_to_csv, start_wandb_run)\n",
    "from travis_attack.tests import check_no_nans_or_infs\n",
    "from travis_attack.models import save_pp_model, resume_pp_model, get_vm_probs, get_start_end_special_token_ids, get_nli_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, numpy as np, pandas as pd, gc,sys, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict, Dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from torch.distributions import Categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "import logging \n",
    "logger = logging.getLogger(\"travis_attack.trainer\")\n",
    "\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from travis_attack.utils import set_seed, set_session_options, setup_logging\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models, get_optimizer\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from snoop import pp\n",
    "import snoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Trainer: \n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, ref_pp_model,\n",
    "                 sts_model, nli_tokenizer, nli_model, optimizer, \n",
    "                 ds, initial_eval=True, log_code=True, use_cpu=False): \n",
    "        store_attr()\n",
    "        self._cfg = self.cfg; del self.cfg;\n",
    "        self.epoch,self.acc_num,self.global_step,self.eval_num,self.param_norm = 0,0,0,0,0\n",
    "        self._reset_batch_dicts()\n",
    "        #resume_pp_model(f\"{path_checkpoints}devout-durian-172_39\")\n",
    "        self._setup_data_stores()\n",
    "        self._setup_gradient_accumulation_variables()\n",
    "        self.start_end_token_d = get_start_end_special_token_ids(self.pp_tokenizer)\n",
    "        \n",
    "    def train(self): \n",
    "        self._setup_wandb_run()\n",
    "        ## we set num_processes=1 because we are running on 1 GPU only and we must specify the argument\n",
    "        # we can't use FP16 with PEGASUS model, so full precision is needed. \n",
    "        #%lprun -f _training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(_training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1)\n",
    "        notebook_launcher(self._training_function, args=(), num_processes=1, use_fp16=False)  \n",
    "    \n",
    "    def _reset_batch_dicts(self): \n",
    "        # train_batch_d holds all info to write to csv, time_d has times, wandb_d has everything to log to wandb\n",
    "        # there will be overlap between them. \n",
    "        self.batch_d,self.batch_time_d,self.batch_wandb_d = dict(),dict(),dict()\n",
    "    \n",
    "    def _setup_wandb_run(self, log_code=True): \n",
    "        \"\"\"Init wandb run, set up paths, create dir for model artifacts if needed, \"\"\"\n",
    "        self.run, self._cfg = start_wandb_run(self._cfg, log_code=self.log_code)\n",
    "        if self._cfg.wandb['log_grads']: wandb.watch(self.pp_model, log='gradients', log_freq=self._cfg.wandb['log_grads_freq'])      \n",
    "        \n",
    "    def _setup_data_stores(self): \n",
    "        \"\"\"Setup dict `self.data_d` to store observations. Setup column names for wandb tables. \"\"\"\n",
    "        # Raw observation data (lists of dicts, later becomes pandas df)\n",
    "        self.data_d = dict() \n",
    "        for split in self._cfg.splits + ['training_step']:   self.data_d[split] = [] \n",
    "        # Data containers and data loaders\n",
    "        self.eval_epoch_df_d = dict(train=[], valid=[], test=[]) # each eval epoch dataframe appended to here \n",
    "\n",
    "            \n",
    "    def _setup_gradient_accumulation_variables(self):\n",
    "        \"\"\"acc_global_l is a list of all batch sizes encountered during training. \n",
    "            \"\"\"\n",
    "        self.acc_global_l = self._cfg.dl_batch_sizes['train'] * self._cfg.n_train_epochs\n",
    "        assert len(self.acc_global_l) ==  self._cfg.n_train_steps\n",
    "        # Check if there will be leftover batches\n",
    "        self._cfg.acc_leftover_batches =  self._cfg.n_train_steps % self._cfg.acc_steps\n",
    "        if self._cfg.acc_leftover_batches != 0: \n",
    "            msg = f\"Config set to do gradient accumulation every {self._cfg.acc_steps} batches, and there are \\\n",
    "            {self._cfg.n_train_steps} total training steps, so there will be {self._cfg.acc_leftover_batches} batches at \\\n",
    "            the end that will not be trained on.\"\n",
    "            warnings.warn(msg)\n",
    "        self._reset_acc_lists()\n",
    "\n",
    "    def _reset_acc_lists(self):\n",
    "        \"\"\"call this at start and every time you call opt step\"\"\"\n",
    "        # acc_current_l is a list of the batch sizes in the current accumulation batch.\n",
    "        last_step = (self._cfg.n_train_steps - 1) - self._cfg.acc_leftover_batches\n",
    "        if self.global_step == 0:   # at start of training\n",
    "            self.acc_current_l = self.acc_global_l[self.global_step:self._cfg.acc_steps]\n",
    "            assert len(self.acc_current_l) == self._cfg.acc_steps\n",
    "        else: \n",
    "            self.acc_current_l = self.acc_global_l[(self.global_step+1):(self.global_step+self._cfg.acc_steps+1)]\n",
    "            if self.global_step == last_step:  assert len(self.acc_current_l) == self._cfg.acc_leftover_batches\n",
    "            else:                              assert len(self.acc_current_l) == self._cfg.acc_steps\n",
    "        self.acc_current_n_examples = sum(self.acc_current_l)       \n",
    "        \n",
    "    def _eval_save_log_test_set(self): \n",
    "        \"\"\"Eval on test set, convert to df, save to file, and log to wandb summary\"\"\"\n",
    "        self._eval_dl(split='test')\n",
    "        self.data_d[\"test\"] = self._convert_data_d_to_df(\"test\")\n",
    "        self._set_df_colorder(\"test\")\n",
    "        self.data_d[\"test\"].to_csv(f\"{self._cfg.path_run}test.csv\", index=False)\n",
    "        self._add_wandb_run_summary_statistics()\n",
    "\n",
    "    def _training_function(self): \n",
    "        self.accelerator = Accelerator(cpu=self.use_cpu)\n",
    "        self._cfg.device = self.accelerator.device\n",
    "        self.vm_model,self.pp_model,self.ref_pp_model,self.sts_model,self.optimizer,self.ds.dld_tkn['train'] = self.accelerator.prepare(\n",
    "            self.vm_model,self.pp_model,self.ref_pp_model,self.sts_model,self.optimizer,self.ds.dld_tkn['train'])\n",
    "        \n",
    "        logger.debug(show_gpu(f'GPU memory usage after loading models:'))\n",
    "        progress_bar = tqdm(range(self._cfg.n_train_steps))\n",
    "        self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none) \n",
    "        \n",
    "        # initial eval (at epoch 0)\n",
    "        if self.initial_eval:\n",
    "            logger.info(\"Launching initial eval run: train\")\n",
    "            self._eval_dl(split='train')\n",
    "            logger.info(\"Launching initial eval run: valid\")\n",
    "            self._eval_dl(split='valid')\n",
    "            self._compute_and_log_eval_metrics()\n",
    "        \n",
    "        for self.epoch in range(1, self._cfg.n_train_epochs+1): \n",
    "            logger.info(f\"Now on epoch {self.epoch} of {self._cfg.n_train_epochs}\")\n",
    "            if not self.pp_model.training: self.pp_model.train()\n",
    "            with timecode() as time_train_one_epoch:\n",
    "                for self.batch_num, (data, raw) in enumerate(zip(self.ds.dld_tkn['train'], self.ds.dld_raw['train'])): \n",
    "                    self._reset_batch_dicts()\n",
    "                    self._training_step(data, raw) \n",
    "                    if self._batch_for_opt_step(): self._reset_acc_lists()\n",
    "                    self.acc_num = (self.acc_num + 1) % self._cfg.acc_steps\n",
    "                    self.global_step += 1\n",
    "                    progress_bar.update(1) \n",
    "                    \n",
    "                    \n",
    "            wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                       'time/train_one_epoch_thoroughput': len(self.ds.dsd_tkn['train']) / time_train_one_epoch.t,\n",
    "                       'epoch': self.epoch}, commit=True)\n",
    "\n",
    "            if self._cfg.wandb['log_grads'] and self.epoch % self._cfg.wandb_log_grads_freq == 0: \n",
    "                plt = self._plot_grad_flow(self.pp_model.named_parameters())\n",
    "                wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "                del plt \n",
    "            #gc.collect() \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if self._cfg.save_model_while_training and (self.epoch + 1) % self._cfg.save_model_freq == 0:  save_model(epoch)\n",
    "\n",
    "            # Evaluation loop\n",
    "            if self.epoch % self._cfg.eval_freq == 0: \n",
    "                self.eval_num += 1\n",
    "                with timecode() as time_eval_train:\n",
    "                    self._eval_dl(split='train') \n",
    "                with timecode() as time_eval_valid:\n",
    "                    self._eval_dl(split='valid')\n",
    "                with timecode() as time_eval_compute_metrics: \n",
    "                    self._compute_and_log_eval_metrics()\n",
    "                with timecode() as time_eval_gc_collect:\n",
    "                    gc.collect() \n",
    "                with timecode() as time_eval_empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                           'time/eval_train_thoroughput': len(self.ds.dsd_tkn['train']) / time_eval_train.t,\n",
    "                           'time/eval_valid_thoroughput': len(self.ds.dsd_tkn['valid']) / time_eval_valid.t, \n",
    "                           'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                           'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "                           'time/eval_compute_metrics': time_eval_compute_metrics.t,\n",
    "                           'epoch': self.epoch}, commit=True)\n",
    "        \n",
    "        self._eval_save_log_test_set()\n",
    "    \n",
    "    def _training_step(self, data, raw): \n",
    "        \"\"\"Forward pass, loss function, backwards pass, parameter update (with gradient accumulation optional), \n",
    "        recording results, wandb logging. \n",
    "        \"\"\"\n",
    "        if not self.pp_model.training: self.pp_model.train()\n",
    "        if not self.vm_model.training: self.vm_model.train()\n",
    "        with timecode() as self.batch_time_d['time_generate_pp']:\n",
    "            pp_output, pp_l = self._pp_model_forward(data)\n",
    "        \n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after forward pass: '))\n",
    "        \n",
    "        # autocast is used by accelerate to allow mixed-precision loss functions.\n",
    "        # drop it if we deprecate fp16 support (because it isn't supported for models like PEGASUS)\n",
    "        #with self.accelerator.autocast():\n",
    "        \n",
    "        with timecode() as self.batch_time_d['time_loss_fn']:\n",
    "            loss_batch = self._loss_fn(data, raw, pp_output, pp_l)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_backwards']:\n",
    "            self.accelerator.backward(loss_batch) \n",
    "\n",
    "        with timecode() as self.batch_time_d['time_calc_gradient_norm']: \n",
    "            self.grad_norm =  self._get_gradient_update_norm()\n",
    "        \n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after backwards pass: '))\n",
    "        with timecode() as self.batch_time_d['time_opt_step_and_calc_param_norm']:\n",
    "            if self._batch_for_opt_step():  \n",
    "                self._opt_step_and_calc_param_norm()\n",
    "            else: \n",
    "                self.param_norm = 0       \n",
    "                with timecode() as self.batch_time_d['time_opt_step']: pass\n",
    "            \n",
    "        self._prepare_train_batch_d(raw, data, pp_l)\n",
    "        self.data_d['training_step'].append(self.batch_d)\n",
    "        self._wandb_log_training_step()\n",
    "    \n",
    "    def _opt_step_and_calc_param_norm(self): \n",
    "        ## record initial parameters\n",
    "        params_all = [o.detach() for o in self.pp_model.parameters() if o[1].requires_grad]\n",
    "        params_initial = [ p.clone() for p in params_all]\n",
    "        # step + zero grad\n",
    "        with timecode() as self.batch_time_d['time_opt_step']: \n",
    "            self.optimizer.step()\n",
    "            self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none)\n",
    "        # record norm\n",
    "        self.param_norm = 0 \n",
    "        for (p_init, p_new) in zip(params_initial, params_all): \n",
    "            self.param_norm += (p_new - p_init).data.norm(2).item() ** 2\n",
    "        self.param_norm = self.grad_norm ** 0.5 \n",
    "    \n",
    "    def _batch_for_opt_step(self): return self.acc_num == (self._cfg.acc_steps - 1)\n",
    "    \n",
    "    def _add_batch_vars_to_batch_d(self, raw, data, pp_l): \n",
    "        # Add basics. (results are already added elsewhere)\n",
    "        self.batch_d = merge_dicts(self.batch_d, { 'idx': raw['idx'],\n",
    "            'epoch': self.epoch, 'batch_num': self.batch_num, 'global_step': self.global_step,\n",
    "            'acc_num': self.acc_num, \"acc_batch_n_examples\": self.acc_current_n_examples, \n",
    "            \"orig_l\": raw['text'], \n",
    "            \"orig_label\": data['label'].cpu().tolist(), \n",
    "            \"orig_truelabel_probs\": data['orig_truelabel_probs'].cpu().tolist(),\n",
    "            'orig_length': self.orig_length, 'orig_batch_size': self.orig_batch_size, \n",
    "            \"pp_l\": pp_l, 'pp_length': self.pp_length, 'pp_batch_size': self.pp_batch_size\n",
    "        })\n",
    "        \n",
    "    def _prepare_train_batch_d(self, raw, data, pp_l): \n",
    "        self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "        # Add times (only for training, not eval)\n",
    "        for k, v in self.batch_time_d.items(): self.batch_time_d[k] = v.t  # extract time from timecode object\n",
    "        self.batch_d = merge_dicts(self.batch_d, self.batch_time_d)\n",
    "    \n",
    "    def _wandb_log_training_step(self): \n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, {\n",
    "            'vm_scores_hist':       Histogram(self.batch_d['vm_score']), \n",
    "            'vm_scores_mean':       np.mean(  self.batch_d['vm_score']),\n",
    "            'sts_scores_hist':      Histogram(self.batch_d['sts_score']),\n",
    "            'sts_scores_mean':      np.mean(  self.batch_d['sts_score']), \n",
    "            'rewards_hist':         Histogram(self.batch_d['reward']),\n",
    "            'rewards_mean':         np.mean(  self.batch_d['reward']), \n",
    "            'pp_logp_hist':         Histogram(self.batch_d['pp_logp']),\n",
    "            'pp_logp_mean':         np.mean(  self.batch_d['pp_logp']),\n",
    "            'ref_logp_hist':        Histogram(self.batch_d['ref_logp']),\n",
    "            'ref_logp_mean':        np.mean(  self.batch_d['ref_logp']),\n",
    "            'kl_div_hist' :         Histogram(self.batch_d['kl_div']),\n",
    "            'kl_div_mean':          np.mean(  self.batch_d['kl_div']),\n",
    "            'reward_penalty_hist':               Histogram(self.batch_d['reward_penalty']),\n",
    "            'reward_penalty_mean':               np.mean(  self.batch_d['reward_penalty']), \n",
    "            'reward_with_penalty_hist':          Histogram(self.batch_d['reward_with_penalty']),\n",
    "            'reward_with_penalty_mean':          np.mean(  self.batch_d['reward_with_penalty']),\n",
    "            'loss_hist'   :         Histogram(self.batch_d['loss']), \n",
    "            'pp_letter_diff_hist':     Histogram(self.batch_d['pp_letter_diff']),\n",
    "            'pp_letter_diff_mean':     np.mean(  self.batch_d['pp_letter_diff']), \n",
    "            'pp_letter_percent_hist':  Histogram(self.batch_d['pp_letter_percent']),\n",
    "            'pp_letter_percent_mean':  np.mean(  self.batch_d['pp_letter_percent']),\n",
    "            'contradiction_score_hist':     Histogram(self.batch_d['contradiction_score']),\n",
    "            'contradiction_score_mean':     np.mean(  self.batch_d['contradiction_score']), \n",
    "            'acc_batch_sizes':      Histogram(self.acc_current_l), \n",
    "            \"gradient_norm\":        self.grad_norm, \n",
    "            \"parameter_norm\":       self.param_norm\n",
    "        })\n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, self.batch_d)\n",
    "        not_for_wandb_keys = ['orig_l', 'orig_label','orig_truelabel_probs', 'pp_l', 'loss', 'pp_logp','ref_logp', 'kl_div', 'reward_with_penalty', 'reward_penalty',\n",
    "                              'reward', 'sts_score', 'vm_score', 'pp_letter_diff', 'pp_letter_percent','contradiction_score',\n",
    "                              'pp_predclass_probs', 'label_flip', 'pp_predclass', 'pp_truelabel_probs']\n",
    "        for k in not_for_wandb_keys:  self.batch_wandb_d.pop(k, None)\n",
    "        wandb.log(self.batch_wandb_d, commit=True)\n",
    "        \n",
    "    def _convert_data_d_to_df(self, data_d_key): \n",
    "        df = pd.DataFrame(self.data_d[data_d_key])\n",
    "        ### Check all lists have the same number of elements in their row \n",
    "        # last batch will have different number of elements to the batch size\n",
    "        nonscalar_cols = df.columns[[o == np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        df_nonscalar_cols = df[nonscalar_cols]\n",
    "        # sometimes if we have one element in the last batch, the tolist() function returns a scalar instead of a list \n",
    "        # so we handle that case here\n",
    "        def scalar2list(x): return x if type(x) == list else [x]\n",
    "        if len(df_nonscalar_cols.iloc[-1][\"idx\"]) == 1: df_nonscalar_cols.iloc[-1] = df_nonscalar_cols.iloc[-1].apply(scalar2list) \n",
    "        df_lengths = df_nonscalar_cols.applymap(len)\n",
    "        assert df_lengths.eq(df_lengths.iloc[:,0], axis=0).all(None)\n",
    "        \n",
    "        ### Put in dataframes\n",
    "        # expand lists and broadcast scalars\n",
    "        scalar_cols = df.columns[[o != np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        df_expanded = unpack_nested_lists_in_df(df, scalar_cols)\n",
    "        # check shape of new dataframe is correct \n",
    "        if data_d_key == \"training_step\": \n",
    "            if self.epoch == 0: \n",
    "                df_shape = (self._cfg.ds_length[\"train\"],                       df.shape[1])\n",
    "            else: \n",
    "                df_shape = (self._cfg.ds_length[\"train\"] * self._cfg.eval_freq, df.shape[1])\n",
    "        elif data_d_key in [\"train\", \"valid\", \"test\"]: \n",
    "                 df_shape = (self._cfg.ds_length[data_d_key],                   df.shape[1])\n",
    "        assert df_expanded.shape == df_shape\n",
    "        return df_expanded\n",
    "    \n",
    "    def _pp_model_forward(self, data): \n",
    "        pp_output, pp_l = self._get_paraphrases(data['input_ids'], data['attention_mask'])\n",
    "       # self._assert_start_and_end_tokens_are_correct(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        # Keep the below line here because then both training and eval can access it\n",
    "        self._update_batch_size_and_length_variables(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _assert_start_and_end_tokens_are_correct(self, orig_ids, pp_ids):\n",
    "        \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "        right special tokens (depends on tokenizer)\"\"\"\n",
    "        # Input\n",
    "        if self.start_end_token_d['input_start_id'] is not None: \n",
    "            assert torch.all(orig_ids[:,0] == self.start_end_token_d['input_start_id'])\n",
    "        # can probs rewrite this to make it nicer but it's fine for now\n",
    "        assert torch.all(torch.logical_or(orig_ids[:,-1] == self.start_end_token_d['input_end_id'][0], \n",
    "                                          orig_ids[:,-1] == self.start_end_token_d['input_end_id'][1]))\n",
    "\n",
    "        # Output\n",
    "        assert torch.all(pp_ids[:,0] == self.start_end_token_d['output_start_id'])\n",
    "        assert torch.all(torch.logical_or(pp_ids[:,-1] == self.start_end_token_d['output_end_id'][0], \n",
    "                                          pp_ids[:,-1] == self.start_end_token_d['output_end_id'][1]))\n",
    "        \n",
    "    def _update_batch_size_and_length_variables(self, orig_ids, pp_ids): \n",
    "        # Update variables\n",
    "        # for greedy search self.pp_length is equal to self.orig_batch_size but this won't be for beam search\n",
    "        self.orig_batch_size     = orig_ids.shape[0]\n",
    "        self.orig_length         = orig_ids.shape[1]\n",
    "        self.pp_batch_size       = pp_ids.shape[0]\n",
    "        self.pp_length           = pp_ids.shape[1] \n",
    "    \n",
    "    def _get_paraphrases(self, orig_ids, attention_mask):\n",
    "        \"\"\"Wrapper for generating paraphrases (pp's).\"\"\"\n",
    "        pp_output = self.pp_model.generate_with_grad(input_ids=orig_ids, attention_mask=attention_mask, num_return_sequences=1, num_beams=1,\n",
    "                                                     **self._cfg.pp, return_dict_in_generate=True, output_scores=True, remove_invalid_values=False, \n",
    "                                                     pad_token_id = self.pp_tokenizer.pad_token_id,eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "        pp_l = self.pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _loss_fn(self, data, raw, pp_output, pp_l): \n",
    "        with timecode() as self.batch_time_d['time_reward_fn']:\n",
    "            reward = self._reward_fn(data, raw, pp_l)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_pp_logp']:\n",
    "            pp_logp = self._get_pp_logp(pp_output)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_ref_logprobs']: \n",
    "            ref_logp = self._get_ref_logprobs(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "            \n",
    "        kl_div =  torch.clip(pp_logp - ref_logp, min=0)  # sometimes this is negative, not sure why. \n",
    "        with timecode() as self.batch_time_d['time_loss_fn_loss_calc']:\n",
    "            if   self._cfg.reward_penalty_type == \"kl_div\": \n",
    "                reward_penalty = - (self._cfg.kl_coef       * kl_div)       # KL div is positive, this term is negative\n",
    "            elif self._cfg.reward_penalty_type == \"ref_logp\": \n",
    "                reward_penalty =    self._cfg.ref_logp_coef * ref_logp      # ref_logp is negative, this term is negative\n",
    "                \n",
    "            reward_with_penalty = torch.clip(reward + reward_penalty, min=0) \n",
    "            loss       = -reward_with_penalty * pp_logp\n",
    "            loss_sum   = torch.sum(loss)  # we scale it later\n",
    "            loss_batch = loss_sum / self.acc_current_n_examples  # for gradient accumulation     \n",
    "  \n",
    "        self.batch_d['pp_logp']     =                      pp_logp.detach().cpu().tolist()\n",
    "        self.batch_d['ref_logp']    =                     ref_logp.detach().cpu().tolist()\n",
    "        self.batch_d['kl_div']      =                 kl_div.detach().cpu().tolist()\n",
    "        self.batch_d['reward_penalty'] =            reward_penalty.detach().cpu().tolist()\n",
    "        self.batch_d['reward_with_penalty'] =  reward_with_penalty.detach().cpu().tolist()\n",
    "        self.batch_d['loss']       =                          loss.detach().cpu().tolist()\n",
    "        self.batch_d['loss_sum']   =                      loss_sum.detach().cpu().tolist()\n",
    "        self.batch_d['loss_batch']   =                  loss_batch.detach().cpu().tolist()\n",
    "        return loss_batch\n",
    "        \n",
    "    def _get_vm_scores(self, pp_l, labels, orig_truelabel_probs): \n",
    "        pp_probs = get_vm_probs(pp_l, self._cfg, self.vm_tokenizer, self.vm_model, return_predclass=False)\n",
    "        pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "        pp_truelabel_probs   = torch.gather(pp_probs, 1, labels[:,None]).squeeze()\n",
    "        pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "        label_flip = ((pp_predclass != labels) * 1)\n",
    "        vm_scores = (orig_truelabel_probs - pp_truelabel_probs)\n",
    "        return dict(pp_truelabel_probs=pp_truelabel_probs, pp_predclass=pp_predclass, pp_predclass_probs=pp_predclass_probs, vm_scores=vm_scores, label_flip=label_flip)\n",
    "        \n",
    "    def _get_pp_letter_diff(self, pp_l, orig_n_letters): \n",
    "        pp_n_letters = np.array([len(o) for o in pp_l])\n",
    "        orig_letters = np.array(orig_n_letters)\n",
    "        pp_letter_diff    = orig_n_letters - pp_n_letters\n",
    "        pp_letter_percent = pp_n_letters / orig_n_letters \n",
    "        return dict(pp_letter_diff=pp_letter_diff, pp_letter_percent=pp_letter_percent)\n",
    "    \n",
    "    def _get_contradiction_scores(self, orig_l, pp_l):\n",
    "        contradiction_scores = get_nli_probs(orig_l, pp_l, self._cfg, self.nli_tokenizer, self.nli_model)[:,  self._cfg.contra_label]\n",
    "        return contradiction_scores\n",
    "    \n",
    "    def _get_sts_scores_one_to_many(self, pp_l, orig_sts_embeddings): \n",
    "        \"\"\"Calculate STS scores when there is one orig and a list of pp_l\"\"\"\n",
    "        orig_sts_embeddings = orig_sts_embeddings.to(self._cfg.device)\n",
    "        pp_sts_embeddings = self.sts_model.encode(pp_l, convert_to_tensor=True, device=self._cfg.device, show_progress_bar=False)\n",
    "        sts_scores = pytorch_cos_sim(orig_sts_embeddings, pp_sts_embeddings).cpu().tolist()\n",
    "        return sts_scores \n",
    "    \n",
    "    def _is_valid_pp(self, sts_score, pp_letter_diff, contradiction_score): \n",
    "        if sts_score           < self._cfg.sts_threshold:                              return False\n",
    "        if contradiction_score > self._cfg.contradiction_threshold:                    return False  \n",
    "        if pp_letter_diff >   self._cfg.pp_letter_diff_threshold:                      return False \n",
    "        if pp_letter_diff < - self._cfg.pp_letter_diff_threshold:                      return False\n",
    "        return True\n",
    "    \n",
    "    def _get_reward(self, vm_scores, sts_scores, pp_letter_diff, contradiction_scores): \n",
    "        def reward_fn_contradiction_and_letter_diff(vm_score, sts_score, pp_letter_diff, contradiction_score): \n",
    "            if not self._is_valid_pp(sts_score, pp_letter_diff, contradiction_score): return 0\n",
    "            reward = self._cfg.reward_base + vm_score * self._cfg.reward_vm_multiplier  \n",
    "            return min(max(self._cfg.reward_clip_min, reward), self._cfg.reward_clip_max)\n",
    "        \n",
    "        def calc_reward(vm_scores, sts_scores, pp_letter_diff, contradiction_scores):\n",
    "            if self._cfg.reward_fn == \"reward_fn_contradiction_and_letter_diff\": reward_fn = reward_fn_contradiction_and_letter_diff\n",
    "            return torch.tensor([reward_fn(vm, sts, ldiff, contra) for vm,sts,ldiff,contra in zip(vm_scores, sts_scores, pp_letter_diff, contradiction_scores)],\n",
    "                                device=self._cfg.device)\n",
    "        rewards = calc_reward(vm_scores, sts_scores, pp_letter_diff, contradiction_scores)\n",
    "        return rewards\n",
    "    \n",
    "    \n",
    "    def _reward_fn(self, data, raw, pp_l): \n",
    "        \"\"\"Used for training, need 1-1 \"\"\"\n",
    "        # Victim model probability differences between orig and pp\n",
    "        # #TODO: update this whole function\n",
    "        with timecode() as self.batch_time_d['time_vm_scores']:\n",
    "            pp_probs = get_vm_probs(pp_l, self._cfg, self.vm_tokenizer, self.vm_model, return_predclass=False)\n",
    "            pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "            pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "            pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "            label_flip = ((pp_predclass != data['label']) * 1)\n",
    "            vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "            \n",
    "        # STS scores\n",
    "        with timecode() as self.batch_time_d['time_sts_scores']:\n",
    "            pp_embeddings  = self.sts_model.encode(pp_l, batch_size=len(raw), convert_to_tensor=True, show_progress_bar=False, device=self._cfg.device)\n",
    "            # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "            sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "        \n",
    "        with timecode() as self.batch_time_d['time_pp_letter_diff']:\n",
    "            pp_letters = np.array([len(o) for o in pp_l])\n",
    "            orig_letters = data['n_letters'].cpu().numpy()\n",
    "            pp_letter_diff    = orig_letters - pp_letters\n",
    "            pp_letter_percent = pp_letters / orig_letters \n",
    "            \n",
    "            \n",
    "        with timecode() as self.batch_time_d['time_contradiction_scores']: \n",
    "            contradiction_scores = get_nli_probs(raw['text'], pp_l, self._cfg, self.nli_tokenizer, self.nli_model)[:,  self._cfg.contra_label]\n",
    "\n",
    "        # Reward calculation\n",
    "        \n",
    "        def reward_fn_contradiction_and_letter_diff(vm_score, sts_score, pp_letter_diff, contradiction_score): \n",
    "            if sts_score           < self._cfg.sts_threshold:                              return 0\n",
    "            if contradiction_score > self._cfg.contradiction_threshold:                    return 0  \n",
    "            if pp_letter_diff >   self._cfg.pp_letter_diff_threshold:                      return 0 \n",
    "            if pp_letter_diff < - self._cfg.pp_letter_diff_threshold:                      return 0 \n",
    "            reward = self._cfg.reward_base + vm_score * self._cfg.reward_vm_multiplier  \n",
    "            return min(max(self._cfg.reward_clip_min, reward), self._cfg.reward_clip_max)\n",
    "        \n",
    "        \n",
    "        def calc_reward(vm_scores, sts_scores, pp_letter_diff, contradiction_scores):\n",
    "            if self._cfg.reward_fn == \"reward_fn_contradiction_and_letter_diff\": reward_fn = reward_fn_contradiction_and_letter_diff\n",
    "            return torch.tensor([reward_fn(vm, sts, ldiff, contra) for vm,sts,ldiff,contra in zip(vm_scores, sts_scores, pp_letter_diff, contradiction_scores)],\n",
    "                                device=self._cfg.device)\n",
    "        \n",
    "        rewards = calc_reward(vm_scores, sts_scores, pp_letter_diff, contradiction_scores)\n",
    "        self.batch_d['pp_truelabel_probs']  = pp_truelabel_probs.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass']        = pp_predclass.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass_probs']  = pp_predclass_probs.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip']          = label_flip.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip_fraction'] = np.mean(self.batch_d['label_flip'])\n",
    "        self.batch_d['reward']              = rewards.detach().cpu().tolist()\n",
    "        self.batch_d['vm_score']            = vm_scores.detach().cpu().tolist()\n",
    "        self.batch_d['sts_score']           = sts_scores.detach().cpu().tolist()\n",
    "        self.batch_d['pp_letter_diff']      = pp_letter_diff.tolist()\n",
    "        self.batch_d['pp_letter_percent']   = pp_letter_percent.tolist()\n",
    "        self.batch_d['contradiction_score'] = contradiction_scores.cpu().tolist()\n",
    "        return rewards\n",
    "         \n",
    "    def _get_pp_logp(self, pp_output): \n",
    "        \"\"\"log(p(pp|orig)) basically.\n",
    "        works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "        ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "        # and has no probability attached to it, so we remove it. \n",
    "        seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "        assert seq_without_first_tkn.shape == torch.Size([self.orig_batch_size, self.pp_length - 1])\n",
    "\n",
    "        ### Convert from tuple of scores to one big tensor of scores \n",
    "        scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "        ### TESTS \n",
    "        # We check shape and that there is no +inf or nan in scores. \n",
    "        # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "        assert scores_stacked.shape == torch.Size([self.orig_batch_size, (self.pp_length - 1), self._cfg.vocab_size])\n",
    "        assert torch.all(~torch.isnan(scores_stacked))\n",
    "        assert torch.all(~torch.isposinf(scores_stacked))\n",
    "        \n",
    "#         # Rough check that every time the eos token occurs before min_length it is -inf for all elements in batch\n",
    "#         # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "#         # shouldn't be set to -inf\n",
    "#         # Not a 100% test but very likely to identify\n",
    "#         idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "#         assert len(idx_neginf[idx_neginf[:,2] == self.pp_tokenizer.eos_token_id, :]) == \\\n",
    "#                   (self._cfg.pp[\"min_length\"] -1) * self.orig_batch_size  \n",
    "#         del idx_neginf\n",
    "\n",
    "        ### Take log softmax of scores and then extract those that correspond \n",
    "        # to the generated sequences    \n",
    "        scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "        seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "        ### TESTS \n",
    "        # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "        assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "        assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "        self._check_scores_log_softmax_sums(scores_log_softmax)\n",
    "        # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "        assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "        # Check that the last token probability corresponds to a possible end token\n",
    "        # this has to be tested before the attention mask is multiplied with it because if the \n",
    "        # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "     #   output_end_ids = self.start_end_token_d['output_end_id']\n",
    "     #   assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "     #   del output_end_ids\n",
    "        ## THIS ONE IS LONG - a test rather than assert \n",
    "        # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "        #                                             seq_token_log_probs) \n",
    "\n",
    "        ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "        # sequence probabilities so that we don't consider probability of padding tokens \n",
    "        # when getting sequence probabilities. \n",
    "        # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "        # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "        # since  -inf * 0 = nan \n",
    "        attention_mask = self.pp_model._prepare_attention_mask_for_generation(\n",
    "            seq_without_first_tkn, self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id\n",
    "        )\n",
    "        seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-20)\n",
    "        seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "        ### TESTS\n",
    "        assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "        # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "        assert all(seq_without_first_tkn[attention_mask == 0] == self.pp_tokenizer.pad_token_id)\n",
    "        check_no_nans_or_infs(seq_token_log_probs)\n",
    "        # check that we aren't picking extrememly rare tokens\n",
    "        # assert torch.all(seq_token_log_probs  > -10)  \n",
    "\n",
    "        ### Get sequence probabilities by summing up token log probabilities \n",
    "        seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "        ## TESTS \n",
    "        assert seq_log_prob.shape == torch.Size([self.pp_batch_size])\n",
    "        check_no_nans_or_infs(seq_log_prob)\n",
    "        \n",
    "        # normalise for length\n",
    "        logprobs_normalised = seq_log_prob / attention_mask.sum(1)  # normalise for length of generated sequence\n",
    "        \n",
    "        if self.pp_model.training:  # don't bother logging or calculate entropy, token_probs in eval mode\n",
    "            if self._cfg.wandb['log_token_entropy']:\n",
    "                with timecode() as self.batch_time_d['time_log_entropy']:\n",
    "                    self.batch_wandb_d['ent_hist'] = self._get_entropy_hist(scores_stacked, attention_mask) \n",
    "            if self._cfg.wandb['log_token_probabilities']: \n",
    "                with timecode() as self.batch_time_d['time_log_token_probabilities']:\n",
    "                    self.batch_wandb_d = merge_dicts(self.batch_wandb_d, \n",
    "                        self._get_token_probability_metrics(scores_log_softmax, attention_mask, k=3))\n",
    "        return logprobs_normalised\n",
    "    \n",
    "    def _get_ref_logprobs(self, orig_ids, pp_ids): \n",
    "     #   orig_input_ids = self.pp_tokenizer(orig_l, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "       # pp_input_ids   = self.pp_tokenizer(pp_l,   return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "        decoder_start_token_ids = torch.tensor([self.ref_pp_model.config.decoder_start_token_id], device=self._cfg.device).repeat(self.orig_batch_size, 1)\n",
    "        pp_ids = torch.cat([decoder_start_token_ids, pp_ids], 1)\n",
    "        logprobs = []\n",
    "        for i in range(pp_ids.shape[1] - 1): \n",
    "            decoder_input_ids = pp_ids[:, 0:(i+1)]\n",
    "            outputs = self.ref_pp_model(input_ids=orig_ids, decoder_input_ids=decoder_input_ids)\n",
    "            token_logprobs = outputs.logits[:,i,:].log_softmax(1)\n",
    "            pp_next_token_ids = pp_ids[:,i+1].unsqueeze(-1)\n",
    "            pp_next_token_logprobs = torch.gather(token_logprobs, 1, pp_next_token_ids).detach().squeeze(-1)\n",
    "            logprobs.append(pp_next_token_logprobs)\n",
    "        logprobs = torch.stack(logprobs, 1)   \n",
    "        attention_mask = self.ref_pp_model._prepare_attention_mask_for_generation(pp_ids[:,1:],\n",
    "                self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id)\n",
    "        logprobs = logprobs * attention_mask        \n",
    "        logprobs_sum = logprobs.sum(1)\n",
    "        logprobs_normalised = logprobs_sum / attention_mask.sum(1)  # normalise for length of generated sequence\n",
    "        return logprobs_normalised\n",
    "   \n",
    "    def _check_scores_log_softmax_sums(self, scores_log_softmax):\n",
    "        sums = scores_log_softmax.exp().sum(2)\n",
    "        # check that the axes is right\n",
    "        # we want to sum over token probabilities at each generation step, so we \n",
    "        # should end up with a shape [self.orig_batch_size, self.pp_length]\n",
    "        assert sums.shape[0] == self.orig_batch_size  \n",
    "        assert sums.shape[1] == self.pp_length - 1\n",
    "        # check that they sum to 1 along the self.pp_length axis (or close enough at least) \n",
    "        assert torch.allclose(sums, torch.ones(sums.size(), device=self._cfg.device), atol = 5e-2)\n",
    "\n",
    "    def _check_seq_token_log_prob_values_are_correct(self, seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "        \"\"\"Just enumerates and checks values\n",
    "        Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i_ex in range(self.orig_batch_size):\n",
    "            for i_step in range(self.pp_length - 1):\n",
    "                i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "                l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "        assert all(l)    \n",
    "    \n",
    "    def _get_entropy_hist(self, scores_stacked, attention_mask): \n",
    "        ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "        assert ent.shape == attention_mask.shape == torch.Size([self.pp_batch_size, self.pp_length - 1])\n",
    "        ent = ent * attention_mask  # stop values after eos token from contributing to ent score \n",
    "        # first remove structure (otherwise we have ragged arrays), then remove corresponding attention mask values\n",
    "        # we can't just filter by ent[ent != 0] because we might have zero tokens during the sequence\n",
    "        att_flat= attention_mask.flatten()\n",
    "        indices = torch.nonzero(att_flat)\n",
    "        ent_flat = ent.flatten()[indices].flatten()\n",
    "        assert ent_flat.shape[0] == (torch.sum(att_flat)*1).item()\n",
    "        # check everything we filter out is zero \n",
    "        torch.isclose(ent.flatten()[torch.nonzero(~(att_flat > 0))].sum(), torch.tensor(0.), 1e-3)\n",
    "        return Histogram(ent_flat.detach().cpu().tolist())\n",
    "\n",
    "    def _get_token_probability_metrics(self, scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, _ = torch.topk(scores_log_softmax, largest=True, k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  \n",
    "        tkn_kmaxprob = torch.nan_to_num(tkn_kmaxprob, nan=None, posinf=None, neginf=-20)\n",
    "        assert tkn_kmaxprob.shape == torch.Size([self.pp_batch_size, self.pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_histogram\"] = Histogram(probs.detach().cpu().tolist())\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [1e-5, 1e-6, 1e-7, 1e-8, 1e-9]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/self._cfg.vocab_size)) / allprobs.shape[0]).item()\n",
    "        return token_prob_d\n",
    "    \n",
    "    def _eval_dl(self, split): \n",
    "        \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "        # Put models in eval mode and do the forward pass \n",
    "        # Current logic: push all batches together into one big list.   \n",
    "        self._reset_batch_dicts()\n",
    "        if self.pp_model.training: self.pp_model.eval()\n",
    "        if self.vm_model.training: self.vm_model.eval()\n",
    "        # The \"train_eval\" dataloader is the same as train but a bigger batch size and explicitly no shuffling\n",
    "        dl_key = \"train_eval\" if split == \"train\" else split\n",
    "        dl_raw = self.ds.dld_raw[dl_key]\n",
    "        dl_tkn = self.ds.dld_tkn[dl_key]\n",
    "        with torch.no_grad(): \n",
    "            for self.batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "                logger.debug(f\"EVAL: {split} with dl_key {dl_key}\")\n",
    "                logger.debug(f\"Elements in data_d[{split}]: {len(self.data_d[split])}\")\n",
    "                logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loading data: '))\n",
    "                assert data['input_ids'].shape[0] == len(raw['text_with_prefix'])\n",
    "                self._reset_batch_dicts()\n",
    "                assert len(self.batch_d) == len(self.batch_time_d) == len(self.batch_wandb_d) == 0 \n",
    "                for k, v in data.items():\n",
    "                    # Eval data isn't loaded on GPU by default unlike train data. This is because train dataloader goes \n",
    "                    # through accelerator `prepare` function, but eval dataloaders don't. So here we load the data onto GPU \n",
    "                    if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "                pp_output, pp_l = self._pp_model_forward(data)\n",
    "                _ = self._loss_fn(data, raw, pp_output, pp_l)\n",
    "                self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "                self.data_d[split].append(self.batch_d)\n",
    "                logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "                \n",
    "    def eval_ref_model(self, split): \n",
    "        notebook_launcher(self._eval_function, args=(split, True), num_processes=1, use_fp16=False)\n",
    "        \n",
    "    def _eval_function(self, split, eval_ref_model=False): \n",
    "        ## Setup\n",
    "        model = self.ref_pp_model if eval_ref_model else self.pp_model\n",
    "        if model.training:             model.eval()\n",
    "        if self.vm_model.training:     self.vm_model.eval()\n",
    "        if eval_ref_model: \n",
    "            self.accelerator_eval = Accelerator(cpu=self.use_cpu)\n",
    "            self._cfg.device = self.accelerator_eval.device\n",
    "            self.vm_model,model,self.sts_model = self.accelerator.prepare(self.vm_model,model,self.sts_model)\n",
    "        eval_batch_results = list()  # each eval batch appended to here, list of dicts\n",
    "        dl_key = \"train_eval\" if split == \"train\" else split\n",
    "        dl_raw = ds.dld_raw[dl_key]\n",
    "        dl_tkn = ds.dld_tkn[dl_key]\n",
    "\n",
    "        ## Loop through batches in eval set\n",
    "        for eval_batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "            pp_output = model.generate(input_ids=data['input_ids'].to(self._cfg.device), attention_mask=data['attention_mask'].to(self._cfg.device), \n",
    "                                          **self._cfg.eval_gen_params,   remove_invalid_values=False, \n",
    "                                          pad_token_id = self.pp_tokenizer.pad_token_id,eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "            pp_l = self.pp_tokenizer.batch_decode(pp_output, skip_special_tokens=True)\n",
    "            pp_l_nested = [pp_l[i:i+self._cfg.n_eval_seq] for i in range(0, len(pp_l), self._cfg.n_eval_seq)]  # put paraphrases in nested lists \n",
    "            assert all([len(l) == self._cfg.n_eval_seq for l in pp_l_nested])  # make sure we generate the same number of paraphrases for each\n",
    "            eval_batch_results.append({'idx': raw['idx'], 'orig': raw['text'], 'pp_l':pp_l_nested, 'orig_n_letters': data['n_letters'].tolist(), \n",
    "                                  'label': raw['label'], 'orig_truelabel_probs': data['orig_truelabel_probs'].tolist(), 'orig_sts_embeddings': data['orig_sts_embeddings'] })\n",
    "\n",
    "        ## Convert eval batches to dataframes and create paraphrase identifier `pp_idx`\n",
    "        df = pd.DataFrame(eval_batch_results)    \n",
    "        df = df.apply(pd.Series.explode).reset_index(drop=True)  # This dataframe has one row per original example\n",
    "        def get_pp_idx(row): return [\"orig_\" + str(row['idx']) + \"-epoch_\" + str(epoch) +  \"-pp_\" +  str(pp_i) for pp_i in range(1, len(row['pp_l'])+1)]\n",
    "        df['pp_idx'] = df.apply(get_pp_idx, axis=1)\n",
    "\n",
    "        ## Create seperate dataframe for sts scores and expand original dataframe\n",
    "        df_sts = df[['pp_idx', 'pp_l', 'orig_sts_embeddings']] \n",
    "        df1 = df.drop(columns='orig_sts_embeddings')\n",
    "        scalar_cols = [o for o in df1.columns if o not in ['pp_l', 'pp_idx']]\n",
    "        df_expanded = unpack_nested_lists_in_df(df1, scalar_cols=scalar_cols) # This dataframe has one row per paraphrase\n",
    "\n",
    "        ## Add vm_scores, sts_scores, pp_letter_diff, contradiction scores\n",
    "        ds_expanded = Dataset.from_pandas(df_expanded)\n",
    "        def add_vm_scores_eval(batch): \n",
    "            output = self._get_vm_scores(pp_l=batch['pp_l'], labels=torch.tensor(batch['label'], device = self._cfg.device), \n",
    "                                            orig_truelabel_probs=torch.tensor(batch['orig_truelabel_probs'], device=self._cfg.device))\n",
    "            for k, v in output.items(): batch[k] = v.cpu().tolist() \n",
    "            return batch\n",
    "        def add_pp_letter_diff(batch): \n",
    "            output = self._get_pp_letter_diff(pp_l=batch['pp_l'], orig_n_letters=batch['orig_n_letters'])\n",
    "            for k, v in output.items(): batch[k] = v.tolist() \n",
    "            return batch\n",
    "        def add_contradiction_score(batch): \n",
    "            batch['contradiction_scores'] = self._get_contradiction_scores(orig_l=batch['orig'], pp_l=batch['pp_l']).cpu().tolist()\n",
    "            return batch\n",
    "        ds_expanded = ds_expanded.map(add_vm_scores_eval,        batched=True)\n",
    "        ds_expanded = ds_expanded.map(add_pp_letter_diff,        batched=True)\n",
    "        ds_expanded = ds_expanded.map(add_contradiction_score,   batched=True)\n",
    "        def add_sts_scores_eval(row):  return self._get_sts_scores_one_to_many(row['pp_l'], row['orig_sts_embeddings'])[0]\n",
    "        df_sts['sts_scores'] = df_sts.apply(add_sts_scores_eval, axis=1)\n",
    "\n",
    "        ## Merge together results \n",
    "        df_sts = df_sts.drop(columns = ['pp_l','orig_sts_embeddings'])\n",
    "        df_sts_expanded = df_sts.apply(pd.Series.explode).reset_index(drop=True)\n",
    "        ds_expanded = Dataset.from_pandas(ds_expanded.to_pandas().merge(df_sts_expanded, how='left', on='pp_idx').reset_index(drop=True))\n",
    "\n",
    "        ## Calculate rewards and identify adversarial examples \n",
    "        def add_reward(batch): \n",
    "            batch['reward'] = self._get_reward(vm_scores=batch['vm_scores'], sts_scores=batch['sts_scores'],\n",
    "                      pp_letter_diff=batch['pp_letter_diff'], contradiction_scores=batch['contradiction_scores']).cpu().tolist()\n",
    "            return batch\n",
    "        ds_expanded = ds_expanded.map(add_reward,   batched=True)\n",
    "        def add_is_valid_pp(example): \n",
    "            example['is_valid_pp'] = self._is_valid_pp(sts_score=example['sts_scores'],\n",
    "                 pp_letter_diff=example['pp_letter_diff'], contradiction_score=example['contradiction_scores'])*1\n",
    "            return example \n",
    "        ds_expanded = ds_expanded.map(add_is_valid_pp,   batched=False)\n",
    "        def add_is_adv_example(batch): \n",
    "            batch['is_adv_example'] = (np.array(batch['is_valid_pp']) * np.array(batch['label_flip'])).tolist()\n",
    "            return batch\n",
    "        ds_expanded = ds_expanded.map(add_is_adv_example,   batched=True)\n",
    "\n",
    "        ## Calculate summary statistics\n",
    "        df_expanded = ds_expanded.to_pandas()\n",
    "        eval_metric_cols = ['label_flip', 'is_valid_pp', 'is_adv_example', 'reward', 'vm_scores', 'sts_scores',  'pp_letter_diff', 'contradiction_scores']\n",
    "        agg_metrics = ['mean','std']  # not going to use the median \n",
    "        # avg across each orig \n",
    "        df_grp_stats = df_expanded[['idx'] + eval_metric_cols].groupby('idx').agg(agg_metrics)\n",
    "        df_grp_stats.columns = df_grp_stats.columns = [\"-\".join(a) for a in df_grp_stats.columns.to_flat_index()]\n",
    "        # avg across whole dataset \n",
    "        df_overall_stats = df_expanded[eval_metric_cols].groupby(lambda _ : True).agg(agg_metrics).reset_index(drop=True)\n",
    "        df_overall_stats.columns = df_overall_stats.columns = [\"-\".join(a) + \"-\" + split for a in df_overall_stats.columns.to_flat_index()]\n",
    "        df_overall_metrics = df_overall_stats.iloc[0].to_dict()   ## WANDB this \n",
    "        df_overall_metrics['any_adv_example_proportion' + \"-\" + split] = np.mean((df_grp_stats['is_adv_example-mean'] > 0 ) * 1)\n",
    "        # add epoch key\n",
    "        df_expanded['epoch'] = epoch\n",
    "        df_overall_metrics['epoch'] = epoch\n",
    "\n",
    "        ## Log results to wandb \n",
    "        if split in ['train', 'eval'] and not eval_ref_model: \n",
    "            wandb_eval_d = dict()\n",
    "            mean_only = ['label_flip', 'is_valid_pp', 'is_adv_example']\n",
    "            mean_and_std = ['reward', 'vm_scores', 'sts_scores', 'pp_letter_diff', 'contradiction_scores']\n",
    "            for k in mean_only + mean_and_std: \n",
    "                name = k + \"-mean\"\n",
    "                wandb_eval_d[name + \"-\"+ split + \"-hist\"] = Histogram(df_grp_stats[name].tolist())\n",
    "            for k in mean_and_std:\n",
    "                name = k + \"-std\"\n",
    "                wandb_eval_d[name + \"-\" + split + \"-hist\"] = Histogram(df_grp_stats[name].tolist())\n",
    "            wandb_eval_d = merge_dicts(df_overall_metrics, wandb_eval_d)\n",
    "            wandb.log(wandb_eval_d, commit=True) \n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "        # When to log to wandb? \n",
    "            # During each epoch of training loop for train and valid set \n",
    "            # Not for ref_model. Just save to CSV \n",
    "            # Not for test set. Just save to csv \n",
    "\n",
    "        ## Append paraphrase-level dataframe to file \n",
    "        fname = f\"{self._cfg.path_run}{split}{'_ref_model' if eval_ref_model else ''}.csv\"\n",
    "        append_df_to_csv(df_expanded, path = fname)\n",
    "\n",
    "            \n",
    "    def _add_batch_vars_to_batch_d_eval(self,  raw, data, pp_l):\n",
    "        self.batch_d = merge_dicts(self.batch_d, { 'idx': raw['idx'],\n",
    "            'epoch': self.epoch, 'batch_num': self.batch_num, 'global_step': self.global_step,\n",
    "            'acc_num': self.acc_num, \"acc_batch_n_examples\": self.acc_current_n_examples, \n",
    "            \"orig_l\": raw['text'], \n",
    "            \"orig_label\": data['label'].cpu().tolist(), \n",
    "            \"orig_truelabel_probs\": data['orig_truelabel_probs'].cpu().tolist(),\n",
    "            'orig_length': self.orig_length, 'orig_batch_size': self.orig_batch_size, \n",
    "            \"pp_l\": pp_l, 'pp_length': self.pp_length, 'pp_batch_size': self.pp_batch_size\n",
    "        })\n",
    "        \n",
    "    def _compute_and_log_eval_metrics(self): \n",
    "        \"\"\"Calculate eval metrics for each split and log to wandb, then empty data_d\"\"\"\n",
    "        wandb_d = dict(epoch=self.epoch)\n",
    "        eval_splits = [\"training_step\", 'train', \"valid\"] if self.epoch != 0 else ['train', 'valid']\n",
    "        for split in eval_splits: \n",
    "            # data d -> data frame \n",
    "            self.data_d[split] = self._convert_data_d_to_df(split)\n",
    "            self._set_df_colorder(split)\n",
    "            # calc metrics \n",
    "            df = self.data_d[split][['epoch'] + self._cfg.metrics]\n",
    "            if split == \"training_step\": df = df.query(\"epoch == @self.epoch\")\n",
    "            d = df.mean()[self._cfg.metrics].to_dict()\n",
    "            wandb_d = merge_dicts(wandb_d, {f\"{k}_{split}\": v for k, v in d.items()})\n",
    "            # df append to file + empty data_d\n",
    "            append_df_to_csv(self.data_d[split], path = f\"{self._cfg.path_run}{split}.csv\")\n",
    "            self.data_d[split] = [] \n",
    "        wandb.log(wandb_d, commit=True)    \n",
    "    \n",
    "    def _set_df_colorder(self, data_d_key): \n",
    "        colorder_eval=['idx','epoch', 'orig_l',  'pp_l','orig_truelabel_probs','pp_truelabel_probs',\n",
    "        'pp_predclass_probs','orig_label','pp_predclass','label_flip', 'vm_score','sts_score', 'pp_letter_diff', 'pp_letter_percent',  \n",
    "        'contradiction_score', 'reward', 'pp_logp','ref_logp', 'kl_div', 'reward_penalty',  'reward_with_penalty','loss','batch_num',\n",
    "                       'global_step','acc_num','loss_sum', 'loss_batch', 'label_flip_fraction',\n",
    "                       'orig_length','orig_batch_size','pp_length','pp_batch_size']\n",
    "        if data_d_key == \"training_step\": \n",
    "            colorder_training_step = colorder_eval + [o for o in self.data_d['training_step'].columns if 'time_' in o]\n",
    "            assert len(set(colorder_training_step).difference(set(self.data_d[data_d_key].columns))) == 0 \n",
    "            self.data_d[data_d_key] = self.data_d[data_d_key][colorder_training_step]\n",
    "        else:\n",
    "            assert len(set(colorder_eval).difference(set(self.data_d[data_d_key].columns))) == 0 \n",
    "            self.data_d[data_d_key] = self.data_d[data_d_key][colorder_eval]\n",
    "\n",
    "    def _add_wandb_run_summary_statistics(self):\n",
    "        \"\"\"Compute test metrics for the run and log them to the wandb run summary pane. \"\"\"\n",
    "        ## Summary statistics of the test set \n",
    "        # From the last epoch atm because we don't have early stopping \n",
    "        test_metrics = self.data_d['test'].filter(self._cfg.metrics, axis=1).mean()\n",
    "        for metric, val in zip(test_metrics.index, test_metrics): \n",
    "            self.run.summary[f\"{metric}_avg_test\"] = val        \n",
    "    \n",
    "    def _get_gradient_update_norm(self):\n",
    "        total_norm = 0\n",
    "        for p in [o for o in self.pp_model.parameters() if o[1].requires_grad]:\n",
    "            if p.grad is not None:  # the embed_position layers on encoder/decoder dont keep grad ()\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        return total_norm\n",
    "    \n",
    "    def _plot_grad_flow(self, named_parameters):\n",
    "        '''Plots the gradients flowing through different layers in the net during training.\n",
    "        Can be used for checking for possible gradient vanishing / exploding problems.'''\n",
    "        plt.figure()\n",
    "        ave_grads = []\n",
    "        max_grads= []\n",
    "        layers = []\n",
    "        for n, p in named_parameters:\n",
    "            if(p.requires_grad) and (\"bias\" not in n):\n",
    "                layers.append(n)\n",
    "                ave_grads.append(p.grad.abs().mean())\n",
    "                max_grads.append(p.grad.abs().max())\n",
    "        plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "        plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "        plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "        plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(left=0, right=len(ave_grads))\n",
    "        plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"Average Gradient\")\n",
    "        plt.title(\"Gradient Flow\")\n",
    "        plt.grid(True)\n",
    "        plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                    Line2D([0], [0], color=\"b\", lw=4),\n",
    "                    Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ref_model.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{'_ref_model' if True else ''}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{\"hello\"if True else 2}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted baselines.ipynb.\n",
      "Converted baselines_analysis.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted pp_eval_baselines.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n",
      "Converted test_pp_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
