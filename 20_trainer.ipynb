{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted 35_charts.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch, wandb, gc, numpy as np, pandas as pd,os\n",
    "from wandb.data_types import Histogram\n",
    "from tqdm.auto import tqdm\n",
    "from travis_attack.utils import timecode, show_gpu\n",
    "from travis_attack.tests import check_no_nans_or_infs\n",
    "from travis_attack.models import save_pp_model, resume_pp_model, get_vm_probs, get_start_end_special_token_ids\n",
    "from travis_attack.charts import plot_grad_flow, plot_examples_chart, plot_summary_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from torch.distributions import Categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from travis_attack.utils import set_seed, set_session_options, prepare_logger\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models, get_optimizer\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b253756c445fb811\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a801003624346c4954e33bbb1d6890d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c802946231f72062\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ed50cb72ba4510aa9e841960177b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-43a49c5188c42e69\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00466590571746c0ab9f68fcd4d84f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1395a3ec96b046c19ea1e61b9c70be61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31e0419861a46d292a155f0bb36c0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab06d0c969cd48d99f61b16b15685643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7c6efb628143cb94f1185e14bfe68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa0483146749a99bcf4ccd8728b4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3f7fd42edc4aa889b781a77abecff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8d2914e3ff4ab59ba6d9f876970ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3f589aa04418fb812793ff24165dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f030a7d9c435b8a1d618cf79796c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626263a2817741ffad72b99b12e560e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f807e459ac4856b694f603fb969b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03cd4117f524e87b14512ecacb31ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b3ea7a2b734d4293fe0cdd5b0251b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7877aca1bcbf4ec1b9a61597bbe0fc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfac39c0d29464db5b7d5557643f265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496c15c37194444c8c49c2b6396cf32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1dfc8a21cc427aa8ff4a2e3de3138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bbee3cb56e4bf69166e290e40daecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c797f846a140709c6a7a1d52d31b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fed5c9f91bc460fa07dba0525694499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e4275c0f1a4d27889dd1eeda44e7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "accelerator = Accelerator()\n",
    "cfg = Config()\n",
    "cfg.device = accelerator.device\n",
    "set_seed(cfg.seed)\n",
    "set_session_options()\n",
    "logger = prepare_logger()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)\n",
    "optimizer = get_optimizer(cfg, pp_model)\n",
    "ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model)\n",
    "vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'] = accelerator.prepare(vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'])\n",
    "cfg.n_train_steps = cfg.n_train_epochs * len(ds.dld_tkn['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Trainer: \n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer, accelerator, ds,\n",
    "                logger): \n",
    "        store_attr()\n",
    "        self._cfg = self.cfg; del self.cfg;\n",
    "        self.accumulation_num,self.global_step = 0,0\n",
    "        #resume_pp_model(f\"{path_checkpoints}devout-durian-172_39\")\n",
    "        self._setup_wandb_run()\n",
    "        self._setup_data_stores()\n",
    "        if self._cfg.wandb['plot_examples']: self._setup_wandb_examples_plots()\n",
    "        self.start_end_token_d = get_start_end_special_token_ids(self.pp_tokenizer)\n",
    "        ## TODO: why is num_processes set to 1?\n",
    "        #%lprun -f training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)\n",
    "        notebook_launcher(self.training_function, args=(), \n",
    "                           num_processes=1, use_fp16=self._cfg.use_fp16)\n",
    "        \n",
    "    def _setup_wandb_run(self): \n",
    "        \"\"\"Init wandb run, set up paths, create dir for model artifacts if needed, \"\"\"\n",
    "        ## TODO: set notebook name and add in save_code \n",
    "        self.run = wandb.init(project=self._cfg.wandb['project'], entity=self._cfg.wandb['entity'], \n",
    "                              config=vars(self._cfg), mode=self._cfg.wandb['mode'],\n",
    "                              notes=self._cfg.wandb['run_notes'])\n",
    "        if self._cfg.wandb['log_grads']: \n",
    "            wandb.watch(self.pp_model, log='gradients', log_freq=self._cfg.wandb['log_grads_freq'])\n",
    "        self._cfg.run_name,self._cfg.run_id = self.run.name, self.run.id\n",
    "        self._cfg.path_run = f\"{self._cfg.path_checkpoints}{self.run.name}/\"\n",
    "        if not os.path.exists(self._cfg.path_run): os.makedirs(self._cfg.path_run, exist_ok=True)\n",
    "    \n",
    "    def _setup_data_stores(self): \n",
    "        \"\"\"Setup dict `self.data_d` to store observations. Setup column names for wandb tables.  \n",
    "         \"\"\"\n",
    "        # Raw observation data (lists of dicts, later becomes pandas df)\n",
    "        self.data_d = dict()\n",
    "        # These have to be in the keys of the output from eval_dl\n",
    "        self.table_columns = ['idx', 'orig_l',  'truelabel', 'orig_truelabel_probs', 'epoch', 'pp_l',\n",
    "                     'pp_truelabel_probs', \"pp_predclass\", \"pp_predclass_probs\"] + self._cfg.metrics\n",
    "        self.summary_table_columns = ['epoch','split'] + [f'{m}_avg' for m in self._cfg.metrics]\n",
    "        for key in self._cfg.splits + ['training_summary']:         self.data_d[key]             = [] \n",
    "        if self._cfg.wandb['log_training_step_table']:              self.data_d['training_step'] = []\n",
    "    \n",
    "    def _setup_wandb_examples_plots(self): \n",
    "        \"\"\"If we plot a few examples this sets that up.\"\"\"\n",
    "        def get_examples_plot_idxs(dataset): \n",
    "            \"\"\"Get data indices for the examples plots\"\"\"\n",
    "            return np.random.choice(dataset['idx'], size=self._cfg.wandb['n_examples_plot'], replace=False).tolist()\n",
    "        self.plt_idx_d = dict()\n",
    "        for split in self._cfg.splits:  self.plt_idx_d[split] = get_examples_plot_idxs(self.ds.dsd[split])\n",
    "\n",
    "    def training_function(self): \n",
    "        self.logger.debug(show_gpu(f'GPU memory usage after loading models:'))\n",
    "        progress_bar = tqdm(range(self._cfg.n_train_steps))\n",
    "        self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none) \n",
    "        for self.epoch in range(self._cfg.n_train_epochs): \n",
    "            self.logger.info(f\"Now on epoch {self.epoch} of {self._cfg.n_train_epochs}\")\n",
    "            if not self.pp_model.training: self.pp_model.train()\n",
    "            with timecode() as time_train_one_epoch:\n",
    "                for self.batch_num, (data, raw) in enumerate(zip(self.ds.dld_tkn['train'], self.ds.dld_raw['train'])): \n",
    "                    self.training_step(data, raw) \n",
    "                    self.accumulation_num += 1  ; self.global_step += 1 ;  progress_bar.update(1) \n",
    "                    \n",
    "            wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                       'time/train_one_epoch_thoroughput': len(self.ds.dsd_tkn['train']) / time_train_one_epoch.t,\n",
    "                       'epoch': self.epoch}, commit=True)\n",
    "\n",
    "            if self._cfg.wandb['log_grads'] and self.epoch % self._cfg.wandb_log_grads_freq == 0: \n",
    "                plt = plot_grad_flow(self.pp_model.named_parameters())\n",
    "                wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "                del plt \n",
    "            #gc.collect() \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if self._cfg.save_model_while_training and (self.epoch + 1) % self._cfg.save_model_freq == 0:  save_model(epoch)\n",
    "\n",
    "            # Evaluation loop\n",
    "            if self.epoch % self._cfg.eval_freq == 0: \n",
    "                with timecode() as time_eval_train:\n",
    "                    train_set_preds = self.eval_dl(dl_tkn=self.ds.dld_tkn['train_eval'], \n",
    "                                                   dl_raw=self.ds.dld_raw['train_eval'])\n",
    "\n",
    "                with timecode() as time_eval_valid:\n",
    "                    valid_set_preds = self.eval_dl(dl_tkn=self.ds.dld_tkn['valid'],\n",
    "                                                   dl_raw=self.ds.dld_raw['valid'])\n",
    "\n",
    "                # update the tables every epoch and log them\n",
    "                with timecode() as time_update_training_summary_table:\n",
    "                    self.update_training_summary_table(train_set_preds, split='train')\n",
    "                    self.update_training_summary_table(valid_set_preds, split='valid')\n",
    "                with timecode() as time_add_eval_preds_to_data_d:    \n",
    "                    self.add_preds_to_data_d(train_set_preds, split='train')\n",
    "                    self.add_preds_to_data_d(valid_set_preds, split='valid')\n",
    "                self.plot_wandb_charts()\n",
    "                del train_set_preds, valid_set_preds\n",
    "                with timecode() as time_eval_gc_collect:\n",
    "                    gc.collect() \n",
    "                with timecode() as time_eval_empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                           'time/eval_train_thoroughput': len(self.ds.dsd_tkn['train']) / time_eval_train.t,\n",
    "                           'time/eval_valid_thoroughput': len(self.ds.dsd_tkn['valid']) / time_eval_valid.t,\n",
    "                           'time/eval_update_training_summary_table': time_update_training_summary_table.t, \n",
    "                           'time/eval_add_preds_to_data_d': time_add_eval_preds_to_data_d.t,\n",
    "                           'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                           'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "                   'epoch': self.epoch}, commit=True)\n",
    "        # Eval on test set \n",
    "        test_set_preds = self.eval_dl(dl_tkn = self.ds.dld_tkn['test'], dl_raw=self.ds.dld_raw['test'])\n",
    "        self.add_preds_to_data_d(test_set_preds, split='test')\n",
    "\n",
    "        # Data -> df and save dfs to file \n",
    "        for key in self.data_d.keys():  # splits and sometimes 'training_step' too \n",
    "            self.data_d[key] = pd.DataFrame(self.data_d[key]) # dict of list of dict -> dict of dataframe\n",
    "            self.data_d[key].to_csv(f\"{self._cfg.path_run}{key}.csv\", index=False)\n",
    "        # Save training_summary table to csv too \n",
    "        pd.DataFrame(self.data_d['training_summary']).to_csv(f\"{self._cfg.path_run}training_summary.csv\", index=False)\n",
    "\n",
    "        # plot_wandb_charts()  # don't think I need this\n",
    "        self.add_wandb_run_summary_statistics()     \n",
    "        \n",
    "        self.run.finish()\n",
    "        \n",
    "    def training_step(self, data, raw): \n",
    "        \"\"\"Forward pass, loss function, backwards pass, parameter update (with gradient accumulation optional), \n",
    "        recording results, wandb logging. \n",
    "        \"\"\"\n",
    "        with timecode() as time_generate_pp:\n",
    "            pp_output, pp_l = self.pp_model_forward(data)\n",
    "\n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after forward pass: '))\n",
    "\n",
    "        with self.accelerator.autocast():\n",
    "            with timecode() as time_loss_fn:\n",
    "                if self._cfg.wandb['log_training_step_table']: \n",
    "                    results_d = self.loss_fn(data, raw, pp_output, pp_l, return_components=True)\n",
    "                    loss_batch = results_d['loss_batch']\n",
    "                else: \n",
    "                    loss_batch = self.loss_fn(data, raw, pp_output, pp_l, return_components=False)\n",
    "\n",
    "            loss_batch = loss_batch / self._cfg.accumulation_steps  # Normalize our loss for gradient accumulation\n",
    "\n",
    "        with timecode() as time_backwards:\n",
    "            self.accelerator.backward(loss_batch) \n",
    "\n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after backwards pass: '))\n",
    "        if (self.accumulation_num + 1) % self._cfg.accumulation_steps == 0: \n",
    "            with timecode() as time_opt_step:\n",
    "                self.optimizer.step()\n",
    "            self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none)\n",
    "        if self._cfg.wandb['log_training_step_table']: \n",
    "            with timecode() as time_add_to_training_step_table:\n",
    "                results_d = self.process_results_d1(results_d, raw)\n",
    "                self.add_preds_to_data_d(results_d, split='training_step') \n",
    "\n",
    "        wandb.log({'time/generate_pp': time_generate_pp.t, 'time/loss_fn': time_loss_fn.t, \n",
    "                   'time/backwards_pass': time_backwards.t, 'time/optimizer_step': time_opt_step.t, \n",
    "                   'time/add_to_training_step_table': time_add_to_training_step_table.t, \n",
    "                   'epoch': self.epoch, 'global_step': self.global_step,'batch_num': self.batch_num,\n",
    "                   'orig_length': self.orig_length,'orig_batch_size': self.orig_batch_size,\n",
    "                  'pp_length': self.pp_length, 'pp_batch_size': self.pp_batch_size}\n",
    "                  ,commit=False)\n",
    "        \n",
    "    def pp_model_forward(self, data): \n",
    "        pp_output, pp_l = self.get_paraphrases(data['input_ids'], data['attention_mask'])\n",
    "        self._assert_start_and_end_tokens_are_correct(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        self._update_batch_size_and_length_variables( orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _assert_start_and_end_tokens_are_correct(self, orig_ids, pp_ids):\n",
    "        \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "        right special tokens (depends on tokenizer)\"\"\"\n",
    "        # Input\n",
    "        if self.start_end_token_d['input_start_id'] is not None: \n",
    "            assert torch.all(orig_ids[:,0] == self.start_end_token_d['input_start_id'])\n",
    "        # can probs rewrite this to make it nicer but it's fine for now\n",
    "        assert torch.all(torch.logical_or(orig_ids[:,-1] == self.start_end_token_d['input_end_id'][0], \n",
    "                                          orig_ids[:,-1] == self.start_end_token_d['input_end_id'][1]))\n",
    "\n",
    "        # Output\n",
    "        assert torch.all(pp_ids[:,0] == self.start_end_token_d['output_start_id'])\n",
    "        assert torch.all(torch.logical_or(pp_ids[:,-1] == self.start_end_token_d['output_end_id'][0], \n",
    "                                          pp_ids[:,-1] == self.start_end_token_d['output_end_id'][1]))\n",
    "        \n",
    "    def _update_batch_size_and_length_variables(self, orig_ids, pp_ids): \n",
    "        # Update variables\n",
    "        # for greedy search self.pp_length is equal to self.orig_batch_size but this won't be for beam search\n",
    "        self.orig_batch_size     = orig_ids.shape[0]\n",
    "        self.orig_length         = orig_ids.shape[1]\n",
    "        self.pp_batch_size       = pp_ids.shape[0]\n",
    "        self.pp_length           = pp_ids.shape[1] \n",
    "    \n",
    "    def get_paraphrases(self, orig_ids, attention_mask):\n",
    "        \"\"\"Wrapper for generating paraphrases (pp's).  Only greedy search supported at the moment\"\"\"\n",
    "        pp_output = self.pp_model.generate_with_grad(input_ids=orig_ids, \n",
    "                                                attention_mask=attention_mask, \n",
    "                                                 **self._cfg.pp,\n",
    "                                                 do_sample=False, \n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 output_scores=True,\n",
    "                                                    remove_invalid_values=False, \n",
    "                                                 pad_token_id = self.pp_tokenizer.pad_token_id,\n",
    "                                                 eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "        pp_l = self.pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def loss_fn(self, data, raw, pp_output, pp_l, return_components=False): \n",
    "        with timecode() as time_reward_fn:\n",
    "            d = self.reward_fn(data, raw, pp_l, return_components=return_components)\n",
    "\n",
    "        if self._cfg.normalise_rewards: \n",
    "            d['orig_reward'] = copy.deepcopy(d['reward'])\n",
    "            d['reward'] = (d['reward']-torch.mean(d['reward']))/torch.std(d['reward'])\n",
    "\n",
    "        with timecode() as time_pp_logp:\n",
    "            d['pp_logp'] = self.get_pp_logp(pp_output)\n",
    "\n",
    "        with timecode() as time_loss_fn_loss_calc:\n",
    "            d['loss'] = -d['reward'] * d['pp_logp']\n",
    "            d['loss_batch'] = torch.mean(d['loss'])\n",
    "            if return_components ==  False: return d['loss_batch'] \n",
    "\n",
    "        # remove some items from compgraph\n",
    "        with timecode() as time_loss_fn_detach:\n",
    "            d['pp_logp'] = d['pp_logp'].detach()  \n",
    "            d['loss']    = d['loss'].detach()\n",
    "\n",
    "        if self.pp_model.training:    \n",
    "            pp_logp_cpu = d['pp_logp'].cpu()\n",
    "            wandb.log({'epoch': self.epoch, 'global_step': self.global_step, \n",
    "                       'time/reward_fn': time_reward_fn.t, 'time/pp_logp': time_pp_logp.t, \n",
    "                      'time/loss_fn_loss_calc': time_loss_fn_loss_calc.t, \n",
    "                       'time/pp_logp_detach': time_loss_fn_detach.t, \n",
    "                       'pp_logp': Histogram(pp_logp_cpu),\n",
    "                       'pp_logp_avg': pp_logp_cpu.mean(),\n",
    "                       'loss_examples': Histogram(d['loss'].cpu()),\n",
    "                       'loss_batch': d['loss_batch'].detach().cpu()\n",
    "                      }, \n",
    "                     commit=False)\n",
    "        return d\n",
    "    \n",
    "    def reward_fn(self, data, raw, pp_l, return_components=False): \n",
    "        \"\"\"orig_l, pp_l are lists of original and paraphrase respectively\"\"\"\n",
    "        # Victim model probability differences between orig and pp\n",
    "        with timecode() as time_vm_scores:\n",
    "            pp_probs = get_vm_probs(pp_l, self._cfg, self.vm_tokenizer, self.vm_model, return_predclass=False)\n",
    "            pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "            pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "            pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "            label_flip = ((pp_predclass != data['label']) * 1)\n",
    "            vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "            \n",
    "\n",
    "        # STS scores\n",
    "        with timecode() as time_sts_scores:\n",
    "            pp_embeddings  = self.sts_model.encode(pp_l, batch_size=len(raw), convert_to_tensor=True, device=self._cfg.device)\n",
    "            # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "            sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "\n",
    "        # Reward calculation \n",
    "        rewards = torch.tensor([-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)],device=self._cfg.device)\n",
    "        \n",
    "        if self.pp_model.training:\n",
    "            rewards_detached = rewards.detach().cpu()\n",
    "            label_flip_fraction = label_flip.float().detach().cpu().mean()\n",
    "            wandb.log({'epoch': self.epoch, 'global_step': self.global_step, \n",
    "                       'time/vm_scores': time_vm_scores.t, 'time/sts_scores': time_sts_scores.t,\n",
    "                       'vm_scores': Histogram(vm_scores.detach().cpu()), \n",
    "                       'sts_scores': Histogram(sts_scores.detach().cpu()),\n",
    "                       'rewards': Histogram(rewards_detached),\n",
    "                       'rewards_mean': rewards_detached.mean(), \n",
    "                       'label_flip_fraction': label_flip_fraction,\n",
    "                      }, commit=False)\n",
    "\n",
    "        ## TODO: with a class we can just keep all these variables in self, refactor? then maybe you can log them \n",
    "        #  with wandb histogram?\n",
    "        if return_components: \n",
    "            return {\n",
    "                \"orig_l\": raw['text'],\n",
    "                \"pp_l\": pp_l,  \n",
    "                \"truelabel\": data['label'],\n",
    "                \"orig_truelabel_probs\": data['orig_truelabel_probs'],\n",
    "                \"pp_truelabel_probs\":  pp_truelabel_probs,\n",
    "                \"pp_predclass\": pp_predclass,\n",
    "                \"pp_predclass_probs\": pp_predclass_probs,\n",
    "                \"vm_score\": vm_scores, \n",
    "                \"sts_score\": sts_scores,\n",
    "                \"reward\": rewards,\n",
    "                \"label_flip\": label_flip\n",
    "            }\n",
    "        else:  return {\"reward\": rewards}\n",
    "        \n",
    "    def get_pp_logp(self, pp_output): \n",
    "        \"\"\"log(p(pp|orig)) basically.\n",
    "        works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "        ### TODO: this looks like logp to me, not plogp. Find out if this is right and if so rename, if not, fix\n",
    "        ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "        # and has no probability attached to it, so we remove it. \n",
    "        seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "        assert seq_without_first_tkn.shape == torch.Size([self.orig_batch_size, self.pp_length - 1])\n",
    "\n",
    "        ### Convert from tuple of scores to one big tensor of scores \n",
    "        scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "        ### TESTS \n",
    "        # We check shape and that there is no +inf or nan in scores. \n",
    "        # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "        assert scores_stacked.shape == torch.Size([self.orig_batch_size, (self.pp_length - 1), self._cfg.vocab_size])\n",
    "        assert torch.all(~torch.isnan(scores_stacked))\n",
    "        assert torch.all(~torch.isposinf(scores_stacked))\n",
    "        # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "        # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "        # shouldn't be set to -inf\n",
    "        # Not a 100% test but very likely to identify\n",
    "        idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "        assert len(idx_neginf[idx_neginf[:,2] == self.pp_tokenizer.eos_token_id, :]) == \\\n",
    "                  (self._cfg.pp[\"min_length\"] -1) * self.orig_batch_size  \n",
    "        del idx_neginf\n",
    "\n",
    "        ### Take log softmax of scores and then extract those that correspond \n",
    "        # to the generated sequences    \n",
    "        scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "        seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "        ### TESTS \n",
    "        # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "        assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "        assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "        self._check_scores_log_softmax_sums(scores_log_softmax)\n",
    "        # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "        assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "        # Check that the last token probability corresponds to a possible end token\n",
    "        # this has to be tested before the attention mask is multiplied with it because if the \n",
    "        # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "        output_end_ids = self.start_end_token_d['output_end_id']\n",
    "        assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "        del output_end_ids\n",
    "        ## THIS ONE IS LONG - a test rather than assert \n",
    "        # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "        #                                             seq_token_log_probs) \n",
    "\n",
    "        ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "        # sequence probabilities so that we don't consider probability of padding tokens \n",
    "        # when getting sequence probabilities. \n",
    "        # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "        # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "        # since  -inf * 0 = nan \n",
    "        attention_mask = self.pp_model._prepare_attention_mask_for_generation(\n",
    "            seq_without_first_tkn, self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id\n",
    "        )\n",
    "        seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-10000)\n",
    "        seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "        ### TESTS\n",
    "        assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "        # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "        assert all(seq_without_first_tkn[attention_mask == 0] == self.pp_tokenizer.pad_token_id)\n",
    "        check_no_nans_or_infs(seq_token_log_probs)\n",
    "        # check that we aren't picking extrememly rare tokens\n",
    "        assert torch.all(seq_token_log_probs  > -10)  \n",
    "\n",
    "        ### Get sequence probabilities by summing up token log probabilities \n",
    "        seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "        ## TESTS \n",
    "        assert seq_log_prob.shape == torch.Size([self.pp_batch_size])\n",
    "        check_no_nans_or_infs(seq_log_prob)\n",
    "        \n",
    "        if self.pp_model.training:  # don't bother logging or calculate entropy, token_probs in eval mode\n",
    "            if self._cfg.wandb['log_token_entropy']:\n",
    "                with timecode() as time_log_entropy:\n",
    "                    ent_d = self._get_entropy_metrics(scores_stacked, attention_mask)\n",
    "                ent_d['time/log_entropy'] = time_log_entropy.t\n",
    "                wandb.log(ent_d, commit=False)\n",
    "\n",
    "            if self._cfg.wandb['log_token_probabilities']: \n",
    "                with timecode() as time_log_token_probabilities:\n",
    "                    token_prob_d = self._get_token_probability_metrics(scores_log_softmax, attention_mask, k=3)\n",
    "                token_prob_d['time/log_token_probabilities'] = time_log_token_probabilities.t\n",
    "                wandb.log(token_prob_d, commit=False)\n",
    "        return seq_log_prob\n",
    "   \n",
    "    def _check_scores_log_softmax_sums(self, scores_log_softmax):\n",
    "        sums = scores_log_softmax.exp().sum(2)\n",
    "        # check that the axes is right\n",
    "        # we want to sum over token probabilities at each generation step, so we \n",
    "        # should end up with a shape [self.orig_batch_size, self.pp_length]\n",
    "        assert sums.shape[0] == self.orig_batch_size  \n",
    "        assert sums.shape[1] == self.pp_length - 1\n",
    "        # check that they sum to 1 along the self.pp_length axis\n",
    "        assert torch.allclose(sums, torch.ones(sums.size(), device=self._cfg.device), atol = 1e-4)\n",
    "\n",
    "    def _check_seq_token_log_prob_values_are_correct(self, seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "        \"\"\"Just enumerates and checks values\n",
    "        Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i_ex in range(self.orig_batch_size):\n",
    "            for i_step in range(self.pp_length - 1):\n",
    "                i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "                l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "        assert all(l)    \n",
    "    \n",
    "    def _get_entropy_metrics(self, scores_stacked, attention_mask): \n",
    "        ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "        assert ent.shape == attention_mask.shape == torch.Size([self.pp_batch_size, self.pp_length - 1])\n",
    "        ent = ent * attention_mask  # stop values after eos token from contributing to ent score \n",
    "        # first remove structure (otherwise we have ragged arrays), then remove corresponding attention mask values\n",
    "        # we can't just filter by ent[ent != 0] because we might have zero tokens during the sequence\n",
    "        att_flat= attention_mask.flatten()\n",
    "        indices = torch.nonzero(att_flat)\n",
    "        ent_flat = ent.flatten()[indices].flatten()\n",
    "        assert ent_flat.shape[0] == (torch.sum(att_flat)*1).item()\n",
    "        # check everything we filter out is zero \n",
    "        torch.isclose(ent.flatten()[torch.nonzero(~(att_flat > 0))].sum(), torch.tensor(0.), 1e-3)\n",
    "        ## TODO: can we use WandB histogram to make this easier?\n",
    "        ent_d = dict(\n",
    "      #      ent_min             = ent_flat.quantile(0).item(),\n",
    "            ent_lower_quartile  = ent_flat.quantile(0.25).item(), \n",
    "            ent_median          = ent_flat.median().item(), \n",
    "      #      ent_mean            = ent_flat.mean().item(), \n",
    "            ent_upper_quartile  = ent_flat.quantile(0.75).item(), \n",
    "      #      ent_max             = ent_flat.quantile(1).item(),   # skews the graph\n",
    "            epoch=self.epoch, global_step=self.global_step\n",
    "        )\n",
    "        return ent_d\n",
    "\n",
    "    def _get_token_probability_metrics(self, scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, _ = torch.topk(scores_log_softmax, largest=True, k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  \n",
    "        assert tkn_kmaxprob.shape == torch.Size([self.pp_batch_size, self.pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [0.1, 0.01, 0.001, 0.0001, 0.00001]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/self._cfg.vocab_size)) / allprobs.shape[0]).item()\n",
    "\n",
    "        token_prob_d['epoch']       = self.epoch\n",
    "        token_prob_d['global_step'] = self.global_step\n",
    "        return token_prob_d\n",
    "\n",
    "    def process_results_d1(self, results_d, raw): \n",
    "        \"\"\"Reconfigure results_d so it can be stored easily\"\"\"\n",
    "        ## TODO: do you need this?\n",
    "        results_d['epoch'] = self.epoch\n",
    "        results_d['idx']   = raw['idx']\n",
    "        for k,v in results_d.items(): \n",
    "            if torch.is_tensor(v): \n",
    "                results_d[k] = v.detach().cpu().tolist()\n",
    "            elif type(v) == int or type(v) == float: \n",
    "                # make into list repeated n times\n",
    "                results_d[k] = [v for o in range(self._cfg.batch_size_train)]\n",
    "        return results_d    \n",
    "    \n",
    "    def process_results_d_for_wandb(self,results_d): \n",
    "        ## TODO: figure out what to do with this and process_results_d1. \n",
    "        ## Also rename this. To me it looks like it changes types, shapes, but why?\n",
    "        # If you convert the summary plots to simple wandb charts, can you get rid of this?\n",
    "        \n",
    "        # Flatten batches for each key, depending on datatype (e.g. lists of lists )\n",
    "        for k,v in results_d.items(): \n",
    "            # v[0] is arbitrary - we are just checking the first item in the list to see the type\n",
    "            if type(v) == float or type(v) == int: \n",
    "                next\n",
    "            elif  torch.is_tensor(v[0]): \n",
    "                # case where we have a list of scalars - the cat function doesn't work here \n",
    "                if  v[0].size() == torch.Size([]): x = torch.stack(v)\n",
    "                else:                              x = torch.cat(v)\n",
    "                results_d[k] = x.detach().cpu().squeeze().tolist()  # convert to list (squeeze is for single scalar list)\n",
    "            elif type(v[0]) == list:  # this is True for tensors also, so it has to go after the is_tensor check\n",
    "                results_d[k] = list(itertools.chain(*v)) \n",
    "            elif type(v) == list: \n",
    "                next\n",
    "            else: \n",
    "                raise Exception(\"shouldn't get here\")\n",
    "        return results_d\n",
    "    \n",
    "    def eval_dl(self, dl_tkn, dl_raw): \n",
    "        \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "        # Put models in eval mode and do the forward pass \n",
    "        # Current logic: push all batches together into one big list.     \n",
    "        if self.pp_model.training: self.pp_model.eval()\n",
    "        if self.vm_model.training: self.vm_model.eval()\n",
    "        results_d = defaultdict(list)\n",
    "        with torch.no_grad(): \n",
    "            for eval_batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "                self.logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loading data: '))\n",
    "                for k, v in data.items(): \n",
    "                    if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "\n",
    "                pp_output, pp_l = self.pp_model_forward(data)\n",
    "                d = self.loss_fn(data, raw, pp_output, pp_l, return_components=True)\n",
    "                self.logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "                d['idx'] = raw['idx']\n",
    "\n",
    "                for k,v in d.items(): \n",
    "                    results_d[k].append(v) \n",
    "        del eval_batch_num, data, raw, pp_output, pp_l, d\n",
    "        results_d = self.process_results_d_for_wandb(results_d)\n",
    "        results_d['epoch'] = self.epoch\n",
    "        return results_d\n",
    "    \n",
    "    def add_preds_to_data_d(self, results_d, split):\n",
    "        assert split in self.data_d.keys() or split == \"training_summary\"\n",
    "        d1 = results_d\n",
    "        d1['epoch'] = [self.epoch for i in range(len(d1['pp_l']))]\n",
    "        dcols = [d1[c] for c in self.table_columns]  # filter out loss_batch\n",
    "        assert len(set([len(o) for o in dcols])) == 1  # all lists should be of the same length \n",
    "        for row in zip(*dcols):\n",
    "            d2 = {k:v for k,v in zip(self.table_columns,row)}\n",
    "            self.data_d[split].append(d2)\n",
    "            \n",
    "    def update_training_summary_table(self, results_d, split):\n",
    "        d = dict()\n",
    "        # key names here have to match those in summary_table_columns\n",
    "        ## TODO: either write assert for this or figure out if you need this table (can you plot it instead?)\n",
    "        d['epoch'] = self.epoch\n",
    "        d['split'] = split\n",
    "        for metric in self._cfg.metrics:\n",
    "            d[f'{metric}_avg'] = np.mean(results_d[metric])\n",
    "        self.data_d['training_summary'].append(d)\n",
    "        \n",
    "    def plot_wandb_charts(self): \n",
    "        ## TODO: rename to indicate it only plots summary and examples charts \n",
    "        ## Can you refactor into regular wandb charts?\n",
    "        if self._cfg.wandb['plot_examples']: \n",
    "            # Examples charts \n",
    "            for split in ['train', 'valid']:\n",
    "                df = pd.DataFrame(data_d[split]) if type(self.data_d[split]) is list else self.data_d[split]\n",
    "                df = df.query(\"idx in @plt_idx_d[@split]\").sort_values(['idx', 'epoch'])\n",
    "                for metric in self._cfg.metrics: \n",
    "                    chart = plot_examples_chart(split, table=wandb.Table(dataframe=df), metric=metric)\n",
    "                    wandb.log({f\"individual_examples/{split}_{metric}_vs_epoch_examples\": chart}, commit=False)  \n",
    "                    \n",
    "        ## Summary charts \n",
    "        for metric in self._cfg.metrics: \n",
    "            df = pd.DataFrame(self.data_d['training_summary'])\n",
    "            chart = plot_summary_chart(metric, table=wandb.Table(dataframe=df))\n",
    "            wandb.log({f\"summary_charts/avg_{metric}_vs_epoch\": chart}, \n",
    "                      commit=True if metric == self._cfg.metrics[len(self._cfg.metrics)-1] else False)\n",
    "\n",
    "    def add_wandb_run_summary_statistics(self):\n",
    "        \"\"\"Compute final performance metrics for the run and log them to the wandb run summary pane. \"\"\"\n",
    "        df_summary = pd.DataFrame(self.data_d['training_summary']) \n",
    "        # We calculate the best epoch according to the validation set\n",
    "        best_epoch_idx = df_summary.query(\"split=='valid'\")['loss_avg'].idxmin() \n",
    "        valid_row = df_summary.iloc[best_epoch_idx]\n",
    "        best_epoch = valid_row['epoch'].item()\n",
    "        self.run.summary['best_epoch'] = best_epoch\n",
    "        # iloc transforms 1row df to series (so it is same as  valid_row)\n",
    "        train_row = df_summary.query(\"split=='train' & epoch==@best_epoch\").iloc[0]  \n",
    "        for metric in self._cfg.metrics: \n",
    "            self.run.summary[f\"{metric}_avg_train\"] = train_row[f\"{metric}_avg\"].item()\n",
    "            self.run.summary[f\"{metric}_avg_valid\"] = valid_row[f\"{metric}_avg\"].item()\n",
    "\n",
    "        ## Summary statistics of the test set \n",
    "        # From the last epoch atm because we don't have early stopping \n",
    "        test_metrics = self.data_d['test'].filter(self._cfg.metrics, axis=1).mean()\n",
    "        for metric, val in zip(test_metrics.index, test_metrics): \n",
    "            self.run.summary[f\"{metric}_avg_test\"] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb492db739334aabad4e7da45d64e526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 0 of 4\n",
      "Now on epoch 1 of 4\n",
      "Now on epoch 2 of 4\n",
      "Now on epoch 3 of 4\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer,\n",
    "                  accelerator, ds, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>split</th>\n",
       "      <th>loss_avg</th>\n",
       "      <th>pp_logp_avg</th>\n",
       "      <th>reward_avg</th>\n",
       "      <th>vm_score_avg</th>\n",
       "      <th>sts_score_avg</th>\n",
       "      <th>label_flip_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1.209682</td>\n",
       "      <td>-2.435188</td>\n",
       "      <td>0.479609</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>0.906750</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.927716</td>\n",
       "      <td>-2.088127</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>0.977944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.917068</td>\n",
       "      <td>-2.078872</td>\n",
       "      <td>0.437184</td>\n",
       "      <td>-0.063507</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.822364</td>\n",
       "      <td>-1.847250</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>0.977944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>0.788127</td>\n",
       "      <td>-1.788181</td>\n",
       "      <td>0.437184</td>\n",
       "      <td>-0.063507</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.725989</td>\n",
       "      <td>-1.627906</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>0.977944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>0.675476</td>\n",
       "      <td>-1.534069</td>\n",
       "      <td>0.437184</td>\n",
       "      <td>-0.063507</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.641082</td>\n",
       "      <td>-1.435201</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>0.977944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  split  loss_avg  pp_logp_avg  reward_avg  vm_score_avg  \\\n",
       "0      0  train  1.209682    -2.435188    0.479609     -0.001434   \n",
       "1      0  valid  0.927716    -2.088127    0.437600     -0.063277   \n",
       "2      1  train  0.917068    -2.078872    0.437184     -0.063507   \n",
       "3      1  valid  0.822364    -1.847250    0.437600     -0.063277   \n",
       "4      2  train  0.788127    -1.788181    0.437184     -0.063507   \n",
       "5      2  valid  0.725989    -1.627906    0.437600     -0.063277   \n",
       "6      3  train  0.675476    -1.534069    0.437184     -0.063507   \n",
       "7      3  valid  0.641082    -1.435201    0.437600     -0.063277   \n",
       "\n",
       "   sts_score_avg  label_flip_avg  \n",
       "0       0.906750             0.0  \n",
       "1       0.977944             0.0  \n",
       "2       0.984991             0.0  \n",
       "3       0.977944             0.0  \n",
       "4       0.984991             0.0  \n",
       "5       0.977944             0.0  \n",
       "6       0.984991             0.0  \n",
       "7       0.977944             0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.data_d['training_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-23-235c2c2edc25>\u001b[0m(263)\u001b[0;36mreward_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    261 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    262 \u001b[0;31m            \u001b[0mrewards_detached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 263 \u001b[0;31m            \u001b[0mlabel_flip_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_flip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    264 \u001b[0;31m            wandb.log({'epoch': self.epoch, 'global_step': self.global_step, \n",
      "\u001b[0m\u001b[0;32m    265 \u001b[0;31m                       \u001b[0;34m'time/vm_scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_vm_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time/sts_scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_sts_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> label_flip.float32()\n",
      "*** AttributeError: 'Tensor' object has no attribute 'float32'\n",
      "ipdb> label_flip.float()\n",
      "tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted 35_charts.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
