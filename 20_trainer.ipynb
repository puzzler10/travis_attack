{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, wandb, gc, numpy as np, pandas as pd,os\n",
    "from wandb.data_types import Histogram\n",
    "from tqdm.auto import tqdm\n",
    "from travis_attack.utils import (timecode, show_gpu, merge_dicts, unpack_nested_lists_in_df, \n",
    "                                 display_all, append_df_to_csv)\n",
    "from travis_attack.tests import check_no_nans_or_infs\n",
    "from travis_attack.models import save_pp_model, resume_pp_model, get_vm_probs, get_start_end_special_token_ids\n",
    "from travis_attack.charts import plot_grad_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from torch.distributions import Categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from travis_attack.utils import set_seed, set_session_options, prepare_logger, setup_logging\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models, get_optimizer\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from fastcore.basics import store_attr\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from snoop import pp\n",
    "import snoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentence_transformers.SentenceTransformer: INFO     Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "sentence_transformers.SentenceTransformer: INFO     Did not find folder sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "sentence_transformers.SentenceTransformer: INFO     Search model on server: http://sbert.net/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.zip\n",
      "sentence_transformers.SentenceTransformer: INFO     Load SentenceTransformer from folder: /home/tproth/.cache/torch/sentence_transformers/sbert.net_models_sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2\n",
      "sentence_transformers.SentenceTransformer: INFO     Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "cfg = Config()\n",
    "if cfg.dataset_name == \"rotten_tomatoes\": cfg = cfg.small_ds()\n",
    "set_seed(cfg.seed)\n",
    "set_session_options()\n",
    "#logger = prepare_logger()\n",
    "setup_logging(cfg, disable_other_loggers=False)\n",
    "logger = logging.getLogger(\"travis_attack.trainer\")\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)\n",
    "optimizer = get_optimizer(cfg, pp_model)\n",
    "ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model, load_processed_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Trainer: \n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer, \n",
    "                ds, initial_eval=True, log_code=True): \n",
    "        store_attr()\n",
    "        self._cfg = self.cfg; del self.cfg;\n",
    "        self.epoch,self.acc_num,self.global_step,self.eval_num = 0,0,0,0\n",
    "        self._reset_batch_dicts()\n",
    "        #resume_pp_model(f\"{path_checkpoints}devout-durian-172_39\")\n",
    "        self._setup_data_stores()\n",
    "        self._setup_gradient_accumulation_variables()\n",
    "        self.start_end_token_d = get_start_end_special_token_ids(self.pp_tokenizer)\n",
    "        \n",
    "    def train(self): \n",
    "        self._setup_wandb_run()\n",
    "        ## we set num_processes=1 because we are running on 1 GPU only and we must specify the argument \n",
    "        #%lprun -f _training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(_training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)\n",
    "        notebook_launcher(self._training_function, args=(), \n",
    "                           num_processes=1, use_fp16=self._cfg.use_fp16)\n",
    "    \n",
    "    def _reset_batch_dicts(self): \n",
    "        # train_batch_d holds all info to write to csv, time_d has times, wandb_d has everything to log to wandb\n",
    "        # there will be overlap between them. \n",
    "        self.batch_d,self.batch_time_d,self.batch_wandb_d = dict(),dict(),dict()\n",
    "    \n",
    "    def _setup_wandb_run(self): \n",
    "        \"\"\"Init wandb run, set up paths, create dir for model artifacts if needed, \"\"\"\n",
    "        self.run = wandb.init(project=self._cfg.wandb['project'], entity=self._cfg.wandb['entity'], \n",
    "                              config=vars(self._cfg), mode=self._cfg.wandb['mode'],\n",
    "                              notes=self._cfg.wandb['run_notes'], save_code=True)\n",
    "        if self._cfg.wandb['log_grads']: \n",
    "            wandb.watch(self.pp_model, log='gradients', log_freq=self._cfg.wandb['log_grads_freq'])\n",
    "        self._cfg.run_name,self._cfg.run_id = self.run.name, self.run.id\n",
    "        self._cfg.path_run = f\"{self._cfg.path_checkpoints}{self.run.name}/\"\n",
    "        if not os.path.exists(self._cfg.path_run): os.makedirs(self._cfg.path_run, exist_ok=True)\n",
    "        if self.log_code: self.run.log_code(\".\")\n",
    "        \n",
    "    def _setup_data_stores(self): \n",
    "        \"\"\"Setup dict `self.data_d` to store observations. Setup column names for wandb tables. \"\"\"\n",
    "        # Raw observation data (lists of dicts, later becomes pandas df)\n",
    "        self.data_d = dict()\n",
    "        for split in self._cfg.splits + ['training_step']:   self.data_d[split] = [] \n",
    "            \n",
    "    def _setup_gradient_accumulation_variables(self):\n",
    "        \"\"\"acc_global_l is a list of all batch sizes encountered during training. \n",
    "            \"\"\"\n",
    "        self.acc_global_l = self._cfg.dl_batch_sizes['train'] * self._cfg.n_train_epochs\n",
    "        assert len(self.acc_global_l) ==  self._cfg.n_train_steps\n",
    "        # Check if there will be leftover batches\n",
    "        self._cfg.acc_leftover_batches =  self._cfg.n_train_steps % self._cfg.acc_steps\n",
    "        if self._cfg.acc_leftover_batches != 0: \n",
    "            msg = f\"Config set to do gradient accumulation every {self._cfg.acc_steps} batches, and there are \\\n",
    "            {self._cfg.n_train_steps} total training steps, so there will be {self._cfg.acc_leftover_batches} batches at \\\n",
    "            the end that will not be trained on.\"\n",
    "            warnings.warn(msg)\n",
    "        self._reset_acc_lists()\n",
    "\n",
    "    def _reset_acc_lists(self):\n",
    "        \"\"\"call this at start and every time you call opt step\"\"\"\n",
    "        # acc_current_l is a list of the batch sizes in the current accumulation batch.\n",
    "        last_step = (self._cfg.n_train_steps - 1) - self._cfg.acc_leftover_batches\n",
    "        if self.global_step == 0:   # at start of training\n",
    "            self.acc_current_l = self.acc_global_l[self.global_step:self._cfg.acc_steps]\n",
    "            assert len(self.acc_current_l) == self._cfg.acc_steps\n",
    "        else: \n",
    "            self.acc_current_l = self.acc_global_l[(self.global_step+1):(self.global_step+self._cfg.acc_steps+1)]\n",
    "            if self.global_step == last_step:  assert len(self.acc_current_l) == self._cfg.acc_leftover_batches\n",
    "            else:                              assert len(self.acc_current_l) == self._cfg.acc_steps\n",
    "        self.acc_current_n_examples = sum(self.acc_current_l)       \n",
    "        \n",
    "    def _eval_save_log_test_set(self): \n",
    "        \"\"\"Eval on test set, convert to df, save to file, and log to wandb summary\"\"\"\n",
    "        self._eval_dl(split='test')\n",
    "        self.data_d[\"test\"] = self._convert_data_d_to_df(\"test\")\n",
    "        self._set_df_colorder(\"test\")\n",
    "        self.data_d[\"test\"].to_csv(f\"{self._cfg.path_run}test.csv\", index=False)\n",
    "        self._add_wandb_run_summary_statistics()\n",
    "\n",
    "    def _training_function(self): \n",
    "        self.accelerator = Accelerator()\n",
    "        self._cfg.device = self.accelerator.device\n",
    "        vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'] = self.accelerator.prepare(\n",
    "            self.vm_model,self.pp_model,self.sts_model,self.optimizer,self.ds.dld_tkn['train'])\n",
    "        \n",
    "        logger.debug(show_gpu(f'GPU memory usage after loading models:'))\n",
    "        progress_bar = tqdm(range(self._cfg.n_train_steps))\n",
    "        self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none) \n",
    "        \n",
    "        # initial eval (at epoch 0)\n",
    "        if self.initial_eval:\n",
    "            logger.info(\"Launching initial eval run: train\")\n",
    "            self._eval_dl(split='train')\n",
    "            logger.info(\"Launching initial eval run: valid\")\n",
    "            self._eval_dl(split='valid')\n",
    "            self._compute_and_log_eval_metrics()\n",
    "        \n",
    "        for self.epoch in range(1, self._cfg.n_train_epochs+1): \n",
    "            logger.info(f\"Now on epoch {self.epoch} of {self._cfg.n_train_epochs}\")\n",
    "            if not self.pp_model.training: self.pp_model.train()\n",
    "            with timecode() as time_train_one_epoch:\n",
    "                for self.batch_num, (data, raw) in enumerate(zip(self.ds.dld_tkn['train'], self.ds.dld_raw['train'])): \n",
    "                    self._reset_batch_dicts()\n",
    "                    self._training_step(data, raw) \n",
    "                    if self._batch_for_opt_step(): self._reset_acc_lists()\n",
    "                    self.acc_num = (self.acc_num + 1) % self._cfg.acc_steps\n",
    "                    self.global_step += 1\n",
    "                    progress_bar.update(1) \n",
    "                    \n",
    "                    \n",
    "            wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                       'time/train_one_epoch_thoroughput': len(self.ds.dsd_tkn['train']) / time_train_one_epoch.t,\n",
    "                       'epoch': self.epoch}, commit=True)\n",
    "\n",
    "            if self._cfg.wandb['log_grads'] and self.epoch % self._cfg.wandb_log_grads_freq == 0: \n",
    "                plt = plot_grad_flow(self.pp_model.named_parameters())\n",
    "                wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "                del plt \n",
    "            #gc.collect() \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if self._cfg.save_model_while_training and (self.epoch + 1) % self._cfg.save_model_freq == 0:  save_model(epoch)\n",
    "\n",
    "            # Evaluation loop\n",
    "            if self.epoch % self._cfg.eval_freq == 0: \n",
    "                self.eval_num += 1\n",
    "                with timecode() as time_eval_train:\n",
    "                    self._eval_dl(split='train') # or train_eval?\n",
    "                with timecode() as time_eval_valid:\n",
    "                    self._eval_dl(split='valid')\n",
    "                with timecode() as time_eval_compute_metrics: \n",
    "                    self._compute_and_log_eval_metrics()\n",
    "                with timecode() as time_eval_gc_collect:\n",
    "                    gc.collect() \n",
    "                with timecode() as time_eval_empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                           'time/eval_train_thoroughput': len(self.ds.dsd_tkn['train']) / time_eval_train.t,\n",
    "                           'time/eval_valid_thoroughput': len(self.ds.dsd_tkn['valid']) / time_eval_valid.t, \n",
    "                           'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                           'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "                           'time/eval_compute_metrics': time_eval_compute_metrics.t,\n",
    "                           'epoch': self.epoch}, commit=True)\n",
    "        \n",
    "        self._eval_save_log_test_set()\n",
    "        self.run.finish()\n",
    "    \n",
    "    def _training_step(self, data, raw): \n",
    "        \"\"\"Forward pass, loss function, backwards pass, parameter update (with gradient accumulation optional), \n",
    "        recording results, wandb logging. \n",
    "        \"\"\"\n",
    "        if not self.pp_model.training: self.pp_model.train()\n",
    "        if not self.vm_model.training: self.vm_model.train()\n",
    "        with timecode() as self.batch_time_d['time_generate_pp']:\n",
    "            pp_output, pp_l = self._pp_model_forward(data)\n",
    "        \n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after forward pass: '))\n",
    "        \n",
    "        # autocast is used by accelerate to allow mixed-precision loss functions.\n",
    "        # drop it if we deprecate fp16 support (because it isn't supported for models like PEGASUS)\n",
    "        with self.accelerator.autocast():\n",
    "            with timecode() as self.batch_time_d['time_loss_fn']:\n",
    "                loss_batch = self._loss_fn(data, raw, pp_output, pp_l)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_backwards']:\n",
    "            self.accelerator.backward(loss_batch) \n",
    "\n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after backwards pass: '))\n",
    "        with timecode() as self.batch_time_d['time_opt_step']:\n",
    "            if self._batch_for_opt_step():\n",
    "                self.optimizer.step()\n",
    "                self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none)\n",
    "            \n",
    "        self._prepare_train_batch_d(raw, data, pp_l)\n",
    "        self.data_d['training_step'].append(self.batch_d)\n",
    "        self._wandb_log_training_step()\n",
    "       \n",
    "    def _batch_for_opt_step(self): return self.acc_num == (self._cfg.acc_steps - 1)\n",
    "    \n",
    "    def _add_batch_vars_to_batch_d(self, raw, data, pp_l): \n",
    "        # Add basics. (results are already added elsewhere)\n",
    "        self.batch_d = merge_dicts(self.batch_d, { 'idx': raw['idx'],\n",
    "            'epoch': self.epoch, 'batch_num': self.batch_num, 'global_step': self.global_step,\n",
    "            'acc_num': self.acc_num, \"acc_batch_n_examples\": self.acc_current_n_examples, \n",
    "            \"orig_l\": raw['text'], \n",
    "            \"orig_label\": data['label'].cpu().tolist(), \n",
    "            \"orig_truelabel_probs\": data['orig_truelabel_probs'].cpu().tolist(),\n",
    "            'orig_length': self.orig_length, 'orig_batch_size': self.orig_batch_size, \n",
    "            \"pp_l\": pp_l, 'pp_length': self.pp_length, 'pp_batch_size': self.pp_batch_size\n",
    "        })\n",
    "        \n",
    "    def _prepare_train_batch_d(self, raw, data, pp_l): \n",
    "        self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "        # Add times (only for training, not eval)\n",
    "        for k, v in self.batch_time_d.items(): self.batch_time_d[k] = v.t  # extract time from timecode object\n",
    "        self.batch_d = merge_dicts(self.batch_d, self.batch_time_d)\n",
    "    \n",
    "    def _wandb_log_training_step(self): \n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, {\n",
    "            'vm_scores_hist':       Histogram(self.batch_d['vm_score']), \n",
    "            'vm_scores_mean':       np.mean(  self.batch_d['vm_score']),\n",
    "            'sts_scores_hist':      Histogram(self.batch_d['sts_score']),\n",
    "            'sts_scores_mean':      np.mean(  self.batch_d['sts_score']), \n",
    "            'rewards_hist':         Histogram(self.batch_d['reward']),\n",
    "            'rewards_mean':         np.mean(  self.batch_d['reward']), \n",
    "            'pp_logp_hist':         Histogram(self.batch_d['pp_logp']),\n",
    "            'pp_logp_mean':         np.mean(  self.batch_d['pp_logp']),\n",
    "            'loss_hist'   :         Histogram(self.batch_d['loss']), \n",
    "            'acc_batch_sizes':      Histogram(self.acc_current_l)    \n",
    "        })\n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, self.batch_d)\n",
    "        not_for_wandb_keys = ['orig_l', 'orig_label','orig_truelabel_probs', 'pp_l', 'loss', 'pp_logp', \n",
    "                              'reward', 'sts_score', 'vm_score',\n",
    "                              'pp_predclass_probs', 'label_flip', 'pp_predclass', 'pp_truelabel_probs']\n",
    "        for k in not_for_wandb_keys:  self.batch_wandb_d.pop(k, None)\n",
    "        wandb.log(self.batch_wandb_d, commit=True)\n",
    "        \n",
    "    def _convert_data_d_to_df(self, data_d_key): \n",
    "        df = pd.DataFrame(self.data_d[data_d_key])\n",
    "        # check all lists have the same number of elements in their row \n",
    "        # last batch will have different number of elements to the batch size\n",
    "        nonscalar_cols = df.columns[[o == np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        df_lengths = df[nonscalar_cols].applymap(len)\n",
    "        assert df_lengths.eq(df_lengths.iloc[:,0], axis=0).all(None)\n",
    "        # expand lists and broadcast scalars\n",
    "        scalar_cols = df.columns[[o != np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        df_expanded = unpack_nested_lists_in_df(df, scalar_cols)\n",
    "        # check shape of new dataframe is correct \n",
    "        if data_d_key == \"training_step\": \n",
    "            if self.epoch == 0: \n",
    "                df_shape = (self._cfg.ds_length[\"train\"],                       df.shape[1])\n",
    "            else: \n",
    "                df_shape = (self._cfg.ds_length[\"train\"] * self._cfg.eval_freq, df.shape[1])\n",
    "        elif data_d_key in [\"train\", \"valid\", \"test\"]: \n",
    "                 df_shape = (self._cfg.ds_length[data_d_key],                   df.shape[1])\n",
    "        assert df_expanded.shape == df_shape\n",
    "        return df_expanded\n",
    "    \n",
    "    def _pp_model_forward(self, data): \n",
    "        pp_output, pp_l = self._get_paraphrases(data['input_ids'], data['attention_mask'])\n",
    "        self._assert_start_and_end_tokens_are_correct(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        # Keep the below line here because then both training and eval can access it\n",
    "        self._update_batch_size_and_length_variables(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _assert_start_and_end_tokens_are_correct(self, orig_ids, pp_ids):\n",
    "        \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "        right special tokens (depends on tokenizer)\"\"\"\n",
    "        # Input\n",
    "        if self.start_end_token_d['input_start_id'] is not None: \n",
    "            assert torch.all(orig_ids[:,0] == self.start_end_token_d['input_start_id'])\n",
    "        # can probs rewrite this to make it nicer but it's fine for now\n",
    "        assert torch.all(torch.logical_or(orig_ids[:,-1] == self.start_end_token_d['input_end_id'][0], \n",
    "                                          orig_ids[:,-1] == self.start_end_token_d['input_end_id'][1]))\n",
    "\n",
    "        # Output\n",
    "        assert torch.all(pp_ids[:,0] == self.start_end_token_d['output_start_id'])\n",
    "        assert torch.all(torch.logical_or(pp_ids[:,-1] == self.start_end_token_d['output_end_id'][0], \n",
    "                                          pp_ids[:,-1] == self.start_end_token_d['output_end_id'][1]))\n",
    "        \n",
    "    def _update_batch_size_and_length_variables(self, orig_ids, pp_ids): \n",
    "        # Update variables\n",
    "        # for greedy search self.pp_length is equal to self.orig_batch_size but this won't be for beam search\n",
    "        self.orig_batch_size     = orig_ids.shape[0]\n",
    "        self.orig_length         = orig_ids.shape[1]\n",
    "        self.pp_batch_size       = pp_ids.shape[0]\n",
    "        self.pp_length           = pp_ids.shape[1] \n",
    "    \n",
    "    def _get_paraphrases(self, orig_ids, attention_mask):\n",
    "        \"\"\"Wrapper for generating paraphrases (pp's).  Only greedy search supported at the moment\"\"\"\n",
    "        pp_output = self.pp_model.generate_with_grad(input_ids=orig_ids, \n",
    "                                                attention_mask=attention_mask, \n",
    "                                                 **self._cfg.pp,\n",
    "                                                 do_sample=False, \n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 output_scores=True,\n",
    "                                                 remove_invalid_values=False, \n",
    "                                                 pad_token_id = self.pp_tokenizer.pad_token_id,\n",
    "                                                 eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "        pp_l = self.pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _loss_fn(self, data, raw, pp_output, pp_l): \n",
    "        with timecode() as self.batch_time_d['time_reward_fn']:\n",
    "            reward = self._reward_fn(data, raw, pp_l)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_pp_logp']:\n",
    "            pp_logp = self._get_pp_logp(pp_output)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_loss_fn_loss_calc']:\n",
    "            loss       = -reward * pp_logp\n",
    "            loss_sum   = torch.sum(loss)  # we scale it later\n",
    "            loss_batch = loss_sum / self.acc_current_n_examples  # for gradient accumulation     \n",
    "\n",
    "        self.batch_d['pp_logp']    =        pp_logp.detach().cpu().tolist()\n",
    "        self.batch_d['loss']       =           loss.detach().cpu().tolist()\n",
    "        self.batch_d['loss_sum']   =       loss_sum.detach().cpu().tolist()\n",
    "        self.batch_d['loss_batch']   =   loss_batch.detach().cpu().tolist()\n",
    "        return loss_batch\n",
    "    \n",
    "    def _reward_fn(self, data, raw, pp_l): \n",
    "        \"\"\"\"\"\"\n",
    "        # Victim model probability differences between orig and pp\n",
    "        with timecode() as self.batch_time_d['time_vm_scores']:\n",
    "            pp_probs = get_vm_probs(pp_l, self._cfg, self.vm_tokenizer, self.vm_model, return_predclass=False)\n",
    "            pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "            pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "            pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "            label_flip = ((pp_predclass != data['label']) * 1)\n",
    "            vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "            \n",
    "        # STS scores\n",
    "        with timecode() as self.batch_time_d['time_sts_scores']:\n",
    "            pp_embeddings  = self.sts_model.encode(pp_l, batch_size=len(raw), convert_to_tensor=True, device=self._cfg.device)\n",
    "            # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "            sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "\n",
    "        # Reward calculation \n",
    "        rewards = torch.tensor([0 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)],device=self._cfg.device)\n",
    "\n",
    "        if self._cfg.normalise_rewards: \n",
    "            self.batch_d['reward_unscaled'] = rewards.detach().cpu().tolist()\n",
    "            rewards = (rewards - torch.mean(rewards)) / torch.std(rewards)\n",
    "        \n",
    "        self.batch_d['pp_truelabel_probs']  = pp_truelabel_probs.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass']        = pp_predclass.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass_probs']  = pp_predclass_probs.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip']          = label_flip.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip_fraction'] = np.mean(self.batch_d['label_flip'])\n",
    "        self.batch_d['reward']              = rewards.detach().cpu().tolist()\n",
    "        self.batch_d['vm_score']            = vm_scores.detach().cpu().tolist()\n",
    "        self.batch_d['sts_score']           = sts_scores.detach().cpu().tolist()\n",
    "    \n",
    "        return rewards\n",
    "         \n",
    "    def _get_pp_logp(self, pp_output): \n",
    "        \"\"\"log(p(pp|orig)) basically.\n",
    "        works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "        ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "        # and has no probability attached to it, so we remove it. \n",
    "        seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "        assert seq_without_first_tkn.shape == torch.Size([self.orig_batch_size, self.pp_length - 1])\n",
    "\n",
    "        ### Convert from tuple of scores to one big tensor of scores \n",
    "        scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "        ### TESTS \n",
    "        # We check shape and that there is no +inf or nan in scores. \n",
    "        # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "        assert scores_stacked.shape == torch.Size([self.orig_batch_size, (self.pp_length - 1), self._cfg.vocab_size])\n",
    "        assert torch.all(~torch.isnan(scores_stacked))\n",
    "        assert torch.all(~torch.isposinf(scores_stacked))\n",
    "        # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "        # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "        # shouldn't be set to -inf\n",
    "        # Not a 100% test but very likely to identify\n",
    "        idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "        assert len(idx_neginf[idx_neginf[:,2] == self.pp_tokenizer.eos_token_id, :]) == \\\n",
    "                  (self._cfg.pp[\"min_length\"] -1) * self.orig_batch_size  \n",
    "        del idx_neginf\n",
    "\n",
    "        ### Take log softmax of scores and then extract those that correspond \n",
    "        # to the generated sequences    \n",
    "        scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "        seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "        ### TESTS \n",
    "        # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "        assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "        assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "        self._check_scores_log_softmax_sums(scores_log_softmax)\n",
    "        # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "        assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "        # Check that the last token probability corresponds to a possible end token\n",
    "        # this has to be tested before the attention mask is multiplied with it because if the \n",
    "        # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "        output_end_ids = self.start_end_token_d['output_end_id']\n",
    "        assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "        del output_end_ids\n",
    "        ## THIS ONE IS LONG - a test rather than assert \n",
    "        # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "        #                                             seq_token_log_probs) \n",
    "\n",
    "        ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "        # sequence probabilities so that we don't consider probability of padding tokens \n",
    "        # when getting sequence probabilities. \n",
    "        # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "        # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "        # since  -inf * 0 = nan \n",
    "        attention_mask = self.pp_model._prepare_attention_mask_for_generation(\n",
    "            seq_without_first_tkn, self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id\n",
    "        )\n",
    "        seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-20)\n",
    "        seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "        ### TESTS\n",
    "        assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "        # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "        assert all(seq_without_first_tkn[attention_mask == 0] == self.pp_tokenizer.pad_token_id)\n",
    "        check_no_nans_or_infs(seq_token_log_probs)\n",
    "        # check that we aren't picking extrememly rare tokens\n",
    "        assert torch.all(seq_token_log_probs  > -10)  \n",
    "\n",
    "        ### Get sequence probabilities by summing up token log probabilities \n",
    "        seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "        ## TESTS \n",
    "        assert seq_log_prob.shape == torch.Size([self.pp_batch_size])\n",
    "        check_no_nans_or_infs(seq_log_prob)\n",
    "        \n",
    "        if self.pp_model.training:  # don't bother logging or calculate entropy, token_probs in eval mode\n",
    "            if self._cfg.wandb['log_token_entropy']:\n",
    "                with timecode() as self.batch_time_d['time_log_entropy']:\n",
    "                    self.batch_wandb_d['ent_hist'] = self._get_entropy_hist(scores_stacked, attention_mask) \n",
    "            if self._cfg.wandb['log_token_probabilities']: \n",
    "                with timecode() as self.batch_time_d['time_log_token_probabilities']:\n",
    "                    self.batch_wandb_d = merge_dicts(self.batch_wandb_d, \n",
    "                        self._get_token_probability_metrics(scores_log_softmax, attention_mask, k=3))\n",
    "        return seq_log_prob\n",
    "   \n",
    "    def _check_scores_log_softmax_sums(self, scores_log_softmax):\n",
    "        sums = scores_log_softmax.exp().sum(2)\n",
    "        # check that the axes is right\n",
    "        # we want to sum over token probabilities at each generation step, so we \n",
    "        # should end up with a shape [self.orig_batch_size, self.pp_length]\n",
    "        assert sums.shape[0] == self.orig_batch_size  \n",
    "        assert sums.shape[1] == self.pp_length - 1\n",
    "        # check that they sum to 1 along the self.pp_length axis\n",
    "        assert torch.allclose(sums, torch.ones(sums.size(), device=self._cfg.device), atol = 1e-4)\n",
    "\n",
    "    def _check_seq_token_log_prob_values_are_correct(self, seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "        \"\"\"Just enumerates and checks values\n",
    "        Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i_ex in range(self.orig_batch_size):\n",
    "            for i_step in range(self.pp_length - 1):\n",
    "                i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "                l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "        assert all(l)    \n",
    "    \n",
    "    def _get_entropy_hist(self, scores_stacked, attention_mask): \n",
    "        ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "        assert ent.shape == attention_mask.shape == torch.Size([self.pp_batch_size, self.pp_length - 1])\n",
    "        ent = ent * attention_mask  # stop values after eos token from contributing to ent score \n",
    "        # first remove structure (otherwise we have ragged arrays), then remove corresponding attention mask values\n",
    "        # we can't just filter by ent[ent != 0] because we might have zero tokens during the sequence\n",
    "        att_flat= attention_mask.flatten()\n",
    "        indices = torch.nonzero(att_flat)\n",
    "        ent_flat = ent.flatten()[indices].flatten()\n",
    "        assert ent_flat.shape[0] == (torch.sum(att_flat)*1).item()\n",
    "        # check everything we filter out is zero \n",
    "        torch.isclose(ent.flatten()[torch.nonzero(~(att_flat > 0))].sum(), torch.tensor(0.), 1e-3)\n",
    "        return Histogram(ent_flat.detach().cpu().tolist())\n",
    "\n",
    "    def _get_token_probability_metrics(self, scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, _ = torch.topk(scores_log_softmax, largest=True, k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  \n",
    "        tkn_kmaxprob = torch.nan_to_num(tkn_kmaxprob, nan=None, posinf=None, neginf=-20)\n",
    "        assert tkn_kmaxprob.shape == torch.Size([self.pp_batch_size, self.pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_histogram\"] = Histogram(probs.detach().cpu().tolist())\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [0.1, 0.01, 0.001, 0.0001, 0.00001]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/self._cfg.vocab_size)) / allprobs.shape[0]).item()\n",
    "        return token_prob_d\n",
    "    \n",
    "    def _eval_dl(self, split): \n",
    "        \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "        # Put models in eval mode and do the forward pass \n",
    "        # Current logic: push all batches together into one big list.   \n",
    "        self._reset_batch_dicts()\n",
    "        if self.pp_model.training: self.pp_model.eval()\n",
    "        if self.vm_model.training: self.vm_model.eval()\n",
    "        # The \"train_eval\" dataloader is the same as train but a bigger batch size and explicitly no shuffling\n",
    "        dl_key = \"train_eval\" if split == \"train\" else split\n",
    "        dl_raw = self.ds.dld_raw[dl_key]\n",
    "        dl_tkn = self.ds.dld_tkn[dl_key]\n",
    "        with torch.no_grad(): \n",
    "            for self.batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "                logger.debug(f\"EVAL: {split} with dl_key {dl_key}\")\n",
    "                logger.debug(f\"Elements in data_d[{split}]: {len(self.data_d[split])}\")\n",
    "                logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loading data: '))\n",
    "                assert data['input_ids'].shape[0] == len(raw['text'])\n",
    "                self._reset_batch_dicts()\n",
    "                assert len(self.batch_d) == len(self.batch_time_d) == len(self.batch_wandb_d) == 0 \n",
    "                for k, v in data.items():\n",
    "                    # Eval data isn't loaded on GPU by default unlike train data. This is because train dataloader goes \n",
    "                    # through accelerator `prepare` function, but eval dataloaders don't. So here we load the data onto GPU \n",
    "                    if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "                pp_output, pp_l = self._pp_model_forward(data)\n",
    "                _ = self._loss_fn(data, raw, pp_output, pp_l)\n",
    "                self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "                self.data_d[split].append(self.batch_d)\n",
    "                logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "        \n",
    "    def _compute_and_log_eval_metrics(self): \n",
    "        \"\"\"Calculate eval metrics for each split and log to wandb, then empty data_d\"\"\"\n",
    "        wandb_d = dict(epoch=self.epoch)\n",
    "        eval_splits = [\"training_step\", 'train', \"valid\"] if self.epoch != 0 else ['train', 'valid']\n",
    "        for split in eval_splits: \n",
    "            # data d -> data frame \n",
    "            self.data_d[split] = self._convert_data_d_to_df(split)\n",
    "            self._set_df_colorder(split)\n",
    "            # calc metrics \n",
    "            df = self.data_d[split][['epoch'] + self._cfg.metrics]\n",
    "            if split == \"training_step\": df = df.query(\"epoch == @self.epoch\")\n",
    "            d = df.mean()[self._cfg.metrics].to_dict()\n",
    "            wandb_d = merge_dicts(wandb_d, {f\"{k}_{split}\": v for k, v in d.items()})\n",
    "            # df append to file + empty data_d\n",
    "            append_df_to_csv(self.data_d[split], path = f\"{self._cfg.path_run}{split}.csv\")\n",
    "            self.data_d[split] = [] \n",
    "        wandb.log(wandb_d, commit=True)    \n",
    "    \n",
    "    def _set_df_colorder(self, data_d_key): \n",
    "        colorder_eval=['idx','epoch', 'orig_l',  'pp_l','orig_truelabel_probs','pp_truelabel_probs',\n",
    "        'pp_predclass_probs','orig_label','pp_predclass','label_flip', 'vm_score','sts_score',\n",
    "        'reward', 'pp_logp','loss','batch_num','global_step','acc_num','loss_sum', 'loss_batch', 'label_flip_fraction',\n",
    "        'orig_length','orig_batch_size','pp_length','pp_batch_size']\n",
    "        if data_d_key == \"training_step\": \n",
    "            colorder_training_step = colorder_eval + [o for o in self.data_d['training_step'].columns if 'time_' in o]\n",
    "            assert len(set(colorder_training_step).difference(set(self.data_d[data_d_key].columns))) == 0 \n",
    "            self.data_d[data_d_key] = self.data_d[data_d_key][colorder_training_step]\n",
    "        else:\n",
    "            assert len(set(colorder_eval).difference(set(self.data_d[data_d_key].columns))) == 0 \n",
    "            self.data_d[data_d_key] = self.data_d[data_d_key][colorder_eval]\n",
    "\n",
    "    def _add_wandb_run_summary_statistics(self):\n",
    "        \"\"\"Compute test metrics for the run and log them to the wandb run summary pane. \"\"\"\n",
    "        ## Summary statistics of the test set \n",
    "        # From the last epoch atm because we don't have early stopping \n",
    "        test_metrics = self.data_d['test'].filter(self._cfg.metrics, axis=1).mean()\n",
    "        for metric, val in zip(test_metrics.index, test_metrics): \n",
    "            self.run.summary[f\"{metric}_avg_test\"] = val        \n",
    "            \n",
    "    def get_training_dfs(self): \n",
    "        \"\"\"Return a dict of dataframes with all training and eval data\"\"\"\n",
    "        df_d = dict()\n",
    "        for key in self._cfg.splits + ['training_step']: \n",
    "            df_d[key] = pd.read_csv(f\"{self._cfg.path_run}{key}.csv\")\n",
    "        return df_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.wandb['mode'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-2a1a1f820081>:55: UserWarning: Config set to do gradient accumulation every 4 batches, and there are             9 total training steps, so there will be 1 batches at             the end that will not be trained on.\n",
      "  warnings.warn(msg)\n",
      "wandb.jupyter: ERROR    Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82d7f1bb1a44d2098297863f691d25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainer     : INFO     Now on epoch 1 of 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING FN epoch 1 batch_num 0 global_step 0 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 0 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50647920799f4177af79f8c62b6b2982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 1 batch_num 0 global_step 0 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 0 acc_current_n_examples 41 orig_batch_size 13 loss [2.6579325199127197, 2.882582187652588, 3.808532953262329, 2.601559638977051, 1.8070285320281982, 0.9169818162918091, 2.397155284881592, 2.536442279815674, 3.018293619155884, 1.7645758390426636, 2.1225979328155518, 4.422473430633545, 1.6228030920028687] loss_sum 32.5589599609375 loss_batch 0.7941209673881531\n",
      "TRAINING FN epoch 1 batch_num 1 global_step 1 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 1 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880ed285d8d9450c9139b5082e39edea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 1 batch_num 1 global_step 1 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 1 acc_current_n_examples 41 orig_batch_size 13 loss [2.088310956954956, 1.2175743579864502, 1.6848658323287964, 0.9218475222587585, 0.897847056388855, 0.8152209520339966, 1.2488887310028076, 1.1638776063919067, 1.8707139492034912, 2.0865373611450195, 2.5366291999816895, 1.468750238418579, 1.3512530326843262] loss_sum 19.352317810058594 loss_batch 0.47200772166252136\n",
      "TRAINING FN epoch 1 batch_num 2 global_step 2 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 2 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d970f81f5e98479c866fb85c300f12d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 1 batch_num 2 global_step 2 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 2 acc_current_n_examples 41 orig_batch_size 2 loss [1.0890592336654663, 4.028101921081543] loss_sum 5.117161273956299 loss_batch 0.12480880320072174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d123c773a59d40bd8efcbbb8fdca6653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79122e886949493da4fc1c32756a9bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainer     : INFO     Now on epoch 2 of 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING FN epoch 2 batch_num 0 global_step 3 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 3 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c00e2b25be346ed86728d8bb56647b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPT STEP    epoch 2 batch_num 0 global_step 3 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 3 acc_current_n_examples 41 orig_batch_size 13 loss [3.2344911098480225, 4.9538679122924805, 4.83140754699707, 1.9228928089141846, 1.9292998313903809, 1.569717526435852, 3.133167266845703, 3.093967914581299, 3.583906650543213, 1.8731579780578613, 2.430718421936035, 4.914355754852295, 1.7376790046691895] loss_sum 39.2086296081543 loss_batch 0.9563080072402954\n",
      "WANDB LOG   epoch 2 batch_num 0 global_step 3 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 13, 2, 13] acc_num 3 acc_current_n_examples 41 orig_batch_size 13 loss [3.2344911098480225, 4.9538679122924805, 4.83140754699707, 1.9228928089141846, 1.9292998313903809, 1.569717526435852, 3.133167266845703, 3.093967914581299, 3.583906650543213, 1.8731579780578613, 2.430718421936035, 4.914355754852295, 1.7376790046691895] loss_sum 39.2086296081543 loss_batch 0.9563080072402954\n",
      "TRAINING FN epoch 2 batch_num 1 global_step 4 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 0 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa6a37e91974ebca150a6f6a1bf086e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 2 batch_num 1 global_step 4 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 0 acc_current_n_examples 41 orig_batch_size 13 loss [2.0674896240234375, 1.0390440225601196, 1.2479512691497803, 1.3635417222976685, 0.6955448389053345, 1.3756415843963623, 1.068172812461853, 1.453413724899292, 1.7316007614135742, 1.7547646760940552, 2.0186386108398438, 2.019388198852539, 2.165886402130127] loss_sum 20.001079559326172 loss_batch 0.4878312051296234\n",
      "TRAINING FN epoch 2 batch_num 2 global_step 5 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 1 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dcafaea33a47dd8aa78b9a694825d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 2 batch_num 2 global_step 5 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 1 acc_current_n_examples 41 orig_batch_size 2 loss [0.4963894188404083, 0.0] loss_sum 0.4963894188404083 loss_batch 0.012107058428227901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d478a7504f4c2480eeda4e33327209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc85eea44cf44aca45c40aba82e5f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainer     : INFO     Now on epoch 3 of 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING FN epoch 3 batch_num 0 global_step 6 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 2 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab26d3f34144bbb6780b7e37c7bc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 3 batch_num 0 global_step 6 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 2 acc_current_n_examples 41 orig_batch_size 13 loss [2.361065626144409, 4.704903602600098, 4.999863147735596, 2.5626537799835205, 2.4743404388427734, 1.5723201036453247, 2.113670587539673, 2.0295958518981934, 4.422112941741943, 1.6269975900650024, 1.4496331214904785, 3.659032106399536, 1.1455601453781128] loss_sum 35.12174987792969 loss_batch 0.8566280007362366\n",
      "TRAINING FN epoch 3 batch_num 1 global_step 7 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 3 acc_current_n_examples 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da524fc83204bb795aec6974dee2a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPT STEP    epoch 3 batch_num 1 global_step 7 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 3 acc_current_n_examples 41 orig_batch_size 13 loss [3.25260329246521, 1.719853401184082, 1.809979796409607, 1.1265289783477783, 0.7422451972961426, 1.299055814743042, 1.4299691915512085, 1.436971664428711, 1.9265263080596924, 1.6522576808929443, 2.5445961952209473, 5.304778099060059, 2.4535233974456787] loss_sum 26.698888778686523 loss_batch 0.6511923670768738\n",
      "WANDB LOG   epoch 3 batch_num 1 global_step 7 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [13, 2, 13, 13] acc_num 3 acc_current_n_examples 41 orig_batch_size 13 loss [3.25260329246521, 1.719853401184082, 1.809979796409607, 1.1265289783477783, 0.7422451972961426, 1.299055814743042, 1.4299691915512085, 1.436971664428711, 1.9265263080596924, 1.6522576808929443, 2.5445961952209473, 5.304778099060059, 2.4535233974456787] loss_sum 26.698888778686523 loss_batch 0.6511923670768738\n",
      "TRAINING FN epoch 3 batch_num 2 global_step 8 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [2] acc_num 0 acc_current_n_examples 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2767b095bace4c6198c2326417c2be99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB LOG   epoch 3 batch_num 2 global_step 8 acc_steps 4 acc_leftover_batches 1 acc_global_l [13, 13, 2, 13, 13, 2, 13, 13, 2] acc_current_l [2] acc_num 0 acc_current_n_examples 2 orig_batch_size 2 loss [2.980776071548462, 2.1049318313598633] loss_sum 5.085707664489746 loss_batch 2.542853832244873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633f0c3b3b0146fcbc47770a1cde3657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c59a0c788a481ba2655f8df815bdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b162040914432f8268b30b634903a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer,\n",
    "                  ds, initial_eval=False)\n",
    "trainer.train()\n",
    "\n",
    "# copy log file to models directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concurrent.futures': <Logger concurrent.futures (DEBUG)>,\n",
       " 'concurrent': <logging.PlaceHolder at 0x2b930f175a30>,\n",
       " 'asyncio': <Logger asyncio (DEBUG)>,\n",
       " 'parso.python.diff': <Logger parso.python.diff (DEBUG)>,\n",
       " 'parso.python': <logging.PlaceHolder at 0x2b930fa1ac70>,\n",
       " 'parso': <logging.PlaceHolder at 0x2b930fa1acd0>,\n",
       " 'parso.cache': <Logger parso.cache (DEBUG)>,\n",
       " 'tornado.access': <Logger tornado.access (DEBUG)>,\n",
       " 'tornado': <Logger tornado (DEBUG)>,\n",
       " 'tornado.application': <Logger tornado.application (DEBUG)>,\n",
       " 'tornado.general': <Logger tornado.general (DEBUG)>,\n",
       " 'IPKernelApp': <Logger IPKernelApp (WARNING)>,\n",
       " 'torch.distributed.distributed_c10d': <Logger torch.distributed.distributed_c10d (DEBUG)>,\n",
       " 'torch.distributed': <logging.PlaceHolder at 0x2b93c202b490>,\n",
       " 'torch': <logging.PlaceHolder at 0x2b93c202b4f0>,\n",
       " 'torch.distributed.rpc': <Logger torch.distributed.rpc (DEBUG)>,\n",
       " 'torch.distributed.rpc.api': <Logger torch.distributed.rpc.api (DEBUG)>,\n",
       " 'tqdm.cli': <Logger tqdm.cli (DEBUG)>,\n",
       " 'tqdm': <logging.PlaceHolder at 0x2b93c257bac0>,\n",
       " 'dill': <Logger dill (WARNING)>,\n",
       " 'urllib3.util.retry': <Logger urllib3.util.retry (DEBUG)>,\n",
       " 'urllib3.util': <logging.PlaceHolder at 0x2b93c45e4b80>,\n",
       " 'urllib3': <Logger urllib3 (DEBUG)>,\n",
       " 'urllib3.connection': <Logger urllib3.connection (DEBUG)>,\n",
       " 'urllib3.response': <Logger urllib3.response (DEBUG)>,\n",
       " 'urllib3.connectionpool': <Logger urllib3.connectionpool (DEBUG)>,\n",
       " 'urllib3.poolmanager': <Logger urllib3.poolmanager (DEBUG)>,\n",
       " 'socks': <Logger socks (DEBUG)>,\n",
       " 'requests': <Logger requests (DEBUG)>,\n",
       " 'sentry_sdk.errors': <Logger sentry_sdk.errors (DEBUG)>,\n",
       " 'sentry_sdk': <logging.PlaceHolder at 0x2b93c48b82b0>,\n",
       " 'wandb.util': <Logger wandb.util (DEBUG)>,\n",
       " 'wandb': <Logger wandb (DEBUG)>,\n",
       " 'graphql.execution.executor': <Logger graphql.execution.executor (DEBUG)>,\n",
       " 'graphql.execution': <logging.PlaceHolder at 0x2b93c49710d0>,\n",
       " 'graphql': <logging.PlaceHolder at 0x2b93c4971130>,\n",
       " 'gql.client': <Logger gql.client (DEBUG)>,\n",
       " 'gql': <logging.PlaceHolder at 0x2b93c4957b20>,\n",
       " 'wandb.sdk.lib.retry': <Logger wandb.sdk.lib.retry (DEBUG)>,\n",
       " 'wandb.sdk.lib': <logging.PlaceHolder at 0x2b93c4aa6340>,\n",
       " 'wandb.sdk': <logging.PlaceHolder at 0x2b93c4aa63a0>,\n",
       " 'wandb.sdk.lib.git': <Logger wandb.sdk.lib.git (DEBUG)>,\n",
       " 'git.util': <Logger git.util (DEBUG)>,\n",
       " 'git': <logging.PlaceHolder at 0x2b93c4b3b6d0>,\n",
       " 'git.config': <Logger git.config (DEBUG)>,\n",
       " 'git.cmd': <Logger git.cmd (DEBUG)>,\n",
       " 'git.objects.submodule.base': <Logger git.objects.submodule.base (DEBUG)>,\n",
       " 'git.objects.submodule': <logging.PlaceHolder at 0x2b93c4bd6cd0>,\n",
       " 'git.objects': <logging.PlaceHolder at 0x2b93c4bd6d60>,\n",
       " 'git.objects.commit': <Logger git.objects.commit (DEBUG)>,\n",
       " 'git.objects.submodule.root': <Logger git.objects.submodule.root (DEBUG)>,\n",
       " 'git.remote': <Logger git.remote (DEBUG)>,\n",
       " 'git.repo.base': <Logger git.repo.base (DEBUG)>,\n",
       " 'git.repo': <logging.PlaceHolder at 0x2b93c4c133a0>,\n",
       " 'wandb.sdk.internal.internal_api': <Logger wandb.sdk.internal.internal_api (DEBUG)>,\n",
       " 'wandb.sdk.internal': <logging.PlaceHolder at 0x2b93c4957370>,\n",
       " 'wandb.sdk.lib.ipython': <Logger wandb.sdk.lib.ipython (DEBUG)>,\n",
       " 'wandb.apis.public': <Logger wandb.apis.public (DEBUG)>,\n",
       " 'wandb.apis': <logging.PlaceHolder at 0x2b938adbd7f0>,\n",
       " 'wandb.sdk.lib.debug_log': <Logger wandb.sdk.lib.debug_log (DEBUG)>,\n",
       " 'wandb.sdk.internal.meta': <Logger wandb.sdk.internal.meta (DEBUG)>,\n",
       " 'wandb.sdk.internal.tpu': <Logger wandb.sdk.internal.tpu (DEBUG)>,\n",
       " 'wandb.sdk.internal.tb_watcher': <Logger wandb.sdk.internal.tb_watcher (DEBUG)>,\n",
       " 'wandb.sdk.internal.handler': <Logger wandb.sdk.internal.handler (DEBUG)>,\n",
       " 'wandb.sdk.internal.internal_util': <Logger wandb.sdk.internal.internal_util (DEBUG)>,\n",
       " 'watchdog.observers.inotify_buffer': <Logger watchdog.observers.inotify_buffer (DEBUG)>,\n",
       " 'watchdog.observers': <logging.PlaceHolder at 0x2b9391724760>,\n",
       " 'watchdog': <logging.PlaceHolder at 0x2b93917247c0>,\n",
       " 'wandb.filesync.dir_watcher': <Logger wandb.filesync.dir_watcher (DEBUG)>,\n",
       " 'wandb.filesync': <logging.PlaceHolder at 0x2b9391712f10>,\n",
       " 'wandb.sdk.internal.file_stream': <Logger wandb.sdk.internal.file_stream (DEBUG)>,\n",
       " 'wandb.filesync.upload_job': <Logger wandb.filesync.upload_job (DEBUG)>,\n",
       " 'wandb.sdk.internal.file_pusher': <Logger wandb.sdk.internal.file_pusher (DEBUG)>,\n",
       " 'wandb.sdk.internal.sender': <Logger wandb.sdk.internal.sender (DEBUG)>,\n",
       " 'wandb.sdk.internal.datastore': <Logger wandb.sdk.internal.datastore (DEBUG)>,\n",
       " 'wandb.sdk.internal.writer': <Logger wandb.sdk.internal.writer (DEBUG)>,\n",
       " 'wandb.sdk.internal.internal': <Logger wandb.sdk.internal.internal (DEBUG)>,\n",
       " 'wandb.agents.pyagent': <Logger wandb.agents.pyagent (DEBUG)>,\n",
       " 'wandb.agents': <logging.PlaceHolder at 0x2b9391794e20>,\n",
       " 'wandb.wandb_agent': <Logger wandb.wandb_agent (DEBUG)>,\n",
       " 'wandb.jupyter': <Logger wandb.jupyter (DEBUG)>,\n",
       " 'huggingface_hub': <Logger huggingface_hub (WARNING)>,\n",
       " 'huggingface_hub.file_download': <Logger huggingface_hub.file_download (WARNING)>,\n",
       " 'huggingface_hub.repository': <Logger huggingface_hub.repository (WARNING)>,\n",
       " 'huggingface_hub.hub_mixin': <Logger huggingface_hub.hub_mixin (WARNING)>,\n",
       " 'huggingface_hub.inference_api': <Logger huggingface_hub.inference_api (WARNING)>,\n",
       " 'huggingface_hub.keras_mixin': <Logger huggingface_hub.keras_mixin (WARNING)>,\n",
       " 'transformers': <Logger transformers (WARNING)>,\n",
       " 'transformers.file_utils': <Logger transformers.file_utils (WARNING)>,\n",
       " 'transformers.configuration_utils': <Logger transformers.configuration_utils (WARNING)>,\n",
       " 'transformers.tokenization_utils_base': <Logger transformers.tokenization_utils_base (WARNING)>,\n",
       " 'transformers.tokenization_utils': <Logger transformers.tokenization_utils (WARNING)>,\n",
       " 'transformers.models.bert.configuration_bert': <Logger transformers.models.bert.configuration_bert (WARNING)>,\n",
       " 'transformers.models.bert': <logging.PlaceHolder at 0x2b93ca9f2160>,\n",
       " 'transformers.models': <logging.PlaceHolder at 0x2b93ca9f21c0>,\n",
       " 'transformers.models.layoutlm.configuration_layoutlm': <Logger transformers.models.layoutlm.configuration_layoutlm (WARNING)>,\n",
       " 'transformers.models.layoutlm': <logging.PlaceHolder at 0x2b9393928e80>,\n",
       " 'transformers.models.bert.tokenization_bert': <Logger transformers.models.bert.tokenization_bert (WARNING)>,\n",
       " 'transformers.models.layoutlm.tokenization_layoutlm': <Logger transformers.models.layoutlm.tokenization_layoutlm (WARNING)>,\n",
       " 'transformers.models.t5.tokenization_t5': <Logger transformers.models.t5.tokenization_t5 (WARNING)>,\n",
       " 'transformers.models.t5': <logging.PlaceHolder at 0x2b93cae5dd30>,\n",
       " 'transformers.tokenization_utils_fast': <Logger transformers.tokenization_utils_fast (WARNING)>,\n",
       " 'transformers.models.t5.tokenization_t5_fast': <Logger transformers.models.t5.tokenization_t5_fast (WARNING)>,\n",
       " 'transformers.models.auto.dynamic': <Logger transformers.models.auto.dynamic (WARNING)>,\n",
       " 'transformers.models.auto': <logging.PlaceHolder at 0x2b93cb16fa00>,\n",
       " 'transformers.models.auto.configuration_auto': <Logger transformers.models.auto.configuration_auto (WARNING)>,\n",
       " 'transformers.models.auto.auto_factory': <Logger transformers.models.auto.auto_factory (WARNING)>,\n",
       " 'transformers.models.auto.modeling_auto': <Logger transformers.models.auto.modeling_auto (WARNING)>,\n",
       " 'transformers.models.encoder_decoder.configuration_encoder_decoder': <Logger transformers.models.encoder_decoder.configuration_encoder_decoder (WARNING)>,\n",
       " 'transformers.models.encoder_decoder': <logging.PlaceHolder at 0x2b93cb18cc10>,\n",
       " 'transformers.models.auto.tokenization_auto': <Logger transformers.models.auto.tokenization_auto (WARNING)>,\n",
       " '/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/nltk/metrics/agreement.py': <Logger /home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/nltk/metrics/agreement.py (DEBUG)>,\n",
       " '/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/nltk/metrics/agreement': <logging.PlaceHolder at 0x2b93d4d5a4c0>,\n",
       " '/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3': <logging.PlaceHolder at 0x2b93d4d5a520>,\n",
       " 'nltk.parse.nonprojectivedependencyparser': <Logger nltk.parse.nonprojectivedependencyparser (DEBUG)>,\n",
       " 'nltk.parse': <logging.PlaceHolder at 0x2b93d5874ca0>,\n",
       " 'nltk': <logging.PlaceHolder at 0x2b93d5874d00>,\n",
       " 'sklearn': <Logger sklearn (DEBUG)>,\n",
       " 'sklearn.datasets._covtype': <Logger sklearn.datasets._covtype (DEBUG)>,\n",
       " 'sklearn.datasets': <logging.PlaceHolder at 0x2b93c2608f10>,\n",
       " 'sklearn.datasets._kddcup99': <Logger sklearn.datasets._kddcup99 (DEBUG)>,\n",
       " 'sklearn.datasets._lfw': <Logger sklearn.datasets._lfw (DEBUG)>,\n",
       " 'sklearn.datasets._twenty_newsgroups': <Logger sklearn.datasets._twenty_newsgroups (DEBUG)>,\n",
       " 'sklearn.datasets._species_distributions': <Logger sklearn.datasets._species_distributions (DEBUG)>,\n",
       " 'sklearn.datasets._california_housing': <Logger sklearn.datasets._california_housing (DEBUG)>,\n",
       " 'sklearn.datasets._rcv1': <Logger sklearn.datasets._rcv1 (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.BinaryClassificationEvaluator': <Logger sentence_transformers.evaluation.BinaryClassificationEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation': <logging.PlaceHolder at 0x2b93d714c9a0>,\n",
       " 'sentence_transformers': <logging.PlaceHolder at 0x2b93d714ca00>,\n",
       " 'sentence_transformers.evaluation.EmbeddingSimilarityEvaluator': <Logger sentence_transformers.evaluation.EmbeddingSimilarityEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.util': <Logger sentence_transformers.util (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.InformationRetrievalEvaluator': <Logger sentence_transformers.evaluation.InformationRetrievalEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.LabelAccuracyEvaluator': <Logger sentence_transformers.evaluation.LabelAccuracyEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.MSEEvaluator': <Logger sentence_transformers.evaluation.MSEEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.MSEEvaluatorFromDataFrame': <Logger sentence_transformers.evaluation.MSEEvaluatorFromDataFrame (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.ParaphraseMiningEvaluator': <Logger sentence_transformers.evaluation.ParaphraseMiningEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.TranslationEvaluator': <Logger sentence_transformers.evaluation.TranslationEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.TripletEvaluator': <Logger sentence_transformers.evaluation.TripletEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.evaluation.RerankingEvaluator': <Logger sentence_transformers.evaluation.RerankingEvaluator (DEBUG)>,\n",
       " 'sentence_transformers.models.BoW': <Logger sentence_transformers.models.BoW (DEBUG)>,\n",
       " 'sentence_transformers.models': <logging.PlaceHolder at 0x2b93d717f2e0>,\n",
       " 'sentence_transformers.models.WordEmbeddings': <Logger sentence_transformers.models.WordEmbeddings (DEBUG)>,\n",
       " 'sentence_transformers.models.WordWeights': <Logger sentence_transformers.models.WordWeights (DEBUG)>,\n",
       " 'sentence_transformers.SentenceTransformer': <Logger sentence_transformers.SentenceTransformer (DEBUG)>,\n",
       " 'absl': <ABSLLogger absl (DEBUG)>,\n",
       " 'tensorflow': <Logger tensorflow (INFO)>,\n",
       " 'PIL.Image': <Logger PIL.Image (DEBUG)>,\n",
       " 'PIL': <logging.PlaceHolder at 0x2b93fee28d00>,\n",
       " 'tensorboard': <Logger tensorboard (DEBUG)>,\n",
       " 'botocore': <Logger botocore (DEBUG)>,\n",
       " 'botocore.compat': <Logger botocore.compat (DEBUG)>,\n",
       " 'urllib3.contrib.pyopenssl': <Logger urllib3.contrib.pyopenssl (DEBUG)>,\n",
       " 'urllib3.contrib': <logging.PlaceHolder at 0x2b93ff4a30a0>,\n",
       " 'botocore.httpsession': <Logger botocore.httpsession (DEBUG)>,\n",
       " 'botocore.utils': <Logger botocore.utils (DEBUG)>,\n",
       " 'botocore.awsrequest': <Logger botocore.awsrequest (DEBUG)>,\n",
       " 'botocore.hooks': <Logger botocore.hooks (DEBUG)>,\n",
       " 'botocore.history': <Logger botocore.history (DEBUG)>,\n",
       " 'botocore.parsers': <Logger botocore.parsers (DEBUG)>,\n",
       " 'botocore.response': <Logger botocore.response (DEBUG)>,\n",
       " 'botocore.endpoint': <Logger botocore.endpoint (DEBUG)>,\n",
       " 'botocore.credentials': <Logger botocore.credentials (DEBUG)>,\n",
       " 'bcdocs': <Logger bcdocs (DEBUG)>,\n",
       " 'botocore.waiter': <Logger botocore.waiter (DEBUG)>,\n",
       " 'botocore.auth': <Logger botocore.auth (DEBUG)>,\n",
       " 'botocore.paginate': <Logger botocore.paginate (DEBUG)>,\n",
       " 'botocore.args': <Logger botocore.args (DEBUG)>,\n",
       " 'botocore.discovery': <Logger botocore.discovery (DEBUG)>,\n",
       " 'botocore.retries.special': <Logger botocore.retries.special (DEBUG)>,\n",
       " 'botocore.retries': <logging.PlaceHolder at 0x2b93ff944400>,\n",
       " 'botocore.retries.standard': <Logger botocore.retries.standard (DEBUG)>,\n",
       " 'botocore.retries.adaptive': <Logger botocore.retries.adaptive (DEBUG)>,\n",
       " 'botocore.client': <Logger botocore.client (DEBUG)>,\n",
       " 'botocore.configprovider': <Logger botocore.configprovider (DEBUG)>,\n",
       " 'botocore.retryhandler': <Logger botocore.retryhandler (DEBUG)>,\n",
       " 'botocore.handlers': <Logger botocore.handlers (DEBUG)>,\n",
       " 'botocore.loaders': <Logger botocore.loaders (DEBUG)>,\n",
       " 'botocore.regions': <Logger botocore.regions (DEBUG)>,\n",
       " 'botocore.monitoring': <Logger botocore.monitoring (DEBUG)>,\n",
       " 'botocore.session': <Logger botocore.session (DEBUG)>,\n",
       " 'boto3.resources.model': <Logger boto3.resources.model (DEBUG)>,\n",
       " 'boto3.resources': <logging.PlaceHolder at 0x2b93ff9aed30>,\n",
       " 'boto3': <Logger boto3 (DEBUG)>,\n",
       " 'boto3.resources.action': <Logger boto3.resources.action (DEBUG)>,\n",
       " 'boto3.resources.base': <Logger boto3.resources.base (DEBUG)>,\n",
       " 'boto3.resources.collection': <Logger boto3.resources.collection (DEBUG)>,\n",
       " 'boto3.resources.factory': <Logger boto3.resources.factory (DEBUG)>,\n",
       " 'transformers.optimization': <Logger transformers.optimization (WARNING)>,\n",
       " 'sentence_transformers.datasets.ParallelSentencesDataset': <Logger sentence_transformers.datasets.ParallelSentencesDataset (DEBUG)>,\n",
       " 'sentence_transformers.datasets': <logging.PlaceHolder at 0x2b93d713ab80>,\n",
       " 'sentence_transformers.datasets.SentenceLabelDataset': <Logger sentence_transformers.datasets.SentenceLabelDataset (DEBUG)>,\n",
       " 'sentence_transformers.cross_encoder.CrossEncoder': <Logger sentence_transformers.cross_encoder.CrossEncoder (DEBUG)>,\n",
       " 'sentence_transformers.cross_encoder': <logging.PlaceHolder at 0x2b93ffa62640>,\n",
       " 'matplotlib.animation': <Logger matplotlib.animation (DEBUG)>,\n",
       " 'matplotlib': <Logger matplotlib (DEBUG)>,\n",
       " 'matplotlib.rcsetup': <Logger matplotlib.rcsetup (DEBUG)>,\n",
       " 'matplotlib.artist': <Logger matplotlib.artist (DEBUG)>,\n",
       " 'matplotlib.lines': <Logger matplotlib.lines (DEBUG)>,\n",
       " 'matplotlib.ticker': <Logger matplotlib.ticker (DEBUG)>,\n",
       " 'matplotlib.afm': <Logger matplotlib.afm (DEBUG)>,\n",
       " 'matplotlib.font_manager': <Logger matplotlib.font_manager (DEBUG)>,\n",
       " 'matplotlib.dviread': <Logger matplotlib.dviread (DEBUG)>,\n",
       " 'matplotlib.mathtext': <Logger matplotlib.mathtext (DEBUG)>,\n",
       " 'matplotlib.textpath': <Logger matplotlib.textpath (DEBUG)>,\n",
       " 'matplotlib.text': <Logger matplotlib.text (DEBUG)>,\n",
       " 'matplotlib.texmanager': <Logger matplotlib.texmanager (DEBUG)>,\n",
       " 'matplotlib.backend_tools': <Logger matplotlib.backend_tools (DEBUG)>,\n",
       " 'matplotlib.backend_managers': <Logger matplotlib.backend_managers (DEBUG)>,\n",
       " 'matplotlib.backend_bases': <Logger matplotlib.backend_bases (DEBUG)>,\n",
       " 'matplotlib.blocking_input': <Logger matplotlib.blocking_input (DEBUG)>,\n",
       " 'matplotlib._layoutbox': <Logger matplotlib._layoutbox (DEBUG)>,\n",
       " 'matplotlib.gridspec': <Logger matplotlib.gridspec (DEBUG)>,\n",
       " 'matplotlib._constrained_layout': <Logger matplotlib._constrained_layout (DEBUG)>,\n",
       " 'matplotlib.colorbar': <Logger matplotlib.colorbar (DEBUG)>,\n",
       " 'PIL.PngImagePlugin': <Logger PIL.PngImagePlugin (DEBUG)>,\n",
       " 'matplotlib.image': <Logger matplotlib.image (DEBUG)>,\n",
       " 'matplotlib.style.core': <Logger matplotlib.style.core (DEBUG)>,\n",
       " 'matplotlib.style': <logging.PlaceHolder at 0x2b94002e8d00>,\n",
       " 'matplotlib.category': <Logger matplotlib.category (DEBUG)>,\n",
       " 'matplotlib.dates': <Logger matplotlib.dates (DEBUG)>,\n",
       " 'matplotlib.axis': <Logger matplotlib.axis (DEBUG)>,\n",
       " 'matplotlib.axes._base': <Logger matplotlib.axes._base (DEBUG)>,\n",
       " 'matplotlib.axes': <logging.PlaceHolder at 0x2b940046ef10>,\n",
       " 'matplotlib.axes._axes': <Logger matplotlib.axes._axes (DEBUG)>,\n",
       " 'matplotlib.figure': <Logger matplotlib.figure (DEBUG)>,\n",
       " 'matplotlib.pyplot': <Logger matplotlib.pyplot (DEBUG)>,\n",
       " 'fsspec': <Logger fsspec (DEBUG)>,\n",
       " 'datasets': <Logger datasets (WARNING)>,\n",
       " 'datasets.utils.file_utils': <Logger datasets.utils.file_utils (WARNING)>,\n",
       " 'datasets.utils': <logging.PlaceHolder at 0x2b9403a084c0>,\n",
       " 'datasets.utils.info_utils': <Logger datasets.utils.info_utils (WARNING)>,\n",
       " 'datasets.utils.py_utils': <Logger datasets.utils.py_utils (WARNING)>,\n",
       " 'datasets.utils.download_manager': <Logger datasets.utils.download_manager (WARNING)>,\n",
       " 'datasets.utils.mock_download_manager': <Logger datasets.utils.mock_download_manager (WARNING)>,\n",
       " 'datasets.utils.deprecation_utils': <Logger datasets.utils.deprecation_utils (WARNING)>,\n",
       " 'datasets.config': <Logger datasets.config (WARNING)>,\n",
       " 'datasets.table': <Logger datasets.table (WARNING)>,\n",
       " 'datasets.arrow_reader': <Logger datasets.arrow_reader (WARNING)>,\n",
       " 'aiohttp.access': <Logger aiohttp.access (DEBUG)>,\n",
       " 'aiohttp': <logging.PlaceHolder at 0x2b9408437d30>,\n",
       " 'aiohttp.client': <Logger aiohttp.client (DEBUG)>,\n",
       " 'aiohttp.internal': <Logger aiohttp.internal (DEBUG)>,\n",
       " 'aiohttp.server': <Logger aiohttp.server (DEBUG)>,\n",
       " 'aiohttp.web': <Logger aiohttp.web (DEBUG)>,\n",
       " 'aiohttp.websocket': <Logger aiohttp.websocket (DEBUG)>,\n",
       " 'charset_normalizer': <Logger charset_normalizer (DEBUG)>,\n",
       " 'datasets.utils.streaming_download_manager': <Logger datasets.utils.streaming_download_manager (WARNING)>,\n",
       " 'datasets.features.features': <Logger datasets.features.features (WARNING)>,\n",
       " 'datasets.features': <logging.PlaceHolder at 0x2b9409088fa0>,\n",
       " 'datasets.tasks': <Logger datasets.tasks (WARNING)>,\n",
       " 'datasets.info': <Logger datasets.info (WARNING)>,\n",
       " 'datasets.arrow_writer': <Logger datasets.arrow_writer (WARNING)>,\n",
       " 'datasets.fingerprint': <Logger datasets.fingerprint (WARNING)>,\n",
       " 'datasets.formatting': <Logger datasets.formatting (WARNING)>,\n",
       " 'datasets.search': <Logger datasets.search (WARNING)>,\n",
       " 'datasets.arrow_dataset': <Logger datasets.arrow_dataset (WARNING)>,\n",
       " 'datasets.data_files': <Logger datasets.data_files (WARNING)>,\n",
       " 'datasets.dataset_dict': <Logger datasets.dataset_dict (WARNING)>,\n",
       " 'datasets.builder': <Logger datasets.builder (WARNING)>,\n",
       " 'datasets.combine': <Logger datasets.combine (WARNING)>,\n",
       " 'datasets.metric': <Logger datasets.metric (WARNING)>,\n",
       " 'datasets.packaged_modules.csv.csv': <Logger datasets.packaged_modules.csv.csv (WARNING)>,\n",
       " 'datasets.packaged_modules.csv': <logging.PlaceHolder at 0x2b94091c57f0>,\n",
       " 'datasets.packaged_modules': <logging.PlaceHolder at 0x2b94091c5850>,\n",
       " 'datasets.packaged_modules.json.json': <Logger datasets.packaged_modules.json.json (WARNING)>,\n",
       " 'datasets.packaged_modules.json': <logging.PlaceHolder at 0x2b94091d50a0>,\n",
       " 'datasets.packaged_modules.parquet.parquet': <Logger datasets.packaged_modules.parquet.parquet (WARNING)>,\n",
       " 'datasets.packaged_modules.parquet': <logging.PlaceHolder at 0x2b94091d5b50>,\n",
       " 'datasets.packaged_modules.text.text': <Logger datasets.packaged_modules.text.text (WARNING)>,\n",
       " 'datasets.packaged_modules.text': <logging.PlaceHolder at 0x2b94091db070>,\n",
       " 'datasets.utils.patching': <Logger datasets.utils.patching (WARNING)>,\n",
       " 'datasets.streaming': <Logger datasets.streaming (WARNING)>,\n",
       " 'datasets.load': <Logger datasets.load (WARNING)>,\n",
       " 'datasets.inspect': <Logger datasets.inspect (WARNING)>,\n",
       " 'accelerate.accelerator': <Logger accelerate.accelerator (DEBUG)>,\n",
       " 'accelerate': <logging.PlaceHolder at 0x2b94092016a0>,\n",
       " 'trainer': <Logger trainer (DEBUG)>,\n",
       " 'transformers.models.distilbert.configuration_distilbert': <Logger transformers.models.distilbert.configuration_distilbert (WARNING)>,\n",
       " 'transformers.models.distilbert': <logging.PlaceHolder at 0x2b94092e1820>,\n",
       " 'transformers.models.distilbert.tokenization_distilbert': <Logger transformers.models.distilbert.tokenization_distilbert (WARNING)>,\n",
       " 'transformers.models.bert.tokenization_bert_fast': <Logger transformers.models.bert.tokenization_bert_fast (WARNING)>,\n",
       " 'transformers.models.distilbert.tokenization_distilbert_fast': <Logger transformers.models.distilbert.tokenization_distilbert_fast (WARNING)>,\n",
       " 'transformers.models.yoso.configuration_yoso': <Logger transformers.models.yoso.configuration_yoso (WARNING)>,\n",
       " 'transformers.models.yoso': <logging.PlaceHolder at 0x2b940933ea60>,\n",
       " 'transformers.models.nystromformer.configuration_nystromformer': <Logger transformers.models.nystromformer.configuration_nystromformer (WARNING)>,\n",
       " 'transformers.models.nystromformer': <logging.PlaceHolder at 0x2b940933e9d0>,\n",
       " 'transformers.models.qdqbert.configuration_qdqbert': <Logger transformers.models.qdqbert.configuration_qdqbert (WARNING)>,\n",
       " 'transformers.models.qdqbert': <logging.PlaceHolder at 0x2b940933e880>,\n",
       " 'transformers.models.fnet.configuration_fnet': <Logger transformers.models.fnet.configuration_fnet (WARNING)>,\n",
       " 'transformers.models.fnet': <logging.PlaceHolder at 0x2b940933ef70>,\n",
       " 'transformers.models.perceiver.configuration_perceiver': <Logger transformers.models.perceiver.configuration_perceiver (WARNING)>,\n",
       " 'transformers.models.perceiver': <logging.PlaceHolder at 0x2b940930d250>,\n",
       " 'transformers.models.gptj.configuration_gptj': <Logger transformers.models.gptj.configuration_gptj (WARNING)>,\n",
       " 'transformers.models.gptj': <logging.PlaceHolder at 0x2b940930d280>,\n",
       " 'transformers.models.layoutlmv2.configuration_layoutlmv2': <Logger transformers.models.layoutlmv2.configuration_layoutlmv2 (WARNING)>,\n",
       " 'transformers.models.layoutlmv2': <logging.PlaceHolder at 0x2b940930d700>,\n",
       " 'transformers.models.rembert.configuration_rembert': <Logger transformers.models.rembert.configuration_rembert (WARNING)>,\n",
       " 'transformers.models.rembert': <logging.PlaceHolder at 0x2b940930d880>,\n",
       " 'transformers.models.canine.configuration_canine': <Logger transformers.models.canine.configuration_canine (WARNING)>,\n",
       " 'transformers.models.canine': <logging.PlaceHolder at 0x2b940930d970>,\n",
       " 'transformers.models.roformer.configuration_roformer': <Logger transformers.models.roformer.configuration_roformer (WARNING)>,\n",
       " 'transformers.models.roformer': <logging.PlaceHolder at 0x2b940930dca0>,\n",
       " 'transformers.models.bigbird_pegasus.configuration_bigbird_pegasus': <Logger transformers.models.bigbird_pegasus.configuration_bigbird_pegasus (WARNING)>,\n",
       " 'transformers.models.bigbird_pegasus': <logging.PlaceHolder at 0x2b940930dee0>,\n",
       " 'transformers.models.gpt_neo.configuration_gpt_neo': <Logger transformers.models.gpt_neo.configuration_gpt_neo (WARNING)>,\n",
       " 'transformers.models.gpt_neo': <logging.PlaceHolder at 0x2b94111e55e0>,\n",
       " 'transformers.models.big_bird.configuration_big_bird': <Logger transformers.models.big_bird.configuration_big_bird (WARNING)>,\n",
       " 'transformers.models.big_bird': <logging.PlaceHolder at 0x2b94111e5940>,\n",
       " 'transformers.models.convbert.configuration_convbert': <Logger transformers.models.convbert.configuration_convbert (WARNING)>,\n",
       " 'transformers.models.convbert': <logging.PlaceHolder at 0x2b94111e5850>,\n",
       " 'transformers.models.led.configuration_led': <Logger transformers.models.led.configuration_led (WARNING)>,\n",
       " 'transformers.models.led': <logging.PlaceHolder at 0x2b94111e5bb0>,\n",
       " 'transformers.models.ibert.configuration_ibert': <Logger transformers.models.ibert.configuration_ibert (WARNING)>,\n",
       " 'transformers.models.ibert': <logging.PlaceHolder at 0x2b94111d9130>,\n",
       " 'transformers.models.mobilebert.configuration_mobilebert': <Logger transformers.models.mobilebert.configuration_mobilebert (WARNING)>,\n",
       " 'transformers.models.mobilebert': <logging.PlaceHolder at 0x2b94111d9250>,\n",
       " 'transformers.models.roberta.configuration_roberta': <Logger transformers.models.roberta.configuration_roberta (WARNING)>,\n",
       " 'transformers.models.roberta': <logging.PlaceHolder at 0x2b94111d9d90>,\n",
       " 'transformers.models.camembert.configuration_camembert': <Logger transformers.models.camembert.configuration_camembert (WARNING)>,\n",
       " 'transformers.models.camembert': <logging.PlaceHolder at 0x2b94111d9940>,\n",
       " 'transformers.models.xlm_roberta.configuration_xlm_roberta': <Logger transformers.models.xlm_roberta.configuration_xlm_roberta (WARNING)>,\n",
       " 'transformers.models.xlm_roberta': <logging.PlaceHolder at 0x2b94111d9fa0>,\n",
       " 'transformers.models.mbart.configuration_mbart': <Logger transformers.models.mbart.configuration_mbart (WARNING)>,\n",
       " 'transformers.models.mbart': <logging.PlaceHolder at 0x2b940933efa0>,\n",
       " 'transformers.models.megatron_bert.configuration_megatron_bert': <Logger transformers.models.megatron_bert.configuration_megatron_bert (WARNING)>,\n",
       " 'transformers.models.megatron_bert': <logging.PlaceHolder at 0x2b94092e1fd0>,\n",
       " 'transformers.models.mpnet.configuration_mpnet': <Logger transformers.models.mpnet.configuration_mpnet (WARNING)>,\n",
       " 'transformers.models.mpnet': <logging.PlaceHolder at 0x2b94092e1fa0>,\n",
       " 'transformers.models.bart.configuration_bart': <Logger transformers.models.bart.configuration_bart (WARNING)>,\n",
       " 'transformers.models.bart': <logging.PlaceHolder at 0x2b94111c8340>,\n",
       " 'transformers.models.reformer.configuration_reformer': <Logger transformers.models.reformer.configuration_reformer (WARNING)>,\n",
       " 'transformers.models.reformer': <logging.PlaceHolder at 0x2b94111c8520>,\n",
       " 'transformers.models.longformer.configuration_longformer': <Logger transformers.models.longformer.configuration_longformer (WARNING)>,\n",
       " 'transformers.models.longformer': <logging.PlaceHolder at 0x2b94111c89d0>,\n",
       " 'transformers.models.deberta_v2.configuration_deberta_v2': <Logger transformers.models.deberta_v2.configuration_deberta_v2 (WARNING)>,\n",
       " 'transformers.models.deberta_v2': <logging.PlaceHolder at 0x2b94111c8940>,\n",
       " 'transformers.models.deberta.configuration_deberta': <Logger transformers.models.deberta.configuration_deberta (WARNING)>,\n",
       " 'transformers.models.deberta': <logging.PlaceHolder at 0x2b94111c8e80>,\n",
       " 'transformers.models.xlm.configuration_xlm': <Logger transformers.models.xlm.configuration_xlm (WARNING)>,\n",
       " 'transformers.models.xlm': <logging.PlaceHolder at 0x2b94111bc2b0>,\n",
       " 'transformers.models.flaubert.configuration_flaubert': <Logger transformers.models.flaubert.configuration_flaubert (WARNING)>,\n",
       " 'transformers.models.flaubert': <logging.PlaceHolder at 0x2b94111bc070>,\n",
       " 'transformers.models.squeezebert.configuration_squeezebert': <Logger transformers.models.squeezebert.configuration_squeezebert (WARNING)>,\n",
       " 'transformers.models.squeezebert': <logging.PlaceHolder at 0x2b94111bc5e0>,\n",
       " 'transformers.models.openai.configuration_openai': <Logger transformers.models.openai.configuration_openai (WARNING)>,\n",
       " 'transformers.models.openai': <logging.PlaceHolder at 0x2b94111bc850>,\n",
       " 'transformers.models.gpt2.configuration_gpt2': <Logger transformers.models.gpt2.configuration_gpt2 (WARNING)>,\n",
       " 'transformers.models.gpt2': <logging.PlaceHolder at 0x2b94111bcd90>,\n",
       " 'transformers.models.transfo_xl.configuration_transfo_xl': <Logger transformers.models.transfo_xl.configuration_transfo_xl (WARNING)>,\n",
       " 'transformers.models.transfo_xl': <logging.PlaceHolder at 0x2b941116b1c0>,\n",
       " 'transformers.models.xlnet.configuration_xlnet': <Logger transformers.models.xlnet.configuration_xlnet (WARNING)>,\n",
       " 'transformers.models.xlnet': <logging.PlaceHolder at 0x2b941116b2e0>,\n",
       " 'transformers.models.ctrl.configuration_ctrl': <Logger transformers.models.ctrl.configuration_ctrl (WARNING)>,\n",
       " 'transformers.models.ctrl': <logging.PlaceHolder at 0x2b941116b700>,\n",
       " 'transformers.models.electra.configuration_electra': <Logger transformers.models.electra.configuration_electra (WARNING)>,\n",
       " 'transformers.models.electra': <logging.PlaceHolder at 0x2b941116b7c0>,\n",
       " 'transformers.models.funnel.configuration_funnel': <Logger transformers.models.funnel.configuration_funnel (WARNING)>,\n",
       " 'transformers.models.funnel': <logging.PlaceHolder at 0x2b941116bd60>,\n",
       " 'transformers.activations': <Logger transformers.activations (WARNING)>,\n",
       " 'transformers.deepspeed': <Logger transformers.deepspeed (WARNING)>,\n",
       " 'transformers.generation_logits_process': <Logger transformers.generation_logits_process (WARNING)>,\n",
       " 'transformers.generation_utils': <Logger transformers.generation_utils (WARNING)>,\n",
       " 'transformers.modeling_utils': <Logger transformers.modeling_utils (WARNING)>,\n",
       " 'transformers.models.distilbert.modeling_distilbert': <Logger transformers.models.distilbert.modeling_distilbert (WARNING)>,\n",
       " 'transformers.models.gpt2.tokenization_gpt2': <Logger transformers.models.gpt2.tokenization_gpt2 (WARNING)>,\n",
       " 'transformers.models.gpt2.tokenization_gpt2_fast': <Logger transformers.models.gpt2.tokenization_gpt2_fast (WARNING)>,\n",
       " 'transformers.models.roberta.tokenization_roberta': <Logger transformers.models.roberta.tokenization_roberta (WARNING)>,\n",
       " 'transformers.models.roberta.tokenization_roberta_fast': <Logger transformers.models.roberta.tokenization_roberta_fast (WARNING)>,\n",
       " 'transformers.models.bart.tokenization_bart': <Logger transformers.models.bart.tokenization_bart (WARNING)>,\n",
       " 'transformers.models.bart.tokenization_bart_fast': <Logger transformers.models.bart.tokenization_bart_fast (WARNING)>,\n",
       " 'transformers.models.m2m_100.configuration_m2m_100': <Logger transformers.models.m2m_100.configuration_m2m_100 (WARNING)>,\n",
       " 'transformers.models.m2m_100': <logging.PlaceHolder at 0x2b9411154cd0>,\n",
       " 'transformers.models.blenderbot_small.configuration_blenderbot_small': <Logger transformers.models.blenderbot_small.configuration_blenderbot_small (WARNING)>,\n",
       " 'transformers.models.blenderbot_small': <logging.PlaceHolder at 0x2b9411154a90>,\n",
       " 'transformers.models.mt5.configuration_mt5': <Logger transformers.models.mt5.configuration_mt5 (WARNING)>,\n",
       " 'transformers.models.mt5': <logging.PlaceHolder at 0x2b94111651f0>,\n",
       " 'transformers.models.t5.configuration_t5': <Logger transformers.models.t5.configuration_t5 (WARNING)>,\n",
       " 'transformers.models.pegasus.configuration_pegasus': <Logger transformers.models.pegasus.configuration_pegasus (WARNING)>,\n",
       " 'transformers.models.pegasus': <logging.PlaceHolder at 0x2b94111655e0>,\n",
       " 'transformers.models.marian.configuration_marian': <Logger transformers.models.marian.configuration_marian (WARNING)>,\n",
       " 'transformers.models.marian': <logging.PlaceHolder at 0x2b9411165580>,\n",
       " 'transformers.models.blenderbot.configuration_blenderbot': <Logger transformers.models.blenderbot.configuration_blenderbot (WARNING)>,\n",
       " 'transformers.models.blenderbot': <logging.PlaceHolder at 0x2b94111658e0>,\n",
       " 'transformers.models.fsmt.configuration_fsmt': <Logger transformers.models.fsmt.configuration_fsmt (WARNING)>,\n",
       " 'transformers.models.fsmt': <logging.PlaceHolder at 0x2b9411165ac0>,\n",
       " 'transformers.models.prophetnet.configuration_prophetnet': <Logger transformers.models.prophetnet.configuration_prophetnet (WARNING)>,\n",
       " 'transformers.models.prophetnet': <logging.PlaceHolder at 0x2b941d76b130>,\n",
       " 'transformers.models.xlm_prophetnet.configuration_xlm_prophetnet': <Logger transformers.models.xlm_prophetnet.configuration_xlm_prophetnet (WARNING)>,\n",
       " 'transformers.models.xlm_prophetnet': <logging.PlaceHolder at 0x2b94111659d0>,\n",
       " 'transformers.models.bart.modeling_bart': <Logger transformers.models.bart.modeling_bart (WARNING)>,\n",
       " 'transformers.models.swin.configuration_swin': <Logger transformers.models.swin.configuration_swin (WARNING)>,\n",
       " 'transformers.models.swin': <logging.PlaceHolder at 0x2b9411238b80>,\n",
       " 'transformers.models.vilt.configuration_vilt': <Logger transformers.models.vilt.configuration_vilt (WARNING)>,\n",
       " 'transformers.models.vilt': <logging.PlaceHolder at 0x2b9411238bb0>,\n",
       " 'transformers.models.vit_mae.configuration_vit_mae': <Logger transformers.models.vit_mae.configuration_vit_mae (WARNING)>,\n",
       " 'transformers.models.vit_mae': <logging.PlaceHolder at 0x2b9411238d60>,\n",
       " 'transformers.models.imagegpt.configuration_imagegpt': <Logger transformers.models.imagegpt.configuration_imagegpt (WARNING)>,\n",
       " 'transformers.models.imagegpt': <logging.PlaceHolder at 0x2b94112501c0>,\n",
       " 'transformers.models.segformer.configuration_segformer': <Logger transformers.models.segformer.configuration_segformer (WARNING)>,\n",
       " 'transformers.models.segformer': <logging.PlaceHolder at 0x2b9411250100>,\n",
       " 'transformers.models.clip.configuration_clip': <Logger transformers.models.clip.configuration_clip (WARNING)>,\n",
       " 'transformers.models.clip': <logging.PlaceHolder at 0x2b94112506a0>,\n",
       " 'transformers.models.vision_text_dual_encoder.configuration_vision_text_dual_encoder': <Logger transformers.models.vision_text_dual_encoder.configuration_vision_text_dual_encoder (WARNING)>,\n",
       " 'transformers.models.vision_text_dual_encoder': <logging.PlaceHolder at 0x2b9411250460>,\n",
       " 'transformers.models.beit.configuration_beit': <Logger transformers.models.beit.configuration_beit (WARNING)>,\n",
       " 'transformers.models.beit': <logging.PlaceHolder at 0x2b94112502b0>,\n",
       " 'transformers.models.visual_bert.configuration_visual_bert': <Logger transformers.models.visual_bert.configuration_visual_bert (WARNING)>,\n",
       " 'transformers.models.visual_bert': <logging.PlaceHolder at 0x2b9411250be0>,\n",
       " 'transformers.models.deit.configuration_deit': <Logger transformers.models.deit.configuration_deit (WARNING)>,\n",
       " 'transformers.models.deit': <logging.PlaceHolder at 0x2b9411250af0>,\n",
       " 'transformers.models.luke.configuration_luke': <Logger transformers.models.luke.configuration_luke (WARNING)>,\n",
       " 'transformers.models.luke': <logging.PlaceHolder at 0x2b9411250ca0>,\n",
       " 'transformers.models.detr.configuration_detr': <Logger transformers.models.detr.configuration_detr (WARNING)>,\n",
       " 'transformers.models.detr': <logging.PlaceHolder at 0x2b94112651f0>,\n",
       " 'transformers.models.speech_to_text.configuration_speech_to_text': <Logger transformers.models.speech_to_text.configuration_speech_to_text (WARNING)>,\n",
       " 'transformers.models.speech_to_text': <logging.PlaceHolder at 0x2b9411265280>,\n",
       " 'transformers.models.vit.configuration_vit': <Logger transformers.models.vit.configuration_vit (WARNING)>,\n",
       " 'transformers.models.vit': <logging.PlaceHolder at 0x2b94112652b0>,\n",
       " 'transformers.models.wav2vec2.configuration_wav2vec2': <Logger transformers.models.wav2vec2.configuration_wav2vec2 (WARNING)>,\n",
       " 'transformers.models.wav2vec2': <logging.PlaceHolder at 0x2b9411265460>,\n",
       " 'transformers.models.retribert.configuration_retribert': <Logger transformers.models.retribert.configuration_retribert (WARNING)>,\n",
       " 'transformers.models.retribert': <logging.PlaceHolder at 0x2b9411265610>,\n",
       " 'transformers.models.hubert.configuration_hubert': <Logger transformers.models.hubert.configuration_hubert (WARNING)>,\n",
       " 'transformers.models.hubert': <logging.PlaceHolder at 0x2b94112657c0>,\n",
       " 'transformers.models.lxmert.configuration_lxmert': <Logger transformers.models.lxmert.configuration_lxmert (WARNING)>,\n",
       " 'transformers.models.lxmert': <logging.PlaceHolder at 0x2b9411265880>,\n",
       " 'transformers.models.dpr.configuration_dpr': <Logger transformers.models.dpr.configuration_dpr (WARNING)>,\n",
       " 'transformers.models.dpr': <logging.PlaceHolder at 0x2b9411265ee0>,\n",
       " 'transformers.models.splinter.configuration_splinter': <Logger transformers.models.splinter.configuration_splinter (WARNING)>,\n",
       " 'transformers.models.splinter': <logging.PlaceHolder at 0x2b9411265fd0>,\n",
       " 'transformers.models.sew_d.configuration_sew_d': <Logger transformers.models.sew_d.configuration_sew_d (WARNING)>,\n",
       " 'transformers.models.sew_d': <logging.PlaceHolder at 0x2b941d6ca130>,\n",
       " 'transformers.models.sew.configuration_sew': <Logger transformers.models.sew.configuration_sew (WARNING)>,\n",
       " 'transformers.models.sew': <logging.PlaceHolder at 0x2b941d6ca190>,\n",
       " 'transformers.models.unispeech_sat.configuration_unispeech_sat': <Logger transformers.models.unispeech_sat.configuration_unispeech_sat (WARNING)>,\n",
       " 'transformers.models.unispeech_sat': <logging.PlaceHolder at 0x2b9411238400>,\n",
       " 'transformers.models.unispeech.configuration_unispeech': <Logger transformers.models.unispeech.configuration_unispeech (WARNING)>,\n",
       " 'transformers.models.unispeech': <logging.PlaceHolder at 0x2b9411238580>,\n",
       " 'transformers.models.wavlm.configuration_wavlm': <Logger transformers.models.wavlm.configuration_wavlm (WARNING)>,\n",
       " 'transformers.models.wavlm': <logging.PlaceHolder at 0x2b94112388b0>,\n",
       " 'transformers.models.bert.modeling_bert': <Logger transformers.models.bert.modeling_bert (WARNING)>,\n",
       " 'terminado': <Logger terminado (DEBUG)>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.Logger.manager.loggerDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "debug() missing 1 required positional argument: 'msg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d94d6782c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: debug() missing 1 required positional argument: 'msg'"
     ]
    }
   ],
   "source": [
    "logger.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
