{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, wandb, gc, numpy as np, pandas as pd,os\n",
    "from wandb.data_types import Histogram\n",
    "from tqdm.auto import tqdm\n",
    "from travis_attack.utils import timecode, show_gpu, merge_dicts, unpack_nested_lists_in_df, display_all\n",
    "from travis_attack.tests import check_no_nans_or_infs\n",
    "from travis_attack.models import save_pp_model, resume_pp_model, get_vm_probs, get_start_end_special_token_ids\n",
    "from travis_attack.charts import plot_grad_flow, plot_examples_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from torch.distributions import Categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from travis_attack.utils import set_seed, set_session_options, prepare_logger\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models, get_optimizer\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from fastcore.basics import store_attr\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b253756c445fb811\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59c0bcbd75d465eb21daed415be79e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c802946231f72062\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7e72942b7d4f6b8ccd3049254ab7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-43a49c5188c42e69\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a93a2df509e4067bae10a45b9f650f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a5b42ff4a44bafa1ee687dda70b1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e81c40d03947dba954769b6f103c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e626f976b24454b560dc3f19a03c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e408a6dc67b42879b3f1a7cc1bc2698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3566a76746f34a4abcca4d890dd8ddaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2af6ea3bb6c43b2887df02a196cd5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42784a28f79043eeaeb7bee353774702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4137cfb7459545d6950299feb75a5074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125c3281b3e2410386beb66c0b59e3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e737799ceb634848b2d79de275eda498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35a557b0afa47089f271dc9cb71ab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec3a3621434cca9f9eea20af9233a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b3d2ed8136457d83d7a7ed48e44dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448de6b617454eb0bd15e326081d41a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5823b14918b44629ab2fa08ff22a47c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf44df15a24ab994e03e58cacf3010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367d47779db24ba5a3974d313fb8760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba435c5ecc14ebfa3eaed368165a183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468e0b1205284dffa4182023e5ec3ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f3f976510f427c94a879051432bf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67d26dec35749d59601ee6e37f3e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "accelerator = Accelerator()\n",
    "cfg = Config()\n",
    "cfg.device = accelerator.device\n",
    "set_seed(cfg.seed)\n",
    "set_session_options()\n",
    "logger = prepare_logger()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)\n",
    "optimizer = get_optimizer(cfg, pp_model)\n",
    "ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model)\n",
    "vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'] = accelerator.prepare(vm_model,pp_model,sts_model,optimizer,ds.dld_tkn['train'])\n",
    "cfg.n_train_steps = cfg.n_train_epochs * len(ds.dld_tkn['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Trainer: \n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer, accelerator, ds,\n",
    "                logger): \n",
    "        store_attr()\n",
    "        self._cfg = self.cfg; del self.cfg;\n",
    "        self.accumulation_num,self.global_step,self.eval_num = 0,0,0\n",
    "        self._reset_batch_dicts()\n",
    "        #resume_pp_model(f\"{path_checkpoints}devout-durian-172_39\")\n",
    "        self._setup_wandb_run()\n",
    "        self._setup_data_stores()\n",
    "        if self._cfg.wandb['plot_examples']: self._setup_wandb_examples_plots()\n",
    "        self.start_end_token_d = get_start_end_special_token_ids(self.pp_tokenizer)\n",
    "        ## TODO: why is num_processes set to 1?\n",
    "        #%lprun -f training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)\n",
    "        notebook_launcher(self.training_function, args=(), \n",
    "                           num_processes=1, use_fp16=self._cfg.use_fp16)\n",
    "       \n",
    "    def _reset_batch_dicts(self): \n",
    "          # train_batch_d holds all info to write to csv, time_d has times, wandb_d has everything to log to wandb\n",
    "        # there will be overlap between them. \n",
    "        self.batch_d,self.batch_time_d,self.batch_wandb_d = dict(),dict(),dict()\n",
    "    \n",
    "    def _setup_wandb_run(self): \n",
    "        \"\"\"Init wandb run, set up paths, create dir for model artifacts if needed, \"\"\"\n",
    "        ## TODO: set notebook name and add in save_code \n",
    "        self.run = wandb.init(project=self._cfg.wandb['project'], entity=self._cfg.wandb['entity'], \n",
    "                              config=vars(self._cfg), mode=self._cfg.wandb['mode'],\n",
    "                              notes=self._cfg.wandb['run_notes'])\n",
    "        if self._cfg.wandb['log_grads']: \n",
    "            wandb.watch(self.pp_model, log='gradients', log_freq=self._cfg.wandb['log_grads_freq'])\n",
    "        self._cfg.run_name,self._cfg.run_id = self.run.name, self.run.id\n",
    "        self._cfg.path_run = f\"{self._cfg.path_checkpoints}{self.run.name}/\"\n",
    "        if not os.path.exists(self._cfg.path_run): os.makedirs(self._cfg.path_run, exist_ok=True)\n",
    "    \n",
    "    def _setup_data_stores(self): \n",
    "        \"\"\"Setup dict `self.data_d` to store observations. Setup column names for wandb tables. \"\"\"\n",
    "        # Raw observation data (lists of dicts, later becomes pandas df)\n",
    "        self.data_d = dict()\n",
    "        for split in self._cfg.splits + ['training_step']:   self.data_d[split] = [] \n",
    "    \n",
    "    def _setup_wandb_examples_plots(self): \n",
    "        \"\"\"If we plot a few examples this sets that up.\"\"\"\n",
    "        def get_examples_plot_idxs(dataset): \n",
    "            \"\"\"Get data indices for the examples plots\"\"\"\n",
    "            return np.random.choice(dataset['idx'], size=self._cfg.wandb['n_examples_plot'], replace=False).tolist()\n",
    "        self.plt_idx_d = dict()\n",
    "        for split in self._cfg.splits:  self.plt_idx_d[split] = get_examples_plot_idxs(self.ds.dsd[split])\n",
    "\n",
    "    def training_function(self): \n",
    "        self.logger.debug(show_gpu(f'GPU memory usage after loading models:'))\n",
    "        progress_bar = tqdm(range(self._cfg.n_train_steps))\n",
    "        self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none) \n",
    "        for self.epoch in range(self._cfg.n_train_epochs): \n",
    "            self.logger.info(f\"Now on epoch {self.epoch} of {self._cfg.n_train_epochs}\")\n",
    "            if not self.pp_model.training: self.pp_model.train()\n",
    "            with timecode() as time_train_one_epoch:\n",
    "                for self.batch_num, (data, raw) in enumerate(zip(self.ds.dld_tkn['train'], self.ds.dld_raw['train'])): \n",
    "                    self._reset_batch_dicts()\n",
    "                    self.training_step(data, raw) \n",
    "                    self.accumulation_num += 1  ; self.global_step += 1 ;  progress_bar.update(1) \n",
    "                    \n",
    "            wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                       'time/train_one_epoch_thoroughput': len(self.ds.dsd_tkn['train']) / time_train_one_epoch.t,\n",
    "                       'epoch': self.epoch}, commit=True)\n",
    "\n",
    "            if self._cfg.wandb['log_grads'] and self.epoch % self._cfg.wandb_log_grads_freq == 0: \n",
    "                plt = plot_grad_flow(self.pp_model.named_parameters())\n",
    "                wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "                del plt \n",
    "            #gc.collect() \n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            if self._cfg.save_model_while_training and (self.epoch + 1) % self._cfg.save_model_freq == 0:  save_model(epoch)\n",
    "\n",
    "            # Evaluation loop\n",
    "            if self.epoch % self._cfg.eval_freq == 0: \n",
    "                self.eval_num += 1\n",
    "                with timecode() as time_eval_train:\n",
    "                    self.eval_dl(split='train') # or train_eval?\n",
    "                with timecode() as time_eval_valid:\n",
    "                    self.eval_dl(split='valid')\n",
    "                self.plot_wandb_charts()\n",
    "                with timecode() as time_eval_gc_collect:\n",
    "                    gc.collect() \n",
    "                with timecode() as time_eval_empty_cache:\n",
    "                    torch.cuda.empty_cache()\n",
    "                wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                           'time/eval_train_thoroughput': len(self.ds.dsd_tkn['train']) / time_eval_train.t,\n",
    "                           'time/eval_valid_thoroughput': len(self.ds.dsd_tkn['valid']) / time_eval_valid.t, \n",
    "                           'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                           'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "                   'epoch': self.epoch}, commit=True)\n",
    "        # Eval on test set \n",
    "        self.eval_dl(split='test')\n",
    "\n",
    "        # Data -> df and save dfs to file \n",
    "        for key in self.data_d.keys():  # splits and sometimes 'training_step' too \n",
    "            self.data_d[key] = self._convert_data_d_to_df(key)\n",
    "            self._set_df_colorder(key)\n",
    "            self.data_d[key].to_csv(f\"{self._cfg.path_run}{key}.csv\", index=False)\n",
    "        \n",
    "        # plot_wandb_charts()  # don't think I need this\n",
    "        self.add_wandb_run_summary_statistics()     \n",
    "        self.run.finish()\n",
    "        \n",
    "    def training_step(self, data, raw): \n",
    "        \"\"\"Forward pass, loss function, backwards pass, parameter update (with gradient accumulation optional), \n",
    "        recording results, wandb logging. \n",
    "        \"\"\"\n",
    "        if not self.pp_model.training: self.pp_model.train()\n",
    "        if not self.vm_model.training: self.vm_model.train()\n",
    "            \n",
    "        with timecode() as self.batch_time_d['time_generate_pp']:\n",
    "            pp_output, pp_l = self.pp_model_forward(data)\n",
    "\n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after forward pass: '))\n",
    "\n",
    "        with self.accelerator.autocast():\n",
    "            with timecode() as self.batch_time_d['time_loss_fn']:\n",
    "                loss_batch = self.loss_fn(data, raw, pp_output, pp_l)\n",
    "            loss_batch = loss_batch / self._cfg.accumulation_steps  # Normalize our loss for gradient accumulation\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_backwards']:\n",
    "            self.accelerator.backward(loss_batch) \n",
    "\n",
    "        logger.debug(show_gpu(f'TRAIN, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after backwards pass: '))\n",
    "        if (self.accumulation_num + 1) % self._cfg.accumulation_steps == 0: \n",
    "            with timecode() as self.batch_time_d['time_opt_step']:\n",
    "                self.optimizer.step()\n",
    "            self.pp_model.zero_grad(set_to_none=self._cfg.zero_grad_with_none)\n",
    "            \n",
    "        self._prepare_train_batch_d(raw, data, pp_l)\n",
    "        self.data_d['training_step'].append(self.batch_d)\n",
    "        \n",
    "        self._wandb_log_training_step()\n",
    "    \n",
    "    def _add_batch_vars_to_batch_d(self, raw, data, pp_l): \n",
    "        # Add basics. (results are already added elsewhere)\n",
    "        self.batch_d = merge_dicts(self.batch_d, { 'idx': raw['idx'],\n",
    "            'epoch': self.epoch, 'batch_num': self.batch_num, 'global_step': self.global_step,\n",
    "            'accumulation_num': self.accumulation_num,  \"orig_l\": raw['text'], \n",
    "            \"orig_label\": data['label'].cpu().tolist(), \n",
    "            \"orig_truelabel_probs\": data['orig_truelabel_probs'].cpu().tolist(),\n",
    "            'orig_length': self.orig_length, 'orig_batch_size': self.orig_batch_size, \n",
    "            \"pp_l\": pp_l, 'pp_length': self.pp_length, 'pp_batch_size': self.pp_batch_size\n",
    "        })\n",
    "        \n",
    "    def _prepare_train_batch_d(self, raw, data, pp_l): \n",
    "        self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "        # Add times (only for training, not eval)\n",
    "        for k, v in self.batch_time_d.items(): self.batch_time_d[k] = v.t  # extract time from timecode object\n",
    "        self.batch_d = merge_dicts(self.batch_d, self.batch_time_d)\n",
    "    \n",
    "    def _wandb_log_training_step(self): \n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, {\n",
    "            'vm_scores_hist':       Histogram(self.batch_d['vm_score']), \n",
    "            'vm_scores_mean':       np.mean(  self.batch_d['vm_score']),\n",
    "            'sts_scores_hist':      Histogram(self.batch_d['sts_score']),\n",
    "            'sts_scores_mean':      np.mean(  self.batch_d['sts_score']), \n",
    "            'rewards_hist':         Histogram(self.batch_d['reward']),\n",
    "            'rewards_mean':         np.mean(  self.batch_d['reward']), \n",
    "            'pp_logp_hist':         Histogram(self.batch_d['pp_logp']),\n",
    "            'pp_logp_mean':         np.mean(  self.batch_d['pp_logp']),\n",
    "            'loss_hist'   :         Histogram(self.batch_d['loss'])})\n",
    "        self.batch_wandb_d = merge_dicts(self.batch_wandb_d, self.batch_d)\n",
    "        not_for_wandb_keys = ['orig_l', 'orig_label','orig_truelabel_probs', 'pp_l', 'loss', 'pp_logp', \n",
    "                              'reward', 'sts_score', 'vm_score',\n",
    "                              'pp_predclass_probs', 'label_flip', 'pp_predclass', 'pp_truelabel_probs']\n",
    "        for k in not_for_wandb_keys:  self.batch_wandb_d.pop(k, None)\n",
    "        wandb.log(self.batch_wandb_d, commit=True)\n",
    "        \n",
    "    def _wandb_log_eval_step(self): \n",
    "        ### TODO: implement\n",
    "        pass\n",
    "        wandb.log(self.batch_wandb_d, commit=True)\n",
    "        \n",
    "    def _convert_data_d_to_df(self, data_d_key): \n",
    "        df = pd.DataFrame(self.data_d[data_d_key])\n",
    "        # check all lists have the same number of elements\n",
    "        nonscalar_cols = df.columns[[o == np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        assert (df[nonscalar_cols].applymap(len) == cfg.batch_size_train).all(None)\n",
    "        # expand lists and broadcast scalars\n",
    "        scalar_cols = df.columns[[o != np.dtype('object') for o in df.head(1).dtypes]].tolist()\n",
    "        df_expanded = unpack_nested_lists_in_df(df, scalar_cols)\n",
    "        # check shape of new dataframe is correct \n",
    "        if data_d_key == \"training_step\": \n",
    "            df_shape = (self._cfg.ds_length[\"train\"]    * cfg.n_train_epochs, df.shape[1])\n",
    "        elif data_d_key in [\"train\", \"valid\"]: \n",
    "            df_shape = (self._cfg.ds_length[data_d_key] * self.eval_num,      df.shape[1])\n",
    "        elif data_d_key == \"test\": \n",
    "            df_shape = (self._cfg.ds_length[\"test\"],                           df.shape[1])\n",
    "        assert df_expanded.shape == df_shape\n",
    "        return df_expanded\n",
    "    \n",
    "    def pp_model_forward(self, data): \n",
    "        pp_output, pp_l = self.get_paraphrases(data['input_ids'], data['attention_mask'])\n",
    "        self._assert_start_and_end_tokens_are_correct(orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        self._update_batch_size_and_length_variables( orig_ids=data['input_ids'], pp_ids=pp_output.sequences)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def _assert_start_and_end_tokens_are_correct(self, orig_ids, pp_ids):\n",
    "        \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "        right special tokens (depends on tokenizer)\"\"\"\n",
    "        # Input\n",
    "        if self.start_end_token_d['input_start_id'] is not None: \n",
    "            assert torch.all(orig_ids[:,0] == self.start_end_token_d['input_start_id'])\n",
    "        # can probs rewrite this to make it nicer but it's fine for now\n",
    "        assert torch.all(torch.logical_or(orig_ids[:,-1] == self.start_end_token_d['input_end_id'][0], \n",
    "                                          orig_ids[:,-1] == self.start_end_token_d['input_end_id'][1]))\n",
    "\n",
    "        # Output\n",
    "        assert torch.all(pp_ids[:,0] == self.start_end_token_d['output_start_id'])\n",
    "        assert torch.all(torch.logical_or(pp_ids[:,-1] == self.start_end_token_d['output_end_id'][0], \n",
    "                                          pp_ids[:,-1] == self.start_end_token_d['output_end_id'][1]))\n",
    "        \n",
    "    def _update_batch_size_and_length_variables(self, orig_ids, pp_ids): \n",
    "        # Update variables\n",
    "        # for greedy search self.pp_length is equal to self.orig_batch_size but this won't be for beam search\n",
    "        self.orig_batch_size     = orig_ids.shape[0]\n",
    "        self.orig_length         = orig_ids.shape[1]\n",
    "        self.pp_batch_size       = pp_ids.shape[0]\n",
    "        self.pp_length           = pp_ids.shape[1] \n",
    "    \n",
    "    def get_paraphrases(self, orig_ids, attention_mask):\n",
    "        \"\"\"Wrapper for generating paraphrases (pp's).  Only greedy search supported at the moment\"\"\"\n",
    "        pp_output = self.pp_model.generate_with_grad(input_ids=orig_ids, \n",
    "                                                attention_mask=attention_mask, \n",
    "                                                 **self._cfg.pp,\n",
    "                                                 do_sample=False, \n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 output_scores=True,\n",
    "                                                 remove_invalid_values=False, \n",
    "                                                 pad_token_id = self.pp_tokenizer.pad_token_id,\n",
    "                                                 eos_token_id = self.pp_tokenizer.eos_token_id)\n",
    "        pp_l = self.pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "        return pp_output, pp_l\n",
    "    \n",
    "    def loss_fn(self, data, raw, pp_output, pp_l): \n",
    "        with timecode() as self.batch_time_d['time_reward_fn']:\n",
    "            reward = self.reward_fn(data, raw, pp_l)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_pp_logp']:\n",
    "            pp_logp = self.get_pp_logp(pp_output)\n",
    "\n",
    "        with timecode() as self.batch_time_d['time_loss_fn_loss_calc']:\n",
    "            loss = -reward * pp_logp\n",
    "            loss_batch = torch.mean(loss)\n",
    "\n",
    "        self.batch_d['pp_logp']    =    pp_logp.detach().cpu().tolist()\n",
    "        self.batch_d['loss']       =       loss.detach().cpu().tolist()\n",
    "        self.batch_d['loss_batch'] = loss_batch.detach().cpu().tolist()\n",
    "        return loss_batch\n",
    "    \n",
    "    def reward_fn(self, data, raw, pp_l): \n",
    "        \"\"\"\"\"\"\n",
    "        # Victim model probability differences between orig and pp\n",
    "        with timecode() as self.batch_time_d['time_vm_scores']:\n",
    "            pp_probs = get_vm_probs(pp_l, self._cfg, self.vm_tokenizer, self.vm_model, return_predclass=False)\n",
    "            pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "            pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "            pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "            label_flip = ((pp_predclass != data['label']) * 1)\n",
    "            vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "            \n",
    "        # STS scores\n",
    "        with timecode() as self.batch_time_d['time_sts_scores']:\n",
    "            pp_embeddings  = self.sts_model.encode(pp_l, batch_size=len(raw), convert_to_tensor=True, device=self._cfg.device)\n",
    "            # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "            sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "\n",
    "        # Reward calculation \n",
    "        rewards = torch.tensor([-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)],device=self._cfg.device)\n",
    "\n",
    "        if self._cfg.normalise_rewards: \n",
    "            self.batch_d['reward_unscaled'] = rewards.detach().cpu().tolist()\n",
    "            rewards = (rewards - torch.mean(rewards)) / torch.std(rewards)\n",
    "        \n",
    "        self.batch_d['pp_truelabel_probs']  = pp_truelabel_probs.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass']        = pp_predclass.detach().cpu().tolist()\n",
    "        self.batch_d['pp_predclass_probs']  = pp_predclass_probs.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip']          = label_flip.detach().cpu().tolist()\n",
    "        self.batch_d['label_flip_fraction'] = np.mean(self.batch_d['label_flip'])\n",
    "        self.batch_d['reward']              = rewards.detach().cpu().tolist()\n",
    "        self.batch_d['vm_score']            = vm_scores.detach().cpu().tolist()\n",
    "        self.batch_d['sts_score']           = sts_scores.detach().cpu().tolist()\n",
    "    \n",
    "        return rewards\n",
    "         \n",
    "    def get_pp_logp(self, pp_output): \n",
    "        \"\"\"log(p(pp|orig)) basically.\n",
    "        works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "        ### TODO: this looks like logp to me, not plogp. Find out if this is right and if so rename, if not, fix\n",
    "        ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "        # and has no probability attached to it, so we remove it. \n",
    "        seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "        assert seq_without_first_tkn.shape == torch.Size([self.orig_batch_size, self.pp_length - 1])\n",
    "\n",
    "        ### Convert from tuple of scores to one big tensor of scores \n",
    "        scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "        ### TESTS \n",
    "        # We check shape and that there is no +inf or nan in scores. \n",
    "        # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "        assert scores_stacked.shape == torch.Size([self.orig_batch_size, (self.pp_length - 1), self._cfg.vocab_size])\n",
    "        assert torch.all(~torch.isnan(scores_stacked))\n",
    "        assert torch.all(~torch.isposinf(scores_stacked))\n",
    "        # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "        # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "        # shouldn't be set to -inf\n",
    "        # Not a 100% test but very likely to identify\n",
    "        idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "        assert len(idx_neginf[idx_neginf[:,2] == self.pp_tokenizer.eos_token_id, :]) == \\\n",
    "                  (self._cfg.pp[\"min_length\"] -1) * self.orig_batch_size  \n",
    "        del idx_neginf\n",
    "\n",
    "        ### Take log softmax of scores and then extract those that correspond \n",
    "        # to the generated sequences    \n",
    "        scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "        seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "        ### TESTS \n",
    "        # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "        assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "        assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "        self._check_scores_log_softmax_sums(scores_log_softmax)\n",
    "        # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "        assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "        # Check that the last token probability corresponds to a possible end token\n",
    "        # this has to be tested before the attention mask is multiplied with it because if the \n",
    "        # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "        output_end_ids = self.start_end_token_d['output_end_id']\n",
    "        assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "        del output_end_ids\n",
    "        ## THIS ONE IS LONG - a test rather than assert \n",
    "        # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "        #                                             seq_token_log_probs) \n",
    "\n",
    "        ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "        # sequence probabilities so that we don't consider probability of padding tokens \n",
    "        # when getting sequence probabilities. \n",
    "        # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "        # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "        # since  -inf * 0 = nan \n",
    "        attention_mask = self.pp_model._prepare_attention_mask_for_generation(\n",
    "            seq_without_first_tkn, self.pp_tokenizer.pad_token_id, self.pp_tokenizer.eos_token_id\n",
    "        )\n",
    "        seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-10000)\n",
    "        seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "        ### TESTS\n",
    "        assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "        # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "        assert all(seq_without_first_tkn[attention_mask == 0] == self.pp_tokenizer.pad_token_id)\n",
    "        check_no_nans_or_infs(seq_token_log_probs)\n",
    "        # check that we aren't picking extrememly rare tokens\n",
    "        assert torch.all(seq_token_log_probs  > -10)  \n",
    "\n",
    "        ### Get sequence probabilities by summing up token log probabilities \n",
    "        seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "        ## TESTS \n",
    "        assert seq_log_prob.shape == torch.Size([self.pp_batch_size])\n",
    "        check_no_nans_or_infs(seq_log_prob)\n",
    "        \n",
    "        if self.pp_model.training:  # don't bother logging or calculate entropy, token_probs in eval mode\n",
    "            if self._cfg.wandb['log_token_entropy']:\n",
    "                with timecode() as self.batch_time_d['time_log_entropy']:\n",
    "                    self.batch_wandb_d['ent_hist'] = self._get_entropy_hist(scores_stacked, attention_mask) \n",
    "            if self._cfg.wandb['log_token_probabilities']: \n",
    "                with timecode() as self.batch_time_d['time_log_token_probabilities']:\n",
    "                    self.batch_wandb_d = merge_dicts(self.batch_wandb_d, \n",
    "                        self._get_token_probability_metrics(scores_log_softmax, attention_mask, k=3))\n",
    "        return seq_log_prob\n",
    "   \n",
    "    def _check_scores_log_softmax_sums(self, scores_log_softmax):\n",
    "        sums = scores_log_softmax.exp().sum(2)\n",
    "        # check that the axes is right\n",
    "        # we want to sum over token probabilities at each generation step, so we \n",
    "        # should end up with a shape [self.orig_batch_size, self.pp_length]\n",
    "        assert sums.shape[0] == self.orig_batch_size  \n",
    "        assert sums.shape[1] == self.pp_length - 1\n",
    "        # check that they sum to 1 along the self.pp_length axis\n",
    "        assert torch.allclose(sums, torch.ones(sums.size(), device=self._cfg.device), atol = 1e-4)\n",
    "\n",
    "    def _check_seq_token_log_prob_values_are_correct(self, seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "        \"\"\"Just enumerates and checks values\n",
    "        Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "        \"\"\"\n",
    "        l = []\n",
    "        for i_ex in range(self.orig_batch_size):\n",
    "            for i_step in range(self.pp_length - 1):\n",
    "                i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "                l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "        assert all(l)    \n",
    "    \n",
    "    def _get_entropy_hist(self, scores_stacked, attention_mask): \n",
    "        ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "        assert ent.shape == attention_mask.shape == torch.Size([self.pp_batch_size, self.pp_length - 1])\n",
    "        ent = ent * attention_mask  # stop values after eos token from contributing to ent score \n",
    "        # first remove structure (otherwise we have ragged arrays), then remove corresponding attention mask values\n",
    "        # we can't just filter by ent[ent != 0] because we might have zero tokens during the sequence\n",
    "        att_flat= attention_mask.flatten()\n",
    "        indices = torch.nonzero(att_flat)\n",
    "        ent_flat = ent.flatten()[indices].flatten()\n",
    "        assert ent_flat.shape[0] == (torch.sum(att_flat)*1).item()\n",
    "        # check everything we filter out is zero \n",
    "        torch.isclose(ent.flatten()[torch.nonzero(~(att_flat > 0))].sum(), torch.tensor(0.), 1e-3)\n",
    "        return Histogram(ent_flat.detach().cpu().tolist())\n",
    "#         ent_d = dict(\n",
    "#       #      ent_min             = ent_flat.quantile(0).item(),\n",
    "#             ent_lower_quartile  = ent_flat.quantile(0.25).item(), \n",
    "#             ent_median          = ent_flat.median().item(), \n",
    "#       #      ent_mean            = ent_flat.mean().item(), \n",
    "#             ent_upper_quartile  = ent_flat.quantile(0.75).item(), \n",
    "#       #      ent_max             = ent_flat.quantile(1).item(),   # skews the graph\n",
    "#             epoch=self.epoch, global_step=self.global_step\n",
    "#         )\n",
    "#         return ent_d\n",
    "        print('tmp')\n",
    "\n",
    "    def _get_token_probability_metrics(self, scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, _ = torch.topk(scores_log_softmax, largest=True, k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  \n",
    "        assert tkn_kmaxprob.shape == torch.Size([self.pp_batch_size, self.pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_histogram\"] = Histogram(probs.detach().cpu().tolist())\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [0.1, 0.01, 0.001, 0.0001, 0.00001]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/self._cfg.vocab_size)) / allprobs.shape[0]).item()\n",
    "        return token_prob_d\n",
    "    \n",
    "    def eval_dl(self,split): \n",
    "        \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "        ### TODO: delete redundant stuff\n",
    "        # Put models in eval mode and do the forward pass \n",
    "        # Current logic: push all batches together into one big list.   \n",
    "        self._reset_batch_dicts()\n",
    "        if self.pp_model.training: self.pp_model.eval()\n",
    "        if self.vm_model.training: self.vm_model.eval()\n",
    "        dl_raw = self.ds.dld_raw[split]\n",
    "        dl_tkn = self.ds.dld_tkn[split]\n",
    "        with torch.no_grad(): \n",
    "            for self.batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "                self.logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loading data: '))\n",
    "                for k, v in data.items():\n",
    "                    ## TODO: do you need this line?\n",
    "                    if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "                pp_output, pp_l = self.pp_model_forward(data)\n",
    "                _ = self.loss_fn(data, raw, pp_output, pp_l)\n",
    "                self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "                self.data_d[split].append(self.batch_d)\n",
    "                self.logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "        self._wandb_log_eval_step()  # not implemented yet\n",
    "        \n",
    "    def plot_wandb_charts(self): \n",
    "        ## TODO: rename to indicate it only plots summary and examples charts \n",
    "        ## Can you refactor into regular wandb charts?\n",
    "        if self._cfg.wandb['plot_examples']: \n",
    "            # Examples charts \n",
    "            for split in ['train', 'valid']:\n",
    "                df = pd.DataFrame(data_d[split]) if type(self.data_d[split]) is list else self.data_d[split]\n",
    "                df = df.query(\"idx in @plt_idx_d[@split]\").sort_values(['idx', 'epoch'])\n",
    "                for metric in self._cfg.metrics: \n",
    "                    chart = plot_examples_chart(split, table=wandb.Table(dataframe=df), metric=metric)\n",
    "                    wandb.log({f\"individual_examples/{split}_{metric}_vs_epoch_examples\": chart}, commit=False)  \n",
    "    \n",
    "    def _set_df_colorder(self, data_d_key): \n",
    "        colorder_eval=['idx','epoch', 'orig_l',  'pp_l','orig_truelabel_probs','pp_truelabel_probs',\n",
    "        'pp_predclass_probs','orig_label','pp_predclass','label_flip', 'vm_score','sts_score',\n",
    "        'reward', 'pp_logp','loss','batch_num','global_step','accumulation_num','loss_batch', 'label_flip_fraction',\n",
    "        'orig_length','orig_batch_size','pp_length','pp_batch_size']\n",
    "        colorder_training_step = colorder_eval + [o for o in trainer.data_d['training_step'].columns if 'time_' in o]\n",
    "        if data_d_key == \"training_step\": self.data_d[data_d_key] = self.data_d[data_d_key][colorder_training_step]\n",
    "        else:                             self.data_d[data_d_key] = self.data_d[data_d_key][colorder_eval]\n",
    "\n",
    "    def add_wandb_run_summary_statistics(self):\n",
    "        \"\"\"Compute test metrics for the run and log them to the wandb run summary pane. \"\"\"\n",
    "        ## Summary statistics of the test set \n",
    "        # From the last epoch atm because we don't have early stopping \n",
    "        test_metrics = self.data_d['test'].filter(self._cfg.metrics, axis=1).mean()\n",
    "        for metric, val in zip(test_metrics.index, test_metrics): \n",
    "            self.run.summary[f\"{metric}_avg_test\"] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.wandb['mode'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab532fc7b94746ec9d033a31da69b1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 0 of 4\n",
      "Now on epoch 1 of 4\n",
      "Now on epoch 2 of 4\n",
      "Now on epoch 3 of 4\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, optimizer,\n",
    "                  accelerator, ds, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>epoch</th>\n",
       "      <th>orig_l</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>orig_truelabel_probs</th>\n",
       "      <th>pp_truelabel_probs</th>\n",
       "      <th>pp_predclass_probs</th>\n",
       "      <th>orig_label</th>\n",
       "      <th>pp_predclass</th>\n",
       "      <th>label_flip</th>\n",
       "      <th>vm_score</th>\n",
       "      <th>sts_score</th>\n",
       "      <th>reward</th>\n",
       "      <th>pp_logp</th>\n",
       "      <th>loss</th>\n",
       "      <th>batch_num</th>\n",
       "      <th>global_step</th>\n",
       "      <th>accumulation_num</th>\n",
       "      <th>loss_batch</th>\n",
       "      <th>label_flip_fraction</th>\n",
       "      <th>orig_length</th>\n",
       "      <th>orig_batch_size</th>\n",
       "      <th>pp_length</th>\n",
       "      <th>pp_batch_size</th>\n",
       "      <th>time_generate_pp</th>\n",
       "      <th>time_loss_fn</th>\n",
       "      <th>time_reward_fn</th>\n",
       "      <th>time_vm_scores</th>\n",
       "      <th>time_sts_scores</th>\n",
       "      <th>time_pp_logp</th>\n",
       "      <th>time_log_entropy</th>\n",
       "      <th>time_log_token_probabilities</th>\n",
       "      <th>time_loss_fn_loss_calc</th>\n",
       "      <th>time_backwards</th>\n",
       "      <th>time_opt_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I like this movie</td>\n",
       "      <td>I do not like this movie.</td>\n",
       "      <td>0.892648</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0221628</td>\n",
       "      <td>0.968495</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>-0.97145</td>\n",
       "      <td>0.464873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129559</td>\n",
       "      <td>0.039314</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I do not like this movie</td>\n",
       "      <td>I hate this apple.</td>\n",
       "      <td>0.838063</td>\n",
       "      <td>0.883751</td>\n",
       "      <td>0.883751</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0456876</td>\n",
       "      <td>0.986515</td>\n",
       "      <td>0.454929</td>\n",
       "      <td>-0.997901</td>\n",
       "      <td>0.453974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129559</td>\n",
       "      <td>0.039314</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I love this apple</td>\n",
       "      <td>I love this apple.</td>\n",
       "      <td>0.84649</td>\n",
       "      <td>0.910314</td>\n",
       "      <td>0.910314</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0638236</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.436599</td>\n",
       "      <td>-0.564599</td>\n",
       "      <td>0.246503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129559</td>\n",
       "      <td>0.039314</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>I hate this apple</td>\n",
       "      <td>I like this movie.</td>\n",
       "      <td>0.706167</td>\n",
       "      <td>0.828523</td>\n",
       "      <td>0.828523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.122356</td>\n",
       "      <td>0.991577</td>\n",
       "      <td>0.378675</td>\n",
       "      <td>-0.729189</td>\n",
       "      <td>0.276126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129559</td>\n",
       "      <td>0.039314</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I like this movie</td>\n",
       "      <td>I do not like this movie.</td>\n",
       "      <td>0.892648</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0221628</td>\n",
       "      <td>0.968495</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>-1.22811</td>\n",
       "      <td>0.587696</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.128399</td>\n",
       "      <td>0.038738</td>\n",
       "      <td>0.029656</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.125024</td>\n",
       "      <td>0.005739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx  epoch                    orig_l                       pp_l  \\\n",
       "0   0      0         I like this movie  I do not like this movie.   \n",
       "1   1      0  I do not like this movie         I hate this apple.   \n",
       "2   2      0         I love this apple         I love this apple.   \n",
       "3   3      0         I hate this apple         I like this movie.   \n",
       "4   0      1         I like this movie  I do not like this movie.   \n",
       "\n",
       "  orig_truelabel_probs pp_truelabel_probs pp_predclass_probs orig_label  \\\n",
       "0             0.892648           0.914811           0.914811          0   \n",
       "1             0.838063           0.883751           0.883751          0   \n",
       "2              0.84649           0.910314           0.910314          1   \n",
       "3             0.706167           0.828523           0.828523          1   \n",
       "4             0.892648           0.914811           0.914811          0   \n",
       "\n",
       "  pp_predclass label_flip   vm_score sts_score    reward   pp_logp      loss  \\\n",
       "0            0          0 -0.0221628  0.968495  0.478535  -0.97145  0.464873   \n",
       "1            0          0 -0.0456876  0.986515  0.454929 -0.997901  0.453974   \n",
       "2            1          0 -0.0638236  0.993377  0.436599 -0.564599  0.246503   \n",
       "3            1          0  -0.122356  0.991577  0.378675 -0.729189  0.276126   \n",
       "4            0          0 -0.0221628  0.968495  0.478535  -1.22811  0.587696   \n",
       "\n",
       "   batch_num  global_step  accumulation_num  loss_batch  label_flip_fraction  \\\n",
       "0          0            0                 0    0.360369                  0.0   \n",
       "1          0            0                 0    0.360369                  0.0   \n",
       "2          0            0                 0    0.360369                  0.0   \n",
       "3          0            0                 0    0.360369                  0.0   \n",
       "4          0            1                 1    0.385783                  0.0   \n",
       "\n",
       "   orig_length  orig_batch_size  pp_length  pp_batch_size  time_generate_pp  \\\n",
       "0            8                4          9              4          0.129559   \n",
       "1            8                4          9              4          0.129559   \n",
       "2            8                4          9              4          0.129559   \n",
       "3            8                4          9              4          0.129559   \n",
       "4            8                4          9              4          0.128399   \n",
       "\n",
       "   time_loss_fn  time_reward_fn  time_vm_scores  time_sts_scores  \\\n",
       "0      0.039314        0.030290        0.008350         0.021297   \n",
       "1      0.039314        0.030290        0.008350         0.021297   \n",
       "2      0.039314        0.030290        0.008350         0.021297   \n",
       "3      0.039314        0.030290        0.008350         0.021297   \n",
       "4      0.038738        0.029656        0.008089         0.020940   \n",
       "\n",
       "   time_pp_logp  time_log_entropy  time_log_token_probabilities  \\\n",
       "0      0.008887          0.001839                      0.004882   \n",
       "1      0.008887          0.001839                      0.004882   \n",
       "2      0.008887          0.001839                      0.004882   \n",
       "3      0.008887          0.001839                      0.004882   \n",
       "4      0.008943          0.001697                      0.004800   \n",
       "\n",
       "   time_loss_fn_loss_calc  time_backwards  time_opt_step  \n",
       "0                0.000056        0.127689       0.005714  \n",
       "1                0.000056        0.127689       0.005714  \n",
       "2                0.000056        0.127689       0.005714  \n",
       "3                0.000056        0.127689       0.005714  \n",
       "4                0.000053        0.125024       0.005739  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(trainer.data_d['training_step'][colorder_training_step].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>epoch</th>\n",
       "      <th>orig_l</th>\n",
       "      <th>pp_l</th>\n",
       "      <th>orig_truelabel_probs</th>\n",
       "      <th>pp_truelabel_probs</th>\n",
       "      <th>pp_predclass_probs</th>\n",
       "      <th>orig_label</th>\n",
       "      <th>pp_predclass</th>\n",
       "      <th>label_flip</th>\n",
       "      <th>vm_score</th>\n",
       "      <th>sts_score</th>\n",
       "      <th>reward</th>\n",
       "      <th>pp_logp</th>\n",
       "      <th>loss</th>\n",
       "      <th>batch_num</th>\n",
       "      <th>global_step</th>\n",
       "      <th>accumulation_num</th>\n",
       "      <th>loss_batch</th>\n",
       "      <th>label_flip_fraction</th>\n",
       "      <th>orig_length</th>\n",
       "      <th>orig_batch_size</th>\n",
       "      <th>pp_length</th>\n",
       "      <th>pp_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I like this movie</td>\n",
       "      <td>I do not like this movie.</td>\n",
       "      <td>0.892648</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0221628</td>\n",
       "      <td>0.968495</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>-1.04343</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.331394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I do not like this movie</td>\n",
       "      <td>I hate this apple.</td>\n",
       "      <td>0.838063</td>\n",
       "      <td>0.883751</td>\n",
       "      <td>0.883751</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0456876</td>\n",
       "      <td>0.986515</td>\n",
       "      <td>0.454929</td>\n",
       "      <td>-0.675827</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.331394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I love this apple</td>\n",
       "      <td>I love this apple.</td>\n",
       "      <td>0.84649</td>\n",
       "      <td>0.910314</td>\n",
       "      <td>0.910314</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0638236</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.436599</td>\n",
       "      <td>-0.394324</td>\n",
       "      <td>0.172162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.331394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>I hate this apple</td>\n",
       "      <td>I like this movie.</td>\n",
       "      <td>0.706167</td>\n",
       "      <td>0.828523</td>\n",
       "      <td>0.828523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.122356</td>\n",
       "      <td>0.991577</td>\n",
       "      <td>0.378675</td>\n",
       "      <td>-0.915422</td>\n",
       "      <td>0.346647</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.331394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I like this movie</td>\n",
       "      <td>I do not like this movie.</td>\n",
       "      <td>0.892648</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0.914811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0221628</td>\n",
       "      <td>0.968495</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>-0.92115</td>\n",
       "      <td>0.440803</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx  epoch                    orig_l                       pp_l  \\\n",
       "0   0      0         I like this movie  I do not like this movie.   \n",
       "1   1      0  I do not like this movie         I hate this apple.   \n",
       "2   2      0         I love this apple         I love this apple.   \n",
       "3   3      0         I hate this apple         I like this movie.   \n",
       "4   0      1         I like this movie  I do not like this movie.   \n",
       "\n",
       "  orig_truelabel_probs pp_truelabel_probs pp_predclass_probs orig_label  \\\n",
       "0             0.892648           0.914811           0.914811          0   \n",
       "1             0.838063           0.883751           0.883751          0   \n",
       "2              0.84649           0.910314           0.910314          1   \n",
       "3             0.706167           0.828523           0.828523          1   \n",
       "4             0.892648           0.914811           0.914811          0   \n",
       "\n",
       "  pp_predclass label_flip   vm_score sts_score    reward   pp_logp      loss  \\\n",
       "0            0          0 -0.0221628  0.968495  0.478535  -1.04343  0.499316   \n",
       "1            0          0 -0.0456876  0.986515  0.454929 -0.675827  0.307453   \n",
       "2            1          0 -0.0638236  0.993377  0.436599 -0.394324  0.172162   \n",
       "3            1          0  -0.122356  0.991577  0.378675 -0.915422  0.346647   \n",
       "4            0          0 -0.0221628  0.968495  0.478535  -0.92115  0.440803   \n",
       "\n",
       "   batch_num  global_step  accumulation_num  loss_batch  label_flip_fraction  \\\n",
       "0          0            1                 1    0.331394                  0.0   \n",
       "1          0            1                 1    0.331394                  0.0   \n",
       "2          0            1                 1    0.331394                  0.0   \n",
       "3          0            1                 1    0.331394                  0.0   \n",
       "4          0            2                 2    0.292266                  0.0   \n",
       "\n",
       "   orig_length  orig_batch_size  pp_length  pp_batch_size  \n",
       "0            8                4          9              4  \n",
       "1            8                4          9              4  \n",
       "2            8                4          9              4  \n",
       "3            8                4          9              4  \n",
       "4            8                4          9              4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(trainer.data_d['train'][colorder_eval].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_flip_fraction',\n",
       " 'loss_batch',\n",
       " 'epoch',\n",
       " 'batch_num',\n",
       " 'global_step',\n",
       " 'accumulation_num',\n",
       " 'orig_length',\n",
       " 'orig_batch_size',\n",
       " 'pp_length',\n",
       " 'pp_batch_size',\n",
       " 'time_generate_pp',\n",
       " 'time_loss_fn',\n",
       " 'time_reward_fn',\n",
       " 'time_vm_scores',\n",
       " 'time_sts_scores',\n",
       " 'time_pp_logp',\n",
       " 'time_log_entropy',\n",
       " 'time_log_token_probabilities',\n",
       " 'time_loss_fn_loss_calc',\n",
       " 'time_backwards',\n",
       " 'time_opt_step',\n",
       " 'pp_truelabel_probs',\n",
       " 'pp_predclass',\n",
       " 'pp_predclass_probs',\n",
       " 'label_flip',\n",
       " 'reward',\n",
       " 'vm_score',\n",
       " 'sts_score',\n",
       " 'pp_logp',\n",
       " 'loss',\n",
       " 'idx',\n",
       " 'orig_l',\n",
       " 'orig_label',\n",
       " 'orig_truelabel_probs',\n",
       " 'pp_l']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.data_d['training_step'].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-18-f60db6496910>\u001b[0m(206)\u001b[0;36m_convert_data_d_to_df\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    204 \u001b[0;31m        \u001b[0mdf_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_nested_lists_in_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    205 \u001b[0;31m        \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_d_key\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata_d_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 206 \u001b[0;31m        \u001b[0;32massert\u001b[0m \u001b[0mdf_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_train_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdf_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    208 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> split\n",
      "'test'\n",
      "ipdb> self._cfg.ds_length[split]\n",
      "4\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
