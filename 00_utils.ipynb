{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains useful functions for the rest of the scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import GPUtil\n",
    "import torch\n",
    "import time \n",
    "import torchsnooper\n",
    "from timeit import default_timer as timer\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "#from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class timecode:\n",
    "    \"\"\"This class is used for timing code\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.t0 = timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t = timer() - self.t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.001036524772644\n",
      "2.946704626083374e-06\n"
     ]
    }
   ],
   "source": [
    "with timecode() as t: \n",
    "    time.sleep(1)\n",
    "print(t.t)\n",
    "\n",
    "with timecode() as t: \n",
    "    x = [0,1,2,3,4]\n",
    "    y = [o+1 for o in x]\n",
    "print(t.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_device_info(): \n",
    "    \"\"\"\n",
    "    Prints some statistics around versions and the GPU's available for\n",
    "    the host machine\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import sys\n",
    "    print(\"######## Diagnostics and version information ######## \")\n",
    "    print('__Python VERSION:', sys.version)\n",
    "    print('__pyTorch VERSION:', torch.__version__)\n",
    "    print('__CUDA VERSION', )\n",
    "    from subprocess import call\n",
    "    # call([\"nvcc\", \"--version\"]) does not work\n",
    "    #! nvcc --version\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__Devices')\n",
    "    call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "    print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "    print ('Available devices ', torch.cuda.device_count())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name())\n",
    "    print ('Current cuda device ', torch.cuda.current_device())\n",
    "    print(\"#################################################################\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector. \n",
    "    Useful when running into an out of memory error on the GPU. \"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                            \" GPU\" if obj.is_cuda else \"\",\n",
    "                                            \" pinned\" if obj.is_pinned else \"\",\n",
    "                                            pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s â†’ %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                    type(obj.data).__name__, \n",
    "                                                    \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                    \" pinned\" if obj.data.is_pinned else \"\",\n",
    "                                                    \" grad\" if obj.requires_grad else \"\", \n",
    "                                                    \" volatile\" if obj.volatile else \"\",\n",
    "                                                    pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export      \n",
    "class Monitor(Thread):\n",
    "    \"\"\"Use this to check that you are using the GPU during your pytorch functions and to track memory usage \n",
    "    of the GPU's as well.\"\"\" \n",
    "    def __init__(self, delay):\n",
    "        super(Monitor, self).__init__()\n",
    "        self.stopped = False\n",
    "        self.delay = delay # Time between calls to GPUtil\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped:\n",
    "            GPUtil.showUtilization()\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_gpu(msg):\n",
    "    \"\"\"\n",
    "    ref: https://github.com/huggingface/transformers/issues/1742#issue-518262673\n",
    "    put in logger.info()\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    def query(field):\n",
    "        return(subprocess.check_output(\n",
    "            ['nvidia-smi', f'--query-gpu={field}',\n",
    "                '--format=csv,nounits,noheader'], \n",
    "            encoding='utf-8'))\n",
    "    def to_int(result):\n",
    "        return int(result.strip().split('\\n')[0])\n",
    "    \n",
    "    used = to_int(query('memory.used'))\n",
    "    total = to_int(query('memory.total'))\n",
    "    pct = used/total\n",
    "    return f\"{msg} {100*pct:2.1f}% ({used} out of {total})\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def round_t(t, dp=2):\n",
    "    \"\"\"Return rounded tensors for easy viewing. t is a tensor, dp=decimal places\"\"\"\n",
    "    if t.device.type == \"cuda\": t=t.cpu()\n",
    "    return t.detach().numpy().round(dp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63, 0.99],\n",
       "       [0.35, 0.64],\n",
       "       [0.94, 0.83],\n",
       "       [0.86, 0.16],\n",
       "       [0.92, 0.41],\n",
       "       [0.01, 0.72],\n",
       "       [0.57, 0.63],\n",
       "       [0.11, 0.33],\n",
       "       [0.19, 0.57],\n",
       "       [0.26, 0.95]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_t(torch.rand((10,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 10_data_cleaning.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
