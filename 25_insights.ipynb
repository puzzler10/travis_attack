{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-experiment insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, wandb, spacy, textstat, psutil, pandas as pd, numpy as np, matplotlib.pyplot as plt, editdistance, plotly.express as px\n",
    "import functools, operator, string, seaborn as sns\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "from difflib import ndiff\n",
    "from IPython.display import Markdown, display\n",
    "from lexicalrichness import LexicalRichness\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from IPython.core.debugger import set_trace\n",
    "from travis_attack.utils import display_all, resume_wandb_run, merge_dicts\n",
    "from fastcore.basics import patch_to\n",
    "\n",
    "import logging \n",
    "logger = logging.getLogger(\"travis_attack.insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#run = resume_wandb_run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and add metrics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_training_dfs(path_run, postprocessed=False): \n",
    "    \"\"\"Return a dict of dataframes with all training and eval data\"\"\"\n",
    "    df_d = dict()\n",
    "    for key in ['training_step', 'train', 'valid', 'test']:\n",
    "        try: \n",
    "            if postprocessed: \n",
    "                fname = f\"{path_run}{key}_postprocessed.pkl\" \n",
    "                df_d[key] = pd.read_pickle(fname)\n",
    "            else:\n",
    "                fname = f\"{path_run}{key}.csv\"\n",
    "                df_d[key] = pd.read_csv(fname)\n",
    "        except FileNotFoundError: \n",
    "            pass\n",
    "    logger.info(f'Dataframes have shapes {[f\"{k}: {df.shape}\" for (k, df) in df_d.items()]}')\n",
    "    return df_d \n",
    "\n",
    "def postprocess_df(df, filter_idx=None, num_proc=min(8, psutil.cpu_count())): \n",
    "    \"\"\"set df to one of training_step, train, valid, test\n",
    "    filter_idx - for testing (remove later) \"\"\"\n",
    "    # num_proc=8 seems pretty good - diminishing returns and we may as well leave some CPU for others \n",
    "    df = df.sort_values(by=['idx', \"epoch\"], axis=0)\n",
    "    if filter_idx is not None:   df = df.query(\"idx <= @filter_idx\")  # for testing purposes\n",
    "    # Getting weird behaviour with group_by's so binning some of the numeric values\n",
    "    for col in ['sts_score','vm_score','reward', 'pp_truelabel_probs']:  df.loc[:, col] = df.loc[:, col].round(5)\n",
    "    # Add metrics\n",
    "    df = _add_number_of_unique_pps_per_idx(df)\n",
    "    df = _add_number_of_pp_changes_per_idx(df)\n",
    "    df = _add_epoch_of_first_label_flip(   df)\n",
    "    df = _add_text_metrics(df, num_proc=num_proc)\n",
    "    return df\n",
    "\n",
    "def _add_number_of_unique_pps_per_idx(df): \n",
    "    df_grp = df.groupby(\"idx\").agg({\"pp_l\":\"nunique\"})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_l\":\"idx_n_unique_pp\"})\n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df\n",
    "\n",
    "def _add_number_of_pp_changes_per_idx(df): \n",
    "    df['pp_changed'] = df.sort_values([\"idx\",\"epoch\"]).groupby('idx')['pp_l'].shift().ne(df['pp_l']).astype(int)\n",
    "    df_grp = df.groupby('idx').agg({'pp_changed': 'sum'})\n",
    "    df_grp= df_grp.rename(columns = {\"pp_changed\":\"idx_n_pp_changes\"})\n",
    "    df_grp['idx_n_pp_changes'] -= 1  # The first paraphrase isn't a change\n",
    "    df = df.drop('pp_changed', 1) # don't need this anymore\n",
    "    df = df.merge(df_grp, left_on='idx', right_index=True, how='left')\n",
    "    return df \n",
    "\n",
    "def _add_epoch_of_first_label_flip(df): \n",
    "    rownum_of_first_flip = df.groupby('idx')[['epoch','label_flip']].idxmax()['label_flip'] ## works since idxmax returns first max\n",
    "    df_grp = df[['idx','epoch']].loc[rownum_of_first_flip]\n",
    "    df_grp= df_grp.rename(columns = {\"epoch\":\"epoch_of_first_label_flip\"})\n",
    "    df = df.merge(df_grp, left_on='idx', right_on='idx', how='left')\n",
    "    return df\n",
    "\n",
    "def _add_text_metrics(df, num_proc=min(8, psutil.cpu_count())):\n",
    "    df = _add_text_metrics_for_column(df, \"orig_l\", suffix=\"orig\", num_proc=num_proc)\n",
    "    df = _add_text_metrics_for_column(df, \"pp_l\",   suffix=\"pp\",   num_proc=num_proc)\n",
    "    logger.info(\"Calculating metric differences between orig and pp\")\n",
    "    for k in _get_text_metrics(\"some arbritary text here\").keys():  df[f\"{k}_diff\"] = df[f\"{k}_orig\"] - df[f\"{k}_pp\"]\n",
    "    df = _add_text_pair_metrics(df, num_proc=num_proc)  \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual column metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_text_metrics_for_column(df, cname, suffix, num_proc): \n",
    "    logger.info(f\"Adding text metrics for column {cname}\")\n",
    "    ds_cname = Dataset.from_pandas(df[cname].drop_duplicates().to_frame())\n",
    "    ds_cname = _get_text_metrics_for_ds(ds_cname, cname=cname, suffix=suffix, num_proc=num_proc)    \n",
    "    df = pd.merge(df, pd.DataFrame(ds_cname), how='left', on=[cname])\n",
    "    return df\n",
    "\n",
    "def _get_text_metrics_for_ds(ds, cname, suffix, num_proc):\n",
    "    x = ds.map(_get_text_metrics, input_columns = [cname], batched=False, num_proc=num_proc)\n",
    "    colnames_mapping = dict()\n",
    "    for k in x.column_names: colnames_mapping[k] = k + f\"_{suffix}\" if k != cname else k    # rename columns\n",
    "    return x.rename_columns(colnames_mapping)\n",
    "\n",
    "def _get_text_metrics(text):    \n",
    "    d = defaultdict(lambda: 0)\n",
    "    d['n_words'] = LexicalRichness(text).words\n",
    "    d['n_sentences'] = textstat.sentence_count(text)\n",
    "    def get_chartype_count(text, strset): return len(list(filter(functools.partial(operator.contains, strset), text))) \n",
    "    d['n_punctuation'] = get_chartype_count(text, strset=string.punctuation)\n",
    "    d['n_digits']      = get_chartype_count(text, strset=string.digits)\n",
    "    d['n_letters']     = get_chartype_count(text, strset=string.ascii_letters)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pair metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_text_pair_metrics(df, num_proc): \n",
    "    logger.info(\"Calculating text pair statistics for (orig, pp) unique pairs\")\n",
    "    ds_pairs = Dataset.from_pandas(df[['orig_l','pp_l']].drop_duplicates())\n",
    "    ds_pairs = _get_text_pair_metrics_for_ds(ds_pairs, num_proc=num_proc)\n",
    "    df = pd.merge(df, pd.DataFrame(ds_pairs), how='left', on=['orig_l', 'pp_l'])\n",
    "    return df\n",
    "\n",
    "def _get_text_pair_metrics_for_ds(ds, num_proc): \n",
    "    return ds.map(_get_text_pair_metrics, input_columns = [\"orig_l\", \"pp_l\"], batched=False, num_proc=num_proc)\n",
    "\n",
    "def _get_text_pair_metrics(orig, pp):\n",
    "    d = _get_removals_insertions_unchanged_phrases(orig, pp)\n",
    "    d['edit_distance_token_level'] = _get_token_level_edit_distance(orig, pp)\n",
    "    return d\n",
    "\n",
    "def _get_removals_insertions_unchanged_phrases(orig, pp): \n",
    "    orig_t,pp_t  = [tkn.text for tkn in nlp(orig)],[tkn.text for tkn in nlp(pp)]\n",
    "    diff = [x for x in ndiff(orig_t, pp_t)]\n",
    "    ins_idx,ins_tkns,ins_tkn_grps,ins_phrases = _get_subsequences(diff, \"insertions\")\n",
    "    rem_idx,rem_tkns,rem_tkn_grps,rem_phrases = _get_subsequences(diff, \"removals\")\n",
    "    unc_idx,unc_tkns,unc_tkn_grps,unc_phrases = _get_subsequences(diff, \"unchanged\")\n",
    "    return {'removals_idx': rem_idx, \n",
    "            'removals': rem_phrases,\n",
    "            'insertions_idx': ins_idx,\n",
    "            'insertions': ins_phrases, \n",
    "            'unchanged_idx': unc_idx,\n",
    "            'unchanged': unc_phrases, \n",
    "            'n_segments_inserted': len(ins_tkn_grps),\n",
    "            'n_segments_removed': len(rem_tkn_grps),\n",
    "            'n_tokens_inserted': len(ins_tkns), \n",
    "            'n_tokens_removed': len(rem_tkns),\n",
    "            'is_truncation': _is_truncation(rem_idx, unc_idx),\n",
    "            'any_phrase_capitalised': _any_phrase_capitalised(rem_phrases, ins_phrases),\n",
    "            'any_phrase_decapitalised': _any_phrase_capitalised(ins_phrases, rem_phrases)}\n",
    "    \n",
    "    \n",
    "def _join_punctuation(seq, characters=set(string.punctuation)):\n",
    "    \"Generator to join tokens respecting punctuation, but doesn't work that well.\"\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "    yield current\n",
    "\n",
    "def _get_subsequences(diff, sign): \n",
    "    op = {\"insertions\": \"+\", \"removals\": \"-\", \"unchanged\": \" \"}[sign]\n",
    "    idx,tokens = [],[]\n",
    "    for i, o in enumerate(diff): \n",
    "        if o[0] == op:  idx.append(i); tokens.append(o[2:])  \n",
    "    ## Group tokens that go together        \n",
    "    token_groups = []\n",
    "    # bit of a mystery this bit but seems to work. just need 1-1 mapping between data and tokens \n",
    "    for k, g in groupby(zip(enumerate(idx), tokens), lambda ix: ix[0][0] - ix[0][1]):\n",
    "        token_groups.append(list(map(operator.itemgetter(1), g)))\n",
    "    phrases = [' '.join(_join_punctuation(l)) for l in token_groups]\n",
    "    return idx, tokens, token_groups, phrases\n",
    "\n",
    "def _is_truncation(rem_idx, unc_idx):\n",
    "    \"\"\"determines if a given phrase is trunctated or not. unc_idx = unchanged_idx, rem_idx = removals_idx \"\"\"\n",
    "    if len(rem_idx) == 0 or len(unc_idx) == 0: return False \n",
    "    if max(unc_idx) < max(rem_idx):  return True \n",
    "    else:                            return False\n",
    "\n",
    "def _any_phrase_capitalised(lower_case_phrases, upper_case_phrases): \n",
    "    \"\"\"tests if any of the phrases in lower_case_phrases, when capitalised, are present in upper_case_phrases\"\"\"\n",
    "    for lc_p in lower_case_phrases: \n",
    "        for uc_p in upper_case_phrases: \n",
    "            if lc_p.capitalize() == uc_p: return True \n",
    "    return False \n",
    "\n",
    "def _get_token_level_edit_distance(s1, s2): \n",
    "    l1,l2 = [o.text for o in nlp(s1)],[o.text for o in nlp(s2)]\n",
    "    return editdistance.eval(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test some of these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertions ['A']\n",
      "Removals ['a']\n",
      "Any phrase decapitalised: False\n",
      "Any phrase capitalised: True\n",
      "\n",
      "Insertions ['a', 'the duck is nice']\n",
      "Removals ['A']\n",
      "Any phrase decapitalised: True\n",
      "Any phrase capitalised: False\n",
      "\n",
      "Insertions ['a']\n",
      "Removals ['look at that', 'A']\n",
      "Any phrase decapitalised: True\n",
      "Any phrase capitalised: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_any_phrase_capitalised():\n",
    "    ins1 = ['A']\n",
    "    rem1 = ['a']\n",
    "    ins2 = ['a', \"the duck is nice\"]\n",
    "    rem2= ['A']\n",
    "    ins3 = ['a']\n",
    "    rem3 = ['look at that', 'A']\n",
    "    for ins, rem in zip([ins1,ins2,ins3], [rem1,rem2,rem3]):\n",
    "        print(\"Insertions\", ins)\n",
    "        print(\"Removals\", rem)\n",
    "        print(\"Any phrase decapitalised:\", _any_phrase_capitalised(ins, rem))\n",
    "        print(\"Any phrase capitalised:\", _any_phrase_capitalised(rem, ins))\n",
    "        print(\"\")\n",
    "test_any_phrase_capitalised()       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### hello i am tom ######\n",
      "hello i am mike\n",
      "Edit distance: 1\n",
      "hello i'm tom \n",
      "Edit distance: 1\n",
      "hello my name is tom\n",
      "Edit distance: 3\n",
      "hello, i am tom\n",
      "Edit distance: 1\n",
      "hello i tom\n",
      "Edit distance: 1\n",
      "I am tom, don't forget it\n",
      "Edit distance: 7\n"
     ]
    }
   ],
   "source": [
    "def test_get_token_level_edit_distance(): \n",
    "    s1 = \"hello i am tom\"\n",
    "    s2 = \"hello i am mike\"\n",
    "    s3 = \"hello i'm tom \"\n",
    "    s4 = \"hello my name is tom\"\n",
    "    s5 = \"hello, i am tom\"\n",
    "    s6 = \"hello i tom\"\n",
    "    s7 = \"I am tom, don't forget it\"\n",
    "    print(\"######\", s1, \"######\")\n",
    "    for s in [s2,s3,s4,s5,s6,s7]:\n",
    "        print(s)\n",
    "        print (\"Edit distance:\", _get_token_level_edit_distance(s1, s))        \n",
    "test_get_token_level_edit_distance()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def create_and_log_wandb_postrun_plots(df_d): \n",
    "    df_concat = _prepare_df_concat(df_d)\n",
    "    wandb_plot_d = _prepare_wandb_postrun_plots(df_concat)\n",
    "    wandb.log(wandb_plot_d)\n",
    "\n",
    "def _prepare_df_concat(df_d):\n",
    "    for k,df in df_d.items(): \n",
    "        if  k == \"training_step\": df_d[k]['data_split'] = k \n",
    "        else:                     df_d[k]['data_split'] = f\"eval_{k}\" \n",
    "    df_concat = pd.concat(df_d.values()).reset_index(drop=True)\n",
    "    df_concat.loc[df_concat.epoch_of_first_label_flip == 0, 'epoch_of_first_label_flip'] = None  # stop wrong spike at 0\n",
    "    return df_concat\n",
    "\n",
    "def _prepare_wandb_postrun_plots(df_concat):\n",
    "    fig_l = []\n",
    "    hist_config_dicts = [\n",
    "        {\n",
    "            'cname': 'epoch_of_first_label_flip', \n",
    "            'xlabel': \"Epoch of first label flip\", \n",
    "            'desc': \"Cumulative prob epoch of first label flip for each original example\",\n",
    "            'cumulative': True,\n",
    "        },\n",
    "        {\n",
    "            'cname': 'idx_n_unique_pp', \n",
    "            'xlabel': \"Unique paraphrases per original example\", \n",
    "            \"desc\": \"Number of generated unique paraphrases per original example during training\", \n",
    "            'cumulative': False,\n",
    "        },\n",
    "        {\n",
    "            'cname': 'idx_n_pp_changes', \n",
    "            'xlabel': \"Paraphrase changes per original example\", \n",
    "            \"desc\": \"Number of paraphrase changes per original example during training\", \n",
    "            'cumulative': False,\n",
    "        }]\n",
    "    for d in hist_config_dicts:  fig_l.append({f\"pp_metrics/{d['cname']}\": _plot_idx_hist(df_concat, d['cname'],d['xlabel'],d['cumulative'])})\n",
    "    line_cnames = [o for o in df_concat.columns if \"_diff\" in o] + \\\n",
    "        [\"is_truncation\", 'any_phrase_capitalised', 'any_phrase_decapitalised', 'n_segments_inserted', \n",
    "         'n_segments_removed', 'n_tokens_inserted', 'n_tokens_removed','edit_distance_token_level']\n",
    "    for cname in line_cnames: fig_l.append({f\"pp_metrics/{cname}\": _plot_epoch_line_charts(df_concat, cname)})\n",
    "    return {k:v for d in fig_l for k,v in d.items()}\n",
    "\n",
    "def _plot_idx_hist(df_concat, cname, xlabel, cumulative=False): \n",
    "    df1 = df_concat[['data_split','idx', cname]].drop_duplicates()\n",
    "    fig = px.histogram(df1, x=cname, color='data_split', marginal=\"box\",\n",
    "                       labels={cname: xlabel},cumulative=cumulative, barmode='group', \n",
    "                      histnorm='probability', color_discrete_sequence=px.colors.qualitative.Dark24)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_layout(font_size=8)\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "    fig.update_layout(autosize=True)\n",
    "    return fig \n",
    "\n",
    "def _plot_epoch_line_charts(df_concat, cname): \n",
    "    df1 = df_concat[['data_split','epoch', cname]]\n",
    "    df_grp = df1.groupby(['data_split', 'epoch']).agg('mean').reset_index()\n",
    "    fig = px.line(df_grp, x=\"epoch\", y=cname, color='data_split', labels={cname: cname + \"_avg\"},\n",
    "                 color_discrete_sequence=px.colors.qualitative.Dark24)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_layout(font_size=8)\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "### NOT USED \n",
    "def pretty_print_pp_batch_and_next_token_probabilities(pp_output, tkn_kmaxidx, tkn_kmaxprob, generated_length): \n",
    "    \"\"\"Goes through each paraphrase and shows at each timestep the next likely tokens. \n",
    "    Only will work for greedy search. \n",
    "    e.g. [\n",
    "    \"<pad> ['▁My, 0.289', '▁I, 0.261', '▁Hello, 0.07'] | Entropy: 4.23 \",\n",
    "     \"<pad> My ['▁name, 0.935', '▁Name, 0.005', 'name, 0.002'] | Entropy: 0.80 \"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    from pprint import pprint\n",
    "    str_d = defaultdict(list)\n",
    "    for i_tkn in range(0, generated_length-1): \n",
    "        ids = pp_output.sequences[:, :(i_tkn+1)]\n",
    "        partial_pp = pp_tokenizer.batch_decode(ids)\n",
    "        kth_ids,kth_probs = tkn_kmaxidx[:, i_tkn, :], tkn_kmaxprob[:, i_tkn, :]\n",
    "        kth_tkns = get_tokens_from_token_ids_batch(pp_tokenizer, kth_ids)\n",
    "\n",
    "        # enumerates examples in batch\n",
    "        z = zip(partial_pp, kth_tkns, kth_probs, ent.detach())\n",
    "        for i_ex, (ex_sen, ex_next_tkns, ex_next_probs, ex_e) in enumerate(z): \n",
    "            # Form nice formatted string mixing together tokens and probabilities\n",
    "            tkn_tuples_l = [(tkn, round_t(prob,3)) for tkn, prob in zip(ex_next_tkns, ex_next_probs)]\n",
    "            tkn_str = ['%s, %s' % t for t in tkn_tuples_l]\n",
    "            # Add to dict of lists and add on entropy term. \n",
    "            str_d[i_ex].append(f\"{ex_sen} {tkn_str} | Entropy: {ex_e[i_tkn]:.2f} \")\n",
    "\n",
    "    for v in str_d.values():  pprint(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted baselines.ipynb.\n",
      "Converted baselines_analysis.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
