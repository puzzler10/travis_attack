{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, random, pandas as pd, os, warnings, shutil, uuid\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "from IPython.display import display, HTML\n",
    "from travis_attack.models import get_vm_probs, prepare_models\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.utils import robust_rmtree, timecode\n",
    "from IPython.core.debugger import set_trace\n",
    "import logging\n",
    "logger = logging.getLogger(\"travis_attack.data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prep_dsd_simple(cfg): \n",
    "    \"\"\"Load the simple dataset and package it up in a DatasetDict (dsd) \n",
    "    with splits for train, valid, test.\"\"\"\n",
    "    dsd = DatasetDict()\n",
    "    for s in ['train', 'valid', 'test']:  \n",
    "        dsd[s] = load_dataset('csv', \n",
    "            data_files=f\"{cfg.path_data}simple_dataset_{s}.csv\", keep_in_memory=False)['train']\n",
    "    return dsd\n",
    "\n",
    "def prep_dsd_rotten_tomatoes(cfg):\n",
    "    \"\"\"Load the rotten tomatoes dataset and package it up in a DatasetDict (dsd) \n",
    "    with splits for train, valid, test.\"\"\"\n",
    "    dsd = load_dataset(\"rotten_tomatoes\")\n",
    "    dsd['valid'] = dsd.pop('validation')  # \"valid\" is easier than \"validation\" \n",
    "    # make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "    #for _,ds in dsd.items(): assert ds.features[cfg.label_cname].num_classes == cfg.vm_num_labels \n",
    "    return dsd \n",
    "\n",
    "def prep_dsd_financial(cfg):\n",
    "    \"\"\"Load the financial dataset and package it up in a DatasetDict (dsd) \n",
    "    with splits for train, valid, test.\"\"\"\n",
    "    dsd = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
    "    dsd = get_train_valid_test_split(dsd)\n",
    "    dsd = dsd.rename_column('sentence', 'text')\n",
    "   # for _,ds in dsd.items(): assert ds.features[cfg.label_cname].num_classes == cfg.vm_num_labels\n",
    "    return dsd\n",
    "\n",
    "def prep_dsd_raw_snli(cfg): \n",
    "    ## For snli\n",
    "    # remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "    # ds_train = ds_train.filter(remove_minus1_labels)\n",
    "    # valid = valid.filter(remove_minus1_labels)\n",
    "    # test = test.filter(remove_minus1_labels)\n",
    "    raise NotImplementedError(\"SNLI not implemented yet.\")\n",
    "    \n",
    "def get_train_valid_test_split(dsd, train_size=0.8): \n",
    "        dsd1 = dsd['train'].train_test_split(train_size=train_size)\n",
    "        dsd2 = dsd1['test'].train_test_split(train_size=0.5)\n",
    "        return DatasetDict({\n",
    "            'train': dsd1['train'],\n",
    "            'valid': dsd2['train'],\n",
    "            'test': dsd2['test']\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ProcessedDataset: \n",
    "    \"\"\"Class that wraps a raw dataset (e.g. from huggingface datasets) and performs preprocessing on it.\"\"\"\n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model,\n",
    "                 load_processed_from_file=True): \n",
    "        \"\"\"load_processed_from_file: set to true to load completed version from file, false will process the data. \"\"\"\n",
    "        self._cfg,self._vm_tokenizer,self._vm_model,self._pp_tokenizer,self._sts_model = cfg,vm_tokenizer,vm_model,pp_tokenizer,sts_model\n",
    "        shard_suffix = f\"_{self._cfg.n_shards}_shards\" if self._cfg.use_small_ds else \"\"\n",
    "        self.cache_path_raw = f\"{self._cfg.path_data_cache}{self._cfg.dataset_name}_raw{shard_suffix}\"\n",
    "        self.cache_path_tkn = f\"{self._cfg.path_data_cache}{self._cfg.dataset_name}_tkn{shard_suffix}\"\n",
    "        \n",
    "        logger.info(f\"Will load dataset {self._cfg.dataset_name} with use_small_ds set to {self._cfg.use_small_ds}\")\n",
    "        \n",
    "        if load_processed_from_file:\n",
    "            if os.path.exists(self.cache_path_raw) and os.path.exists(self.cache_path_tkn):\n",
    "                logger.info(\"Cache file found for processed dataset, so loading that dataset.\")\n",
    "                self.dsd_raw = load_from_disk(self.cache_path_raw) \n",
    "                self.dsd_tkn = load_from_disk(self.cache_path_tkn)\n",
    "                self._prep_dataloaders()\n",
    "            else: \n",
    "                warnings.warn(\"Cache file not found, so will now process the raw dataset.\")\n",
    "                self._preprocess_dataset() \n",
    "        else:   \n",
    "            self._preprocess_dataset() \n",
    "        self._update_cfg()\n",
    "        \n",
    "        logger.debug(f\"Dataset lengths: {self._cfg.ds_length}\")\n",
    "        logger.debug(f\"Total training epochs:{self._cfg.n_train_steps}\")\n",
    "        logger.debug(f\"Last batch size in each epoch is: {self._cfg.dl_leftover_batch_size}\")\n",
    "        logger.debug(f\"Dataloader batch sizes are: {self._cfg.dl_batch_sizes}\") \n",
    "    \n",
    "    def _preprocess_dataset(self): \n",
    "        \"\"\"Add columns, tokenize, transform, prepare dataloaders, and do other preprocessing tasks.\"\"\"\n",
    "        if   self._cfg.dataset_name == \"simple\":          dsd = prep_dsd_simple(self._cfg)\n",
    "        elif self._cfg.dataset_name == \"rotten_tomatoes\": dsd = prep_dsd_rotten_tomatoes(self._cfg)\n",
    "        elif self._cfg.dataset_name == \"financial\":       dsd = prep_dsd_financial(self._cfg)\n",
    "        else: raise Exception(\"cfg.dataset_name not valid\")\n",
    "        dsd = dsd.map(self._add_idx, batched=True, with_indices=True)  # add idx column\n",
    "        if self._cfg.use_small_ds: dsd = self._shard_dsd(dsd)  # do after adding idx so it's consistent across runs\n",
    "        # add VM score & filter out misclassified examples.\n",
    "        # use a common variable dsd, add all columns, and later filter columns to get dsd_raw and dsd_tkn\n",
    "        dsd = dsd.map(self._add_vm_orig_score, batched=True)  \n",
    "        if self._cfg.remove_misclassified_examples:  dsd = dsd.filter(lambda x: x['orig_vm_predclass'] == x['label']) \n",
    "        dsd = dsd.map(self._add_sts_orig_embeddings, batched=True)  # add STS score \n",
    "        dsd = dsd.map(self._tokenize_fn,             batched=True)  # tokenize\n",
    "        dsd = dsd.map(self._add_n_tokens,            batched=True)  # add n_tokens\n",
    "        dsd = dsd.map(self._add_n_letters,           batched=True)  # add n_letters\n",
    "        if self._cfg.bucket_by_length: dsd = dsd.sort(\"n_tokens\", reverse=True)  # sort by n_tokens (high to low), useful for cuda memory caching\n",
    "        # Split dsd into dsd_raw and dsd_tkn\n",
    "        assert dsd.column_names['train'] == dsd.column_names['valid'] == dsd.column_names['test']\n",
    "        self.cnames_dsd_raw = ['idx', 'text', 'label']\n",
    "        self.cnames_dsd_tkn = [o for o in dsd.column_names['train'] if o != 'text'] \n",
    "        self.dsd_raw = dsd.remove_columns([o for o in  dsd['train'].column_names if o not in self.cnames_dsd_raw])\n",
    "        self.dsd_tkn = dsd.remove_columns([\"text\"])\n",
    "        for s in self._cfg.splits: assert len(self.dsd_raw[s]) == len(self.dsd_tkn[s])  # check ds has same number of elements in raw and tkn\n",
    "        self._cache_processed_ds()\n",
    "        self._prep_dataloaders()\n",
    "        \n",
    "    def _prep_dataloaders(self): \n",
    "        self.dld_raw = self._get_dataloaders_dict(self.dsd_raw, collate_fn=self._collate_fn_raw)  # dict of data loaders that serve raw text\n",
    "        self.dld_tkn = self._get_dataloaders_dict(self.dsd_tkn, collate_fn=self._collate_fn_tkn)  # dict of data loaders that serve tokenized text\n",
    "    \n",
    "    def _add_idx(self, batch, idx):\n",
    "        \"\"\"Add row numbers\"\"\"\n",
    "        batch['idx'] = idx \n",
    "        return batch   \n",
    "    \n",
    "    def _add_n_tokens(self, batch): \n",
    "        \"\"\"Add the number of tokens of the orig text \"\"\"\n",
    "        batch['n_tokens'] = [len(o) for o in batch['input_ids']]\n",
    "        return batch \n",
    "    \n",
    "    def _add_n_letters(self, batch): \n",
    "        batch['n_letters'] = [len(o) for o in batch['text']]\n",
    "        return batch\n",
    "    \n",
    "    def _add_sts_orig_embeddings(self, batch): \n",
    "        \"\"\"Add the sts embeddings of the orig text\"\"\"\n",
    "        batch['orig_sts_embeddings'] = self._sts_model.encode(batch['text'], batch_size=64, convert_to_tensor=False)\n",
    "        return batch\n",
    "    \n",
    "    def _add_vm_orig_score(self, batch): \n",
    "        \"\"\"Add the vm score of the orig text\"\"\"\n",
    "        labels = torch.tensor(batch[self._cfg.label_cname], device=self._cfg.device)\n",
    "        orig_probs,orig_predclass = get_vm_probs(batch['text'], self._cfg, self._vm_tokenizer,\n",
    "                                                 self._vm_model, return_predclass=True)\n",
    "        batch['orig_truelabel_probs'] = torch.gather(orig_probs,1, labels[:,None]).squeeze().cpu().tolist()\n",
    "        batch['orig_vm_predclass'] = orig_predclass.cpu().tolist()\n",
    "        return batch\n",
    "    \n",
    "    def _tokenize_fn(self, batch):  \n",
    "        \"\"\"Tokenize a batch of orig text using the paraphrase tokenizer.\"\"\"\n",
    "        return self._pp_tokenizer(batch['text'], truncation=True, max_length=self._cfg.orig_max_length)  \n",
    "    \n",
    "    def _collate_fn_tkn(self, x): \n",
    "        \"\"\"Collate function used by the DataLoader that serves tokenized data. \n",
    "        x is a list (with length batch_size) of dicts. Keys should be the same across dicts.\n",
    "        I guess an error is raised if not. \"\"\"\n",
    "        # check all keys are the same in the list. the assert is quick (~1e-5 seconds)\n",
    "        for o in x: assert set(o) == set(x[0])\n",
    "        d = dict()\n",
    "        for k in x[0].keys():  d[k] = [o[k] for o in x]\n",
    "        return self._pp_tokenizer.pad(d, pad_to_multiple_of=self._cfg.orig_padding_multiple, return_tensors=\"pt\")\n",
    "\n",
    "    def _collate_fn_raw(self, x): \n",
    "        \"\"\"Collate function used by the DataLoader that serves raw data. x is a list of data.\"\"\"\n",
    "        d = dict()\n",
    "        for o in x: assert set(o) == set(x[0])  # check all keys are the same in list\n",
    "        for k in x[0].keys(): d[k] = [o[k] for o in x]\n",
    "        return d \n",
    "\n",
    "    def _get_sampler(self, ds): \n",
    "        \"\"\"Returns a RandomSampler. Used so we can keep the same shuffle order across multiple data loaders.\n",
    "        Used when self._cfg.shuffle_train = True\"\"\"\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        return RandomSampler(ds, generator=g)\n",
    "    \n",
    "    def _shard_dsd(self, dsd):\n",
    "        \"\"\"Replaces dsd with a smaller shard of itself.\"\"\"\n",
    "        for k,v in dsd.items():  \n",
    "            dsd[k] = v.shard(self._cfg.n_shards, 0, contiguous=self._cfg.shard_contiguous)\n",
    "        return dsd\n",
    "        \n",
    "    def _get_dataloaders_dict(self, dsd, collate_fn): \n",
    "        \"\"\"Prepare a dict of dataloaders for train, valid and test\"\"\"\n",
    "        if self._cfg.bucket_by_length and self._cfg.shuffle_train:  raise Exception(\"Can only do one of bucket by length or shuffle\")\n",
    "        d = dict()\n",
    "        for split, ds in dsd.items(): \n",
    "            if self._cfg.shuffle_train:\n",
    "                if split == \"train\": \n",
    "                    sampler = self.get_sampler(ds)\n",
    "                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_train, \n",
    "                                           sampler=sampler, collate_fn=collate_fn, \n",
    "                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "                else: \n",
    "                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_eval, \n",
    "                                           shuffle=False, collate_fn=collate_fn, \n",
    "                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "            if self._cfg.bucket_by_length: \n",
    "                batch_size = self._cfg.batch_size_train if split == \"train\" else self._cfg.batch_size_eval\n",
    "                d[split] =  DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, \n",
    "                                       num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "\n",
    "        # Add eval dataloader for train: same as train but bigger batch size and explicitly no shuffling.\n",
    "        d['train_eval'] = DataLoader(dsd['train'], batch_size=self._cfg.batch_size_eval, shuffle=False,\n",
    "                                    collate_fn=collate_fn, \n",
    "                                     num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "        return d \n",
    "    \n",
    "    def _update_cfg(self): \n",
    "        self._cfg.ds_length,self._cfg.dl_n_batches,self._cfg.dl_leftover_batch_size,self._cfg.dl_batch_sizes = dict(),dict(),dict(),dict()\n",
    "        def get_dl_batch_sizes(batch_size, dl_n_batches): \n",
    "            if self._cfg.dl_leftover_batch_size[k] == 0: \n",
    "                return [batch_size for i in range(dl_n_batches)]\n",
    "            else: \n",
    "                l = [batch_size for i in range(dl_n_batches - 1)]\n",
    "                l.append(self._cfg.dl_leftover_batch_size[k])\n",
    "                return l\n",
    "                \n",
    "        for k,v in self.dsd_raw.items(): self._cfg.ds_length[k] = len(v)   # Dataset lengths\n",
    "        for k,v in self.dld_raw.items(): \n",
    "            self._cfg.dl_n_batches[k] = len(v)   # Dataloader lengths \n",
    "            # Dataloader last batch size and list of batch sizes\n",
    "            ds_k = \"train\" if k == \"train_eval\" else k \n",
    "            if k == \"train\": \n",
    "                self._cfg.dl_leftover_batch_size[k] = self._cfg.ds_length[ds_k] % self._cfg.batch_size_train\n",
    "                self._cfg.dl_batch_sizes[k]     = get_dl_batch_sizes(self._cfg.batch_size_train, self._cfg.dl_n_batches[k])\n",
    "            else: \n",
    "                self._cfg.dl_leftover_batch_size[k] = self._cfg.ds_length[ds_k] % self._cfg.batch_size_eval\n",
    "                self._cfg.dl_batch_sizes[k]     = get_dl_batch_sizes(self._cfg.batch_size_eval, self._cfg.dl_n_batches[k])\n",
    "            \n",
    "        # Total number of training steps\n",
    "        self._cfg.n_train_steps = self._cfg.n_train_epochs * self._cfg.dl_n_batches['train']\n",
    "    \n",
    "    def _cache_processed_ds(self):\n",
    "        def _reset_dir(path): \n",
    "            if os.path.exists(path) and os.path.isdir(path):    \n",
    "                # So deleting the old files sometimes throws errors because of race conditions, I think \n",
    "                # so as a workaround we will just move files to old directories and then periodicallly clean them. \n",
    "                #                robust_rmtree(path, logger=None, max_retries=6)  \n",
    "                path_old_files = f\"{self._cfg.path_data_cache}old_files/\"\n",
    "                os.makedirs(path_old_files, exist_ok=True)\n",
    "                shutil.move(path, f\"{path_old_files}{uuid.uuid4().hex}\") \n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        _reset_dir(self.cache_path_raw)\n",
    "        _reset_dir(self.cache_path_tkn)\n",
    "        self.dsd_raw.save_to_disk(dataset_dict_path = self.cache_path_raw)\n",
    "        self.dsd_tkn.save_to_disk(dataset_dict_path = self.cache_path_tkn)\n",
    "        \n",
    "    def show_random_elements(self, ds, num_examples=10):\n",
    "        \"\"\"Print some elements in a nice format so you can take a look at them. \n",
    "        Split is one of 'train', 'test', 'valid'. \n",
    "        Use for a dataset `ds` from the `dataset` package.  \"\"\"\n",
    "        assert num_examples <= len(ds), \"Can't pick more elements than there are in the dataset.\"\n",
    "        picks = []\n",
    "        for _ in range(num_examples):\n",
    "            pick = random.randint(0, len(ds)-1)\n",
    "            while pick in picks:\n",
    "                pick = random.randint(0, len(ds)-1)\n",
    "            picks.append(pick)\n",
    "        df = pd.DataFrame(ds[picks])\n",
    "        for column, typ in ds.features.items():\n",
    "            if isinstance(typ, ClassLabel):\n",
    "                df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_financial = Config().adjust_config_for_financial_dataset()\n",
    "#cfg_financial.device = 'cpu'\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg_financial = prepare_models(cfg_financial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (/data/tproth/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a7cc4b89ba4a62beabb25468cb4afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef5f3a6aa9f421aab2f4a415968fd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5d0c5404a84149ad7a3d5991ddaa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7082cada150f4de285b68fc6222fe875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302282b40f174824aef0fd2f734a35f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031221aa9d7b4f8eb24f23abd3d4bba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d577db91924bb89a7d9ac49bd56ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48927504a4a1439a850c8db2ccea8002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3cfc01c53d4ed2b3bf11cee3d71a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffdc732c32e4eac8c54d343dfc9d539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cc08494994c3893e06c04b7ffea71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7fbef620fa47e1a5420ec566e4dc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165d4ba77fb04fe8b8b6a60dbf8390c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1209cffb64480dbed25446943b0601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001a0984da704c46b28d812a2795d3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2167b4e9a9d44d18a61bb02e3ea7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb4dccd53c4bf48d74f739464e2fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb459776d24a10852f3f9230132582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e64bd22a3742ee9fa37ef6461be7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f036b3f108b482dbd347f3b7837714f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d63b737d834c009f79e7bdadbda469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9f92bfeea24e45b8abe14b984377da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d41e1e099d42fab3dcdde718facbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f7190e7d1e4af481b482fcfd50b398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4c68ca938c4d5abaca125eb7e43660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aa9697af5148098ce713981301dacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fd54297af641ab963fb21253a91296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac359f6e6f8348efbb43ee13c1ea7176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ProcessedDataset(cfg_financial, vm_tokenizer, vm_model, pp_tokenizer, sts_model, \n",
    "                      load_processed_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined a class `ProcessedDataset` that will load and preprocess a dataset. But before processing the dataset you must load both the config object and all models/tokenizers, so we do this first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from travis_attack.models import prepare_models\n",
    "cfg = Config()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are two choices for dataset: \n",
    "\n",
    "* `simple`, a dataset of simple sentences with four elements each in the train, test and valid splits\n",
    "* `rotten_tomatoes`, a dataset of movie reviews scraped from the Rotten Tomatoes site. \n",
    "\n",
    "The dataset is specified by the config class. There are two ways to do this. \n",
    "\n",
    "1. Edit the `self.dataset_name` variable in the Config class to either `simple` or `rotten_tomatoes`. An error will be thrown if the name is not one of these two. This is the best way to use when doing runs.   \n",
    "2. Use the `adjust_dataset_...` methods of the config class: e.g. `cfg = cfg.adjust_dataset_for_rotten_tomatoes_dataset() = Config()`. This is easiest for automated testing so we will do this here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the config is specified and loaded, create an object of class `ProcessedDataset` by passing the config, models and tokenizers as variables. This will do all preprocessing automatically in creating the object (the preprocessing code is in the `__init__()` function of the class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b253756c445fb811\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc7b64ea8fb4b7180361763e80a519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c802946231f72062\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1065a5ad4c1e40799a747a4fcb759f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-43a49c5188c42e69\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435b7ec15bfa416d9c7bfc6ec280630f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16acc6afb1fa490cabd30e4d2ab43beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fb99f3b40142739ec6791671bae7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c84c3bb0d24d478168dfb58ad6c51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f197b1a80346b0aff670b4fc643c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31416f0169584d06a5efab7a78012e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b340df67f56448cb84226fc6948cbd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ba53e9dd0e49d59d68cacfb002b58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e28affa9d245fdaaad2c90ff4bf6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cacfe9c13974b9b99068ff1c1d36eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5c4f852b9c46a981ab617d09ec9d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1287a3d1284230864b44a8d9bbf945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b7528d943044c3a164828ce88e5048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90905d3042c347b992d4d6470e70253d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7672b4faf4e7468e817a1ff3c5782c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f2636e1de641c894bd7e2408f80e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259a45186bc7490abbec08379aa5ecf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae81db6f11649098622616d1867f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0270fccb068c4280b9defa349fc6476f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb2bfaffc02450aaf620a88c8687b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d040b1565b4316ad9c3c931d147ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124ac02c4ed04a63ad543efb53793152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41e9c20e8594aa698bee8ff7be8c3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bce037cddb4ec6a584465f3bbb17e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd23135185be4935949b418557f50066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6c4f94e0bb4a55ab405fbd973675ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa48e8791be4e17832de91768ea2cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab749e9c138e434aaf4eaaae0260b412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg_simple = cfg.adjust_config_for_simple_dataset()\n",
    "ds = ProcessedDataset(cfg_simple, vm_tokenizer, vm_model, pp_tokenizer, sts_model, \n",
    "                      load_processed_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the small dataset adjust the config before creating the `ProcessedDataset` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b273876b3f5f451fb17a665870e14d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0a89aeddb5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcfg_rt_small_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_config_for_rotten_tomatoes_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m ds = ProcessedDataset(cfg_rt_small_ds, vm_tokenizer, vm_model, pp_tokenizer, sts_model,\n\u001b[0m\u001b[1;32m      3\u001b[0m                       load_processed_from_file=False)\n",
      "\u001b[0;32m<ipython-input-5-ef5328328ebc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model, load_processed_from_file)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ef5328328ebc>\u001b[0m in \u001b[0;36m_preprocess_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;34m\"\"\"Add columns, tokenize, transform, prepare dataloaders, and do other preprocessing tasks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m:\u001b[0m          \u001b[0mdsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_dsd_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rotten_tomatoes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_dsd_rotten_tomatoes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"financial\"\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mdsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_dsd_financial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cfg.dataset_name not valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ef5328328ebc>\u001b[0m in \u001b[0;36m_prep_dsd_rotten_tomatoes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdsd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \"valid\" is easier than \"validation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# make sure that all datasets have the same number of labels as what the victim model predicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_cname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvm_num_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdsd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg_rt_small_ds = cfg.adjust_config_for_rotten_tomatoes_dataset().small_ds()\n",
    "ds = ProcessedDataset(cfg_rt_small_ds, vm_tokenizer, vm_model, pp_tokenizer, sts_model,\n",
    "                      load_processed_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access raw data with `ds.dsd_raw` and processed data with the `ds.dsd_tkn`. (The dsd here stands for \"DatasetDict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_tkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access elements by indexing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_raw['valid'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_tkn['valid'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately you can look at some random elements of a dataset with the `ds.show_random_elements()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_random_elements(ds.dsd_raw['train'], num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_random_elements(ds.dsd_tkn['train'], num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing DataLoaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access dataloaders for the raw text in `ds.dsd_raw` with `ds.dld_raw`, and for the tokenised text in `ds.dsd_tkn` with `ds_dld_tkn`. Both of these are dictionaries of dataloaders with keys `['train', 'valid', 'test', 'train_eval]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataloader dict has keys:\", ds.dld_raw.keys())\n",
    "batch_raw = next(iter(ds.dld_raw['train']))\n",
    "batch_tkn = next(iter(ds.dld_tkn['train']))\n",
    "print(batch_raw.keys())\n",
    "print(batch_tkn.keys())\n",
    "print(\"Tokenised input is of shape:\", batch_tkn['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted benchmarks.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
