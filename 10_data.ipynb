{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, random, pandas as pd, os, warnings, shutil, uuid\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "from IPython.display import display, HTML\n",
    "from travis_attack.models import get_vm_probs\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.utils import robust_rmtree, timecode\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"travis_attack.data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import inspect\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ProcessedDataset: \n",
    "    \"\"\"Class that wraps a raw dataset (e.g. from huggingface datasets) and performs preprocessing on it.\"\"\"\n",
    "    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model,\n",
    "                 load_processed_from_file=True): \n",
    "        \"\"\"load_processed_from_file: set to true to load completed version from file, false will process the data. \"\"\"\n",
    "        self._cfg,self._vm_tokenizer,self._vm_model,self._pp_tokenizer,self._sts_model = cfg,vm_tokenizer,vm_model,pp_tokenizer,sts_model\n",
    "        shard_suffix = f\"_{self._cfg.n_shards}_shards\" if self._cfg.use_small_ds else \"\"\n",
    "        self.cache_path_raw = f\"{self._cfg.path_data_cache}{self._cfg.dataset_name}_raw{shard_suffix}\"\n",
    "        self.cache_path_tkn = f\"{self._cfg.path_data_cache}{self._cfg.dataset_name}_tkn{shard_suffix}\"\n",
    "        \n",
    "        logger.info(f\"Will load dataset {self._cfg.dataset_name} with use_small_ds set to {self._cfg.use_small_ds}\")\n",
    "        \n",
    "        if load_processed_from_file:\n",
    "            if os.path.exists(self.cache_path_raw) and os.path.exists(self.cache_path_tkn):\n",
    "                logger.info(\"Cache file found for processed dataset, so loading that dataset.\")\n",
    "                self.dsd_raw = load_from_disk(self.cache_path_raw) \n",
    "                self.dsd_tkn = load_from_disk(self.cache_path_tkn)\n",
    "                self._prep_dataloaders()\n",
    "            else: \n",
    "                warnings.warn(\"Cache file not found, so will now process the raw dataset.\")\n",
    "                self._preprocess_dataset() \n",
    "        else:   \n",
    "            self._preprocess_dataset() \n",
    "        self._update_cfg()\n",
    "        \n",
    "        logger.debug(f\"Dataset lengths: {self._cfg.ds_length}\")\n",
    "        logger.debug(f\"Total training epochs:{self._cfg.n_train_steps}\")\n",
    "        logger.debug(f\"Last batch size in each epoch is: {self._cfg.dl_last_batch_size}\")\n",
    "        logger.debug(f\"Dataloader batch sizes are: {self._cfg.dl_batch_sizes}\") \n",
    "            \n",
    "            \n",
    "    def _prep_dsd_simple(self): \n",
    "        \"\"\"Load the simple dataset and package it up in a DatasetDict (dsd) \n",
    "        with splits for train, valid, test.\"\"\"\n",
    "        dsd = DatasetDict()\n",
    "        for s in self._cfg.splits:  \n",
    "            dsd[s] = load_dataset('csv', \n",
    "                data_files=f\"{self._cfg.path_data}simple_dataset_{s}.csv\", keep_in_memory=False)['train']\n",
    "        return dsd\n",
    "        \n",
    "    def _prep_dsd_rotten_tomatoes(self):\n",
    "        \"\"\"Load the rotten tomatoes dataet and package it up in a DatasetDict (dsd) \n",
    "        with splits for train, valid, test.\"\"\"\n",
    "        dsd = load_dataset(\"rotten_tomatoes\")\n",
    "        dsd['valid'] = dsd.pop('validation')  # \"valid\" is easier than \"validation\" \n",
    "        # make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "        for _,ds in dsd.items(): assert ds.features[self._cfg.label_cname].num_classes == self._cfg.vm_num_labels \n",
    "        return dsd \n",
    "    \n",
    "    def _prep_dsd_raw_snli(self): \n",
    "        ## For snli\n",
    "        # remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "        # ds_train = ds_train.filter(remove_minus1_labels)\n",
    "        # valid = valid.filter(remove_minus1_labels)\n",
    "        # test = test.filter(remove_minus1_labels)\n",
    "        raise NotImplementedError(\"SNLI not implemented yet.\")\n",
    "    \n",
    "    def _preprocess_dataset(self): \n",
    "        \"\"\"Add columns, tokenize, transform, prepare dataloaders, and do other preprocessing tasks.\"\"\"\n",
    "        if   self._cfg.dataset_name == \"simple\":          dsd = self._prep_dsd_simple()\n",
    "        elif self._cfg.dataset_name == \"rotten_tomatoes\": dsd = self._prep_dsd_rotten_tomatoes()\n",
    "        else: raise Exception(\"cfg.dataset_name must be either 'simple' or 'rotten_tomatoes'\")\n",
    "        dsd = dsd.map(self._add_idx, batched=True, with_indices=True)  # add idx column\n",
    "        if self._cfg.use_small_ds: dsd = self._shard_dsd(dsd)  # do after adding idx so it's consistent across runs\n",
    "        # add VM score & filter out misclassified examples.\n",
    "        # use a common variable dsd, add all columns, and later filter columns to get dsd_raw and dsd_tkn\n",
    "        dsd = dsd.map(self._add_vm_orig_score, batched=True)  \n",
    "        if self._cfg.remove_misclassified_examples:  dsd = dsd.filter(lambda x: x['orig_vm_predclass'] == x['label']) \n",
    "        dsd = dsd.map(self._add_sts_orig_embeddings, batched=True)  # add STS score \n",
    "        dsd = dsd.map(self._tokenize_fn,             batched=True)  # tokenize\n",
    "        dsd = dsd.map(self._add_n_tokens,            batched=True)  # add n_tokens\n",
    "        if self._cfg.bucket_by_length: dsd = dsd.sort(\"n_tokens\", reverse=True)  # sort by n_tokens (high to low), useful for cuda memory caching\n",
    "        # Split dsd into dsd_raw and dsd_tkn\n",
    "        assert dsd.column_names['train'] == dsd.column_names['valid'] == dsd.column_names['test']\n",
    "        self.cnames_dsd_raw = ['idx', 'text', 'label']\n",
    "        self.cnames_dsd_tkn = [o for o in dsd.column_names['train'] if o != 'text'] \n",
    "        self.dsd_raw = dsd.remove_columns([o for o in  dsd['train'].column_names if o not in self.cnames_dsd_raw])\n",
    "        self.dsd_tkn = dsd.remove_columns([\"text\"])\n",
    "        for s in self._cfg.splits: assert len(self.dsd_raw[s]) == len(self.dsd_tkn[s])  # check ds has same number of elements in raw and tkn\n",
    "        self._cache_processed_ds()\n",
    "        self._prep_dataloaders()\n",
    "        \n",
    "    def _prep_dataloaders(self): \n",
    "        self.dld_raw = self._get_dataloaders_dict(self.dsd_raw, collate_fn=self._collate_fn_raw)  # dict of data loaders that serve raw text\n",
    "        self.dld_tkn = self._get_dataloaders_dict(self.dsd_tkn, collate_fn=self._collate_fn_tkn)  # dict of data loaders that serve tokenized text\n",
    "        \n",
    "    def _add_idx(self, batch, idx):\n",
    "        \"\"\"Add row numbers\"\"\"\n",
    "        batch['idx'] = idx \n",
    "        return batch   \n",
    "    \n",
    "    def _add_n_tokens(self, batch): \n",
    "        \"\"\"Add the number of tokens of the orig text \"\"\"\n",
    "        batch['n_tokens'] = [len(o) for o in batch['input_ids']]\n",
    "        return batch \n",
    "    \n",
    "    def _add_sts_orig_embeddings(self, batch): \n",
    "        \"\"\"Add the sts embeddings of the orig text\"\"\"\n",
    "        batch['orig_sts_embeddings'] = self._sts_model.encode(batch[self._cfg.orig_cname], batch_size=64, convert_to_tensor=False)\n",
    "        return batch\n",
    "    \n",
    "    def _add_vm_orig_score(self, batch): \n",
    "        \"\"\"Add the vm score of the orig text\"\"\"\n",
    "        labels = torch.tensor(batch[self._cfg.label_cname], device=self._cfg.device)\n",
    "        orig_probs,orig_predclass = get_vm_probs(batch[self._cfg.orig_cname], self._cfg, self._vm_tokenizer,\n",
    "                                                 self._vm_model, return_predclass=True)\n",
    "        batch['orig_truelabel_probs'] = torch.gather(orig_probs,1, labels[:,None]).squeeze().cpu().tolist()\n",
    "        batch['orig_vm_predclass'] = orig_predclass.cpu().tolist()\n",
    "        return batch\n",
    "    \n",
    "    def _tokenize_fn(self, batch):  \n",
    "        \"\"\"Tokenize a batch of orig text using the paraphrase tokenizer.\"\"\"\n",
    "        return self._pp_tokenizer(batch[self._cfg.orig_cname], truncation=True, max_length=self._cfg.orig_max_length)  \n",
    "    \n",
    "    def _collate_fn_tkn(self, x): \n",
    "        \"\"\"Collate function used by the DataLoader that serves tokenized data. \n",
    "        x is a list (with length batch_size) of dicts. Keys should be the same across dicts.\n",
    "        I guess an error is raised if not. \"\"\"\n",
    "        # check all keys are the same in the list. the assert is quick (~1e-5 seconds)\n",
    "        for o in x: assert set(o) == set(x[0])\n",
    "        d = dict()\n",
    "        for k in x[0].keys():  d[k] = [o[k] for o in x]\n",
    "        return self._pp_tokenizer.pad(d, pad_to_multiple_of=self._cfg.orig_padding_multiple, return_tensors=\"pt\")\n",
    "\n",
    "    def _collate_fn_raw(self, x): \n",
    "        \"\"\"Collate function used by the DataLoader that serves raw data. x is a list of data.\"\"\"\n",
    "        d = dict()\n",
    "        for o in x: assert set(o) == set(x[0])  # check all keys are the same in list\n",
    "        for k in x[0].keys(): d[k] = [o[k] for o in x]\n",
    "        return d \n",
    "\n",
    "    def _get_sampler(self, ds): \n",
    "        \"\"\"Returns a RandomSampler. Used so we can keep the same shuffle order across multiple data loaders.\n",
    "        Used when self._cfg.shuffle_train = True\"\"\"\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        return RandomSampler(ds, generator=g)\n",
    "    \n",
    "    def _shard_dsd(self, dsd):\n",
    "        \"\"\"Replaces dsd with a smaller shard of itself.\"\"\"\n",
    "        for k,v in dsd.items():  \n",
    "            dsd[k] = v.shard(self._cfg.n_shards, 0, contiguous=self._cfg.shard_contiguous)\n",
    "        return dsd\n",
    "        \n",
    "    def _get_dataloaders_dict(self, dsd, collate_fn): \n",
    "        \"\"\"Prepare a dict of dataloaders for train, valid and test\"\"\"\n",
    "        if self._cfg.bucket_by_length and self._cfg.shuffle_train:  raise Exception(\"Can only do one of bucket by length or shuffle\")\n",
    "        d = dict()\n",
    "        for split, ds in dsd.items(): \n",
    "            if self._cfg.shuffle_train:\n",
    "                if split == \"train\": \n",
    "                    sampler = self.get_sampler(ds)\n",
    "                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_train, \n",
    "                                           sampler=sampler, collate_fn=collate_fn, \n",
    "                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "                else: \n",
    "                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_eval, \n",
    "                                           shuffle=False, collate_fn=collate_fn, \n",
    "                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "            if self._cfg.bucket_by_length: \n",
    "                batch_size = self._cfg.batch_size_train if split == \"train\" else self._cfg.batch_size_eval\n",
    "                d[split] =  DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, \n",
    "                                       num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "\n",
    "        # Add eval dataloader for train: same as train but bigger batch size and explicitly no shuffling.\n",
    "        d['train_eval'] = DataLoader(dsd['train'], batch_size=self._cfg.batch_size_eval, shuffle=False,\n",
    "                                    collate_fn=collate_fn, \n",
    "                                     num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory) \n",
    "        return d \n",
    "    \n",
    "    def _update_cfg(self): \n",
    "        self._cfg.ds_length,self._cfg.dl_n_batches,self._cfg.dl_last_batch_size,self._cfg.dl_batch_sizes = dict(),dict(),dict(),dict()\n",
    "        def get_dl_batch_sizes(batch_size, dl_n_batches): \n",
    "            l = [batch_size for i in range(dl_n_batches - 1)]\n",
    "            l.append(self._cfg.dl_last_batch_size[k])\n",
    "            return l\n",
    "                \n",
    "        for k,v in self.dsd_raw.items(): self._cfg.ds_length[k] = len(v)   # Dataset lengths\n",
    "        for k,v in self.dld_raw.items(): \n",
    "            self._cfg.dl_n_batches[k] = len(v)   # Dataloader lengths \n",
    "            # Dataloader last batch size and list of batch sizes\n",
    "            ds_k = \"train\" if k == \"train_eval\" else k \n",
    "            if k == \"train\": \n",
    "                self._cfg.dl_last_batch_size[k] = self._cfg.ds_length[ds_k] % self._cfg.batch_size_train\n",
    "                self._cfg.dl_batch_sizes[k]     = get_dl_batch_sizes(self._cfg.batch_size_train, self._cfg.dl_n_batches[k])\n",
    "            else: \n",
    "                self._cfg.dl_last_batch_size[k] = self._cfg.ds_length[ds_k] % self._cfg.batch_size_eval\n",
    "                self._cfg.dl_batch_sizes[k]     = get_dl_batch_sizes(self._cfg.batch_size_eval, self._cfg.dl_n_batches[k])\n",
    "            \n",
    "        # Total number of training steps\n",
    "        self._cfg.n_train_steps = self._cfg.n_train_epochs * self._cfg.dl_n_batches['train']\n",
    "    \n",
    "    def _cache_processed_ds(self):\n",
    "        def _reset_dir(path): \n",
    "            if os.path.exists(path) and os.path.isdir(path):    \n",
    "                # So deleting the old files sometimes throws errors because of race conditions, I think \n",
    "                # so as a workaround we will just move files to old directories and then periodicallly clean them. \n",
    "                #                robust_rmtree(path, logger=None, max_retries=6)  \n",
    "                path_old_files = f\"{self._cfg.path_data_cache}old_files/\"\n",
    "                os.makedirs(path_old_files, exist_ok=True)\n",
    "                shutil.move(path, f\"{path_old_files}{uuid.uuid4().hex}\") \n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        _reset_dir(self.cache_path_raw)\n",
    "        _reset_dir(self.cache_path_tkn)\n",
    "        self.dsd_raw.save_to_disk(dataset_dict_path = self.cache_path_raw)\n",
    "        self.dsd_tkn.save_to_disk(dataset_dict_path = self.cache_path_tkn)\n",
    "        \n",
    "    def show_random_elements(self, ds, num_examples=10):\n",
    "        \"\"\"Print some elements in a nice format so you can take a look at them. \n",
    "        Split is one of 'train', 'test', 'valid'. \n",
    "        Use for a dataset `ds` from the `dataset` package.  \"\"\"\n",
    "        assert num_examples <= len(ds), \"Can't pick more elements than there are in the dataset.\"\n",
    "        picks = []\n",
    "        for _ in range(num_examples):\n",
    "            pick = random.randint(0, len(ds)-1)\n",
    "            while pick in picks:\n",
    "                pick = random.randint(0, len(ds)-1)\n",
    "            picks.append(pick)\n",
    "        df = pd.DataFrame(ds[picks])\n",
    "        for column, typ in ds.features.items():\n",
    "            if isinstance(typ, ClassLabel):\n",
    "                df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined a class `ProcessedDataset` that will load and preprocess a dataset. But before processing the dataset you must load both the config object and all models/tokenizers, so we do this first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from travis_attack.models import prepare_models\n",
    "cfg = Config()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are two choices for dataset: \n",
    "\n",
    "* `simple`, a dataset of simple sentences with four elements each in the train, test and valid splits\n",
    "* `rotten_tomatoes`, a dataset of movie reviews scraped from the Rotten Tomatoes site. \n",
    "\n",
    "The dataset is specified by the config class. There are two ways to do this. \n",
    "\n",
    "1. Edit the `self.dataset_name` variable in the Config class to either `simple` or `rotten_tomatoes`. An error will be thrown if the name is not one of these two. This is the best way to use when doing runs.   \n",
    "2. Use the `adjust_dataset_...` methods of the config class: e.g. `cfg = cfg.adjust_dataset_for_rotten_tomatoes_dataset() = Config()`. This is easiest for automated testing so we will do this here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the config is specified and loaded, create an object of class `ProcessedDataset` by passing the config, models and tokenizers as variables. This will do all preprocessing automatically in creating the object (the preprocessing code is in the `__init__()` function of the class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_simple = cfg.adjust_config_for_simple_dataset()\n",
    "ds = ProcessedDataset(cfg_simple, vm_tokenizer, vm_model, pp_tokenizer, sts_model, \n",
    "                      load_processed_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the small dataset adjust the config before creating the `ProcessedDataset` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_rt_small_ds = cfg.adjust_config_for_rotten_tomatoes_dataset().small_ds()\n",
    "ds = ProcessedDataset(cfg_rt_small_ds, vm_tokenizer, vm_model, pp_tokenizer, sts_model,\n",
    "                      load_processed_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access raw data with `ds.dsd_raw` and processed data with the `ds.dsd_tkn`. (The dsd here stands for \"DatasetDict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_tkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access elements by indexing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_raw['valid'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dsd_tkn['valid'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately you can look at some random elements of a dataset with the `ds.show_random_elements()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_random_elements(ds.dsd_raw['train'], num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_random_elements(ds.dsd_tkn['train'], num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing DataLoaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access dataloaders for the raw text in `ds.dsd_raw` with `ds.dld_raw`, and for the tokenised text in `ds.dsd_tkn` with `ds_dld_tkn`. Both of these are dictionaries of dataloaders with keys `['train', 'valid', 'test', 'train_eval]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataloader dict has keys:\", ds.dld_raw.keys())\n",
    "batch_raw = next(iter(ds.dld_raw['train']))\n",
    "batch_tkn = next(iter(ds.dld_tkn['train']))\n",
    "print(batch_raw.keys())\n",
    "print(batch_tkn.keys())\n",
    "print(\"Tokenised input is of shape:\", batch_tkn['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
