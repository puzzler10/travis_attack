{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import datetime\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_fail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a class `Config` to hold hyperparameters and global variables.\n",
    "\n",
    "Design from https://github.com/cswinter/DeepCodeCraft/blob/master/hyper_params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Config: \n",
    "    def __init__(self): \n",
    "        \"\"\"Set up default parameters\"\"\"\n",
    "        ### Models and datasets\n",
    "        # PP options\n",
    "        # 1. tuner007/pegasus_paraphrase (2.12 GB)\n",
    "        # 2. prithivida/parrot_paraphraser_on_T5 (850 MB)\n",
    "        # 3. ramsrigouthamg/t5-large-paraphraser-diverse-high-quality (2.75 GB)\n",
    "        self.pp_name = \"prithivida/parrot_paraphraser_on_T5\"\n",
    "        self.dataset_name = \"simple\"\n",
    "        # STS options \n",
    "        # 1. sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "        # 2. sentence-transformers/paraphrase-MiniLM-L12-v2\n",
    "        self.sts_name = \"sentence-transformers/paraphrase-MiniLM-L12-v2\"\n",
    "        # NLI options \n",
    "        # 1. microsoft/deberta-base-mnli (~512 MB)\n",
    "        # 2. howey/electra-small-mnli\n",
    "        self.nli_name = \"howey/electra-small-mnli\"\n",
    "        self.cola_name = \"textattack/albert-base-v2-CoLA\"\n",
    "        self._select_vm_model()\n",
    "        \n",
    "        \n",
    "        ### Important parameters\n",
    "        self.seed = 421\n",
    "        self.use_small_ds = False\n",
    "        self.lr = 4e-5   \n",
    "        self.reward_fn = \"reward_fn_contradiction_and_letter_diff\"\n",
    "        self.reward_clip_max = 4\n",
    "        self.reward_clip_min = 0 \n",
    "        self.reward_base = 0 \n",
    "        self.reward_vm_multiplier = 12 \n",
    "        self.sts_threshold = 0.8\n",
    "        self.acceptability_threshold = 0.5  # min \"acceptable\" prob required. \n",
    "        self.contradiction_threshold = 0.2 \n",
    "        self.pp_letter_diff_threshold = 30\n",
    "        \n",
    "        self.reward_penalty_type = \"kl_div\"  # \"kl_div\" or \"ref_logp\"\n",
    "        self.kl_coef = 0.25             # only used if reward_penalty_type == \"kl_div\"\n",
    "        self.ref_logp_coef = 0.05       # only used if reward_penalty_type == \"ref_logp\"\n",
    "        self.max_pp_length = 48\n",
    "        \n",
    "        self.n_eval_seq = 48 \n",
    "        self.decode_method_train = \"sample\"  # \"sample\" or \"greedy\"\n",
    "        self.decode_method_eval = \"sample\"\n",
    "        self.gen_params_train = {\n",
    "            \"min_length\": 2, \n",
    "            \"max_length\": self.max_pp_length, \n",
    "            \"do_sample\": True        if self.decode_method_train == \"sample\" else False,\n",
    "            \"temperature\": 1.1       if self.decode_method_train == \"sample\" else None,\n",
    "            \"top_p\": 0.95            if self.decode_method_train == \"sample\" else None, \n",
    "            \"length_penalty\" : 1.    if self.decode_method_train == \"sample\" else None,\n",
    "            \"repetition_penalty\": 1. if self.decode_method_train == \"greedy\" else None\n",
    "        }\n",
    "        def _get_gen_params_eval(): \n",
    "            common_params = dict(num_return_sequences=self.n_eval_seq, max_length=self.max_pp_length)\n",
    "            gen_params_eval = dict(\n",
    "                beam_search         = dict(**common_params, do_sample=False, num_beams=self.n_eval_seq, \n",
    "                                           top_p=None, temperature=None, length_penalty=None, \n",
    "                                           diversity_penalty=None, num_beam_groups=None), \n",
    "                diverse_beam_search = dict(**common_params, do_sample=False, num_beams=self.n_eval_seq, \n",
    "                                           top_p=None, temperature=None, length_penalty=None,\n",
    "                                           diversity_penalty=1., num_beam_groups=self.n_eval_seq), \n",
    "                sample              = dict(**common_params, do_sample=True,  num_beams=1,      \n",
    "                                           top_p=0.95, temperature=0.8, length_penalty=1,\n",
    "                                           diversity_penalty=None, num_beam_groups=None)\n",
    "            )\n",
    "            return gen_params_eval[self.decode_method_eval]\n",
    "        self.gen_params_eval = _get_gen_params_eval()\n",
    "\n",
    "        \n",
    "        # Early stopping (determined during eval on valid set)\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_min_epochs = 10\n",
    "        self.early_stopping_metric = \"any_adv_example_proportion\"   # don't add -valid to the end of this. \n",
    "        \n",
    "        # Other parameters (usually left untouched)\n",
    "        self.orig_max_length = 32  # longest for pegasus is 60, longest for Parrot is 32\n",
    "        self.pin_memory = True\n",
    "        self.zero_grad_with_none = False\n",
    "        self.pad_token_embeddings = False\n",
    "        self.embedding_padding_multiple = 8\n",
    "        self.orig_padding_multiple = 8   # pad input to multiple of this\n",
    "        self.bucket_by_length = True\n",
    "        self.shuffle_train = False\n",
    "        self.remove_misclassified_examples = True\n",
    "        self.remove_long_orig_examples = True \n",
    "        self.unfreeze_last_n_layers = \"all\"  #counting from the back. set to \"all\" to do no layer freezing, else set to an int \n",
    "        \n",
    "        ### Used for testing\n",
    "        self.n_shards = None\n",
    "        self.shard_contiguous = None\n",
    "        \n",
    "        ### Logging parameters\n",
    "        self.save_model_while_training = False\n",
    "        self.save_model_freq = 10\n",
    "\n",
    "        ### W&B parameters\n",
    "        self.wandb = dict(\n",
    "            project = \"travis_attack\",\n",
    "            entity = \"uts_nlp\",\n",
    "            mode = \"disabled\",  # set to \"disabled\" to turn off wandb, \"online\" to enable it\n",
    "            log_grads = False, \n",
    "            log_grads_freq = 1,  # no effect if wandb_log_grads is False\n",
    "            log_token_entropy = True,\n",
    "            log_token_probabilities = True, \n",
    "            run_notes = f\"\"\n",
    "        )\n",
    "        \n",
    "        ### Devices and GPU settings\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        self.devicenum = torch.cuda.current_device() if self.device.type == 'cuda' else -1\n",
    "        self.n_wkrs = 4 * torch.cuda.device_count()\n",
    "        \n",
    "        ## Globals \n",
    "        self.datetime_run = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "        self.path_data = \"./data/\"\n",
    "        self.path_checkpoints = \"../model_checkpoints/travis_attack/\"\n",
    "        self.path_run = None  # keep as None; this is automatically filled out by trainer (code in utils)\n",
    "        self.path_data_cache = \"/data/tproth/.cache/huggingface/datasets/\"\n",
    "        self.path_logs = f\"./logs/\"\n",
    "        self.path_logfile = self.path_logs + f\"run_{self.datetime_run}.txt\"\n",
    "        self.path_ref_pp_baselines = \"./baselines/ref_pp_baselines/\" \n",
    "        self.path_results = \"./results/\"\n",
    "        \n",
    "        \n",
    "        # Adjust config depending on dataset. \n",
    "        if self.dataset_name   == \"simple\":           self.adjust_config_for_simple_dataset()\n",
    "        elif self.dataset_name == \"rotten_tomatoes\":  self.adjust_config_for_rotten_tomatoes_dataset()\n",
    "        elif self.dataset_name == \"financial\":        self.adjust_config_for_financial_dataset()  \n",
    "                \n",
    "        # Checks\n",
    "        self._validate_n_epochs()\n",
    "        \n",
    "    def _select_vm_model(self): \n",
    "        if   self.dataset_name in [\"rotten_tomatoes\", \"simple\"]:  self.vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "        elif self.dataset_name == \"financial\":                    self.vm_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    def adjust_config_for_simple_dataset(self): \n",
    "        \"\"\"Adjust config for the simple dataset.\"\"\"\n",
    "        self.dataset_name = \"simple\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label'\n",
    "        self.batch_size_train = 4\n",
    "        self.batch_size_eval = 4\n",
    "        self.acc_steps = 2\n",
    "        self.n_train_epochs = 6\n",
    "        self.eval_freq = 1\n",
    "        self._select_vm_model()\n",
    "        return self\n",
    "    \n",
    "    def adjust_config_for_rotten_tomatoes_dataset(self): \n",
    "        \"\"\"Adjust config for the rotten_tomatoes dataset.\"\"\"\n",
    "        self.dataset_name = \"rotten_tomatoes\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label' \n",
    "        self.batch_size_train = 32\n",
    "        self.batch_size_eval = 8\n",
    "        self.acc_steps = 2\n",
    "        self.n_train_epochs = 100\n",
    "        self.eval_freq = 1\n",
    "        self._select_vm_model()\n",
    "        return self    \n",
    "    \n",
    "    def adjust_config_for_financial_dataset(self): \n",
    "        \"\"\"Adjust config for the financial dataset.\"\"\"\n",
    "        self.dataset_name = \"financial\"\n",
    "        self.orig_cname = \"sentence\"\n",
    "        self.label_cname = 'label'\n",
    "        self.batch_size_train = 16\n",
    "        self.batch_size_eval = 32\n",
    "        self.acc_steps = 2\n",
    "        self.n_train_epochs = 4\n",
    "        self.eval_freq = 1\n",
    "        self._select_vm_model()\n",
    "        return self \n",
    "        \n",
    "    def small_ds(self):\n",
    "        \"\"\"Adjust the config to use a small dataset (for testing purposes).\n",
    "        Not possible when using the simple dataset. \"\"\"\n",
    "        if self.dataset_name == \"simple\": \n",
    "            raise Exception(\"Don't shard when using the simple dataset (no need)\")\n",
    "        self.use_small_ds = True  # for testing purposes \n",
    "        self.n_shards = 20\n",
    "        self.shard_contiguous = False\n",
    "        return self\n",
    "    \n",
    "    def _validate_n_epochs(self): \n",
    "        if self.n_train_epochs % self.eval_freq != 0: \n",
    "            raise Exception(\"Set n_train_epochs to a multiple of eval_freq so there are no leftover epochs.\")\n",
    "    \n",
    "    def using_t5(self): \n",
    "        return self.pp_name in [\"prithivida/parrot_paraphraser_on_T5\", \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted baselines.ipynb.\n",
      "Converted baselines_analysis.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted pp_eval_baselines.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n",
      "Converted test_grammar_options.ipynb.\n",
      "Converted test_pp_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "create blank csv \"run_statistics.csv\" with (run_name, params, split,  test_stats, ...)\n",
    "choose parameters including gen_params_train and gen_params_eval\n",
    "for dataset in ['rotten_tomatoes', 'financial_phrasebank']:\n",
    "    for seed in [420, 69, 1337, 80085, 8008135 ]:\n",
    "        create blank csvs \"{path_run}_eval_examples_{train/valid/test}.csv\"  for examples like (ds, seed, epoch, orig, [pp_l], rewards, ...)\n",
    "        create blank csv \"{path_run}_training_step.csv\" for training step examples\n",
    "        eval untrained pp model baseline (train,valid,test) \n",
    "        log to csv \"{path_run}_eval_examples_{train/valid/test}.csv\" with epoch 0\n",
    "        compute baseline for REINFORCE using eval train\n",
    "        compute test set statistics and save to \"run_statistics.csv\"\n",
    "        for epoch in range(1,..., inf): \n",
    "            ## TRAINING \n",
    "            for batch in training_dataloader: \n",
    "                train over examples\n",
    "            append training examples to \"{path_run}_training_step.csv\"\n",
    "            ## EVAL - train \n",
    "            for batch in eval_dataloader_train:\n",
    "                eval train \n",
    "            append train eval examples to \"{path_run}_eval_examples_{train}.csv\" with the epoch\n",
    "            compute statistics and log to wandb\n",
    "            ## EVAL - valid\n",
    "            for batch in eval_dataloader_valid: \n",
    "                eval valid \n",
    "            append valid eval examples to \"{path_run}_eval_examples_{valid}.csv\" with the epoch\n",
    "            compute statistics and log to wandb (keep in memory)\n",
    "            if stopping criteria met (based on valid set): \n",
    "                save model (?)\n",
    "                for batch in eval_dataloader_test: \n",
    "                    eval test \n",
    "                write test eval examples to \"{path_run}_eval_examples_{test}.csv\" with the epoch\n",
    "                compute statistics on test set \n",
    "                log to wandb summary and to \"run_statistics.csv\"\n",
    "                compute statistics on valid set \n",
    "                log valid statistics to wandb\n",
    "            else: \n",
    "                update REINFORCE baseline using eval train\n",
    "    compute post-run stats and upload to wandb\n",
    "statistical tests (per run? overall?)\n",
    "bootstrap tests (per run? overall?)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way is to edit the variables in the config object as you please and then initialise the config object. This will first initialise a set of default values as specified in `__init__()`. Next it calls the methods `adjust_config_for_simple_dataset()` or `adjust_config_for_rotten_tomatoes_dataset()` to overwrite some of these defaults with dataset-specific variables. \n",
    "\n",
    "Once ready, call `cfg = Config()` and access values as attributes of `cfg`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually specify which dataset to use by calling the `adjust_config_...` functions yourself. This is useful for writing test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config().adjust_config_for_simple_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `vars(cfg)` to get all parameters as a dict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(vars(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a small dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do testing on a small dataset you can chain on `use_small_ds()` to adjust the config accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config().adjust_config_for_rotten_tomatoes_dataset().small_ds()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])\n",
    "print(\"Using small dataset?\", cfg.use_small_ds)\n",
    "print(\"How many shards?\", cfg.n_shards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality is disabled for the simple dataset because we only have 4 data points for each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(Config().adjust_config_for_simple_dataset().adjust_config_for_simple_dataset().small_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
