{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_fail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a class `Config` to hold hyperparameters and global variables.\n",
    "\n",
    "Design from https://github.com/cswinter/DeepCodeCraft/blob/master/hyper_params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Config: \n",
    "    def __init__(self): \n",
    "        \"\"\"Set up default parameters\"\"\"\n",
    "          \n",
    "        ### Models and datasets\n",
    "        # options for the pp_model \n",
    "        # 1. tuner007/pegasus_paraphrase\n",
    "        # 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "        # 3. eugenesiow/bart-paraphrase\n",
    "        self.pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "        self.vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "        self.sts_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        self.dataset_name = \"simple\"\n",
    "        \n",
    "        ### Training hyperparameters\n",
    "        self.seed = 420\n",
    "        self.use_fp16 = True\n",
    "        self.lr = 1e-5   \n",
    "        self.normalise_rewards = False\n",
    "        self.pin_memory = True\n",
    "        self.zero_grad_with_none = False\n",
    "        self.pad_token_embeddings = True\n",
    "        self.embedding_padding_multiple = 8\n",
    "        self.orig_padding_multiple = 8   # pad input to multiple of this\n",
    "        self.bucket_by_length = True\n",
    "        self.shuffle_train = False\n",
    "        self.remove_misclassified_examples = True\n",
    "        self.unfreeze_last_n_layers = 2  # counting from the back. set to \"all\" to do no layer freezing. \n",
    "\n",
    "\n",
    "\n",
    "        ### Paraphrase parameters  \n",
    "        self.pp = {\n",
    "            \"num_beams\": 1, \n",
    "            \"num_return_sequences\": 1, \n",
    "            \"num_beam_groups\": 1, \n",
    "            \"diversity_penalty\": 0.,   # must be a float\n",
    "            \"temperature\": 1.5,\n",
    "            \"length_penalty\" : 1,\n",
    "            \"min_length\" : 5,\n",
    "        }\n",
    "        \n",
    "        ### Used for testing\n",
    "        self.use_small_ds = False\n",
    "        self.n_shards = None\n",
    "        self.shard_contiguous = None\n",
    "        \n",
    "        ### Logging parameters\n",
    "        self.save_model_while_training = False\n",
    "        self.save_model_freq = 10\n",
    "        \n",
    "        \n",
    "        ### These parameters don't do anything yet\n",
    "        self.sampling_strategy = \"simple\"  # doesn't do anything\n",
    "        # This makes the reward function more visible\n",
    "        # copy-paste this from reward function\n",
    "        self.reward_strategy = \"[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]\" \n",
    "        \n",
    "        ### W&B parameters\n",
    "        self.wandb = dict(\n",
    "            project = \"travis_attack\",\n",
    "            entity = \"uts_nlp\",\n",
    "            mode = \"online\",  # set to \"disabled\" to turn off wandb, \"online\" to enable it\n",
    "            log_grads = False, \n",
    "            log_grads_freq = 1,  # no effect if wandb_log_grads is False\n",
    "            plot_examples = False,\n",
    "            n_examples_plot = 4,  # number of individual examples to plot curves for\n",
    "            log_token_entropy=True,\n",
    "            log_token_probabilities = True, \n",
    "            run_notes = f\"Reward: {self.reward_strategy}\\nDataset: {self.dataset_name}\"\n",
    "        )\n",
    "        \n",
    "        ### Devices and GPU settings\n",
    "        #### TODO: do you need this with accelerator? does this handle the post-processing analytics too?\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        #device = accelerator.device\n",
    "        self.devicenum = torch.cuda.current_device() if self.device.type == 'cuda' else -1\n",
    "        # When not using Accelerator\n",
    "        #n_wkrs = 4 * torch.cuda.device_count()\n",
    "        # When using Accelerator \n",
    "        self.n_wkrs = 0 \n",
    "        \n",
    "        ## Globals \n",
    "        self.splits = ['train', 'valid', 'test']\n",
    "        self.metrics = ['loss', 'pp_logp', 'reward', 'vm_score', \"sts_score\", 'label_flip']\n",
    "        self.path_data = \"./data/\"\n",
    "        self.path_checkpoints = \"../model_checkpoints/travis_attack/\"\n",
    "        self.path_run = None #keep as None; this is automatically filled out by Trainer class\n",
    "        \n",
    "        # Adjust config depending on dataset. \n",
    "        if self.dataset_name   == \"simple\":           self.adjust_config_for_simple_dataset()\n",
    "        elif self.dataset_name == \"rotten_tomatoes\":  self.adjust_config_for_rotten_tomatoes_dataset()  \n",
    "        \n",
    "    def adjust_config_for_simple_dataset(self): \n",
    "        \"\"\"Adjust config for the simple dataset.\"\"\"\n",
    "        self.dataset_name = \"simple\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label'\n",
    "        self.orig_max_length = 20\n",
    "        self.pp['max_length'] = 20 \n",
    "        self.batch_size_train = 4\n",
    "        self.batch_size_eval = 4\n",
    "        self.accumulation_steps = 1\n",
    "        self.n_train_epochs = 4\n",
    "        self.eval_freq = 1\n",
    "        return self\n",
    "    \n",
    "    def adjust_config_for_rotten_tomatoes_dataset(self): \n",
    "        \"\"\"Adjust config for the rotten_tomatoes dataset.\"\"\"\n",
    "        self.dataset_name = \"rotten_tomatoes\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label'\n",
    "        self.orig_max_length = 64\n",
    "        self.pp['max_length'] = 64 \n",
    "        self.batch_size_train = 16\n",
    "        self.batch_size_eval = 32 \n",
    "        self.accumulation_steps = 1\n",
    "        self.n_train_epochs = 2\n",
    "        self.eval_freq = 1 \n",
    "        return self    \n",
    "        \n",
    "    def small_ds(self):\n",
    "        \"\"\"Adjust the config to use a small dataset (for testing purposes).\n",
    "        Not possible when using the simple dataset. \"\"\"\n",
    "        if self.dataset_name == \"simple\": \n",
    "            raise Exception(\"Don't shard when using the simple dataset (no need)\")\n",
    "        self.use_small_ds = True  # for testing purposes \n",
    "        self.n_shards = 40\n",
    "        self.shard_contiguous = False\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted 35_charts.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way is to edit the variables in the config object as you please and then initialise the config object. This will first initialise a set of default values as specified in `__init__()`. Next it calls the methods `adjust_config_for_simple_dataset()` or `adjust_config_for_rotten_tomatoes_dataset()` to overwrite some of these defaults with dataset-specific variables. \n",
    "\n",
    "Once ready, call `cfg = Config()` and access values as attributes of `cfg`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  rotten_tomatoes\n",
      "Number of train epochs:  2\n",
      "Batch size for train?:  32\n",
      "Max paraphrase length?:  64\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually specify which dataset to use by calling the `adjust_config_...` functions yourself. This is useful for writing test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  simple\n",
      "Number of train epochs:  2\n",
      "Batch size for train?:  4\n",
      "Max paraphrase length?:  20\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().adjust_config_for_simple_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  rotten_tomatoes\n",
      "Number of train epochs:  2\n",
      "Batch size for train?:  32\n",
      "Max paraphrase length?:  64\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `vars(cfg)` to get all parameters as a dict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accumulation_steps': 1,\n",
      " 'batch_size_eval': 128,\n",
      " 'batch_size_train': 32,\n",
      " 'bucket_by_length': True,\n",
      " 'dataset_name': 'rotten_tomatoes',\n",
      " 'device': device(type='cuda'),\n",
      " 'devicenum': 0,\n",
      " 'embedding_padding_multiple': 8,\n",
      " 'eval_freq': 1,\n",
      " 'label_cname': 'label',\n",
      " 'lr': 1e-05,\n",
      " 'metrics': ['loss',\n",
      "             'pp_logp',\n",
      "             'reward',\n",
      "             'vm_score',\n",
      "             'sts_score',\n",
      "             'label_flip'],\n",
      " 'n_shards': None,\n",
      " 'n_train_epochs': 2,\n",
      " 'n_wkrs': 0,\n",
      " 'normalise_rewards': False,\n",
      " 'orig_cname': 'text',\n",
      " 'orig_max_length': 64,\n",
      " 'orig_padding_multiple': 8,\n",
      " 'pad_token_embeddings': True,\n",
      " 'path_checkpoints': '../model_checkpoints/travis_attack/',\n",
      " 'path_data': './data/',\n",
      " 'path_run': None,\n",
      " 'pin_memory': True,\n",
      " 'pp': {'diversity_penalty': 0.0,\n",
      "        'length_penalty': 1,\n",
      "        'max_length': 64,\n",
      "        'min_length': 5,\n",
      "        'num_beam_groups': 1,\n",
      "        'num_beams': 1,\n",
      "        'num_return_sequences': 1,\n",
      "        'temperature': 1.5},\n",
      " 'pp_name': 'eugenesiow/bart-paraphrase',\n",
      " 'remove_misclassified_examples': True,\n",
      " 'reward_strategy': '[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in '\n",
      "                    'zip(vm_scores, sts_scores)]',\n",
      " 'sampling_strategy': 'greedy',\n",
      " 'save_model_freq': 10,\n",
      " 'save_model_while_training': False,\n",
      " 'seed': 420,\n",
      " 'shard_contiguous': None,\n",
      " 'shuffle_train': False,\n",
      " 'splits': ['train', 'valid', 'test'],\n",
      " 'sts_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
      " 'unfreeze_last_n_layers': 2,\n",
      " 'use_fp16': True,\n",
      " 'use_small_ds': False,\n",
      " 'vm_name': 'textattack/distilbert-base-uncased-rotten-tomatoes',\n",
      " 'wandb': {'entity': 'uts_nlp',\n",
      "           'log_grads': False,\n",
      "           'log_grads_freq': 1,\n",
      "           'log_token_entropy': True,\n",
      "           'log_token_probabilities': True,\n",
      "           'log_training_step_table': True,\n",
      "           'mode': 'disabled',\n",
      "           'n_examples_plot': 4,\n",
      "           'plot_examples': False,\n",
      "           'project': 'travis_attack',\n",
      "           'run_notes': 'Reward: [-0.5 if sts < 0.5 else 0.5+v*sts for v,sts '\n",
      "                        'in zip(vm_scores, sts_scores)]\\n'\n",
      "                        'Dataset: rotten_tomatoes'},\n",
      " 'zero_grad_with_none': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(vars(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a small dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do testing on a small dataset you can chain on `use_small_ds()` to adjust the config accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  rotten_tomatoes\n",
      "Number of train epochs:  2\n",
      "Batch size for train?:  32\n",
      "Max paraphrase length?:  64\n",
      "Using small dataset? True\n",
      "How many shards? 40\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().adjust_config_for_rotten_tomatoes_dataset().small_ds()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])\n",
    "print(\"Using small dataset?\", cfg.use_small_ds)\n",
    "print(\"How many shards?\", cfg.n_shards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality is disabled for the simple dataset because we only have 4 data points for each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(Config().adjust_config_for_simple_dataset().adjust_config_for_simple_dataset().small_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted 35_charts.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
