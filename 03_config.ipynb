{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.test import test_fail\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define hyperparameters and config variables. We store them all in a class `config`.\n",
    "\n",
    "Design from https://github.com/cswinter/DeepCodeCraft/blob/master/hyper_params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Config: \n",
    "    def __init__(self): \n",
    "        \"\"\"Set up default parameters\"\"\"\n",
    "          \n",
    "        ### Models and datasets\n",
    "        # options for the pp_model \n",
    "        # 1. tuner007/pegasus_paraphrase\n",
    "        # 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "        # 3. eugenesiow/bart-paraphrase\n",
    "        self.pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "        self.vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "        self.sts_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        self.dataset_name = None\n",
    "        \n",
    "        ### Training hyperparameters\n",
    "        self.seed = 420\n",
    "        self.use_fp16 = True\n",
    "        self.lr = 1e-5   \n",
    "        self.normalise_rewards = False\n",
    "        self.metrics = ['loss', 'pp_logp', 'reward', 'vm_score', \"sts_score\", 'label_flip']\n",
    "        self.pin_memory = True\n",
    "        self.zero_grad_with_none = False\n",
    "        self.pad_token_embeddings = True\n",
    "        self.embedding_padding_multiple = 8\n",
    "        self.padding_multiple = 8 \n",
    "        self.bucket_by_length = True\n",
    "        self.shuffle_train = False\n",
    "        self.remove_misclassified_examples = True\n",
    "\n",
    "\n",
    "        ### Paraphrase parameters  \n",
    "        self.pp = {\n",
    "            \"num_beams\": 1, \n",
    "            \"num_return_sequences\": 1, \n",
    "            \"num_beam_groups\": 1, \n",
    "            \"diversity_penalty\": 0.,   # must be a float\n",
    "            \"temperature\": 1.5,\n",
    "            \"length_penalty\" : 1,\n",
    "            \"min_length\" : 5,\n",
    "        }\n",
    "        \n",
    "        ### Used for testing\n",
    "        self.use_small_ds = False\n",
    "        self.n_shards = None\n",
    "        self.shard_contiguous = None\n",
    "        \n",
    "        ### Logging parameters\n",
    "        self.save_model_while_training = False\n",
    "        self.save_model_freq = 10\n",
    "        \n",
    "        ### W&B parameters\n",
    "        self.wandb = dict(\n",
    "            mode = \"online\",  # set to \"disabled\" to turn off wandb, \"online\" to enable it\n",
    "            log_grads = False, \n",
    "            log_grads_freq = 1,  # no effect if wandb_log_grads is False\n",
    "            plot_examples = False,\n",
    "            n_examples_plot = 4,  # number of individual examples to plot curves for\n",
    "            # log a table to wandb with the examples and rewards the model sees while training. Useful for debugging \n",
    "            # and seeing what is going on, but slows down training time. \n",
    "            log_training_step_table = True,  \n",
    "            log_token_entropy=True,\n",
    "            log_token_probabilities = True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ### Devices and GPU settings\n",
    "        #### TODO: do you need this with accelerator? does this handle the post-processing analytics too?\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        #device = accelerator.device\n",
    "        self.devicenum = torch.cuda.current_device() if self.device.type == 'cuda' else -1\n",
    "        # When not using Accelerator\n",
    "        #n_wkrs = 4 * torch.cuda.device_count()\n",
    "        # When using Accelerator \n",
    "        self.n_wkrs = 0 \n",
    "        \n",
    "        \n",
    "        ### These parameters don't do anything yet\n",
    "        self.sampling_strategy = \"greedy\"  # doesn't do anything\n",
    "        # This makes the reward function more visible\n",
    "        # copy-paste this from reward function\n",
    "        self.reward_strategy = \"[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]\" \n",
    "        self.n_layers_frozen = \"2\"  # counting from the back (doesn't do anything yet)\n",
    "     \n",
    "    \n",
    "    def rotten_tomatoes_dataset(self): \n",
    "        \"\"\"Adjust config for the rotten_tomatoes dataset.\"\"\"\n",
    "        self.dataset_name = \"rotten_tomatoes\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label'\n",
    "        \n",
    "        self.orig_max_length = 64\n",
    "        self.pp['max_length'] = 64 \n",
    "        self.batch_size_train = 32\n",
    "        self.batch_size_eval = 128 \n",
    "        self.accumulation_steps = 1\n",
    "        self.n_train_epochs = 250\n",
    "        self.eval_freq = 1 \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def simple_dataset(self): \n",
    "        \"\"\"Adjust config for the simple dataset.\"\"\"\n",
    "        self.dataset_name = \"simple_dataset\"\n",
    "        self.orig_cname = \"text\"\n",
    "        self.label_cname = 'label'\n",
    "\n",
    "        self.orig_max_length = 20\n",
    "        self.pp['max_length'] = 20 \n",
    "        self.batch_size_train = 2\n",
    "        self.batch_size_eval = 4\n",
    "        self.accumulation_steps = 1\n",
    "        self.eval_freq = 10 \n",
    "        self.n_train_epochs = 60\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def small_ds(self):\n",
    "        \"\"\"Adjust the config to use a small dataset (for testing purposes).\n",
    "        Not possible when using the simple dataset. \"\"\"\n",
    "        if self.dataset_name == \"simple_dataset\": \n",
    "            raise Exception(\"Don't shard when using the simple dataset (no need)\")\n",
    "        self.use_small_ds = True  # for testing purposes \n",
    "        self.n_shards = 60 \n",
    "        self.shard_contiguous = False\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage is pretty straightforward. First initialise the config object and then initialise the dataset by chaining. All hyperparameters and variables are stored as attributes in the class. \n",
    "\n",
    "Currently you can use the `simple` dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  simple_dataset\n",
      "Number of train epochs:  60\n",
      "Batch size for train?:  2\n",
      "Max paraphrase length?:  20\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().simple_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the `rotten_tomatoes` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  rotten_tomatoes\n",
      "Number of train epochs:  250\n",
      "Batch size for train?:  32\n",
      "Max paraphrase length?:  64\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().rotten_tomatoes_dataset()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `vars(cfg)` to get all parameters as a dict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accumulation_steps': 1,\n",
      " 'batch_size_eval': 128,\n",
      " 'batch_size_train': 32,\n",
      " 'bucket_by_length': True,\n",
      " 'dataset_name': 'rotten_tomatoes',\n",
      " 'device': device(type='cuda'),\n",
      " 'devicenum': 0,\n",
      " 'embedding_padding_multiple': 8,\n",
      " 'eval_freq': 1,\n",
      " 'label_cname': 'label',\n",
      " 'lr': 1e-05,\n",
      " 'metrics': ['loss',\n",
      "             'pp_logp',\n",
      "             'reward',\n",
      "             'vm_score',\n",
      "             'sts_score',\n",
      "             'label_flip'],\n",
      " 'n_layers_frozen': '2',\n",
      " 'n_shards': None,\n",
      " 'n_train_epochs': 250,\n",
      " 'n_wkrs': 0,\n",
      " 'normalise_rewards': False,\n",
      " 'orig_cname': 'text',\n",
      " 'orig_max_length': 64,\n",
      " 'pad_token_embeddings': True,\n",
      " 'padding_multiple': 8,\n",
      " 'pin_memory': True,\n",
      " 'pp': {'diversity_penalty': 0.0,\n",
      "        'length_penalty': 1,\n",
      "        'max_length': 64,\n",
      "        'min_length': 5,\n",
      "        'num_beam_groups': 1,\n",
      "        'num_beams': 1,\n",
      "        'num_return_sequences': 1,\n",
      "        'temperature': 1.5},\n",
      " 'pp_name': 'eugenesiow/bart-paraphrase',\n",
      " 'remove_misclassified_examples': True,\n",
      " 'reward_strategy': '[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in '\n",
      "                    'zip(vm_scores, sts_scores)]',\n",
      " 'sampling_strategy': 'greedy',\n",
      " 'save_model_freq': 10,\n",
      " 'save_model_while_training': False,\n",
      " 'seed': 420,\n",
      " 'shard_contiguous': None,\n",
      " 'shuffle_train': False,\n",
      " 'sts_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
      " 'use_fp16': True,\n",
      " 'use_small_ds': False,\n",
      " 'vm_name': 'textattack/distilbert-base-uncased-rotten-tomatoes',\n",
      " 'wandb': {'log_grads': False,\n",
      "           'log_grads_freq': 1,\n",
      "           'log_token_entropy': True,\n",
      "           'log_token_probabilities': True,\n",
      "           'log_training_step_table': True,\n",
      "           'mode': 'online',\n",
      "           'n_examples_plot': 4,\n",
      "           'plot_examples': False},\n",
      " 'zero_grad_with_none': False}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a small dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do testing on a small dataset you can chain on `use_small_ds()` to adjust the config accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name:  rotten_tomatoes\n",
      "Number of train epochs:  250\n",
      "Batch size for train?:  32\n",
      "Max paraphrase length?:  64\n",
      "Using small dataset? True\n",
      "How many shards? 60\n"
     ]
    }
   ],
   "source": [
    "cfg = Config().rotten_tomatoes_dataset().small_ds()\n",
    "print(\"Dataset name: \", cfg.dataset_name)\n",
    "print(\"Number of train epochs: \", cfg.n_train_epochs)\n",
    "print(\"Batch size for train?: \", cfg.batch_size_train)\n",
    "print(\"Max paraphrase length?: \", cfg.pp['max_length'])\n",
    "print(\"Using small dataset?\", cfg.use_small_ds)\n",
    "print(\"How many shards?\", cfg.n_shards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality is disabled for the simple dataset because we only have 4 data points for each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(Config().simple_dataset().small_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
