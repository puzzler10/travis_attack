{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we are trying to adjust parameters of a paraphrase model to generate adversarial examples. \n",
    "### Policy gradients \n",
    "The key parameter update equation is $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)$, where $\\alpha$ is a step size parameter, the parameter vector $\\theta$ is for a model (here a paraphrase model), and $J$ is a loss function. The time step $t$ depends on the problem specification and we will get to it later. \n",
    "\n",
    "Now in my review I have defined the loss function $J(\\theta) = E_\\pi[r(\\tau)]$. Here: \n",
    "* $\\pi$ is the policy, a probability distribution for the next action in a given state; essentially $p(a_t|s_t)$\n",
    "* $\\tau$ is a trajectory, a specific sequence $s_0, a_0, r_1, s_1, a_1, \\ldots$ of the agent in the game. This starts at time $t=0$ and finishes at time $t=T$. \n",
    "* $r(\\tau)$ is the sum of rewards for a trajectory $\\tau$, or in other words, the total reward for the trajectory. \n",
    "\n",
    "For this loss function higher values are better (which might make it a reward function) and so we might have to invert it at some point. \n",
    "\n",
    "To update parameters we must find the gradient $\\nabla_\\theta J(\\theta)$, which measures how $J(\\theta)$ changes when we adjust the parameters of the paraphrase model. The gradient is simplified through some maths to get the policy gradient theorem $$ \\nabla_\\theta J(\\theta) =  \\nabla_\\theta E_\\pi [r(\\tau)]  = E_\\pi \\left[r(\\tau) \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a_t|s_t)  \\right] $$ \n",
    "\n",
    "To calculate this you need to calculate the expectation term, which in turn means evaluating every possible trajectory $\\tau$ and its expected return. Generally this is not possible and instead we turn to estimators.  \n",
    "\n",
    "One of these is REINFORCE. It gives us  $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ where \n",
    "* $G_t$ is the discounted return and is given by $G_t = r_t + \\beta r_{t-1} + \\beta^2 r_{t-2} + \\dots$. It's a rough estimate of $r(\\tau)$. Rewards obtained later in the episode are weighted much higher than rewards obtained earlier. I guess it assumes that the parameters update every timestep. \n",
    "* $S$ is some number of samples.\n",
    "\n",
    "The implementation of REINFORCE and similar estimators depends on how we formulate the problem. Below we present some possible formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation One: Document-level  \n",
    "This is the first implementation we will try. \n",
    "\n",
    "Here we generate a list of paraphrases at each time point. The idea is that there is one paraphrase amongst them that is a good adversarial example. We try to tune the model to produce the best one. \n",
    "\n",
    "This interpretation sees forming the complete paraphrase as one time step. So it isn't token-level but document-level. \n",
    "\n",
    "* Starting state: $s0 = x$, the original example  \n",
    "* Actions: each action is \"choosing\" a paraphrase (or of choosing $n$ paraphrases). The set of all possible paraphrases and their probabilities is the policy. So $\\pi(a|s) = p(x'| x;\\theta)$ where $x'$ is the paraphrase (or list of paraphrases). \n",
    "    * To approximate this probability, what we can do is generate a large list of paraphrases, and for each, the probabilities of generating each token in turn for that paraphrase. This gives a rough \"probability\" of how likely that sequence was. This number is kind of like a weight for how good that paraphrase is, according to the model.  We can then turn the weights into probabilities to get a \"probability\" of the paraphrase. This is dependent on the number of paraphrases generated, so generating a large list is likely to be better for this task. \n",
    "* Reward: The paraphrase moves through the reward function $R(x, x')$) to get the reward $r$. \n",
    "* Time steps: We only have one time step in the game ($T=1$ and $G_t=r$)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are a few variations to this scenario that we can do. For each of these we will formulate the policy and the reward function $R$. Below, $x'$ means paraphrase, $f(x)_y$ means the model confidence of x for the class of the true label $y$, $SS(a,b)$ is the result of a semantic similarity model run over $a$ and $b$, and $\\lambda$ is a hyperparameter.  \n",
    "\n",
    "\n",
    "#### One-paraphrase \n",
    "Here we only generate one paraphrase. This scenario also has a few options. First we generate a list of paraphrases with the probabilities of selecting one. Then we either sample probabilistically from the list or pick the most probable option. \n",
    "\n",
    "In this case the policy $p(x'|x,\\theta)$ is the chance of obtaining a specific paraphrase. For the sampling option this is equal to its sample probability. For the top option this is just the probability of selecting that option. \n",
    "\n",
    "The reward function might look like $R(x,x') = f(x)_y - f(x')_y + \\lambda SS(x, x')$. We could also make the $SS$ factor a step-function above some threshold. \n",
    "\n",
    "The REINFORCE equation $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ becomes $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$ We repeat the process $S$ times where $S$ is ideally as large as possible. We can start with something simple (e.g. $S=10$ or $S=100$) and go from there.  \n",
    "\n",
    "The gradient term $\\nabla \\log p(x'_s|x,\\theta)$ can hopefully be found with autodiff. \n",
    "\n",
    "#### Set of paraphrases\n",
    "In this scenario the paraphrase model is evaluated on performance over a set of paraphrases, which we call $X'$ here. The policy becomes $p(X'|x, \\theta)$, the probability of obtaining that list. We can get this probability by multipling together the \"probability\" of each individual paraphrase, multiplying also by nCr (for r paraphrases out of n total) to account for the lack of order in the list. \n",
    "\n",
    "We can make a number of sub-scenarios here. \n",
    "\n",
    "For the **top-paraphrase in set** condition the paraphrase generator is only measured on the best reward for a paraphrase in its set. The idea is the generator will learn to produce a diverse set of examples, any of which could plausibly be a good adversarial example. Here we only look at best performing paraphrase $x'_m$, which we can find by $x'_m = \\max_i [f(x)_y - f(x'_i)_y]$, then return $R(x,x'_m) = [f(x)_y - f(x'_m)_y] + \\lambda SS(x,x'_m)$ \n",
    "\n",
    "For the **average-paraphrase in set** condition the paraphrase generator is measured on the average reward of the paraphrases in its set. This encourages the generator to consider performance of all examples more-or-less equally. The reward function could be something like $\\frac{1}{k} \\sum_{i=1}^k \\left[ f(x)_y - f(x'_i)_y + \\lambda SS(x, x'_i) \\right]$ \n",
    "\n",
    "A combination of these scenarios is the **top-k/top-p\\% paraphrases in set**. Here we only use the top-$k$ paraphrases, or more generally, the top $p$ percentage of paraphrases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation 2: Token-level\n",
    "This interpretation is at token-level; it sees choosing the next word as the next time step. \n",
    "\n",
    "* Starting state: $s0 = x$, the initial state. But you also have a \"blank slate\" for the paraphrase. So maybe it's a tuple (x, pp) where pp is a paraphrase with no words. Here x is used as the reference for the paraphrase generator.  \n",
    "* Actions: Choose the next word of p. I guess this starts with the \\<START\\> token (or something similar). Then you have the policy $\\pi(a|s)$ which is the same as $p(w_{next}|pp, x; \\theta)$ where $\\theta$ is the paraphrase model parameters, $pp$ is the so-far constructed sentence, and $w_{next}$ is the next token (I say token because I don't know if this model is on the subword or word basis). \n",
    "* Time steps: every token is generated one-by-one and each of these is allocated a time step. This means probably that you also update the parameters after each token generated too. \n",
    "* Reward. The reward is allocated every token. There are many reward functions (see papers on token-level loss functions). Some also incorporate document-level rewards too. \n",
    "* Next state. $s_1$ is again the tuple $(x, pp)$ but now $pp$ has the first word in it. \n",
    "\n",
    "On *teacher forcing*. This is when you have a ground-truth paraphrase and you can use it when generating tokens. This is useful because if the model makes a mistake it doesn't continue down that track but is adjusted back. This stops big divergences (but also might limit the diversity of generated paraphrases). This is used when training a paraphrase model. You have a set of reference paraphrases that are human provided. Here though we only have the original sentence and no references. We could generate adversarial examples and use that to do teacher forcing. Generating them using textattack recipes might work. This is only really used on the token-level rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Updating the paraphrase model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is a choice here. We can either directly update the parameters of the paraphrase model. Or we can fix the parameters and add a new dense layer to the end of the model. We could then train this dense layer to convert paraphrases to adversarial paraphrases. \n",
    "\n",
    "Before trying this out, I am worried that we will destroy the capabilities of the paraphrase generator a bit. We might get semantically invalid or ungrammatical or gibberish text. If so we could try and mitigate it a bit by shaping our reward function to maintain grammatical components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Experiment order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plan is to try the following order: \n",
    "\n",
    "1. One-paraphrase (most probable option). I'll start with this one because it is probably the most simple case. Within this category: \n",
    "    1a. tune existing parameters only (see if the text is recognisable) \n",
    "    1b. add dense layer onto end and try again \n",
    "2. One-paraphrase (sampled). This seems like a logical extension on the first one. \n",
    "3. Paraphrase-set options. (Decide after finishing 1, 2) \n",
    "4. Token-level tuning. (Decide after 1,2,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Layer Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I am uncertain on if to do this or not. \n",
    "\n",
    "* This [paper](https://arxiv.org/abs/1911.03090) indicates that you can get pretty good results by freezing all layers except the last few \n",
    "* Conversely I saw in the transformers documentation that transformers train better if you don't do layer freezing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, load models + datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (5.3.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in /home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings, time\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from collections import defaultdict\n",
    "from types import MethodType\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### Parameters, notes and training settings\n",
    "\n",
    "### These parameters mostly don't do anything but are more notes (for the wanb.init function)\n",
    "debug_run = \"true\"   # doesn't do anything\n",
    "sampling_strategy = \"greedy\"  # doesn't do anything\n",
    "# copy-paste this from reward function\n",
    "reward_strategy = \"[-0.5 if r < 0.15 else 0.5+v*r for v,r in zip(vm_scores, rouge_scores)]\" # doesn't do anything\n",
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "pp_name = \"tdopierre/ProtAugment-ParaphraseGenerator\"\n",
    "#pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "dataset_name = \"rotten_tomatoes\"\n",
    "n_layers_frozen = \"2\"  # counting from the back (doesn't do anything yet)\n",
    "\n",
    "\n",
    "# These parameters only have effect if small_ds = True\n",
    "\n",
    "### Paraphrase parameters  \n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "if dataset_name == \"rotten_tomatoes\":   \n",
    "    pp_model_params['max_length'] = 60\n",
    "    batch_size = 16\n",
    "    n_train_epochs = 3\n",
    "    use_small_ds = True  # for testing purposes\n",
    "    n_shards         = 25    if use_small_ds else None \n",
    "    shard_contiguous = False if use_small_ds else None \n",
    "elif dataset_name == \"simple\":          \n",
    "    pp_model_params['max_length'] = 20\n",
    "    batch_size = 2\n",
    "    n_train_epochs = 5\n",
    "    use_small_ds = False  # for testing purposes\n",
    "    if use_small_ds == True: \n",
    "        raise Exception(\"Don't shard when using the simple dataset (no need)\")\n",
    "\n",
    "\n",
    "### Training parameters\n",
    "seed = 420\n",
    "lr = 1e-4 # Initial learning rate (after the potential warmup period) to use\n",
    "eval_freq = 1  # run eval loop every `eval_freq` epochs\n",
    "accumulation_steps = 4\n",
    "normalise_rewards = False\n",
    "metrics = ['loss', 'pp_logp', 'reward', 'rouge_score', 'vm_score']\n",
    "pin_memory = True\n",
    "zero_grad_with_none=False\n",
    "#weight_decay = 0\n",
    "#lr_scheduler_type = 'none'\n",
    "#n_warmup_steps = 30 \n",
    "\n",
    "### W&B parameters\n",
    "wandb_mode = \"online\"  # set to \"disabled\" to turn off wandb \n",
    "wandb_log_grads = False   \n",
    "wandb_log_grads_freq = 1  # no effect if wandb_log_grads is False\n",
    "wandb_n_examples_plot = 30  # number of individual examples to plot curves for\n",
    "# log a table to wandb with the examples and rewards the model sees while training. Useful for debugging \n",
    "# and seeing what is going on, but slows down training time. \n",
    "wandb_log_training_step_table = False  \n",
    "\n",
    "# Parameter dict\n",
    "config_d = dict(\n",
    "    debug_run = debug_run,\n",
    "    sampling_strategy = sampling_strategy,\n",
    "    reward_strategy = reward_strategy,\n",
    "    pp_name = pp_name,\n",
    "    vm_name = vm_name,\n",
    "    dataset_name = dataset_name, \n",
    "    use_small_ds = use_small_ds,\n",
    "    shard_params =dict(\n",
    "        n_shards = n_shards,\n",
    "        shard_contiguous = shard_contiguous,\n",
    "    ),\n",
    "    n_layers_frozen = n_layers_frozen,\n",
    "    pp_model_params = pp_model_params, \n",
    "    seed = seed,\n",
    "    batch_size = batch_size,\n",
    "    lr = lr, \n",
    "    accumulation_steps=accumulation_steps,\n",
    "    n_train_epochs = n_train_epochs,\n",
    "    eval_freq = eval_freq,\n",
    "    normalise_rewards = normalise_rewards,\n",
    "    metrics = metrics,\n",
    "    zero_grad_with_none=zero_grad_with_none,\n",
    "    wandb_params=dict(\n",
    "        log_grads = wandb_log_grads,\n",
    "        log_grads_freq = wandb_log_grads_freq, \n",
    "        n_examples_plot = wandb_n_examples_plot, \n",
    "        log_training_step_table = wandb_log_training_step_table\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### Misc setup\n",
    "# Paths\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "path_data = \"./data/\"\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Devices and GPU settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "\n",
    "# Configs\n",
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "run_notes = f\"Debug run:{debug_run}\\nReward: {reward_strategy}\\nDataset: {dataset_name}\\\n",
    "\\nSampling strategy: {sampling_strategy}\"\n",
    "\n",
    "# Logging \n",
    "logging.basicConfig(format='%(message)s', stream=sys.stdout) # stdout while we are doing stdout to file piping\n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Other \n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "### Dev code\n",
    "# used to know what to set max_pp_length to. \n",
    "track_pp_sizes = False\n",
    "if track_pp_sizes: \n",
    "    n_train_epochs = 1\n",
    "    use_small_ds = True \n",
    "    n_shards = 2  # half the train dataset gives us a good understanding of this\n",
    "    orig_max_l = []\n",
    "    pp_max_l = []\n",
    "    # After running the code  \n",
    "    # sns.distplot(orig_max_l)\n",
    "    # sns.distplot(pp_max_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Paraphrase (pp) model \n",
    "pp_tokenizer = AutoTokenizer.from_pretrained(pp_name)\n",
    "# takes about 3GB memory space up on the GPU\n",
    "# change the `local_files_only` argument if changing the model name \n",
    "pp_model = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True).to(device)\n",
    "# The no_grad version of generate\n",
    "generate_with_grad = undecorated(pp_model.generate)\n",
    "pp_model.generate_with_grad = MethodType(generate_with_grad, pp_model)\n",
    "\n",
    "## Victim Model (VM)\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name, local_files_only=True).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.debug_utils import DebugUnderflowOverflow\n",
    "# debug_overflow = DebugUnderflowOverflow(pp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw datasets and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def add_idx(x, idx): x['idx'] = idx; return x   # add row numbers\n",
    "\n",
    "def get_standard_dataloaders(): \n",
    "    \"\"\"function for standard train/valid/test dataloader setup\"\"\"\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True,  num_workers=n_wkrs, pin_memory=pin_memory)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=True,  num_workers=n_wkrs, pin_memory=pin_memory)\n",
    "    dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, num_workers=n_wkrs, pin_memory=pin_memory)\n",
    "    return dl_train,dl_valid,dl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435635724ab74716a91d05d2cc2a4d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-719ddcf99ffe3e94.arrow\n",
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-e1940dd71a462589.arrow\n",
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-0f3c403a3c70b6d3.arrow\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == \"rotten_tomatoes\": \n",
    "    ds_dict = load_dataset(\"rotten_tomatoes\")\n",
    "    label_cname = 'label'\n",
    "    for _,ds in ds_dict.items():\n",
    "        # make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "        assert ds.features[label_cname].num_classes == vm_num_labels\n",
    "    ds_dict = ds_dict.map(add_idx, with_indices=True)\n",
    "    ds_train,ds_valid,ds_test = ds_dict['train'],ds_dict['validation'],ds_dict['test']\n",
    "    dl_train,dl_valid,dl_test = get_standard_dataloaders()\n",
    "elif dataset_name == \"simple\": \n",
    "    def load_and_prep_ds_dl(path): \n",
    "        ds = load_dataset('csv',data_files=path)['train']\n",
    "        ds = ds.map(add_idx, with_indices=True)\n",
    "        # no shuffle because just for debugging\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=n_wkrs,  pin_memory=pin_memory) \n",
    "        return ds, dl \n",
    "    ds_train,dl_train = load_and_prep_ds_dl(f\"{path_data}simple_dataset_train.csv\")\n",
    "    ds_valid,dl_valid = load_and_prep_ds_dl(f\"{path_data}simple_dataset_valid.csv\")\n",
    "    ds_test, dl_test  = load_and_prep_ds_dl(f\"{path_data}simple_dataset_test.csv\")\n",
    "elif dataset_name==\"snli\": \n",
    "    next\n",
    "    ## For snli\n",
    "    # remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "    # ds_train = ds_train.filter(remove_minus1_labels)\n",
    "    # valid = valid.filter(remove_minus1_labels)\n",
    "    # test = test.filter(remove_minus1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## If we want to use a smaller dataset for testing we set that up here. \n",
    "if use_small_ds: \n",
    "    ds_train = ds_train.shard(n_shards, 0 , contiguous=shard_contiguous)  \n",
    "    ds_valid = ds_valid.shard(n_shards, 0 , contiguous=shard_contiguous)   \n",
    "    ds_test  = ds_test.shard( n_shards, 0 , contiguous=shard_contiguous)  \n",
    "    dl_train,dl_valid,dl_test = get_standard_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easy referencing later\n",
    "ds_d = dict()\n",
    "ds_d['train'],ds_d['valid'],ds_d['test'] = ds_train,ds_valid,ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop pseudocode\n",
    "\n",
    "The REINFORCE estimator is $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$\n",
    "\n",
    "**Non-batched version (one example), stochastic gradient descent**  \n",
    "Inputs: train, n_pp=1, vm, ppm, $\\alpha = 5e^{-5}$ (saw this rate for $\\alpha$ somewhere  \n",
    "Set eval_mode=true for vm, eval_mode = false for ppm  \n",
    "Freeze all layers of ppm except last 6  \n",
    "Shuffle traning dataset  \n",
    "\n",
    "Loop: take one row $x$ from train\n",
    "* tokenize\n",
    "* do greedy search to get paraphrase pp\n",
    "* get reward using `reward_fn(x, pp)`. $r=R(x,x'_s) = f(x)_y - f(x'_s)_y + \\lambda SS(x, x'_s)$ \n",
    "* update model parameters \n",
    "\n",
    "\n",
    "* generate large UNIVERSE list of paraphrases `pp_l` (e.g. 128) from 'text' column using ppm\n",
    "* extract sequence scores from this list to get a vector of probabilities `pp_probs`\n",
    "* take `log` of `pp_probs` and store in `pp_logprobs`\n",
    "* pick S paraphrases from `pp_l` to get `pp_s`. \n",
    "* Take the corresponding entries from `pp_logprobs`. Get gradient of each entry by looking at .grad attribute. Sum them up and store in a variable `gradsum` \n",
    "* for each `pp` (i.e. $x'_s$) in `pp_s`:\n",
    "    * \n",
    "* Sum up these rewards to get `rewardsum` and add to `gradsum` to get `nablaJ`\n",
    "* Update parameters of paraphrase model with $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\log p(x'_s|x,\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(text):\n",
    "    \"\"\"Wrapper for generating paraphrases (pp's). Most keywords are passed on to pp_model.generate function, \n",
    "    so see docs for that function. \"\"\"\n",
    "    batch = pp_tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    # Only greedy search supported at the moment\n",
    "    generated = pp_model.generate_with_grad(**batch, \n",
    "                                         **pp_model_params,\n",
    "                                         do_sample=False, \n",
    "                                         return_dict_in_generate=True,\n",
    "                                         output_scores=True,\n",
    "                                            remove_invalid_values=False, \n",
    "                                         pad_token_id = pp_tokenizer.pad_token_id,\n",
    "                                         eos_token_id = pp_tokenizer.eos_token_id)\n",
    "    tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)\n",
    "    if track_pp_sizes:  # DEV CODE (can delete later)\n",
    "        orig_max_l.append(batch['input_ids'].shape[1])\n",
    "        pp_max_l.append(generated.sequences.shape[1])\n",
    "    return generated, tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(translated.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "#     if np.any(np.isnan(seq_log_prob.detach().cpu()).tolist()): \n",
    "#         warnings.warn(f\"Warning: NAN's detected in pp_logp calclulations.\\n seq_token_log_probs: {seq_token_log_probs}\")\n",
    "    return seq_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_vm_probs(text): \n",
    "    \"\"\"Used in the reward_fn to get vm_score\"\"\"\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tkns = vm_tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "        logits = vm_model(**tkns).logits\n",
    "        probs = torch.softmax(logits,1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def reward_fn(orig_l, pp_l, truelabel, return_components=False): \n",
    "    \"\"\"orig_l, pp_l are lists of original and paraphrase respectively\"\"\"\n",
    "    # Victim model probability differences between orig and pp\n",
    "    orig_probs,pp_probs = get_vm_probs(orig_l),get_vm_probs(pp_l)\n",
    "    orig_truelabel_probs = torch.gather(orig_probs,1,truelabel[:,None]).squeeze()\n",
    "    pp_truelabel_probs   = torch.gather(pp_probs,1,truelabel[:,None]).squeeze()\n",
    "    vm_scores = (orig_truelabel_probs - pp_truelabel_probs)#.detach().cpu().tolist()\n",
    "    \n",
    "    # ROUGE scores\n",
    "    def get_rouge_score(ref, pred):\n",
    "        return rouge_metric.compute(rouge_types=[\"rougeL\"],\n",
    "            predictions=[pred], references=[ref])['rougeL'].mid.fmeasure \n",
    "    rouge_scores = torch.tensor([get_rouge_score(ref=orig,pred=pp) for orig,pp in zip(orig_l, pp_l)], device=device)\n",
    "\n",
    "    # Reward calculation \n",
    "    rewards = torch.tensor([-0.5 if r < 0.15 else 0.5+v*r for v,r in zip(vm_scores, rouge_scores)],device=device)\n",
    "    \n",
    "    if return_components: \n",
    "        return {\n",
    "            \"orig_l\": orig_l,\n",
    "            \"pp_l\": pp_l,  \n",
    "            \"truelabel\": truelabel,\n",
    "            \"orig_truelabel_probs\":orig_truelabel_probs,\n",
    "            \"pp_truelabel_probs\":  pp_truelabel_probs,\n",
    "            \"vm_score\": vm_scores, \n",
    "            \"rouge_score\": rouge_scores,\n",
    "            \"reward\": rewards\n",
    "        }\n",
    "    else:  return {\"reward\": rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pp_model_forward(text): \n",
    "    generated, pp_text = get_paraphrases(text)\n",
    "    return generated, pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(text, label, generated,  pp_text, return_components=False): \n",
    "    d = reward_fn(orig_l=text, pp_l=pp_text, truelabel=label, return_components=return_components)\n",
    "    if normalise_rewards: \n",
    "        d['orig_reward'] = copy.deepcopy(d['reward'])\n",
    "        d['reward'] = (d['reward']-torch.mean(d['reward']))/torch.std(d['reward'])\n",
    "    d['pp_logp'] = get_pp_logp(generated)\n",
    "    d['loss'] = -d['reward'] * d['pp_logp']\n",
    "    d['loss_batch'] = torch.mean(d['loss'])\n",
    "    if return_components ==  False: return d['loss_batch'] \n",
    "    # remove some items from compgraph\n",
    "    d['pp_logp'] = d['pp_logp'].detach()  \n",
    "    d['loss']    = d['loss'].detach()\n",
    "    gc.collect() \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training_step_old(data): \n",
    "    optimizer.zero_grad(set_to_none=zero_grad_with_none)\n",
    "    label,text = data['label'].to(device),data[\"text\"]\n",
    "    generated, pp_text = pp_model_forward(text)\n",
    "    logger.info(show_gpu(f'Batch {i}, GPU memory usage after forward pass: '))\n",
    "    if wandb_log_training_step_table: \n",
    "        results_d = loss_fn(text, label, generated, pp_text,  return_components=True)\n",
    "        loss_batch = results_d['loss_batch']\n",
    "    else: \n",
    "        loss_batch = loss_fn(text, label, generated, pp_text, return_components=False)\n",
    "    loss_batch.backward()\n",
    "    logger.info(show_gpu(f'Batch {i}, GPU memory usage after backwards pass: '))\n",
    "    optimizer.step()\n",
    "    if wandb_log_training_step_table: \n",
    "        results_d = process_results_d1(results_d)\n",
    "        update_wandb_table(results_d, split='training_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(data): \n",
    "    \"\"\"With gradient accumulation\"\"\"\n",
    "    label,text = data['label'].to(device),data[\"text\"]\n",
    "    generated, pp_text = pp_model_forward(text)\n",
    "    logger.info(show_gpu(f'Batch {i}, GPU memory usage after forward pass: '))\n",
    "    if wandb_log_training_step_table: \n",
    "        results_d = loss_fn(text, label, generated, pp_text,  return_components=True)\n",
    "        loss_batch = results_d['loss_batch']\n",
    "    else: \n",
    "        loss_batch = loss_fn(text, label, generated, pp_text, return_components=False)\n",
    "    loss_batch = loss_batch / accumulation_steps  # Normalize our loss for gradient accumulation\n",
    "    loss_batch.backward()\n",
    "    logger.info(show_gpu(f'Batch {i}, GPU memory usage after backwards pass: '))\n",
    "    if (i+1) % accumulation_steps == 0: \n",
    "        optimizer.step()\n",
    "        pp_model.zero_grad(set_to_none=zero_grad_with_none)\n",
    "    if wandb_log_training_step_table: \n",
    "        results_d = process_results_d1(results_d)\n",
    "        update_wandb_table(results_d, split='training_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                   # Reset gradients tensors\n",
    "# for i, (inputs, labels) in enumerate(training_set):\n",
    "#     predictions = model(inputs)                     # Forward pass\n",
    "#     loss = loss_function(predictions, labels)       # Compute loss function\n",
    "#     loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n",
    "#     loss.backward()                                 # Backward pass\n",
    "#     if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "#         optimizer.step()                            # Now we can do an optimizer step\n",
    "#         model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_results_d1(results_d): \n",
    "    \"\"\"REFACTOR THIS LATER\"\"\"\n",
    "    # wandb logging \n",
    "    results_d['epoch'] = epoch\n",
    "    results_d['idx'] = data['idx']\n",
    "    for k,v in results_d.items(): \n",
    "        if torch.is_tensor(v): \n",
    "            results_d[k] = v.detach().cpu().tolist()\n",
    "        elif type(v) == int or type(v) == float: \n",
    "            # make into list repeated n times\n",
    "            results_d[k] = [v for i in range(batch_size)]\n",
    "    return results_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval and wandb functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table2df(table):  return pd.DataFrame(data=table.data, columns=table.columns)  # wandb table to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_results_d_for_wandb(results_d): \n",
    "    # Flatten batches for each key, depending on datatype (e.g. lists of lists )\n",
    "    for k,v in results_d.items(): \n",
    "        # v[0] is arbitrary - we are just checking the first item in the list to see the type\n",
    "        if type(v) == float or type(v) == int: \n",
    "            next\n",
    "        elif  torch.is_tensor(v[0]): \n",
    "            # case where we have a list of scalars - the cat function doesn't work here \n",
    "            if  v[0].size() == torch.Size([]): x = torch.stack(v)\n",
    "            else:                              x = torch.cat(v)\n",
    "            results_d[k] = x.detach().cpu().squeeze().tolist()  # convert to list (squeeze is for single scalar list)\n",
    "        elif type(v[0]) == list:  # this is True for tensors also, so it has to go after the is_tensor check\n",
    "            results_d[k] = list(itertools.chain(*v)) \n",
    "        elif type(v) == list: \n",
    "            next\n",
    "        else: \n",
    "            raise Exception(\"shouldn't get here\")\n",
    "    return results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_dl(dl): \n",
    "    \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "    # Put models in eval mode and do the forward pass \n",
    "    # Current logic: push all batches together into one big list.     \n",
    "    if pp_model.training: pp_model.eval()\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    results_d = defaultdict(list)\n",
    "    with torch.no_grad(): \n",
    "        for i, data in enumerate(dl):\n",
    "            label,text = data['label'].to(device),data[\"text\"]\n",
    "          #  logger.info(show_gpu(f'EVAL, batch {i}, GPU memory usage after loading data: '))\n",
    "            generated, pp_text = pp_model_forward(text)\n",
    "            d = loss_fn(text, label, generated,  pp_text, return_components=True)\n",
    "          #  logger.info(show_gpu(f'EVAL, batch {i}, GPU memory usage after loss_fn pass: '))\n",
    "            d['idx'] = data['idx']\n",
    "                      \n",
    "            for k,v in d.items(): \n",
    "                results_d[k].append(v)\n",
    "            #if '300' in d['idx'] or 300 in d['idx']: \n",
    "          #  print(epoch, i)\n",
    "          #  print(d)\n",
    "          #  print(results_d)\n",
    "    del d, label, text, i, data\n",
    "    results_d = process_results_d_for_wandb(results_d)\n",
    "            \n",
    "    # Calculate additional metrics \n",
    "    results_d['epoch'] = epoch\n",
    "  #  print(\"after process\", results_d)\n",
    "    return results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_wandb_table(results_d, split):\n",
    "    if split not in table_d.keys() or split == \"training_summary\": # training summary table logic is elsewhere\n",
    "        raise Exception(\"split not in table keys or split == training_summary \") \n",
    "    table = table_d[split]\n",
    "        \n",
    "    # Need epoch to be repeated to the same length as the rest of the fields \n",
    "    # (this isn't the batch size because we concat a bunch of stuff)\n",
    "    # we don't want to change the `epoch` key because it screws up logging of the other metrics. \n",
    "    # So we make a new dict.\n",
    "    # d1 = copy.deepcopy(results_d)\n",
    "    d1 = results_d\n",
    "    d1['epoch'] = [epoch for i in range(len(d1['pp_l']))]\n",
    "    dcols = [d1[c] for c in table_columns]\n",
    "    assert len(set([len(o) for o in dcols])) == 1  # all lists should be of the same length \n",
    "    \n",
    "    for row in zip(*dcols):\n",
    "        table.add_data(*row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_training_summary_table(results_d, split):\n",
    "    d = dict()\n",
    "    # key names here have to match those in summary_table_columns\n",
    "    d['epoch'] = epoch\n",
    "    d['split'] = split\n",
    "    for metric in metrics:\n",
    "        d[f'{metric}_avg'] = np.mean(results_d[metric])\n",
    "    table_d['training_summary'].add_data(*[d[c] for c in summary_table_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log_wandb_tables(): \n",
    "    \"\"\"Log wandb tables to the UI\"\"\"\n",
    "    d = dict()\n",
    "    for k, v in table_d.items(): \n",
    "        if k == \"training_step\": d[f\"training/{k}_table\"] = v\n",
    "        else                   : d[f\"eval/{k}_table\"] = v\n",
    "    run.log(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_examples_chart(split, table, metric):\n",
    "   # spec = \"uts_nlp/line_chart_with_idx_filter\"\n",
    "    spec = \"uts_nlp/line_chart_v2\"\n",
    "    fields = {\"x\": \"epoch\",'groupKeys': 'idx'}\n",
    "    fields['y'] = f\"{metric}\"\n",
    "    string_fields = dict()\n",
    "    #string_fields = {'n_idx':wandb_n_examples_plot }\n",
    "    string_fields['title'] = f\"{split}_{metric} vs epoch (examples)\"\n",
    "    chart = wandb.plot_table(vega_spec_name=spec, data_table=table, \n",
    "                            fields=fields, string_fields=string_fields)\n",
    "    wandb.log({f\"individual_examples/{split}_{metric}_vs_epoch_examples\": chart})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_summary_charts(metric):\n",
    "    spec = \"uts_nlp/line_chart_v2\"\n",
    "    fields = {\"x\": \"epoch\",'groupKeys': 'split'}\n",
    "    fields['y']  = f\"{metric}_avg\"\n",
    "    chart = wandb.plot_table(vega_spec_name=spec, data_table=table_d['training_summary'], \n",
    "                                 fields=fields, string_fields={'title': f\"{metric} vs epoch\"})\n",
    "    wandb.log({f\"summary_charts/avg_{metric}_vs_epoch\": chart})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_wandb_charts(): \n",
    "    # Examples charts \n",
    "    for split in ['train', 'valid']: \n",
    "        df = table2df(table_d[split]).query(\"idx in @plt_idx_d[@split]\").sort_values(['idx', 'epoch'])\n",
    "        for metric in metrics: \n",
    "            plot_examples_chart(split, table=wandb.Table(dataframe=df), metric=metric)\n",
    "        \n",
    "    ## Summary charts \n",
    "    # This is at end because we want it to appear first in the dashboard. \n",
    "    for metric in metrics: \n",
    "        plot_summary_charts(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_wandb_run_summary_statistics():\n",
    "    ## Training summary statistics \n",
    "    df_summary = table2df(table_d['training_summary']) \n",
    "    # We calculate the best epoch according to the validation set\n",
    "    best_epoch_idx = df_summary.query(\"split=='valid'\")['loss_avg'].idxmin() \n",
    "    valid_row = df_summary.iloc[best_epoch_idx]\n",
    "    best_epoch = valid_row['epoch'].item()\n",
    "    run.summary['best_epoch'] = best_epoch\n",
    "    # iloc transforms 1row df to series (so it is same as  valid_row)\n",
    "    train_row = df_summary.query(\"split=='train' & epoch==@best_epoch\").iloc[0]  \n",
    "    for metric in metrics: \n",
    "        run.summary[f\"{metric}_avg_train\"] = train_row[f\"{metric}_avg\"].item()\n",
    "        run.summary[f\"{metric}_avg_valid\"] = valid_row[f\"{metric}_avg\"].item()\n",
    "                                 \n",
    "    ## Summary statistics of the test set \n",
    "    # From the last epoch atm because we don't have early stopping \n",
    "    df_test = table2df(table_d['test']) \n",
    "    test_metrics = df_test.filter(metrics, axis=1).mean()\n",
    "    for metric, val in zip(test_metrics.index, test_metrics): \n",
    "        run.summary[f\"{metric}_avg_test\"] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up models and do layer freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Setup\n",
    "vm_model.eval()\n",
    "pp_model.train()\n",
    "\n",
    "## Layer freezing \n",
    "# Unfreeze last 2 layers of the base model decoder\n",
    "# Not sure if decoder layer norm should be unfrozen or not, but it appears after the\n",
    "#   other parameters in the module ordering, so let's include it for now\n",
    "# Also unfreeze the linear head.  This isn't stored in the base model but rather tacked on top\n",
    "#   and will be fine-tuned for summarisation. \n",
    "\n",
    "if pp_name == \"tuner007/pegasus_paraphrase\":\n",
    "    layer_list = ['decoder.layers.14', 'decoder.layers.15', 'decoder.layer_norm'] \n",
    "elif pp_name == \"tdopierre/ProtAugment-ParaphraseGenerator\":\n",
    "    layer_list = ['decoder.layers.4','decoder.layers.5', 'decoder.layernorm_embedding']\n",
    "for i, (name,param) in enumerate(pp_model.base_model.named_parameters()): \n",
    "    if np.any([o in name for o in layer_list]):   param.requires_grad = True\n",
    "    else:                                         param.requires_grad = False\n",
    "for param in pp_model.lm_head.parameters():       param.requires_grad = True\n",
    "# Not sure if to include this or not. this seems to affect lm_head. i might just leave it as it was for now.\n",
    "# this will freeze the embeddings/lm head. \n",
    "# From here: https://github.com/huggingface/transformers/issues/10479#issuecomment-788964822\n",
    "# self.lm_head is tied (the same parameter as) to self.encoder.embed_tokens and self.decoder.embed_tokens.\n",
    "for param in pp_model.base_model.shared.parameters(): param.requires_grad = False \n",
    "\n",
    "\n",
    "#if pp_name == \"tuner007/pegasus_paraphrase\":\n",
    "### For checking the grad status of the layers\n",
    "# for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)\n",
    "# for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    print(i, name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shared.weight torch.Size([96103, 1024]) True\n",
      "1 encoder.embed_positions.weight torch.Size([60, 1024]) False\n",
      "2 encoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "3 encoder.layers.0.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "4 encoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "5 encoder.layers.0.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "6 encoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "7 encoder.layers.0.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "8 encoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "9 encoder.layers.0.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "10 encoder.layers.0.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "11 encoder.layers.0.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "12 encoder.layers.0.fc1.weight torch.Size([4096, 1024]) False\n",
      "13 encoder.layers.0.fc1.bias torch.Size([4096]) False\n",
      "14 encoder.layers.0.fc2.weight torch.Size([1024, 4096]) False\n",
      "15 encoder.layers.0.fc2.bias torch.Size([1024]) False\n",
      "16 encoder.layers.0.final_layer_norm.weight torch.Size([1024]) False\n",
      "17 encoder.layers.0.final_layer_norm.bias torch.Size([1024]) False\n",
      "18 encoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "19 encoder.layers.1.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "20 encoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "21 encoder.layers.1.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "22 encoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "23 encoder.layers.1.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "24 encoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "25 encoder.layers.1.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "26 encoder.layers.1.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "27 encoder.layers.1.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "28 encoder.layers.1.fc1.weight torch.Size([4096, 1024]) False\n",
      "29 encoder.layers.1.fc1.bias torch.Size([4096]) False\n",
      "30 encoder.layers.1.fc2.weight torch.Size([1024, 4096]) False\n",
      "31 encoder.layers.1.fc2.bias torch.Size([1024]) False\n",
      "32 encoder.layers.1.final_layer_norm.weight torch.Size([1024]) False\n",
      "33 encoder.layers.1.final_layer_norm.bias torch.Size([1024]) False\n",
      "34 encoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "35 encoder.layers.2.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "36 encoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "37 encoder.layers.2.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "38 encoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "39 encoder.layers.2.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "40 encoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "41 encoder.layers.2.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "42 encoder.layers.2.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "43 encoder.layers.2.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "44 encoder.layers.2.fc1.weight torch.Size([4096, 1024]) False\n",
      "45 encoder.layers.2.fc1.bias torch.Size([4096]) False\n",
      "46 encoder.layers.2.fc2.weight torch.Size([1024, 4096]) False\n",
      "47 encoder.layers.2.fc2.bias torch.Size([1024]) False\n",
      "48 encoder.layers.2.final_layer_norm.weight torch.Size([1024]) False\n",
      "49 encoder.layers.2.final_layer_norm.bias torch.Size([1024]) False\n",
      "50 encoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "51 encoder.layers.3.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "52 encoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "53 encoder.layers.3.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "54 encoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "55 encoder.layers.3.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "56 encoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "57 encoder.layers.3.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "58 encoder.layers.3.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "59 encoder.layers.3.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "60 encoder.layers.3.fc1.weight torch.Size([4096, 1024]) False\n",
      "61 encoder.layers.3.fc1.bias torch.Size([4096]) False\n",
      "62 encoder.layers.3.fc2.weight torch.Size([1024, 4096]) False\n",
      "63 encoder.layers.3.fc2.bias torch.Size([1024]) False\n",
      "64 encoder.layers.3.final_layer_norm.weight torch.Size([1024]) False\n",
      "65 encoder.layers.3.final_layer_norm.bias torch.Size([1024]) False\n",
      "66 encoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "67 encoder.layers.4.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "68 encoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "69 encoder.layers.4.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "70 encoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "71 encoder.layers.4.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "72 encoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "73 encoder.layers.4.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "74 encoder.layers.4.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "75 encoder.layers.4.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "76 encoder.layers.4.fc1.weight torch.Size([4096, 1024]) False\n",
      "77 encoder.layers.4.fc1.bias torch.Size([4096]) False\n",
      "78 encoder.layers.4.fc2.weight torch.Size([1024, 4096]) False\n",
      "79 encoder.layers.4.fc2.bias torch.Size([1024]) False\n",
      "80 encoder.layers.4.final_layer_norm.weight torch.Size([1024]) False\n",
      "81 encoder.layers.4.final_layer_norm.bias torch.Size([1024]) False\n",
      "82 encoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "83 encoder.layers.5.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "84 encoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "85 encoder.layers.5.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "86 encoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "87 encoder.layers.5.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "88 encoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "89 encoder.layers.5.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "90 encoder.layers.5.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "91 encoder.layers.5.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "92 encoder.layers.5.fc1.weight torch.Size([4096, 1024]) False\n",
      "93 encoder.layers.5.fc1.bias torch.Size([4096]) False\n",
      "94 encoder.layers.5.fc2.weight torch.Size([1024, 4096]) False\n",
      "95 encoder.layers.5.fc2.bias torch.Size([1024]) False\n",
      "96 encoder.layers.5.final_layer_norm.weight torch.Size([1024]) False\n",
      "97 encoder.layers.5.final_layer_norm.bias torch.Size([1024]) False\n",
      "98 encoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "99 encoder.layers.6.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "100 encoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "101 encoder.layers.6.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "102 encoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "103 encoder.layers.6.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "104 encoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "105 encoder.layers.6.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "106 encoder.layers.6.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "107 encoder.layers.6.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "108 encoder.layers.6.fc1.weight torch.Size([4096, 1024]) False\n",
      "109 encoder.layers.6.fc1.bias torch.Size([4096]) False\n",
      "110 encoder.layers.6.fc2.weight torch.Size([1024, 4096]) False\n",
      "111 encoder.layers.6.fc2.bias torch.Size([1024]) False\n",
      "112 encoder.layers.6.final_layer_norm.weight torch.Size([1024]) False\n",
      "113 encoder.layers.6.final_layer_norm.bias torch.Size([1024]) False\n",
      "114 encoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "115 encoder.layers.7.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "116 encoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "117 encoder.layers.7.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "118 encoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "119 encoder.layers.7.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "120 encoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "121 encoder.layers.7.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "122 encoder.layers.7.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "123 encoder.layers.7.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "124 encoder.layers.7.fc1.weight torch.Size([4096, 1024]) False\n",
      "125 encoder.layers.7.fc1.bias torch.Size([4096]) False\n",
      "126 encoder.layers.7.fc2.weight torch.Size([1024, 4096]) False\n",
      "127 encoder.layers.7.fc2.bias torch.Size([1024]) False\n",
      "128 encoder.layers.7.final_layer_norm.weight torch.Size([1024]) False\n",
      "129 encoder.layers.7.final_layer_norm.bias torch.Size([1024]) False\n",
      "130 encoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "131 encoder.layers.8.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "132 encoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "133 encoder.layers.8.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "134 encoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "135 encoder.layers.8.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "136 encoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "137 encoder.layers.8.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "138 encoder.layers.8.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "139 encoder.layers.8.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "140 encoder.layers.8.fc1.weight torch.Size([4096, 1024]) False\n",
      "141 encoder.layers.8.fc1.bias torch.Size([4096]) False\n",
      "142 encoder.layers.8.fc2.weight torch.Size([1024, 4096]) False\n",
      "143 encoder.layers.8.fc2.bias torch.Size([1024]) False\n",
      "144 encoder.layers.8.final_layer_norm.weight torch.Size([1024]) False\n",
      "145 encoder.layers.8.final_layer_norm.bias torch.Size([1024]) False\n",
      "146 encoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "147 encoder.layers.9.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "148 encoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "149 encoder.layers.9.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "150 encoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "151 encoder.layers.9.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "152 encoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "153 encoder.layers.9.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "154 encoder.layers.9.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "155 encoder.layers.9.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "156 encoder.layers.9.fc1.weight torch.Size([4096, 1024]) False\n",
      "157 encoder.layers.9.fc1.bias torch.Size([4096]) False\n",
      "158 encoder.layers.9.fc2.weight torch.Size([1024, 4096]) False\n",
      "159 encoder.layers.9.fc2.bias torch.Size([1024]) False\n",
      "160 encoder.layers.9.final_layer_norm.weight torch.Size([1024]) False\n",
      "161 encoder.layers.9.final_layer_norm.bias torch.Size([1024]) False\n",
      "162 encoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "163 encoder.layers.10.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "164 encoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "165 encoder.layers.10.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "166 encoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "167 encoder.layers.10.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "168 encoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "169 encoder.layers.10.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "170 encoder.layers.10.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "171 encoder.layers.10.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "172 encoder.layers.10.fc1.weight torch.Size([4096, 1024]) False\n",
      "173 encoder.layers.10.fc1.bias torch.Size([4096]) False\n",
      "174 encoder.layers.10.fc2.weight torch.Size([1024, 4096]) False\n",
      "175 encoder.layers.10.fc2.bias torch.Size([1024]) False\n",
      "176 encoder.layers.10.final_layer_norm.weight torch.Size([1024]) False\n",
      "177 encoder.layers.10.final_layer_norm.bias torch.Size([1024]) False\n",
      "178 encoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "179 encoder.layers.11.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "180 encoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "181 encoder.layers.11.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "182 encoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "183 encoder.layers.11.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "184 encoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "185 encoder.layers.11.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "186 encoder.layers.11.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "187 encoder.layers.11.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "188 encoder.layers.11.fc1.weight torch.Size([4096, 1024]) False\n",
      "189 encoder.layers.11.fc1.bias torch.Size([4096]) False\n",
      "190 encoder.layers.11.fc2.weight torch.Size([1024, 4096]) False\n",
      "191 encoder.layers.11.fc2.bias torch.Size([1024]) False\n",
      "192 encoder.layers.11.final_layer_norm.weight torch.Size([1024]) False\n",
      "193 encoder.layers.11.final_layer_norm.bias torch.Size([1024]) False\n",
      "194 encoder.layers.12.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "195 encoder.layers.12.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "196 encoder.layers.12.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "197 encoder.layers.12.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "198 encoder.layers.12.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "199 encoder.layers.12.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "200 encoder.layers.12.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "201 encoder.layers.12.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "202 encoder.layers.12.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "203 encoder.layers.12.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "204 encoder.layers.12.fc1.weight torch.Size([4096, 1024]) False\n",
      "205 encoder.layers.12.fc1.bias torch.Size([4096]) False\n",
      "206 encoder.layers.12.fc2.weight torch.Size([1024, 4096]) False\n",
      "207 encoder.layers.12.fc2.bias torch.Size([1024]) False\n",
      "208 encoder.layers.12.final_layer_norm.weight torch.Size([1024]) False\n",
      "209 encoder.layers.12.final_layer_norm.bias torch.Size([1024]) False\n",
      "210 encoder.layers.13.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "211 encoder.layers.13.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "212 encoder.layers.13.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "213 encoder.layers.13.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "214 encoder.layers.13.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "215 encoder.layers.13.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "216 encoder.layers.13.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "217 encoder.layers.13.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "218 encoder.layers.13.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "219 encoder.layers.13.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "220 encoder.layers.13.fc1.weight torch.Size([4096, 1024]) False\n",
      "221 encoder.layers.13.fc1.bias torch.Size([4096]) False\n",
      "222 encoder.layers.13.fc2.weight torch.Size([1024, 4096]) False\n",
      "223 encoder.layers.13.fc2.bias torch.Size([1024]) False\n",
      "224 encoder.layers.13.final_layer_norm.weight torch.Size([1024]) False\n",
      "225 encoder.layers.13.final_layer_norm.bias torch.Size([1024]) False\n",
      "226 encoder.layers.14.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "227 encoder.layers.14.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "228 encoder.layers.14.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "229 encoder.layers.14.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "230 encoder.layers.14.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "231 encoder.layers.14.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "232 encoder.layers.14.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "233 encoder.layers.14.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "234 encoder.layers.14.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "235 encoder.layers.14.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "236 encoder.layers.14.fc1.weight torch.Size([4096, 1024]) False\n",
      "237 encoder.layers.14.fc1.bias torch.Size([4096]) False\n",
      "238 encoder.layers.14.fc2.weight torch.Size([1024, 4096]) False\n",
      "239 encoder.layers.14.fc2.bias torch.Size([1024]) False\n",
      "240 encoder.layers.14.final_layer_norm.weight torch.Size([1024]) False\n",
      "241 encoder.layers.14.final_layer_norm.bias torch.Size([1024]) False\n",
      "242 encoder.layers.15.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "243 encoder.layers.15.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "244 encoder.layers.15.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "245 encoder.layers.15.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "246 encoder.layers.15.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "247 encoder.layers.15.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "248 encoder.layers.15.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "249 encoder.layers.15.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "250 encoder.layers.15.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "251 encoder.layers.15.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "252 encoder.layers.15.fc1.weight torch.Size([4096, 1024]) False\n",
      "253 encoder.layers.15.fc1.bias torch.Size([4096]) False\n",
      "254 encoder.layers.15.fc2.weight torch.Size([1024, 4096]) False\n",
      "255 encoder.layers.15.fc2.bias torch.Size([1024]) False\n",
      "256 encoder.layers.15.final_layer_norm.weight torch.Size([1024]) False\n",
      "257 encoder.layers.15.final_layer_norm.bias torch.Size([1024]) False\n",
      "258 encoder.layer_norm.weight torch.Size([1024]) False\n",
      "259 encoder.layer_norm.bias torch.Size([1024]) False\n",
      "260 decoder.embed_positions.weight torch.Size([60, 1024]) False\n",
      "261 decoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "262 decoder.layers.0.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "263 decoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "264 decoder.layers.0.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "265 decoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "266 decoder.layers.0.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "267 decoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "268 decoder.layers.0.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "269 decoder.layers.0.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "270 decoder.layers.0.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "271 decoder.layers.0.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "272 decoder.layers.0.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "273 decoder.layers.0.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "274 decoder.layers.0.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "275 decoder.layers.0.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "276 decoder.layers.0.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "277 decoder.layers.0.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "278 decoder.layers.0.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "279 decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "280 decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "281 decoder.layers.0.fc1.weight torch.Size([4096, 1024]) False\n",
      "282 decoder.layers.0.fc1.bias torch.Size([4096]) False\n",
      "283 decoder.layers.0.fc2.weight torch.Size([1024, 4096]) False\n",
      "284 decoder.layers.0.fc2.bias torch.Size([1024]) False\n",
      "285 decoder.layers.0.final_layer_norm.weight torch.Size([1024]) False\n",
      "286 decoder.layers.0.final_layer_norm.bias torch.Size([1024]) False\n",
      "287 decoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "288 decoder.layers.1.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "289 decoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "290 decoder.layers.1.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "291 decoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "292 decoder.layers.1.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "293 decoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "294 decoder.layers.1.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "295 decoder.layers.1.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "296 decoder.layers.1.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "297 decoder.layers.1.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "298 decoder.layers.1.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "299 decoder.layers.1.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "300 decoder.layers.1.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "301 decoder.layers.1.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "302 decoder.layers.1.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "303 decoder.layers.1.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "304 decoder.layers.1.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "305 decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "306 decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "307 decoder.layers.1.fc1.weight torch.Size([4096, 1024]) False\n",
      "308 decoder.layers.1.fc1.bias torch.Size([4096]) False\n",
      "309 decoder.layers.1.fc2.weight torch.Size([1024, 4096]) False\n",
      "310 decoder.layers.1.fc2.bias torch.Size([1024]) False\n",
      "311 decoder.layers.1.final_layer_norm.weight torch.Size([1024]) False\n",
      "312 decoder.layers.1.final_layer_norm.bias torch.Size([1024]) False\n",
      "313 decoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "314 decoder.layers.2.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "315 decoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "316 decoder.layers.2.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "317 decoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "318 decoder.layers.2.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "319 decoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "320 decoder.layers.2.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "321 decoder.layers.2.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "322 decoder.layers.2.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "323 decoder.layers.2.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "324 decoder.layers.2.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "325 decoder.layers.2.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "326 decoder.layers.2.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "327 decoder.layers.2.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "328 decoder.layers.2.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "329 decoder.layers.2.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "330 decoder.layers.2.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "331 decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "332 decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "333 decoder.layers.2.fc1.weight torch.Size([4096, 1024]) False\n",
      "334 decoder.layers.2.fc1.bias torch.Size([4096]) False\n",
      "335 decoder.layers.2.fc2.weight torch.Size([1024, 4096]) False\n",
      "336 decoder.layers.2.fc2.bias torch.Size([1024]) False\n",
      "337 decoder.layers.2.final_layer_norm.weight torch.Size([1024]) False\n",
      "338 decoder.layers.2.final_layer_norm.bias torch.Size([1024]) False\n",
      "339 decoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "340 decoder.layers.3.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "341 decoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "342 decoder.layers.3.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "343 decoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "344 decoder.layers.3.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "345 decoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "346 decoder.layers.3.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "347 decoder.layers.3.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "348 decoder.layers.3.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "349 decoder.layers.3.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "350 decoder.layers.3.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "351 decoder.layers.3.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "352 decoder.layers.3.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "353 decoder.layers.3.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "354 decoder.layers.3.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "355 decoder.layers.3.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "356 decoder.layers.3.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "357 decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "358 decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "359 decoder.layers.3.fc1.weight torch.Size([4096, 1024]) False\n",
      "360 decoder.layers.3.fc1.bias torch.Size([4096]) False\n",
      "361 decoder.layers.3.fc2.weight torch.Size([1024, 4096]) False\n",
      "362 decoder.layers.3.fc2.bias torch.Size([1024]) False\n",
      "363 decoder.layers.3.final_layer_norm.weight torch.Size([1024]) False\n",
      "364 decoder.layers.3.final_layer_norm.bias torch.Size([1024]) False\n",
      "365 decoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "366 decoder.layers.4.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "367 decoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "368 decoder.layers.4.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "369 decoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "370 decoder.layers.4.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "371 decoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "372 decoder.layers.4.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "373 decoder.layers.4.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "374 decoder.layers.4.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "375 decoder.layers.4.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "376 decoder.layers.4.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "377 decoder.layers.4.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "378 decoder.layers.4.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "379 decoder.layers.4.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "380 decoder.layers.4.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "381 decoder.layers.4.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "382 decoder.layers.4.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "383 decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "384 decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "385 decoder.layers.4.fc1.weight torch.Size([4096, 1024]) False\n",
      "386 decoder.layers.4.fc1.bias torch.Size([4096]) False\n",
      "387 decoder.layers.4.fc2.weight torch.Size([1024, 4096]) False\n",
      "388 decoder.layers.4.fc2.bias torch.Size([1024]) False\n",
      "389 decoder.layers.4.final_layer_norm.weight torch.Size([1024]) False\n",
      "390 decoder.layers.4.final_layer_norm.bias torch.Size([1024]) False\n",
      "391 decoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "392 decoder.layers.5.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "393 decoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "394 decoder.layers.5.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "395 decoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "396 decoder.layers.5.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "397 decoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "398 decoder.layers.5.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "399 decoder.layers.5.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "400 decoder.layers.5.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "401 decoder.layers.5.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "402 decoder.layers.5.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "403 decoder.layers.5.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "404 decoder.layers.5.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "405 decoder.layers.5.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "406 decoder.layers.5.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "407 decoder.layers.5.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "408 decoder.layers.5.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "409 decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "410 decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "411 decoder.layers.5.fc1.weight torch.Size([4096, 1024]) False\n",
      "412 decoder.layers.5.fc1.bias torch.Size([4096]) False\n",
      "413 decoder.layers.5.fc2.weight torch.Size([1024, 4096]) False\n",
      "414 decoder.layers.5.fc2.bias torch.Size([1024]) False\n",
      "415 decoder.layers.5.final_layer_norm.weight torch.Size([1024]) False\n",
      "416 decoder.layers.5.final_layer_norm.bias torch.Size([1024]) False\n",
      "417 decoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "418 decoder.layers.6.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "419 decoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "420 decoder.layers.6.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "421 decoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "422 decoder.layers.6.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "423 decoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "424 decoder.layers.6.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "425 decoder.layers.6.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "426 decoder.layers.6.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "427 decoder.layers.6.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "428 decoder.layers.6.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "429 decoder.layers.6.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "430 decoder.layers.6.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "431 decoder.layers.6.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "432 decoder.layers.6.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "433 decoder.layers.6.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "434 decoder.layers.6.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "435 decoder.layers.6.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "436 decoder.layers.6.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "437 decoder.layers.6.fc1.weight torch.Size([4096, 1024]) False\n",
      "438 decoder.layers.6.fc1.bias torch.Size([4096]) False\n",
      "439 decoder.layers.6.fc2.weight torch.Size([1024, 4096]) False\n",
      "440 decoder.layers.6.fc2.bias torch.Size([1024]) False\n",
      "441 decoder.layers.6.final_layer_norm.weight torch.Size([1024]) False\n",
      "442 decoder.layers.6.final_layer_norm.bias torch.Size([1024]) False\n",
      "443 decoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "444 decoder.layers.7.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "445 decoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "446 decoder.layers.7.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "447 decoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "448 decoder.layers.7.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "449 decoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "450 decoder.layers.7.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "451 decoder.layers.7.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "452 decoder.layers.7.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "453 decoder.layers.7.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "454 decoder.layers.7.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "455 decoder.layers.7.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "456 decoder.layers.7.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "457 decoder.layers.7.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "458 decoder.layers.7.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "459 decoder.layers.7.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "460 decoder.layers.7.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "461 decoder.layers.7.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "462 decoder.layers.7.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "463 decoder.layers.7.fc1.weight torch.Size([4096, 1024]) False\n",
      "464 decoder.layers.7.fc1.bias torch.Size([4096]) False\n",
      "465 decoder.layers.7.fc2.weight torch.Size([1024, 4096]) False\n",
      "466 decoder.layers.7.fc2.bias torch.Size([1024]) False\n",
      "467 decoder.layers.7.final_layer_norm.weight torch.Size([1024]) False\n",
      "468 decoder.layers.7.final_layer_norm.bias torch.Size([1024]) False\n",
      "469 decoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "470 decoder.layers.8.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "471 decoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "472 decoder.layers.8.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "473 decoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "474 decoder.layers.8.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "475 decoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "476 decoder.layers.8.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "477 decoder.layers.8.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "478 decoder.layers.8.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "479 decoder.layers.8.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "480 decoder.layers.8.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "481 decoder.layers.8.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "482 decoder.layers.8.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "483 decoder.layers.8.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "484 decoder.layers.8.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "485 decoder.layers.8.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "486 decoder.layers.8.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "487 decoder.layers.8.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "488 decoder.layers.8.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "489 decoder.layers.8.fc1.weight torch.Size([4096, 1024]) False\n",
      "490 decoder.layers.8.fc1.bias torch.Size([4096]) False\n",
      "491 decoder.layers.8.fc2.weight torch.Size([1024, 4096]) False\n",
      "492 decoder.layers.8.fc2.bias torch.Size([1024]) False\n",
      "493 decoder.layers.8.final_layer_norm.weight torch.Size([1024]) False\n",
      "494 decoder.layers.8.final_layer_norm.bias torch.Size([1024]) False\n",
      "495 decoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "496 decoder.layers.9.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "497 decoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "498 decoder.layers.9.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "499 decoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "500 decoder.layers.9.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "501 decoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "502 decoder.layers.9.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "503 decoder.layers.9.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "504 decoder.layers.9.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "505 decoder.layers.9.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "506 decoder.layers.9.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "507 decoder.layers.9.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "508 decoder.layers.9.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "509 decoder.layers.9.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "510 decoder.layers.9.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "511 decoder.layers.9.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "512 decoder.layers.9.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "513 decoder.layers.9.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "514 decoder.layers.9.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "515 decoder.layers.9.fc1.weight torch.Size([4096, 1024]) False\n",
      "516 decoder.layers.9.fc1.bias torch.Size([4096]) False\n",
      "517 decoder.layers.9.fc2.weight torch.Size([1024, 4096]) False\n",
      "518 decoder.layers.9.fc2.bias torch.Size([1024]) False\n",
      "519 decoder.layers.9.final_layer_norm.weight torch.Size([1024]) False\n",
      "520 decoder.layers.9.final_layer_norm.bias torch.Size([1024]) False\n",
      "521 decoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "522 decoder.layers.10.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "523 decoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "524 decoder.layers.10.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "525 decoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "526 decoder.layers.10.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "527 decoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "528 decoder.layers.10.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "529 decoder.layers.10.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "530 decoder.layers.10.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "531 decoder.layers.10.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "532 decoder.layers.10.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "533 decoder.layers.10.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "534 decoder.layers.10.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "535 decoder.layers.10.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "536 decoder.layers.10.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "537 decoder.layers.10.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "538 decoder.layers.10.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "539 decoder.layers.10.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "540 decoder.layers.10.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "541 decoder.layers.10.fc1.weight torch.Size([4096, 1024]) False\n",
      "542 decoder.layers.10.fc1.bias torch.Size([4096]) False\n",
      "543 decoder.layers.10.fc2.weight torch.Size([1024, 4096]) False\n",
      "544 decoder.layers.10.fc2.bias torch.Size([1024]) False\n",
      "545 decoder.layers.10.final_layer_norm.weight torch.Size([1024]) False\n",
      "546 decoder.layers.10.final_layer_norm.bias torch.Size([1024]) False\n",
      "547 decoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "548 decoder.layers.11.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "549 decoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "550 decoder.layers.11.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "551 decoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "552 decoder.layers.11.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "553 decoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "554 decoder.layers.11.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "555 decoder.layers.11.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "556 decoder.layers.11.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "557 decoder.layers.11.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "558 decoder.layers.11.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "559 decoder.layers.11.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "560 decoder.layers.11.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "561 decoder.layers.11.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "562 decoder.layers.11.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "563 decoder.layers.11.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "564 decoder.layers.11.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "565 decoder.layers.11.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "566 decoder.layers.11.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "567 decoder.layers.11.fc1.weight torch.Size([4096, 1024]) False\n",
      "568 decoder.layers.11.fc1.bias torch.Size([4096]) False\n",
      "569 decoder.layers.11.fc2.weight torch.Size([1024, 4096]) False\n",
      "570 decoder.layers.11.fc2.bias torch.Size([1024]) False\n",
      "571 decoder.layers.11.final_layer_norm.weight torch.Size([1024]) False\n",
      "572 decoder.layers.11.final_layer_norm.bias torch.Size([1024]) False\n",
      "573 decoder.layers.12.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "574 decoder.layers.12.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "575 decoder.layers.12.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "576 decoder.layers.12.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "577 decoder.layers.12.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "578 decoder.layers.12.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "579 decoder.layers.12.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "580 decoder.layers.12.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "581 decoder.layers.12.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "582 decoder.layers.12.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "583 decoder.layers.12.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "584 decoder.layers.12.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "585 decoder.layers.12.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "586 decoder.layers.12.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "587 decoder.layers.12.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "588 decoder.layers.12.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "589 decoder.layers.12.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "590 decoder.layers.12.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "591 decoder.layers.12.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "592 decoder.layers.12.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "593 decoder.layers.12.fc1.weight torch.Size([4096, 1024]) False\n",
      "594 decoder.layers.12.fc1.bias torch.Size([4096]) False\n",
      "595 decoder.layers.12.fc2.weight torch.Size([1024, 4096]) False\n",
      "596 decoder.layers.12.fc2.bias torch.Size([1024]) False\n",
      "597 decoder.layers.12.final_layer_norm.weight torch.Size([1024]) False\n",
      "598 decoder.layers.12.final_layer_norm.bias torch.Size([1024]) False\n",
      "599 decoder.layers.13.self_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "600 decoder.layers.13.self_attn.k_proj.bias torch.Size([1024]) False\n",
      "601 decoder.layers.13.self_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "602 decoder.layers.13.self_attn.v_proj.bias torch.Size([1024]) False\n",
      "603 decoder.layers.13.self_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "604 decoder.layers.13.self_attn.q_proj.bias torch.Size([1024]) False\n",
      "605 decoder.layers.13.self_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "606 decoder.layers.13.self_attn.out_proj.bias torch.Size([1024]) False\n",
      "607 decoder.layers.13.self_attn_layer_norm.weight torch.Size([1024]) False\n",
      "608 decoder.layers.13.self_attn_layer_norm.bias torch.Size([1024]) False\n",
      "609 decoder.layers.13.encoder_attn.k_proj.weight torch.Size([1024, 1024]) False\n",
      "610 decoder.layers.13.encoder_attn.k_proj.bias torch.Size([1024]) False\n",
      "611 decoder.layers.13.encoder_attn.v_proj.weight torch.Size([1024, 1024]) False\n",
      "612 decoder.layers.13.encoder_attn.v_proj.bias torch.Size([1024]) False\n",
      "613 decoder.layers.13.encoder_attn.q_proj.weight torch.Size([1024, 1024]) False\n",
      "614 decoder.layers.13.encoder_attn.q_proj.bias torch.Size([1024]) False\n",
      "615 decoder.layers.13.encoder_attn.out_proj.weight torch.Size([1024, 1024]) False\n",
      "616 decoder.layers.13.encoder_attn.out_proj.bias torch.Size([1024]) False\n",
      "617 decoder.layers.13.encoder_attn_layer_norm.weight torch.Size([1024]) False\n",
      "618 decoder.layers.13.encoder_attn_layer_norm.bias torch.Size([1024]) False\n",
      "619 decoder.layers.13.fc1.weight torch.Size([4096, 1024]) False\n",
      "620 decoder.layers.13.fc1.bias torch.Size([4096]) False\n",
      "621 decoder.layers.13.fc2.weight torch.Size([1024, 4096]) False\n",
      "622 decoder.layers.13.fc2.bias torch.Size([1024]) False\n",
      "623 decoder.layers.13.final_layer_norm.weight torch.Size([1024]) False\n",
      "624 decoder.layers.13.final_layer_norm.bias torch.Size([1024]) False\n",
      "625 decoder.layers.14.self_attn.k_proj.weight torch.Size([1024, 1024]) True\n",
      "626 decoder.layers.14.self_attn.k_proj.bias torch.Size([1024]) True\n",
      "627 decoder.layers.14.self_attn.v_proj.weight torch.Size([1024, 1024]) True\n",
      "628 decoder.layers.14.self_attn.v_proj.bias torch.Size([1024]) True\n",
      "629 decoder.layers.14.self_attn.q_proj.weight torch.Size([1024, 1024]) True\n",
      "630 decoder.layers.14.self_attn.q_proj.bias torch.Size([1024]) True\n",
      "631 decoder.layers.14.self_attn.out_proj.weight torch.Size([1024, 1024]) True\n",
      "632 decoder.layers.14.self_attn.out_proj.bias torch.Size([1024]) True\n",
      "633 decoder.layers.14.self_attn_layer_norm.weight torch.Size([1024]) True\n",
      "634 decoder.layers.14.self_attn_layer_norm.bias torch.Size([1024]) True\n",
      "635 decoder.layers.14.encoder_attn.k_proj.weight torch.Size([1024, 1024]) True\n",
      "636 decoder.layers.14.encoder_attn.k_proj.bias torch.Size([1024]) True\n",
      "637 decoder.layers.14.encoder_attn.v_proj.weight torch.Size([1024, 1024]) True\n",
      "638 decoder.layers.14.encoder_attn.v_proj.bias torch.Size([1024]) True\n",
      "639 decoder.layers.14.encoder_attn.q_proj.weight torch.Size([1024, 1024]) True\n",
      "640 decoder.layers.14.encoder_attn.q_proj.bias torch.Size([1024]) True\n",
      "641 decoder.layers.14.encoder_attn.out_proj.weight torch.Size([1024, 1024]) True\n",
      "642 decoder.layers.14.encoder_attn.out_proj.bias torch.Size([1024]) True\n",
      "643 decoder.layers.14.encoder_attn_layer_norm.weight torch.Size([1024]) True\n",
      "644 decoder.layers.14.encoder_attn_layer_norm.bias torch.Size([1024]) True\n",
      "645 decoder.layers.14.fc1.weight torch.Size([4096, 1024]) True\n",
      "646 decoder.layers.14.fc1.bias torch.Size([4096]) True\n",
      "647 decoder.layers.14.fc2.weight torch.Size([1024, 4096]) True\n",
      "648 decoder.layers.14.fc2.bias torch.Size([1024]) True\n",
      "649 decoder.layers.14.final_layer_norm.weight torch.Size([1024]) True\n",
      "650 decoder.layers.14.final_layer_norm.bias torch.Size([1024]) True\n",
      "651 decoder.layers.15.self_attn.k_proj.weight torch.Size([1024, 1024]) True\n",
      "652 decoder.layers.15.self_attn.k_proj.bias torch.Size([1024]) True\n",
      "653 decoder.layers.15.self_attn.v_proj.weight torch.Size([1024, 1024]) True\n",
      "654 decoder.layers.15.self_attn.v_proj.bias torch.Size([1024]) True\n",
      "655 decoder.layers.15.self_attn.q_proj.weight torch.Size([1024, 1024]) True\n",
      "656 decoder.layers.15.self_attn.q_proj.bias torch.Size([1024]) True\n",
      "657 decoder.layers.15.self_attn.out_proj.weight torch.Size([1024, 1024]) True\n",
      "658 decoder.layers.15.self_attn.out_proj.bias torch.Size([1024]) True\n",
      "659 decoder.layers.15.self_attn_layer_norm.weight torch.Size([1024]) True\n",
      "660 decoder.layers.15.self_attn_layer_norm.bias torch.Size([1024]) True\n",
      "661 decoder.layers.15.encoder_attn.k_proj.weight torch.Size([1024, 1024]) True\n",
      "662 decoder.layers.15.encoder_attn.k_proj.bias torch.Size([1024]) True\n",
      "663 decoder.layers.15.encoder_attn.v_proj.weight torch.Size([1024, 1024]) True\n",
      "664 decoder.layers.15.encoder_attn.v_proj.bias torch.Size([1024]) True\n",
      "665 decoder.layers.15.encoder_attn.q_proj.weight torch.Size([1024, 1024]) True\n",
      "666 decoder.layers.15.encoder_attn.q_proj.bias torch.Size([1024]) True\n",
      "667 decoder.layers.15.encoder_attn.out_proj.weight torch.Size([1024, 1024]) True\n",
      "668 decoder.layers.15.encoder_attn.out_proj.bias torch.Size([1024]) True\n",
      "669 decoder.layers.15.encoder_attn_layer_norm.weight torch.Size([1024]) True\n",
      "670 decoder.layers.15.encoder_attn_layer_norm.bias torch.Size([1024]) True\n",
      "671 decoder.layers.15.fc1.weight torch.Size([4096, 1024]) True\n",
      "672 decoder.layers.15.fc1.bias torch.Size([4096]) True\n",
      "673 decoder.layers.15.fc2.weight torch.Size([1024, 4096]) True\n",
      "674 decoder.layers.15.fc2.bias torch.Size([1024]) True\n",
      "675 decoder.layers.15.final_layer_norm.weight torch.Size([1024]) True\n",
      "676 decoder.layers.15.final_layer_norm.bias torch.Size([1024]) True\n",
      "677 decoder.layer_norm.weight torch.Size([1024]) True\n",
      "678 decoder.layer_norm.bias torch.Size([1024]) True\n",
      "0 weight torch.Size([96103, 1024]) True\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.size(), param.requires_grad)\n",
    "for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    print(i, name, param.size(), param.requires_grad)\n",
    "#for i, (name, param) in enumerate(pp_model.named_parameters()):    print(i, name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x2b6087521eb0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.0136,  0.0027, -0.0099,  ...,  0.0021,  0.1076,  0.0093],\n",
      "        [-0.0104, -0.0187, -0.0175,  ..., -0.0140, -0.0049, -0.0037],\n",
      "        [ 0.0818, -0.0393,  0.0106,  ...,  0.0582,  0.0077,  0.0356],\n",
      "        ...,\n",
      "        [ 0.0149, -0.0256, -0.0199,  ..., -0.0053,  0.0234, -0.0057],\n",
      "        [ 0.0076, -0.0403, -0.0265,  ...,  0.0048,  0.0126, -0.0031],\n",
      "        [ 0.0129, -0.0226, -0.0269,  ...,  0.0227,  0.0032,  0.0016]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for param in pp_model.base_model.shared.named_parameters(): \n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Set up optimiser and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code below taken from https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue_no_trainer.py#L363\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "# no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [p for n, p in pp_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": weight_decay,\n",
    "#     },\n",
    "#     {\n",
    "#         \"params\": [p for n, p in pp_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": 0.0,\n",
    "#     },\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# For now we just keep this simple\n",
    "optimizer = AdamW(pp_model.parameters(), lr=lr)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=lr_scheduler_type,\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=n_warmup_steps,\n",
    "#     num_training_steps=n_train_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muts_nlp\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/uts_nlp/travis_attack/runs/1hd6rq5x\" target=\"_blank\">whole-salad-81</a></strong> to <a href=\"https://wandb.ai/uts_nlp/travis_attack\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Launch run and configure what it is tracking\n",
    "run = wandb.init(project=\"travis_attack\", entity=\"uts_nlp\", config=config_d,\n",
    "                 mode=wandb_mode, notes=run_notes)\n",
    "if wandb_log_grads: wandb.watch(pp_model, log='gradients', log_freq=wandb_log_grads_freq)\n",
    "\n",
    "### Set up tables \n",
    "table_d = dict()\n",
    "\n",
    "# These have to be in the keys of the output from eval_dl\n",
    "table_columns = ['idx', 'orig_l',  'truelabel', 'orig_truelabel_probs', 'epoch', 'pp_l',\n",
    "             'pp_truelabel_probs'] + metrics\n",
    "def make_table(cols): return wandb.Table(columns=cols)\n",
    "for key in splits:                table_d[key]             = make_table(table_columns) \n",
    "if wandb_log_training_step_table: table_d['training_step'] = make_table(table_columns) \n",
    "summary_table_columns = ['epoch','split'] + [f'{m}_avg' for m in metrics]\n",
    "table_d['training_summary'] = make_table(summary_table_columns)\n",
    "\n",
    "## Get indices for the examples plots\n",
    "def get_examples_plot_idxs(ds): return ds['idx'][0:wandb_n_examples_plot]\n",
    "plt_idx_d = dict()\n",
    "for split in splits: plt_idx_d[split] = get_examples_plot_idxs(ds_d[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up other miscellaneous things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")\n",
    "n_train_steps = n_train_epochs * len(dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU memory usage after loading models: 16.3% (3958 out of 24220)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebdfe00ca514153aa25d89714ca8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 0 of 3\n",
      "Batch 0, GPU memory usage after forward pass:  22.6% (5474 out of 24220)\n",
      "Batch 0, GPU memory usage after backwards pass:  28.6% (6934 out of 24220)\n",
      "Batch 1, GPU memory usage after forward pass:  28.6% (6934 out of 24220)\n",
      "Batch 1, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 2, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 2, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 3, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 3, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 4, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 4, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 5, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 5, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 6, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 6, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 7, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 7, GPU memory usage after backwards pass:  30.1% (7288 out of 24220)\n",
      "Batch 8, GPU memory usage after forward pass:  30.1% (7295 out of 24220)\n",
      "Batch 8, GPU memory usage after backwards pass:  30.1% (7295 out of 24220)\n",
      "Batch 9, GPU memory usage after forward pass:  30.1% (7295 out of 24220)\n",
      "Batch 9, GPU memory usage after backwards pass:  30.1% (7295 out of 24220)\n",
      "Batch 10, GPU memory usage after forward pass:  30.1% (7295 out of 24220)\n",
      "Batch 10, GPU memory usage after backwards pass:  30.1% (7295 out of 24220)\n",
      "Batch 11, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 11, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 12, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 12, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 13, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 13, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 14, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 14, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 15, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 15, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 16, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 16, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 17, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 17, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 18, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 18, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 19, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 19, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 20, GPU memory usage after forward pass:  30.1% (7282 out of 24220)\n",
      "Batch 20, GPU memory usage after backwards pass:  30.1% (7282 out of 24220)\n",
      "Batch 21, GPU memory usage after forward pass:  30.3% (7338 out of 24220)\n",
      "Batch 21, GPU memory usage after backwards pass:  30.3% (7338 out of 24220)\n",
      "Now on epoch 1 of 3\n",
      "Batch 0, GPU memory usage after forward pass:  29.8% (7222 out of 24220)\n",
      "Batch 0, GPU memory usage after backwards pass:  29.8% (7222 out of 24220)\n",
      "Batch 1, GPU memory usage after forward pass:  29.8% (7222 out of 24220)\n",
      "Batch 1, GPU memory usage after backwards pass:  29.8% (7222 out of 24220)\n",
      "Batch 2, GPU memory usage after forward pass:  29.8% (7222 out of 24220)\n",
      "Batch 2, GPU memory usage after backwards pass:  29.8% (7222 out of 24220)\n",
      "Batch 3, GPU memory usage after forward pass:  29.9% (7250 out of 24220)\n",
      "Batch 3, GPU memory usage after backwards pass:  32.9% (7959 out of 24220)\n",
      "Batch 4, GPU memory usage after forward pass:  32.9% (7959 out of 24220)\n",
      "Batch 4, GPU memory usage after backwards pass:  32.9% (7959 out of 24220)\n",
      "Batch 5, GPU memory usage after forward pass:  32.9% (7959 out of 24220)\n",
      "Batch 5, GPU memory usage after backwards pass:  32.9% (7959 out of 24220)\n",
      "Batch 6, GPU memory usage after forward pass:  32.9% (7965 out of 24220)\n",
      "Batch 6, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 7, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 7, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 8, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 8, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 9, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 9, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 10, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 10, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 11, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 11, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 12, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 12, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 13, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 13, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 14, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 14, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 15, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 15, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 16, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 16, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 17, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 17, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 18, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 18, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 19, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 19, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 20, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 20, GPU memory usage after backwards pass:  32.8% (7952 out of 24220)\n",
      "Batch 21, GPU memory usage after forward pass:  32.8% (7952 out of 24220)\n",
      "Batch 21, GPU memory usage after backwards pass:  32.9% (7965 out of 24220)\n",
      "Now on epoch 2 of 3\n",
      "Batch 0, GPU memory usage after forward pass:  26.2% (6340 out of 24220)\n",
      "Batch 0, GPU memory usage after backwards pass:  31.9% (7732 out of 24220)\n",
      "Batch 1, GPU memory usage after forward pass:  31.9% (7732 out of 24220)\n",
      "Batch 1, GPU memory usage after backwards pass:  31.9% (7732 out of 24220)\n",
      "Batch 2, GPU memory usage after forward pass:  31.9% (7732 out of 24220)\n",
      "Batch 2, GPU memory usage after backwards pass:  31.9% (7718 out of 24220)\n",
      "Batch 3, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 3, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 4, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 4, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 5, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 5, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 6, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 6, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 7, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 7, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 8, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 8, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 9, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 9, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 10, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 10, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 11, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 12, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 12, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 13, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 13, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 14, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 14, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 15, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 15, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 16, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 16, GPU memory usage after backwards pass:  31.9% (7735 out of 24220)\n",
      "Batch 17, GPU memory usage after forward pass:  31.9% (7735 out of 24220)\n",
      "Batch 17, GPU memory usage after backwards pass:  31.9% (7735 out of 24220)\n",
      "Batch 18, GPU memory usage after forward pass:  31.9% (7735 out of 24220)\n",
      "Batch 18, GPU memory usage after backwards pass:  31.9% (7735 out of 24220)\n",
      "Batch 19, GPU memory usage after forward pass:  31.9% (7735 out of 24220)\n",
      "Batch 19, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 20, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 20, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Batch 21, GPU memory usage after forward pass:  31.9% (7722 out of 24220)\n",
      "Batch 21, GPU memory usage after backwards pass:  31.9% (7722 out of 24220)\n",
      "Total time: 225.53737211227417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25767... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.17MB of 1.20MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.97695624530…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2</td></tr><tr><td>loss_avg_test</td><td>3.73249</td></tr><tr><td>loss_avg_train</td><td>3.3791</td></tr><tr><td>loss_avg_valid</td><td>2.79903</td></tr><tr><td>pp_logp_avg_test</td><td>-7.09083</td></tr><tr><td>pp_logp_avg_train</td><td>-6.327</td></tr><tr><td>pp_logp_avg_valid</td><td>-5.19485</td></tr><tr><td>reward_avg_test</td><td>0.53784</td></tr><tr><td>reward_avg_train</td><td>0.52753</td></tr><tr><td>reward_avg_valid</td><td>0.53924</td></tr><tr><td>rouge_score_avg_test</td><td>0.68822</td></tr><tr><td>rouge_score_avg_train</td><td>0.70549</td></tr><tr><td>rouge_score_avg_valid</td><td>0.74377</td></tr><tr><td>vm_score_avg_test</td><td>0.06434</td></tr><tr><td>vm_score_avg_train</td><td>0.0451</td></tr><tr><td>vm_score_avg_valid</td><td>0.04241</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 14 media file(s), 6 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">whole-salad-81</strong>: <a href=\"https://wandb.ai/uts_nlp/travis_attack/runs/1hd6rq5x\" target=\"_blank\">https://wandb.ai/uts_nlp/travis_attack/runs/1hd6rq5x</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211118_134206-1hd6rq5x/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(show_gpu(f' GPU memory usage after loading models:'))\n",
    "progress_bar = tqdm(range(n_train_steps))\n",
    "\n",
    "t0 = time.time()\n",
    "pp_model.zero_grad(set_to_none=zero_grad_with_none) \n",
    "for epoch in range(n_train_epochs): \n",
    "    logger.info(f\"Now on epoch {epoch} of {n_train_epochs}\")\n",
    "    if not pp_model.training: pp_model.train()\n",
    "    for i, data in enumerate(dl_train): \n",
    "        if i % 10 == 0 :   logging.info(f\"Now processing batch {i} out of {len(dl_train)}\")\n",
    "        training_step(data) \n",
    "            \n",
    "        # For debugging\n",
    "        # print_info_on_generated_text()\n",
    "        progress_bar.update(1)   \n",
    "    if wandb_log_grads and epoch % wandb_log_grads_freq == 0: \n",
    "        plt = plot_grad_flow(pp_model.named_parameters())\n",
    "        wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "        del plt \n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()\n",
    "    # Evaluation loop\n",
    "    if epoch % eval_freq == 0: \n",
    "        train_set_preds = eval_dl(dl = dl_train)\n",
    "        valid_set_preds = eval_dl(dl = dl_valid)\n",
    "#         if epoch == 3: \n",
    "#             set_trace()\n",
    "        # can't update the tables every epoch, so store them and do it at the end \n",
    "        update_training_summary_table(train_set_preds, split='train')\n",
    "        update_training_summary_table(valid_set_preds, split='valid')\n",
    "        update_wandb_table(train_set_preds, split='train')\n",
    "        update_wandb_table(valid_set_preds, split='valid')\n",
    "        #del train_set_preds\n",
    "        #del valid_set_preds\n",
    "            \n",
    "# Eval on test set \n",
    "test_set_preds = eval_dl(dl = dl_test)\n",
    "update_wandb_table(test_set_preds, split='test')\n",
    "# Log, plot, and finish up\n",
    "log_wandb_tables()\n",
    "plot_wandb_charts()\n",
    "add_wandb_run_summary_statistics()\n",
    "\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(\"Total time:\", total)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(60, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(60, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194304"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 * 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and debugging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verifying that the weights update each training step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check_parameters_update(dl)  # from utils script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code scraps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiments around plotting average parameter updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_parameter_group_dict(): \n",
    "#     \"\"\"Function to create \"groups\" of parameters. This is useful to check how much a group of \n",
    "#     parameters updates at an epoch. \n",
    "#     Parameter groups are hardcoded into this code for now. \n",
    "#     \"\"\"\n",
    "#     # Identify which parameters should be grouped together\n",
    "#     isolates = ['model.shared.weight',\"model.encoder.embed_positions.weight\", \"model.encoder.layer_norm\",\n",
    "#                 \"model.decoder.embed_positions.weight\", \"model.decoder.layer_norm\"]\n",
    "#     layers_base = [\"model.encoder.layers\", \"model.decoder.layers\"]\n",
    "#     def flatten_list(l): return list(np.concatenate(l).flat)\n",
    "#     layers = flatten_list([[lyr + \".\" + str(o) +\".\" for o in list(range(16))] for lyr in layers_base])\n",
    "#     parameter_groups = layers + isolates\n",
    "#     # Sort the parameter groups by the order they appear in the model \n",
    "#     all_params = [name for name,_ in pp_model.named_parameters()]\n",
    "#     ordering = [np.min(np.where([pg in o for o in all_params])) for pg in parameter_groups]\n",
    "#     parameter_groups = [o for _,o in sorted(zip(ordering, parameter_groups))]\n",
    "#     # Assign each model parameter a parameter group \n",
    "#     group_d = dict()\n",
    "#     for pg in parameter_groups: \n",
    "#         name = pg[:-1] if pg in layers else pg  # remove the \".\" from the end of the name for the numeric layers\n",
    "#         group_d[name] = [o for o in all_params if pg in o]\n",
    "#     return group_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_parameter_update_amount(): \n",
    "#     group_d = get_parameter_group_dict()\n",
    "#     params_all_initial_d = dict(params_all_initial)\n",
    "#     params_all_d = dict(params_all)\n",
    "#     group_d = get_parameter_group_dict()\n",
    "#     df_d = dict()\n",
    "#     for k,param_l in group_d.items(): \n",
    "#         l = list()\n",
    "#         for p in param_l: \n",
    "#             l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "#         l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "#         df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "#     df = pd.DataFrame(df_d)\n",
    "#     df.index = pd.DataFrame([1,2,3]).describe().index\n",
    "#     return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Random code snippets\n",
    "\n",
    "# initial_params = [(name, p.detach().clone()) for (name, p) in pp_model.named_parameters()]\n",
    "# loss, reward, pp_logp = training_step(data) \n",
    "# update_d =  dict()\n",
    "# for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#     update_d[name] = torch.abs(old_p - new_p).detach().flatten()     \n",
    "    \n",
    "#             update_d =  dict()\n",
    "#             for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#                 update_d[name] = torch.abs(old_p - new_p).flatten() \n",
    "#                 print (name, torch.norm(new_p - old_p).item())  \n",
    "            \n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             initial_params_d,current_params_d = dict(initial_params),dict()\n",
    "#             params_all_d = dict(params_all)\n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             df_d = dict()\n",
    "#             for k,param_l in group_d.items(): \n",
    "#                 l = list()\n",
    "#                 for p in param_l: \n",
    "#                     l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "#                 l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "#                 df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "#             df = pd.DataFrame(df_d)\n",
    "#             df.index = pd.DataFrame([1,2,3]).describe().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generating a paraphrase dataset and getting VM predictions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def create_paraphrase_dataset(batch, cname_input, cname_output, num_beams=32,\n",
    "#                               num_return_sequences=32): \n",
    "#     \"\"\"Create paraphrases for each example in the batch. Then repeat the other fields \n",
    "#         so that the resulting datase has the same length as the number of paraphrases. \n",
    "#         Key assumption is \n",
    "#         that the same number of paraphrases is created for each example.\n",
    "#         batch: a dict of examples used by the `map` function from the dataset\n",
    "#         cname_input: What column to create paraphrases of \n",
    "#         cname_output: What to call the column of paraphrases\n",
    "#         other parameters - passed to get_paraphrases. \"\"\"\n",
    "    \n",
    "#     # Generate paraphrases. \n",
    "#     # This can be later extended to add diversity or so on. \n",
    "#     #set_trace()\n",
    "#     pp_l,probs = get_paraphrases(batch[cname_input], num_beams=num_beams,\n",
    "#         num_return_sequences=num_return_sequences)\n",
    "    \n",
    "#     # To return paraphrases as a list of lists for batch input (not done here but might need later)\n",
    "#     #     split_into_sublists = lambda l,n: [l[i:i + n] for i in range(0, len(l), n)]\n",
    "#     #     pp_l = split_into_sublists(pp_l, n_seed_seqs)\n",
    "#     batch[cname_output] = pp_l \n",
    "#     batch[\"probs\"] = probs.to('cpu').numpy()\n",
    "    \n",
    "#     # Repeat each entry in all other columns `num_return_sequences` times so they are the same length\n",
    "#     # as the paraphrase column\n",
    "#     # Only works if the same number of paraphrases is generated for each phrase. \n",
    "#     # Else try something like \n",
    "#         # for o in zip(*batch.values()):\n",
    "#         #     d = dict(zip(batch.keys(), o))\n",
    "#         #     get_paraphrases(batch[cname_input],num_return_sequences=n_seed_seqs,num_beams=n_seed_seqs)\n",
    "#         #     for k,v in d.items(): \n",
    "#         #       return_d[k] += v if k == 'text' else [v for o in range(n_paraphrases)]\n",
    "#         # return return_d\n",
    "#     return_d = defaultdict(list) \n",
    "#     repeat_each_item_n_times = lambda l,n: [o for o in l for i in range(n)]\n",
    "#     for k in batch.keys(): \n",
    "#         if   k == cname_output: return_d[k] = batch[cname_output]\n",
    "#         elif k == \"probs\"     : return_d[k] = batch[\"probs\"]\n",
    "#         else:                   return_d[k] = repeat_each_item_n_times(batch[k], num_return_sequences)\n",
    "#     return return_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_vm_scores(ds_pp, cname_orig, cname_pp, cname_label='label', \n",
    "#                   use_metric=False, monitor=False): \n",
    "#     \"\"\"Get victim model preds+probs for the paraphrase dataset.\n",
    "#     \"\"\"\n",
    "#     assert vm_model.training == False  # checks that model is in eval mode \n",
    "#     if use_metric: \n",
    "#         metric_d = {}\n",
    "#         metric_d['orig'],metric_d['pp'] = load_metric('accuracy'),load_metric('accuracy')\n",
    "#     orig_probs_l,pp_probs_l = [],[]\n",
    "#     if monitor: monitor = Monitor(2)  # track GPU usage and memory\n",
    "    \n",
    "#     def get_vm_preds(x): \n",
    "#         \"\"\"Get predictions for a vector x (here a vector of documents/text). \n",
    "#         Works for a sentiment-analysis dataset (needs to be adjusted for NLI tasks)\"\"\"\n",
    "#         inputs = vm_tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         inputs.to(device)\n",
    "#         outputs = vm_model(**inputs, labels=labels)\n",
    "#         probs = outputs.logits.softmax(1).cpu()\n",
    "#         preds = probs.argmax(1)\n",
    "#         return probs, preds\n",
    "       \n",
    "#     print(\"Getting victim model predictions for both original and paraphrased text.\")\n",
    "#     dl = DataLoader(ds_pp, batch_size=batch_size, shuffle=False, \n",
    "#                     num_workers=n_wkrs, pin_memory=True)\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(dl): \n",
    "#             if i % 50 == 0 : print(\"Now processing batch\", i, \"out of\", len(dl))\n",
    "#             labels,orig,pp = data['label'].to(device),data[cname_orig],data[cname_pp]\n",
    "#             orig_probs, orig_preds = get_vm_preds(orig)            \n",
    "#             pp_probs,   pp_preds   = get_vm_preds(pp)    \n",
    "#             orig_probs_l.append(orig_probs); pp_probs_l.append(pp_probs)\n",
    "#             if use_metric: \n",
    "#                 metric_d['orig'].add_batch(predictions=orig_preds, references=labels)\n",
    "#                 metric_d['pp'].add_batch(  predictions=pp_preds,   references=labels)\n",
    "#     if monitor: monitor.stop()\n",
    "#     def list2tensor(l): return torch.cat(l)\n",
    "#     orig_probs_t,pp_probs_t = list2tensor(orig_probs_l),list2tensor(pp_probs_l)\n",
    "#     if use_metric: return orig_probs_t, pp_probs_t, metric_d\n",
    "#     else:          return orig_probs_t, pp_probs_t, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Generate paraphrase dataset\n",
    "# num_beams = 10\n",
    "# num_return_sequences = 3\n",
    "# cname_input = 'text' # which text column to paraphrase\n",
    "# cname_output= cname_input + '_pp'\n",
    "# date = '20210825'\n",
    "# fname = path_cache + '_rt_train'+ date + '_' + str(num_return_sequences)\n",
    "# if os.path.exists(fname):  \n",
    "#     ds_pp = datasets.load_from_disk(fname)\n",
    "# else:\n",
    "#     ds_pp = train.shard(200, 0, contiguous=True)\n",
    "#     # Have to call with batched=True\n",
    "#     # Need to set a batch size otherwise will run out of memory on the GPU card. \n",
    "#     # 64 seems to work well \n",
    "#     ds_pp = ds_pp.map(\n",
    "#         lambda x: create_paraphrase_dataset(x, \n",
    "#             num_beams=num_beams, num_return_sequences=num_return_sequences,\n",
    "#             cname_input=cname_input, cname_output=cname_output),\n",
    "#         batched=True, batch_size=4) \n",
    "#     ds_pp.save_to_disk(fname)\n",
    "#     gc.collect(); torch.cuda.empty_cache() # free up most of the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Get predictions\n",
    "# cname_orig = cname_input\n",
    "# cname_pp = cname_output\n",
    "# cname_label = 'label'\n",
    "# print_metric = True\n",
    "# fname = path_cache + 'results_df_'+ date + \"_\" + str(num_return_sequences) + \".csv\"\n",
    "# if os.path.exists(fname):    results_df = pd.read_csv(fname)\n",
    "# else: \n",
    "#     #sim_score_t = generate_sim_scores()\n",
    "#     orig_probs_t,pp_probs_t,metric_d = get_vm_scores(ds_pp, cname_orig, \n",
    "#                                                      cname_pp, cname_label,\n",
    "#                                                      monitor=True, use_metric=print_metric)\n",
    "#     if print_metric: \n",
    "#         print(\"orig vm accuracy:\",       metric_d['orig'].compute())\n",
    "#         print(\"paraphrase vm accuracy:\", metric_d['pp'].compute())\n",
    "#     vm_orig_scores  = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], orig_probs_t)])\n",
    "#     vm_pp_scores    = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], pp_probs_t)])\n",
    "#     results_df = pd.DataFrame({\n",
    "#                   cname_orig: ds_pp[cname_orig],\n",
    "#                   cname_pp: ds_pp[cname_pp],\n",
    "#    #               'sim_score': sim_score_t,\n",
    "#                   'label_true': ds_pp[cname_label], \n",
    "#                   'label_vm_orig': orig_probs_t.argmax(1),\n",
    "#                   'label_vm_pp': pp_probs_t.argmax(1),\n",
    "#                   'vm_orig_truelabel': vm_orig_scores,             \n",
    "#                   'vm_pp_truelabel': vm_pp_scores,\n",
    "#                   'vm_truelabel_change': vm_orig_scores - vm_pp_scores,\n",
    "#                   'vm_orig_class0': orig_probs_t[:,0], \n",
    "#                   'vm_orig_class1': orig_probs_t[:,1], \n",
    "#                   'vm_pp_class0': pp_probs_t[:,0], \n",
    "#                   'vm_pp_class1': pp_probs_t[:,1], \n",
    "#                   })\n",
    "# #    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "#     results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing how to keep gradients with `generate` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Testing the `generate_with_grad` function\n",
    "\n",
    "# input_text=\"hello my name is Tom\"\n",
    "# num_return_sequences=1\n",
    "# num_beams=2\n",
    "# return_probs=True\n",
    "# batch = pp_tokenizer(input_text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "# generated = pp_model.generate_with_grad(**batch, return_dict_in_generate=True, output_scores=True,\n",
    "#                               num_return_sequences=num_return_sequences,\n",
    "#                                 num_beams=num_beams,\n",
    "#                                 num_beam_groups=1,\n",
    "#                                 diversity_penalty=0,\n",
    "#                                 temperature=1.5, \n",
    "#                               length_penalty=1)\n",
    "# print(generated)\n",
    "\n",
    "# tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)\n",
    "# print(pp_tokenizer.tokenize(tgt_text[0]))\n",
    "# print(pp_tokenizer.encode(tgt_text[0]))\n",
    "\n",
    "# # Score: score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "# # gradient gets removed (i think) by the line \n",
    "# # beam_hyp.add(\n",
    "# #   input_ids[batch_beam_idx].clone(),\n",
    "# #   next_score.item())\n",
    "\n",
    "\n",
    "# x=generated['scores'][5]\n",
    "# print(x.max(1))\n",
    "# x.max(1).values / (len(generated['scores']) ** 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## An example of how to use greedy_search\n",
    "\n",
    "# from transformers import (\n",
    "# AutoTokenizer,\n",
    "# AutoModelForCausalLM,\n",
    "# LogitsProcessorList,\n",
    "# MinLengthLogitsProcessor,\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# input_prompt = \"Today is a beautiful day, and\"\n",
    "# input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # instantiate logits processors\n",
    "# logits_processor = LogitsProcessorList([\n",
    "#     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "# ])\n",
    "\n",
    "# outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
    "\n",
    "# print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tensorboard setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import datetime \n",
    "# # Create writer and track to run directory \n",
    "# path_runs = './runs/'\n",
    "# log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = SummaryWriter(log_dir = log_dir)\n",
    "# # stuff here logging to tensorboard\n",
    "# #writer.close() # important otherwise Tensorboard eventually shuts down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### WandB artifact tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# log predictions table to wandb, giving it a name\n",
    "#  train_table_artifact = wandb.Artifact(\"train_samples_\" + str(wandb.run.id), type=\"predictions\")\n",
    "#  valid_table_artifact = wandb.Artifact(\"test_samples_\"   + str(wandb.run.id), type=\"predictions\")\n",
    "#train_table_artifact.add(train_table, \"predictions\")\n",
    "#valid_table_artifact.add(valid_table, \"predictions\")\n",
    "#wandb.run.log_artifact(train_table_artifact) \n",
    "#wandb.run.log_artifact(valid_table_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A no_grad version of `model.generate()` adapted from transformers v4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Union\n",
    "# from transformers.generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "# import torch\n",
    "\n",
    "\n",
    "# def generate_with_grad(\n",
    "#     self,\n",
    "#     input_ids: Optional[torch.LongTensor] = None,\n",
    "#     max_length: Optional[int] = None,\n",
    "#     min_length: Optional[int] = None,\n",
    "#     do_sample: Optional[bool] = None,\n",
    "#     early_stopping: Optional[bool] = None,\n",
    "#     num_beams: Optional[int] = None,\n",
    "#     temperature: Optional[float] = None,\n",
    "#     top_k: Optional[int] = None,\n",
    "#     top_p: Optional[float] = None,\n",
    "#     repetition_penalty: Optional[float] = None,\n",
    "#     bad_words_ids: Optional[Iterable[int]] = None,\n",
    "#     bos_token_id: Optional[int] = None,\n",
    "#     pad_token_id: Optional[int] = None,\n",
    "#     eos_token_id: Optional[int] = None,\n",
    "#     length_penalty: Optional[float] = None,\n",
    "#     no_repeat_ngram_size: Optional[int] = None,\n",
    "#     encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "#     num_return_sequences: Optional[int] = None,\n",
    "#     max_time: Optional[float] = None,\n",
    "#     decoder_start_token_id: Optional[int] = None,\n",
    "#     use_cache: Optional[bool] = None,\n",
    "#     num_beam_groups: Optional[int] = None,\n",
    "#     diversity_penalty: Optional[float] = None,\n",
    "#     prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "#     output_attentions: Optional[bool] = None,\n",
    "#     output_hidden_states: Optional[bool] = None,\n",
    "#     output_scores: Optional[bool] = None,\n",
    "#     return_dict_in_generate: Optional[bool] = None,\n",
    "#     forced_bos_token_id: Optional[int] = None,\n",
    "#     forced_eos_token_id: Optional[int] = None,\n",
    "#     remove_invalid_values: Optional[bool] = None,\n",
    "#     **model_kwargs):\n",
    "#     # set init values\n",
    "#     num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "#     num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
    "#     max_length = max_length if max_length is not None else self.config.max_length\n",
    "#     do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "#     num_return_sequences = (\n",
    "#         num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "#     )\n",
    "\n",
    "#     pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "#     bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "#     eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "#     output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "#     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "#     output_hidden_states = (\n",
    "#         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "#     )\n",
    "#     return_dict_in_generate = (\n",
    "#         return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "#     )\n",
    "\n",
    "#     model_kwargs[\"output_attentions\"] = output_attentions\n",
    "#     model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "#     if input_ids is None:\n",
    "#         # init `input_ids` with bos_token_id\n",
    "#         input_ids = self._prepare_input_ids_for_generation(bos_token_id, model_kwargs.get(\"encoder_outputs\"))\n",
    "\n",
    "#     if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "#         # init `attention_mask` depending on `pad_token_id`\n",
    "#         model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "#             input_ids, pad_token_id, eos_token_id\n",
    "#         )\n",
    "\n",
    "#     # special case if pad_token_id is not defined\n",
    "#     if pad_token_id is None and eos_token_id is not None:\n",
    "#         logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "#         pad_token_id = eos_token_id\n",
    "\n",
    "#     # Storing encoder_input_ids for logits_processor that could use them\n",
    "#     encoder_input_ids = input_ids if self.config.is_encoder_decoder else None\n",
    "\n",
    "#     if self.config.is_encoder_decoder:\n",
    "#         # add encoder_outputs to model_kwargs\n",
    "#         model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "#         # set input_ids as decoder_input_ids\n",
    "#         if \"decoder_input_ids\" in model_kwargs:\n",
    "#             input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "#         else:\n",
    "#             input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "#                 input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "#             )\n",
    "\n",
    "# #         if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "# #             raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "#     if input_ids.shape[-1] >= max_length:\n",
    "#         input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "#         logger.warning(\n",
    "#             f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "#             \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "#         )\n",
    "\n",
    "#     # determine generation mode\n",
    "#     is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#     is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#     is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#     is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#     is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "#     if num_beam_groups > num_beams:\n",
    "#         raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "#     if is_group_beam_gen_mode and do_sample is True:\n",
    "#         raise ValueError(\n",
    "#             \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "#         )\n",
    "\n",
    "#     # set model_kwargs\n",
    "#     model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "#     # get distribution pre_processing samplers\n",
    "#     logits_processor = self._get_logits_processor(\n",
    "#         repetition_penalty=repetition_penalty,\n",
    "#         no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "#         encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "#         encoder_input_ids=encoder_input_ids,\n",
    "#         bad_words_ids=bad_words_ids,\n",
    "#         min_length=min_length,\n",
    "#         max_length=max_length,\n",
    "#         eos_token_id=eos_token_id,\n",
    "#         forced_bos_token_id=forced_bos_token_id,\n",
    "#         forced_eos_token_id=forced_eos_token_id,\n",
    "#         prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "#         num_beams=num_beams,\n",
    "#         num_beam_groups=num_beam_groups,\n",
    "#         diversity_penalty=diversity_penalty,\n",
    "#         remove_invalid_values=remove_invalid_values,\n",
    "#     )\n",
    "\n",
    "#     stopping_criteria = self._get_stopping_criteria(\n",
    "#         max_length=max_length,\n",
    "#         max_time=max_time,\n",
    "#     )\n",
    "\n",
    "#     if is_greedy_gen_mode:\n",
    "#         if num_return_sequences > 1:\n",
    "#             raise ValueError(\n",
    "#                 f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "#             )\n",
    "\n",
    "#         # greedy search\n",
    "#         return self.greedy_search(\n",
    "#             input_ids,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_sample_gen_mode:\n",
    "#         # get probability distribution warper\n",
    "#         logits_warper = self._get_logits_warper(\n",
    "#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#         )\n",
    "\n",
    "#         # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids,\n",
    "#             expand_size=num_return_sequences,\n",
    "#             is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#         # sample\n",
    "#         return self.sample(\n",
    "#             input_ids,\n",
    "#             logits_processor=logits_processor,\n",
    "#             logits_warper=logits_warper,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_beam_gen_mode:\n",
    "#         batch_size = input_ids.shape[0]\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "#         if num_return_sequences > num_beams:\n",
    "#             raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#         beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#             num_beam_hyps_to_keep=num_return_sequences,\n",
    "#         )\n",
    "#         # interleave with `num_beams`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "#         )\n",
    "#         #with torchsnooper.snoop(depth=4, max_variable_length=200, normalize=True):\n",
    "#         return self.beam_search(\n",
    "#             input_ids,\n",
    "#             beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_beam_sample_gen_mode:\n",
    "#         logits_warper = self._get_logits_warper(\n",
    "#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#         )\n",
    "\n",
    "#         batch_size = input_ids.shape[0] * num_return_sequences\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#         )\n",
    "\n",
    "#         # interleave with `num_beams * num_return_sequences`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids,\n",
    "#             expand_size=num_beams * num_return_sequences,\n",
    "#             is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#         return self.beam_sample(\n",
    "#             input_ids,\n",
    "#             beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             logits_warper=logits_warper,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_group_beam_gen_mode:\n",
    "#         batch_size = input_ids.shape[0]\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "#         if num_return_sequences > num_beams:\n",
    "#             raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#         if num_beams % num_beam_groups != 0:\n",
    "#             raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "#         diverse_beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#             num_beam_hyps_to_keep=num_return_sequences,\n",
    "#             num_beam_groups=num_beam_groups,\n",
    "#         )\n",
    "#         # interleave with `num_beams`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "#         )\n",
    "#         return self.group_beam_search(\n",
    "#             input_ids,\n",
    "#             diverse_beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old eval/wandb functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_eval_metrics(preds, split):\n",
    "#     ### Might be obselete ###\n",
    "#     print(f\"{split} paraphrases:\", preds['pp_l'])\n",
    "#     print(f\"{split} VM scores:\",    np.round(preds['vm_score'],3))\n",
    "#     print(f\"{split} ROUGE scores:\", np.round(preds['rouge_score'],3))\n",
    "#     if normalise_rewards: print(f\"{split} unnormalised rewards:\", preds['orig_reward'])\n",
    "#     print(f\"{split} rewards:\", round_t(preds['reward'], 3))\n",
    "#     print(f\"{split} avg reward:\", torch.mean(preds['reward']).item())\n",
    "#     print(f\"{split} logp:\", round_t(preds['pp_logp'], 3))\n",
    "#     print(f\"{split} avg logp:\", torch.mean(preds['pp_logp']).item())\n",
    "#     print(f\"{split} loss:\", train_set_preds['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_wandb_metrics(results_d, split): \n",
    "#     \"\"\"flattens lists of metrics to wandb acceptable form. obselete now but might be useful later\"\"\"\n",
    "#     #### MIGHT BE OBSELETE #####\n",
    "#     # Log numeric data \n",
    "#     # Convert all lists of values to wandb format. Scalars are unchanged\n",
    "#     d = dict()\n",
    "#     orig_keys = results_d.keys()\n",
    "#     for k,v in results_d.items(): \n",
    "#         if type(v) is list: \n",
    "#             if type(v[0]) == int or type(v[0]) == float:  # we handle strings differently\n",
    "#                 d1 = {f\"eval/{split}/examples/{k}/{i}\": o for i,o in enumerate(v)}  # list -> dict of len(v) scalars\n",
    "#                 d = {**d, **d1}  # merge dicts\n",
    "#         else: \n",
    "#             d[f\"{split}/{k}\"] = v\n",
    "#     d['epoch'] = epoch\n",
    "#     wandb.log(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "### An attempt to rename wandb columns of a table. This seemed to break something internally inside the table.\n",
    "#\n",
    "#     def rename_wandb_column(table, old, new): \n",
    "#         if old not in table.columns: \n",
    "#             warnings.warn(f\"{old} not in columns of table. Skipping. Columns of table are {table.columns}\")\n",
    "#         else: \n",
    "#             idx = [i for i,o in enumerate(table.columns) if o == old][0]\n",
    "#             table.columns[idx] = new\n",
    "        \n",
    "    # Can't just originally name these 'orig' and 'pp' because they don't match key names used in `eval_dl`\n",
    "    # NOTE: this seems to break the internals of wandb. might have to just live with the names. \n",
    "#     rename_wandb_column(train_table, old='orig_l', new='orig')\n",
    "#     rename_wandb_column(train_table, old='pp_l',   new='pp')\n",
    "#     rename_wandb_column(valid_table,  old='orig_l', new='orig')\n",
    "#     rename_wandb_column(valid_table,  old='pp_l',   new='pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-fdbf7307be58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'translated' is not defined"
     ]
    }
   ],
   "source": [
    "torch.stack(translated.scores, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
