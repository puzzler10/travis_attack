{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Here we are trying to adjust parameters of a paraphrase model to generate adversarial examples. \n",
    "\n",
    "### Policy gradients \n",
    "The key parameter update equation is $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta R(\\theta)$, where $\\alpha$ is a step size parameter, the parameter vector $\\theta$ is for a model (here a paraphrase model), and $J$ is a loss function. The time step $t$ depends on the problem specification and we will get to it later. \n",
    "\n",
    "Now in my review I have defined the loss function $J(\\theta) = E_\\pi[r(\\tau)]$. Here: \n",
    "* $\\pi$ is the policy, a probability distribution for the next action in a given state; essentially $p(a_t|s_t)$\n",
    "* $\\tau$ is a trajectory, a specific sequence $s_0, a_0, r_1, s_1, a_1, \\ldots$ of the agent in the game. This starts at time $t=0$ and finishes at time $t=T$. \n",
    "* $r(\\tau)$ is the sum of rewards for a trajectory $\\tau$, or in other words, the total reward for the trajectory. \n",
    "\n",
    "This is a strange loss function because higher values are better. We might have to invert it at some point. \n",
    "\n",
    "To update parameters we must find the gradient $\\nabla_\\theta J(\\theta)$, which measures how $J(\\theta)$ changes when we adjust the parameters of the paraphrase model. The gradient is simplified through some maths to get the policy gradient theorem $$ \\nabla_\\theta J(\\theta) =  \\nabla_\\theta E_\\pi [r(\\tau)]  = E_\\pi \\left[r(\\tau) \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a_t|s_t)  \\right] $$ \n",
    "\n",
    "To calculate this you need to calculate the expectation term, which in turn means evaluating every possible trajectory $\\tau$ and its expected return. Generally this is not possible and instead we turn to estimators.  \n",
    "\n",
    "One of these is REINFORCE. It gives us  $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ where \n",
    "* $G_t$ is the discounted return and is given by $G_t = r_t + \\beta r_{t-1} + \\beta^2 r_{t-2} + \\dots$. It's a rough estimate of $r(\\tau)$. Rewards obtained later in the episode are weighted much higher than rewards obtained earlier. I guess it assumes that the parameters update every timestep. \n",
    "* $S$ is some number of samples.\n",
    "\n",
    "The implementation of REINFORCE and similar estimators depends on how we formulate the problem. Below we present some possible formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation One: Document-level  \n",
    "This is the first implementation we will try. \n",
    "\n",
    "Here we generate a list of paraphrases at each time point. The idea is that there is one paraphrase amongst them that is a good adversarial example. We try to tune the model to produce the best one. \n",
    "\n",
    "This interpretation sees forming the complete paraphrase as one time step. So it isn't token-level but document-level. \n",
    "\n",
    "* Starting state: $s0 = x$, the original example  \n",
    "* Actions: each action is \"choosing\" a paraphrase (or of choosing $n$ paraphrases. The set of all possible paraphrases and their probabilities is the policy. So $\\pi(a|s) = p(x'| x;\\theta)$ where $x'$ is the paraphrase. \n",
    "    * for a given paraphrase we can get the probabilities of generating each token in turn, then multiply them together to get some kind of \"probability\" of the paraphrase. \n",
    "    * for a list of paraphrases, we can multiply together the \"probability\" of each one to get the probability of obtaining the list (multiplying also by nCr (for r paraphrases out of n total) to account for the lack of order in the list)\n",
    "* Reward: The paraphrase moves through the reward function $R(x, x')$) to get the reward $r$. \n",
    "* Time steps: We only have one time step in the game ($T=1$ and $G_t=r$)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation 2: Token-level\n",
    "This interpretation is at token-level; it sees choosing the next word as the next time step. \n",
    "\n",
    "* Starting state: $s0 = x$, the initial state. But you also have a \"blank slate\" for the paraphrase. So maybe it's a tuple (x, pp) where pp is a paraphrase with no words. Here x is used as the reference for the paraphrase generator.  \n",
    "* Actions: Choose the next word of p. I guess this starts with the \\<START\\> token (or something similar). Then you have the policy $\\pi(a|s)$ which is the same as $p(w_{next}|pp, x; \\theta)$ where $\\theta$ is the paraphrase model parameters, $pp$ is the so-far constructed sentence, and $w_{next}$ is the next token (I say token because I don't know if this model is on the subword or word basis). \n",
    "* Time steps: every token is generated one-by-one and each of these is allocated a time step. This means probably that you also update the parameters after each token generated too. \n",
    "* Reward. The reward is allocated every token. There are many reward functions (see papers on token-level loss functions). Some also incorporate document-level rewards too. \n",
    "* Next state. $s_1$ is again the tuple $(x, pp)$ but now $pp$ has the first word in it. \n",
    "\n",
    "On *teacher forcing*. This is when you have a ground-truth paraphrase and you can use it when generating tokens. This is useful because if the model makes a mistake it doesn't continue down that track but is adjusted back. This stops big divergences (but also might limit the diversity of generated paraphrases). This is used when training a paraphrase model. You have a set of reference paraphrases that are human provided. Here though we only have the original sentence and no references. We could generate adversarial examples and use that to do teacher forcing. Generating them using textattack recipes might work. This is only really used on the token-level rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, load models + datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets, transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pprint import pprint\n",
    "import numpy as np, pandas as pd\n",
    "import scipy\n",
    "from utils import *   # local script \n",
    "import pyarrow\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import seaborn as sns\n",
    "from itertools import repeat\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "\n",
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size = 64\n",
    "pd.set_option(\"display.max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase model (para)\n",
    "para_name = \"tuner007/pegasus_paraphrase\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9c411f7ecd9f3045389de0d9ce984061a1056507703d2e3183b1ac1a90816e4d)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "\n",
    "label_cname = 'label'\n",
    "## For snli\n",
    "# remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "# train = train.filter(remove_minus1_labels)\n",
    "# valid = valid.filter(remove_minus1_labels)\n",
    "# test = test.filter(remove_minus1_labels)\n",
    "\n",
    "# make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "assert train.features[label_cname].num_classes == vm_num_labels\n",
    "assert valid.features[label_cname].num_classes == vm_num_labels\n",
    "assert test.features[ label_cname].num_classes == vm_num_labels\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "* use training dataset\n",
    "* sentiment analysis\n",
    "\n",
    "**Training loop** \n",
    "* get batch (e.g. 16 examples) which we call batch_orig\n",
    "* get paraphrases for each example in batch to make new batch (batch_pp)\n",
    "    * more efficient if we have bigger batches\n",
    "    * start with k=2 paraphrases for now (so the code can handle the multi-paraphrase case) but we will try with 1 and with a few as well. \n",
    "    * will have to later play around with diversity parameters (or maybe they can also be learned with rl too)\n",
    "* get reward \n",
    "    * the *reward function* $R$ takes in k rows of batch_pp, which corresponds to one example of batch_orig\n",
    "    * here are the formulas. $x'$ means paraphrase, $f(x)_y$ means the model confidence of x for the class of the true label $y$, $SS(a,b)$ is the result of a semantic similarity model run over $a$ and $b$, and $\\lambda$ is a hyperparameter that we'll probably have to tune in reward-shaping style.  \n",
    "    * for k=1: $R = f(x)_y - f(x')_y + \\lambda SS(x, x')$\n",
    "    * for k>1 (e.g. 3): some thought needed. ideas: \n",
    "        * only look at best performing paraphrase $x'_m$. find it by $x'_m = \\max_i [f(x)_y - f(x'_i)_y]$, then return $R [f(x)_y - f(x'_m)_y] + \\lambda SS(x,x')$ \n",
    "        * take average of each: $\\frac{1}{k} \\sum_{i=1}^k \\left[ f(x)_y - f(x'_i)_y + \\lambda SS(x, x'_i) \\right]$\n",
    "* update parameters of $f_{x'}$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's map our adversarial problem to the language of reinforcement learning. We start off with an example $x$, and we then generate a paraphrase $x'$ using a paraphrase model $f_p$ with parameters $\\theta$. It seems safe to say that the starting state $s_0$ is represented by $x$, so that $s_0 = x$. The action is then to generate paraphrases. Under this view the policy $\\pi(a|s)$ really only has one option - the \"generate paraphrase\" option - and so this probability distribution is really just 1 on \"generate paraphrase\" and 0 everywhere else. We then obtain a list of paraphrases $l_{x'}$ , which seems to be ordered from \"best to worst\" according to the paraphrase model. The chance of getting a particular paraphrase $p(x'|x; \\theta)$ is dependent on $\\theta$. Under the standard RL model this probability is given by the \"environment\" and is usually hard to model. It is similar to $p(r_{t+1}|s_t,a_t)$ which is the \"environment\" factor in my literature review. \n",
    "#We now tick over to $t=1$ and put $l_{x'}$ into the reward function and obtain a reward $r$. (We tackle reward function design elsewhere). The next state is $s_1= l_{x'}$. It would seem natural to continue the game and generate paraphrases of paraphrases and so on, but we stop here. The trajectory is then $\\tau = x, \\text{gen_x'}, r, l_{x'}, \\dots$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks \n",
    "* ~~clean up text above~~\n",
    "    * ~~Second edit that makes clear the scenario that is being implemented.~~\n",
    "* clean up a bunch of commented code \n",
    "* write out some scenarios \n",
    "    * best-paraphrase reward from set \n",
    "    * avg-paraphrase reward from set \n",
    "    * one paraphrase reward \n",
    "* write reward function\n",
    "* checkpoint: reward for one set of paraphrases \n",
    "* add to git\n",
    "* implement reinforce \n",
    "* ~~make a wiki entry for \"teacher forcing\"~~\n",
    "* merge `get_paraphrases` and `get_paraphrases_and_probs`\n",
    "* run through Trainer tutorial at: https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paraphrases_and_probs(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "    batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                               temperature=1.5, \n",
    "                                 num_beam_groups=num_beam_groups, \n",
    "                                diversity_penalty=diversity_penalty,\n",
    "                                 return_dict_in_generate=True, output_scores=True)\n",
    "    # Sequence scores won't add to 1 across the generated paraphrases, so here we normalise them. \n",
    "    # We also need to take exp for them to work. \n",
    "    seq_probs = torch.exp(translated.sequences_scores) / sum(torch.exp(translated.sequences_scores))\n",
    "    tgt_text = para_tokenizer.batch_decode(translated.sequences, skip_special_tokens=True)\n",
    "    return tgt_text, seq_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Note that some asteroids (the ones behind the asteroids marked 1, 5, and 7) won't have a chance to be vaporized until the next full rotation.\"\n",
    "num_beam_groups = 10\n",
    "num_beams=20\n",
    "num_return_sequences = 20\n",
    "diversity_penalty=1.\n",
    "para, seq_probs = get_paraphrases_and_probs(input_text,num_return_sequences,num_beams,\n",
    "                                            num_beam_groups, diversity_penalty)\n",
    "\n",
    "# batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "# translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "#                                temperature=1.5, \n",
    "#                                  num_beam_groups=num_beam_groups, \n",
    "#                                  do_sample=True,\n",
    "#                                 diversity_penalty=diversity_penalty,\n",
    "#                                  return_dict_in_generate=True, output_scores=True)\n",
    "# seq_probs = torch.exp(translated.sequences_scores)\n",
    "# tgt_text = para_tokenizer.batch_decode(translated.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     28
    ]
   },
   "outputs": [],
   "source": [
    "# Precompute paraphrases for the training set and store them\n",
    "def get_paraphrases(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "    batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                                   temperature=1.5, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty)\n",
    "    tgt_text = para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "# def gen_dataset_paraphrases(x, cname_input, cname_output, n_seed_seqs=32): \n",
    "#     \"\"\" x: one row of a dataset. \n",
    "#     cname_input: column to generate paraphrases for \n",
    "#     cname_output: column name to give output of paraphrases \n",
    "#     n_seed_seqs: rough indicator of how many paraphrases to return. \n",
    "#             For now, keep at 4,8,16,32,64 etc\"\"\"\n",
    "#     # TODO: figure out how to batch this. \n",
    "#     if n_seed_seqs % 4 != 0: raise ValueError(\"keep n_seed_seqs divisible by 4 for now\")\n",
    "#     n = n_seed_seqs/2\n",
    "#     #low diversity (ld) paraphrases \n",
    "#     ld_l = get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "#                             num_beams=int(n))\n",
    "#     #high diversity (hd) paraphrases. We can use num_beam_groups and diversity_penalty as hyperparameters. \n",
    "#     hd_l =  get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "#                             num_beams=int(n), num_beam_groups=int(n),diversity_penalty=50002.5)\n",
    "#     l = ld_l + hd_l \n",
    "#     x[cname_output] = l #TODO: change to list(set(l))             \n",
    "#     return x \n",
    "\n",
    "\n",
    "def create_paraphrase_dataset(batch, cname_input, cname_output, n_seed_seqs=32): \n",
    "    \"\"\"Create `n_seed_seq` paraphrases for each example in the batch. Then repeat the other fields \n",
    "        so that the resulting datase has the same length as the number of paraphrases. Key assumption is \n",
    "        that the same number of paraphrases is created for each example.\n",
    "    batch: a dict of examples used by the `map` function from the dataset\n",
    "    cname_input: What column to create paraphrases of \n",
    "    cname_output: What to call the column of paraphrases\n",
    "    n_seed_seqs: Number of paraphrases to generate. \"\"\"\n",
    "    \n",
    "    # Generate paraphrases. \n",
    "    # This can be later extended to add diversity or so on. \n",
    "    para_l = get_paraphrases(batch[cname_input], n_seed_seqs, n_seed_seqs)\n",
    "    \n",
    "    # To return paraphrases as a list of lists for batch input (not done here but might need later)\n",
    "    #     split_into_sublists = lambda l,n: [l[i:i + n] for i in range(0, len(l), n)]\n",
    "    #     para_l = split_into_sublists(para_l, n_seed_seqs)\n",
    "    batch[cname_output] = para_l \n",
    "    \n",
    "    # Repeat each entry in all other columns `n_seed_seq` times so they are the same length\n",
    "    # as the paraphrase column\n",
    "    # Only works if the same number of paraphrases is generated for each phrase. \n",
    "    # Else try something like \n",
    "        # for o in zip(*batch.values()):\n",
    "        #     d = dict(zip(batch.keys(), o))\n",
    "        #     get_paraphrases(batch[cname_input],num_return_sequences=n_seed_seqs,num_beams=n_seed_seqs)\n",
    "        #     for k,v in d.items(): \n",
    "        #       return_d[k] += v if k == 'text' else [v for o in range(n_paraphrases)]\n",
    "        # return return_d\n",
    "    return_d = defaultdict(list) \n",
    "    repeat_each_item_n_times = lambda l,n: [o for o in l for i in range(n)]\n",
    "    for k in batch.keys(): \n",
    "        if   k == cname_output: return_d[k] = batch[cname_output]\n",
    "        else:                   return_d[k] = repeat_each_item_n_times(batch[k], n_seed_seqs)\n",
    "    return return_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate paraphrase dataset\n",
    "n_seed_seqs = 3\n",
    "cname_input = 'text' # which text column to paraphrase\n",
    "cname_output= cname_input + '_pphrases'\n",
    "date = '20210802'\n",
    "fname = path_cache + '_rt_train'+ date + '_' + str(n_seed_seqs)\n",
    "if os.path.exists(fname):  \n",
    "    ds_pphrases = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    ds_pphrases = train.shard(1, 0, contiguous=True)\n",
    "    # Have to call with batched=True\n",
    "    # Need to set a batch size otherwise will run out of memory on the GPU card. \n",
    "    # 64 seems to work well \n",
    "    ds_pphrases = ds_pphrases.map(\n",
    "        lambda x: create_paraphrase_dataset(x, n_seed_seqs=n_seed_seqs,\n",
    "            cname_input=cname_input, cname_output=cname_output),\n",
    "        batched=True, batch_size=64) \n",
    "    ds_pphrases.save_to_disk(fname)\n",
    "    gc.collect(); torch.cuda.empty_cache() # free up most of the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get model results for the pphrases\n",
    "def get_vm_scores(ds_pphrases, cname_orig, cname_pphrase, cname_label='label'): \n",
    "    \"\"\"Get victim model preds/probs for  \"\"\"\n",
    "    # Get preds and accuracy on the pphrase dataset\n",
    "    print(\"Getting victim model scores.\")\n",
    "    dl = DataLoader(ds_pphrases, batch_size=batch_size, shuffle=False, \n",
    "                    num_workers=n_wkrs, pin_memory=True)\n",
    "  #  metric = load_metric('accuracy')\n",
    "    pphrase_probs_l,orig_probs_l = [],[]\n",
    "    assert vm_model.training == False  # checks that model is in eval mode \n",
    "    #monitor = Monitor(2)  # track GPU usage and memory\n",
    "    \n",
    "    def get_vm_preds(x): \n",
    "        \"\"\"Get predictions for a vector x (here a vector of documents/text). \n",
    "        Works for a sentiment-analysis dataset (needs to be adjusted for NLI tasks)\"\"\"\n",
    "        inputs = vm_tokenizer(x, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        outputs = vm_model(**inputs, labels=labels)\n",
    "        probs = outputs.logits.softmax(1).cpu()\n",
    "        preds = probs.argmax(1)\n",
    "        return probs, preds\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dl): \n",
    "            if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "            labels,orig,pphrases = data['label'].to(device),data[cname_orig],data[cname_pphrase]\n",
    "            \n",
    "            # predictions for original\n",
    "            orig_probs, orig_preds = get_vm_preds(orig)            \n",
    "            orig_probs_l.append(orig_probs)\n",
    "            \n",
    "            # predictions for pphrase\n",
    "            pphrase_probs, pphrase_preds = get_vm_preds(pphrases)            \n",
    "            pphrase_probs_l.append(pphrase_probs)\n",
    "          #  metric.add_batch(predictions=pphrase_preds, references=labels)\n",
    "\n",
    "    # convert lists to tensor\n",
    "    orig_probs_t, pphrase_probs_t = torch.cat(orig_probs_l),torch.cat(pphrase_probs_l)\n",
    "    #monitor.stop()\n",
    "    return orig_probs_t, pphrase_probs_t\n",
    "\n",
    "cname_orig = cname_input\n",
    "cname_pphrase = cname_output\n",
    "cname_label = 'label'\n",
    "fname = path_cache + 'results_df_'+ date + \"_\" + str(n_seed_seqs) + \".csv\"\n",
    "if os.path.exists(fname):    results_df = pd.read_csv(fname)\n",
    "else: \n",
    "    #sim_score_t = generate_sim_scores()\n",
    "    orig_probs_t,pphrase_probs_t = get_vm_scores(ds_pphrases, cname_orig, cname_pphrase, cname_label)\n",
    "    vm_orig_scores    = torch.tensor([r[idx] for idx,r in zip(ds_pphrases[cname_label], orig_probs_t   )])\n",
    "    vm_pphrase_scores = torch.tensor([r[idx] for idx,r in zip(ds_pphrases[cname_label], pphrase_probs_t)])\n",
    "    results_df = pd.DataFrame({\n",
    "                  cname_orig: ds_pphrases[cname_orig],\n",
    "                  cname_pphrase: ds_pphrases[cname_pphrase],\n",
    "   #               'sim_score': sim_score_t,\n",
    "                  'label_true': ds_pphrases[cname_label], \n",
    "                  'label_vm_orig': orig_probs_t.argmax(1),\n",
    "                  'label_vm_pphrase': pphrase_probs_t.argmax(1),\n",
    "                  'vm_orig_truelabel': vm_orig_scores,             \n",
    "                  'vm_pphrase_truelabel': vm_pphrase_scores,\n",
    "                  'vm_truelabel_change': vm_orig_scores - vm_pphrase_scores,\n",
    "                  'vm_orig_class0': orig_probs_t[:,0], \n",
    "                  'vm_orig_class1': orig_probs_t[:,1], \n",
    "   #               'vm_orig_class2': orig_probs_t[:,2],  \n",
    "                  'vm_pphrase_class0': pphrase_probs_t[:,0], \n",
    "                  'vm_pphrase_class1': pphrase_probs_t[:,1], \n",
    "#                  'vm_pphrase_class2': pphrase_probs_t[:,2]     \n",
    "                  })\n",
    "#    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "    results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn_onerow(x): \n",
    "    \"\"\"x is one row of a pandas df\"\"\"\n",
    "    text,pp,lbl_change = x['text'],x['text_pphrases'],x['vm_truelabel_change']\n",
    "    return lbl_change\n",
    "\n",
    "def reward_fn_batch(): \n",
    "    pass \n",
    "\n",
    "def loss_fn(): \n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make a function that calculates loss \n",
    "    * you will have to \"shape\" the reward from this function, which means you will have to try out a number of different things \n",
    "    * for now, try $ |f(p1) - y|^2 + |f(p2) - y|^2 + |f(p3) - y|^2 $ where $f(p1)$ is model confidence for class y. \n",
    "        * this is tricky because the reward isn't calculated over one datapoint any longer but rather a few of them. in addition you have two datasets: the original and the paraphrase. \n",
    "        * maybe the best is to compute the paraphrases inside the reward function. or compute them before but just store them, and then reference it in the reward function. \n",
    "    * later you can add various terms, e.g. BERTScore, or a term for fluency, or the semantic similarity component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
