{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we are trying to adjust parameters of a paraphrase model to generate adversarial examples. \n",
    "### Policy gradients \n",
    "The key parameter update equation is $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)$, where $\\alpha$ is a step size parameter, the parameter vector $\\theta$ is for a model (here a paraphrase model), and $J$ is a loss function. The time step $t$ depends on the problem specification and we will get to it later. \n",
    "\n",
    "Now in my review I have defined the loss function $J(\\theta) = E_\\pi[r(\\tau)]$. Here: \n",
    "* $\\pi$ is the policy, a probability distribution for the next action in a given state; essentially $p(a_t|s_t)$\n",
    "* $\\tau$ is a trajectory, a specific sequence $s_0, a_0, r_1, s_1, a_1, \\ldots$ of the agent in the game. This starts at time $t=0$ and finishes at time $t=T$. \n",
    "* $r(\\tau)$ is the sum of rewards for a trajectory $\\tau$, or in other words, the total reward for the trajectory. \n",
    "\n",
    "For this loss function higher values are better (which might make it a reward function) and so we might have to invert it at some point. \n",
    "\n",
    "To update parameters we must find the gradient $\\nabla_\\theta J(\\theta)$, which measures how $J(\\theta)$ changes when we adjust the parameters of the paraphrase model. The gradient is simplified through some maths to get the policy gradient theorem $$ \\nabla_\\theta J(\\theta) =  \\nabla_\\theta E_\\pi [r(\\tau)]  = E_\\pi \\left[r(\\tau) \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a_t|s_t)  \\right] $$ \n",
    "\n",
    "To calculate this you need to calculate the expectation term, which in turn means evaluating every possible trajectory $\\tau$ and its expected return. Generally this is not possible and instead we turn to estimators.  \n",
    "\n",
    "One of these is REINFORCE. It gives us  $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ where \n",
    "* $G_t$ is the discounted return and is given by $G_t = r_t + \\beta r_{t-1} + \\beta^2 r_{t-2} + \\dots$. It's a rough estimate of $r(\\tau)$. Rewards obtained later in the episode are weighted much higher than rewards obtained earlier. I guess it assumes that the parameters update every timestep. \n",
    "* $S$ is some number of samples.\n",
    "\n",
    "The implementation of REINFORCE and similar estimators depends on how we formulate the problem. Below we present some possible formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation One: Document-level  \n",
    "This is the first implementation we will try. \n",
    "\n",
    "Here we generate a list of paraphrases at each time point. The idea is that there is one paraphrase amongst them that is a good adversarial example. We try to tune the model to produce the best one. \n",
    "\n",
    "This interpretation sees forming the complete paraphrase as one time step. So it isn't token-level but document-level. \n",
    "\n",
    "* Starting state: $s0 = x$, the original example  \n",
    "* Actions: each action is \"choosing\" a paraphrase (or of choosing $n$ paraphrases). The set of all possible paraphrases and their probabilities is the policy. So $\\pi(a|s) = p(x'| x;\\theta)$ where $x'$ is the paraphrase (or list of paraphrases). \n",
    "    * To approximate this probability, what we can do is generate a large list of paraphrases, and for each, the probabilities of generating each token in turn for that paraphrase. This gives a rough \"probability\" of how likely that sequence was. This number is kind of like a weight for how good that paraphrase is, according to the model.  We can then turn the weights into probabilities to get a \"probability\" of the paraphrase. This is dependent on the number of paraphrases generated, so generating a large list is likely to be better for this task. \n",
    "* Reward: The paraphrase moves through the reward function $R(x, x')$) to get the reward $r$. \n",
    "* Time steps: We only have one time step in the game ($T=1$ and $G_t=r$)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are a few variations to this scenario that we can do. For each of these we will formulate the policy and the reward function $R$. Below, $x'$ means paraphrase, $f(x)_y$ means the model confidence of x for the class of the true label $y$, $SS(a,b)$ is the result of a semantic similarity model run over $a$ and $b$, and $\\lambda$ is a hyperparameter.  \n",
    "\n",
    "\n",
    "#### One-paraphrase \n",
    "Here we only generate one paraphrase. This scenario also has a few options. First we generate a list of paraphrases with the probabilities of selecting one. Then we either sample probabilistically from the list or pick the most probable option. \n",
    "\n",
    "In this case the policy $p(x'|x,\\theta)$ is the chance of obtaining a specific paraphrase. For the sampling option this is equal to its sample probability. For the top option this is just the probability of selecting that option. \n",
    "\n",
    "The reward function might look like $R(x,x') = f(x)_y - f(x')_y + \\lambda SS(x, x')$. We could also make the $SS$ factor a step-function above some threshold. \n",
    "\n",
    "The REINFORCE equation $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ becomes $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$ We repeat the process $S$ times where $S$ is ideally as large as possible. We can start with something simple (e.g. $S=10$ or $S=100$) and go from there.  \n",
    "\n",
    "The gradient term $\\nabla \\log p(x'_s|x,\\theta)$ can hopefully be found with autodiff. \n",
    "\n",
    "#### Set of paraphrases\n",
    "In this scenario the paraphrase model is evaluated on performance over a set of paraphrases, which we call $X'$ here. The policy becomes $p(X'|x, \\theta)$, the probability of obtaining that list. We can get this probability by multipling together the \"probability\" of each individual paraphrase, multiplying also by nCr (for r paraphrases out of n total) to account for the lack of order in the list. \n",
    "\n",
    "We can make a number of sub-scenarios here. \n",
    "\n",
    "For the **top-paraphrase in set** condition the paraphrase generator is only measured on the best reward for a paraphrase in its set. The idea is the generator will learn to produce a diverse set of examples, any of which could plausibly be a good adversarial example. Here we only look at best performing paraphrase $x'_m$, which we can find by $x'_m = \\max_i [f(x)_y - f(x'_i)_y]$, then return $R(x,x'_m) = [f(x)_y - f(x'_m)_y] + \\lambda SS(x,x'_m)$ \n",
    "\n",
    "For the **average-paraphrase in set** condition the paraphrase generator is measured on the average reward of the paraphrases in its set. This encourages the generator to consider performance of all examples more-or-less equally. The reward function could be something like $\\frac{1}{k} \\sum_{i=1}^k \\left[ f(x)_y - f(x'_i)_y + \\lambda SS(x, x'_i) \\right]$ \n",
    "\n",
    "A combination of these scenarios is the **top-k/top-p\\% paraphrases in set**. Here we only use the top-$k$ paraphrases, or more generally, the top $p$ percentage of paraphrases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation 2: Token-level\n",
    "This interpretation is at token-level; it sees choosing the next word as the next time step. \n",
    "\n",
    "* Starting state: $s0 = x$, the initial state. But you also have a \"blank slate\" for the paraphrase. So maybe it's a tuple (x, pp) where pp is a paraphrase with no words. Here x is used as the reference for the paraphrase generator.  \n",
    "* Actions: Choose the next word of p. I guess this starts with the \\<START\\> token (or something similar). Then you have the policy $\\pi(a|s)$ which is the same as $p(w_{next}|pp, x; \\theta)$ where $\\theta$ is the paraphrase model parameters, $pp$ is the so-far constructed sentence, and $w_{next}$ is the next token (I say token because I don't know if this model is on the subword or word basis). \n",
    "* Time steps: every token is generated one-by-one and each of these is allocated a time step. This means probably that you also update the parameters after each token generated too. \n",
    "* Reward. The reward is allocated every token. There are many reward functions (see papers on token-level loss functions). Some also incorporate document-level rewards too. \n",
    "* Next state. $s_1$ is again the tuple $(x, pp)$ but now $pp$ has the first word in it. \n",
    "\n",
    "On *teacher forcing*. This is when you have a ground-truth paraphrase and you can use it when generating tokens. This is useful because if the model makes a mistake it doesn't continue down that track but is adjusted back. This stops big divergences (but also might limit the diversity of generated paraphrases). This is used when training a paraphrase model. You have a set of reference paraphrases that are human provided. Here though we only have the original sentence and no references. We could generate adversarial examples and use that to do teacher forcing. Generating them using textattack recipes might work. This is only really used on the token-level rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Updating the paraphrase model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is a choice here. We can either directly update the parameters of the paraphrase model. Or we can fix the parameters and add a new dense layer to the end of the model. We could then train this dense layer to convert paraphrases to adversarial paraphrases. \n",
    "\n",
    "Before trying this out, I am worried that we will destroy the capabilities of the paraphrase generator a bit. We might get semantically invalid or ungrammatical or gibberish text. If so we could try and mitigate it a bit by shaping our reward function to maintain grammatical components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Experiment order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plan is to try the following order: \n",
    "\n",
    "1. One-paraphrase (most probable option). I'll start with this one because it is probably the most simple case. Within this category: \n",
    "    1a. tune existing parameters only (see if the text is recognisable) \n",
    "    1b. add dense layer onto end and try again \n",
    "2. One-paraphrase (sampled). This seems like a logical extension on the first one. \n",
    "3. Paraphrase-set options. (Decide after finishing 1, 2) \n",
    "4. Token-level tuning. (Decide after 1,2,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Layer Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I am uncertain on if to do this or not. \n",
    "\n",
    "* This [paper](https://arxiv.org/abs/1911.03090) indicates that you can get pretty good results by freezing all layers except the last few \n",
    "* Conversely I saw in the transformers documentation that transformers train better if you don't do layer freezing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, load models + datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from torch.distributions import Categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "import utils; from utils import *   # local script\n",
    "from tests import *   # local script\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#### Parameters, notes and training settings\n",
    "\n",
    "### These parameters mostly don't do anything but are more notes (for the wanb.init function)\n",
    "debug_run = \"true\"   # doesn't do anything\n",
    "sampling_strategy = \"greedy\"  # doesn't do anything\n",
    "# copy-paste this from reward function\n",
    "reward_strategy = \"[-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]\" # doesn't do anything\n",
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "#pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "sts_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "dataset_name = \"simple\"\n",
    "n_layers_frozen = \"2\"  # counting from the back (doesn't do anything yet)\n",
    "use_fp16 = True\n",
    "save_model_while_training = False\n",
    "save_model_freq = 10\n",
    "\n",
    "# These parameters only have effect if small_ds = True\n",
    "\n",
    "### Paraphrase parameters  \n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "if dataset_name == \"rotten_tomatoes\":\n",
    "    max_length = 64\n",
    "    pp_model_params['max_length'] = max_length\n",
    "    #batch_size_train = 32\n",
    "    #batch_size_eval = 128\n",
    "    batch_size_train = 16\n",
    "    batch_size_eval = 32\n",
    "    accumulation_steps = 1\n",
    "    n_train_epochs = 2\n",
    "    eval_freq = 1 \n",
    "    use_small_ds = True  # for testing purposes\n",
    "    n_shards         = 60    if use_small_ds else None \n",
    "    shard_contiguous = False if use_small_ds else None \n",
    "elif dataset_name == \"simple\":\n",
    "    max_length = 20\n",
    "    pp_model_params['max_length'] = max_length\n",
    "    batch_size_train = 2\n",
    "    batch_size_eval = 4\n",
    "    accumulation_steps = 1\n",
    "    eval_freq = 10 \n",
    "    n_train_epochs = 60\n",
    "    use_small_ds = False  # for testing purposes\n",
    "    n_shards         =  None \n",
    "    shard_contiguous =  None \n",
    "    if use_small_ds == True: \n",
    "        raise Exception(\"Don't shard when using the simple dataset (no need)\")\n",
    "\n",
    "\n",
    "### Training parameters\n",
    "seed = 420\n",
    "lr = 3e-5 # Initial learning rate (after the potential warmup period) to use\n",
    "normalise_rewards = False\n",
    "metrics = ['loss', 'pp_logp', 'reward', 'vm_score', \"sts_score\", 'label_flip']\n",
    "pin_memory = True\n",
    "zero_grad_with_none = False\n",
    "pad_token_embeddings = True\n",
    "padding_multiple = 8 \n",
    "bucket_by_length = True\n",
    "shuffle_train = False\n",
    "remove_misclassified_examples = True\n",
    "#weight_decay = 0\n",
    "#lr_scheduler_type = 'none'\n",
    "#n_warmup_steps = 30 \n",
    "\n",
    "### W&B parameters\n",
    "wandb_mode = \"online\"  # set to \"disabled\" to turn off wandb, \"online\" to enable it\n",
    "wandb_log_grads = False   \n",
    "wandb_log_grads_freq = 1  # no effect if wandb_log_grads is False\n",
    "wandb_plot_examples = False\n",
    "wandb_n_examples_plot = 4  # number of individual examples to plot curves for\n",
    "# log a table to wandb with the examples and rewards the model sees while training. Useful for debugging \n",
    "# and seeing what is going on, but slows down training time. \n",
    "wandb_log_training_step_table = True  \n",
    "wandb_log_token_entropy=True\n",
    "wandb_log_token_probabilities = True\n",
    "\n",
    "# Parameter dict\n",
    "config_d = dict(\n",
    "    debug_run = debug_run,\n",
    "    sampling_strategy = sampling_strategy,\n",
    "    reward_strategy = reward_strategy,\n",
    "    pp_name = pp_name,\n",
    "    vm_name = vm_name,\n",
    "    sts_name=sts_name,\n",
    "    dataset_name = dataset_name, \n",
    "    use_small_ds = use_small_ds,\n",
    "    shard_params = dict(\n",
    "        n_shards = n_shards,\n",
    "        shard_contiguous = shard_contiguous,\n",
    "    ),\n",
    "    n_layers_frozen = n_layers_frozen,\n",
    "    pp_model_params = pp_model_params, \n",
    "    seed = seed,\n",
    "    batch_size_train = batch_size_train,\n",
    "    batch_size_eval = batch_size_eval,\n",
    "    fp16 = use_fp16,\n",
    "    max_length = max_length,\n",
    "    lr = lr, \n",
    "    accumulation_steps=accumulation_steps,\n",
    "    n_train_epochs = n_train_epochs,\n",
    "    eval_freq = eval_freq,\n",
    "    normalise_rewards = normalise_rewards,\n",
    "    metrics = metrics,\n",
    "    pin_memory = pin_memory,\n",
    "    zero_grad_with_none = zero_grad_with_none,\n",
    "    pad_token_embeddings = pad_token_embeddings,\n",
    "    padding_multiple = padding_multiple,\n",
    "    bucket_by_length = bucket_by_length,\n",
    "    shuffle_train = shuffle_train,\n",
    "    remove_misclassified_examples = remove_misclassified_examples, \n",
    "    wandb_params=dict(\n",
    "        log_grads = wandb_log_grads,\n",
    "        log_grads_freq = wandb_log_grads_freq, \n",
    "        plot_examples = wandb_plot_examples, \n",
    "        n_examples_plot = wandb_n_examples_plot, \n",
    "        log_training_step_table = wandb_log_training_step_table\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#### Misc setup\n",
    "# Paths\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "path_data = \"./data/\"\n",
    "path_checkpoints = \"../model_checkpoints/travis_attack/\"\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Devices and GPU settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#device = accelerator.device\n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "# When not using Accelerator\n",
    "#n_wkrs = 4 * torch.cuda.device_count()\n",
    "# When using Accelerator \n",
    "n_wkrs = 0 \n",
    "\n",
    "# Configs\n",
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "wandb.Table.MAX_ARTIFACT_ROWS = 1000000\n",
    "wandb.Table.MAX_ROWS = 1000000\n",
    "run_notes = f\"Debug run:{debug_run}\\nReward: {reward_strategy}\\nDataset: {dataset_name}\\\n",
    "\\nSampling strategy: {sampling_strategy}\"\n",
    "\n",
    "# Logging \n",
    "logging.basicConfig(format='%(message)s', stream=sys.stdout) # stdout while we are doing stdout to file piping\n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Other \n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "### Dev code\n",
    "# used to know what to set max_pp_length to. \n",
    "track_pp_sizes = False\n",
    "if track_pp_sizes: \n",
    "    n_train_epochs = 1\n",
    "    use_small_ds = True \n",
    "    n_shards = 2  # half the train dataset gives us a good understanding of this\n",
    "    orig_max_l = []\n",
    "    pp_max_l = []\n",
    "    # After running the code  \n",
    "    # sns.distplot(orig_max_l)\n",
    "    # sns.distplot(pp_max_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Paraphrase (pp) model \n",
    "pp_tokenizer = AutoTokenizer.from_pretrained(pp_name)\n",
    "# takes about 3GB memory space up on the GPU\n",
    "# change the `local_files_only` argument if changing the model name \n",
    "pp_model = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "# The no_grad version of generate\n",
    "generate_with_grad = undecorated(pp_model.generate)\n",
    "pp_model.generate_with_grad = MethodType(generate_with_grad, pp_model)\n",
    "\n",
    "## Victim Model (VM)\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name, local_files_only=True).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels\n",
    "\n",
    "## Semantic Textual Similarity (STS) model\n",
    "sts_model = SentenceTransformer(sts_name)\n",
    "\n",
    "# Pad vocab to multiple of 8 (for better tensor core efficiency in fp16)\n",
    "if pad_token_embeddings: \n",
    "    def pad_token_embeddings_to_multiple_of_n(model, n):\n",
    "        def get_new_vocab_size(model): return int((np.floor(model.config.vocab_size / n) + 1) * n)\n",
    "        model.resize_token_embeddings(get_new_vocab_size(model))\n",
    "    pad_token_embeddings_to_multiple_of_n(pp_model, 8)\n",
    "    pad_token_embeddings_to_multiple_of_n(vm_model, 8)\n",
    "    # sts_model is from SentenceTransformers so needs a bit of unwrapping to access the base huggingface model \n",
    "    pad_token_embeddings_to_multiple_of_n(sts_model._first_module().auto_model, 8) \n",
    "# unlike pp_tokenizer.vocab_size this includes the padding \n",
    "vocab_size = pp_model.get_input_embeddings().num_embeddings  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw datasets and create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     6,
     10,
     21,
     28,
     33,
     37,
     43,
     49
    ]
   },
   "outputs": [],
   "source": [
    "def add_idx(x, idx): x['idx'] = idx; return x   # add row numbers\n",
    "\n",
    "def add_n_tokens(x): x['n_tokens'] = [len(o) for o in x['input_ids']]; return x  # number of tokens of \"text\" field \n",
    "\n",
    "def tokenize_fn(x):  return pp_tokenizer(x['text'], truncation=True, max_length=max_length)\n",
    "\n",
    "def prep_small_ds(ds_dict):\n",
    "    for k,v in ds_dict.items():  ds_dict[k] = v.shard(n_shards, 0, contiguous=shard_contiguous)\n",
    "    return ds_dict\n",
    "\n",
    "def get_vm_probs(text, return_predclass=False): \n",
    "    \"\"\"Used also by the reward_fn to get vm_score\"\"\"\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tkns = vm_tokenizer(text, truncation=True, padding=True, pad_to_multiple_of=padding_multiple,\n",
    "                            return_tensors=\"pt\").to(device)\n",
    "        logits = vm_model(**tkns).logits\n",
    "        probs = torch.softmax(logits,1)\n",
    "        if return_predclass:    return probs, torch.argmax(probs,1)\n",
    "        else:                   return probs\n",
    "\n",
    "def get_vm_orig_score(batch): \n",
    "    labels = torch.tensor(batch['label'], device=device)\n",
    "    orig_probs,orig_predclass = get_vm_probs(text = batch['text'], return_predclass=True)\n",
    "    batch['orig_truelabel_probs'] = torch.gather(orig_probs,1, labels[:,None]).squeeze().cpu().tolist()\n",
    "    batch['orig_vm_predclass'] = orig_predclass.cpu().tolist()\n",
    "    return batch\n",
    "\n",
    "def get_sampler(ds): \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return RandomSampler(ds, generator=g)\n",
    "\n",
    "def get_sts_orig_embeddings(batch): \n",
    "    batch['orig_sts_embeddings'] = sts_model.encode(batch['text'], batch_size=64, convert_to_tensor=False)\n",
    "    return batch\n",
    "\n",
    "def collate_fn_tkn(x): \n",
    "    d = dict()\n",
    "    for k in ['idx', 'attention_mask', 'input_ids', 'label', 'orig_truelabel_probs', 'orig_sts_embeddings']: \n",
    "        d[k] = [o[k] for o in x]\n",
    "    return pp_tokenizer.pad(d, pad_to_multiple_of=padding_multiple, return_tensors=\"pt\")\n",
    "\n",
    "def collate_fn_raw(x): \n",
    "    d = dict()\n",
    "    for k in ['text', 'idx']: \n",
    "        d[k] = [o[k] for o in x]\n",
    "    return d \n",
    "\n",
    "def get_dataloaders_dict(ds_dict, collate_fn): \n",
    "    if bucket_by_length and shuffle_train:  raise Exception(\"Can only do one of bucket by length or shuffle\")\n",
    "    d = dict()\n",
    "    for split, ds in ds_dict.items(): \n",
    "        if shuffle_train:\n",
    "            if split == \"train\": \n",
    "                sampler = get_sampler(ds)\n",
    "                d[split] =  DataLoader(ds, batch_size=batch_size_train, sampler=sampler, collate_fn=collate_fn, \n",
    "                                       num_workers=n_wkrs, pin_memory=pin_memory) \n",
    "            else: \n",
    "                d[split] =  DataLoader(ds, batch_size=batch_size_eval, shuffle=False, collate_fn=collate_fn, \n",
    "                                       num_workers=n_wkrs, pin_memory=pin_memory) \n",
    "        if bucket_by_length: \n",
    "            batch_size = batch_size_train if split == \"train\" else batch_size_eval\n",
    "            d[split] =  DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, \n",
    "                                   num_workers=n_wkrs, pin_memory=pin_memory) \n",
    "    \n",
    "    # Add eval dataloader for train \n",
    "    d['train_eval'] =DataLoader(ds_dict['train'], batch_size=batch_size_eval, shuffle=False,\n",
    "                                collate_fn=collate_fn, num_workers=n_wkrs, pin_memory=pin_memory) \n",
    "    return d \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, tokenise, preprocess datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b253756c445fb811\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fb739c35e4bce8a4ab11e1c299b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c802946231f72062\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917c73605150442bbab07e1a3d0b9038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-43a49c5188c42e69\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0995942b64d04cc097fc630d19584130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/csv/default-b253756c445fb811/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-a5edd4f7b5f30765.arrow\n",
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/csv/default-c802946231f72062/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-b5c4ff556910996b.arrow\n",
      "Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/csv/default-43a49c5188c42e69/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-5a12731c4f785a54.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf0cf765c26487abd6bd5dcf70bf4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176487130db04d9792967dc55292e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd6e82cc66d4265b66b1fa15dee4ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180b08aeced44aac92eda1531a7b07dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0ca89412d1405f9447d6430ecb3278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8208092ae74d9f9a10384719e35a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e711f11aff47949e4199d13eda2174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac141b3b7d1a4b778d920cf9e42099cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705a899ccd2f402b9149e4296d235f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b64ab3ff6a942e493ec9a536ab3c0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ba81f2b4f546c8ac4d67dd10717e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e287fae1c2c48afa00281ab43c833a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dcc79a1424442eb30ef430aa6e81c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bd6b57b4e44afd88da5d15b83886a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9c47aafcfe422f9255b86a7a0e2211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dataset_name == \"rotten_tomatoes\": \n",
    "    dsd_raw = load_dataset(\"rotten_tomatoes\")\n",
    "    dsd_raw['valid'] = dsd_raw.pop('validation')  # \"valid\" is easier than \"validation\"\n",
    "    label_cname = 'label'\n",
    "    for _,ds in dsd_raw.items():\n",
    "        # make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "        assert ds.features[label_cname].num_classes == vm_num_labels    \n",
    "elif dataset_name == \"simple\": \n",
    "    dsd_raw = DatasetDict()\n",
    "    for s in splits:  dsd_raw[s] = load_dataset('csv', data_files=f\"{path_data}simple_dataset_{s}.csv\")['train']\n",
    "elif dataset_name==\"snli\": \n",
    "    next\n",
    "    ## For snli\n",
    "    # remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "    # ds_train = ds_train.filter(remove_minus1_labels)\n",
    "    # valid = valid.filter(remove_minus1_labels)\n",
    "    # test = test.filter(remove_minus1_labels)\n",
    "dsd = dsd_raw.map(add_idx, batched=True, with_indices=True)\n",
    "if use_small_ds: dsd = prep_small_ds(dsd)  # do after adding idx so it's consistent across runs\n",
    "dsd = dsd.map(get_vm_orig_score,       batched=True)\n",
    "if remove_misclassified_examples: dsd = dsd.filter(lambda x: x['orig_vm_predclass'] == x['label'])\n",
    "dsd = dsd.map(get_sts_orig_embeddings, batched=True)\n",
    "dsd = dsd.map(tokenize_fn, batched=True) \n",
    "dsd = dsd.map(add_n_tokens, batched=True)\n",
    "if bucket_by_length: dsd = dsd.sort(\"n_tokens\", reverse=True)\n",
    "dld_raw = get_dataloaders_dict(dsd, collate_fn=collate_fn_raw) \n",
    "dld_tkn = get_dataloaders_dict(dsd, collate_fn=collate_fn_tkn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop pseudocode\n",
    "\n",
    "The REINFORCE estimator is $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\log p(x'_s|x,\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     18,
     54,
     73,
     163,
     303,
     316,
     324
    ]
   },
   "outputs": [],
   "source": [
    "def get_entropy_metrics(scores_stacked, attention_mask): \n",
    "    ent = Categorical(logits = scores_stacked).entropy().detach()\n",
    "    assert ent.shape == torch.Size([pp_batch_size, pp_length - 1])\n",
    "    ent = ent * attention_mask  # stop values after eos token from contributing\n",
    "    ent = (ent * attention_mask).flatten()  # remove structure (we want this across all batch values)\n",
    "    ent = ent[ent != 0]   # remove the values corresponding to eos tokens from summary stats \n",
    "    assert len(ent) == len(attention_mask[attention_mask == 1].flatten())\n",
    "    ent_d = dict(\n",
    "        ent_min             = ent.quantile(0).item(),\n",
    "        ent_lower_quartile  = ent.quantile(0.25).item(), \n",
    "        ent_median          = ent.median().item(), \n",
    "        ent_mean            = ent.mean().item(), \n",
    "        ent_upper_quartile  = ent.quantile(0.75).item(), \n",
    "        ent_max             = ent.quantile(1).item(), \n",
    "        epoch=epoch, global_step=global_step\n",
    "    )\n",
    "    return ent_d\n",
    "\n",
    "def get_token_probability_metrics(scores_log_softmax, attention_mask, k=3): \n",
    "        token_prob_d = dict()\n",
    "        tkn_kmaxprob, tkn_kmaxidx = torch.topk(scores_log_softmax,largest=True,  k=k, dim=2)\n",
    "        tkn_kmaxprob = tkn_kmaxprob.detach()  # log these \n",
    "        assert tkn_kmaxprob.shape == torch.Size([pp_batch_size, pp_length - 1, k])\n",
    "\n",
    "        # % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1\n",
    "        top_probs = tkn_kmaxprob[:,:,0].exp()\n",
    "        top_probs = (top_probs * attention_mask).flatten()\n",
    "        top_probs = top_probs[top_probs != 0]\n",
    "        prob_threshold_l = [0.99, 0.975, 0.95, 0.90, 0.75, 0.5, 0.3, 0.1]\n",
    "        for p in prob_threshold_l: \n",
    "            token_prob_d[f\"top_token_prob_over_{str(p)}\"] = (torch.sum(top_probs > p) / top_probs.shape[0]).item()\n",
    "\n",
    "        # avg + median + lower + upper quartile of first, second, third choice probs\n",
    "        tkn_kmaxprob_mask = tkn_kmaxprob * attention_mask[:,:,None]  # broadcasting over kth dim\n",
    "        for i in range(k): \n",
    "            probs = tkn_kmaxprob_mask[:,:, i].flatten()\n",
    "            probs = probs[probs != 0]\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_mean\"] = probs.mean().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_median\"] = probs.median().item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.25_quantile\"] = probs.quantile(0.25).item()\n",
    "            token_prob_d[f\"rank_{i+1}_token_prob_0.75_quantile\"] = probs.quantile(0.75).item()\n",
    "\n",
    "        # tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob \n",
    "        allprobs = (scores_log_softmax.detach().exp() * attention_mask[:,:,None]).flatten()\n",
    "        allprobs = allprobs[allprobs != 0]\n",
    "        for p in [0.1, 0.01, 0.001, 0.0001, 0.00001]: \n",
    "            token_prob_d[f\"%_of_tokens_above_prob_{p}\"] =  (torch.sum(allprobs > p) / allprobs.shape[0]).item()\n",
    "        token_prob_d[f\"%_of_tokens_above_prob_1/vocab_size\"] = \\\n",
    "            (torch.sum(allprobs > (1/vocab_size)) / allprobs.shape[0]).item()\n",
    "            \n",
    "        token_prob_d['epoch'] = epoch\n",
    "        token_prob_d['global_step'] = global_step\n",
    "        return token_prob_d\n",
    "\n",
    "def get_paraphrases(input_ids,attention_mask):\n",
    "    \"\"\"Wrapper for generating paraphrases (pp's). Most keywords are passed on to pp_model.generate function, \n",
    "    so see docs for that function. \"\"\"\n",
    "    # Only greedy search supported at the moment\n",
    "    pp_output = pp_model.generate_with_grad(input_ids=input_ids, \n",
    "                                            attention_mask=attention_mask, \n",
    "                                             **pp_model_params,\n",
    "                                             do_sample=False, \n",
    "                                             return_dict_in_generate=True,\n",
    "                                             output_scores=True,\n",
    "                                                remove_invalid_values=False, \n",
    "                                             pad_token_id = pp_tokenizer.pad_token_id,\n",
    "                                             eos_token_id = pp_tokenizer.eos_token_id)\n",
    "    pp_l = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "    if track_pp_sizes:  # DEV CODE (can delete later)\n",
    "        orig_max_l.append(batch['input_ids'].shape[1])\n",
    "        pp_max_l.append(pp_output.sequences.shape[1])\n",
    "    return pp_output, pp_l\n",
    "\n",
    "def get_pp_logp(pp_output,log_times=True): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    ### TODO: this looks like logp to me, not plogp. Find out if this is right and if so rename, if not, fix\n",
    "    ### We want to align tokens with token probabilities. The first token is given at the start \n",
    "    # and has no probability attached to it, so we remove it. \n",
    "    seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "    assert seq_without_first_tkn.shape == torch.Size([orig_batch_size, pp_length - 1])\n",
    "    \n",
    "    ### Convert from tuple of scores to one big tensor of scores \n",
    "    scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "    ### TESTS \n",
    "    # We check shape and that there is no +inf or nan in scores. \n",
    "    # Scores can have -inf in them - see explanation in `exploring_generation`.  \n",
    "    assert scores_stacked.shape == torch.Size([orig_batch_size, (pp_length - 1), vocab_size])\n",
    "    assert torch.all(~torch.isnan(scores_stacked))\n",
    "    assert torch.all(~torch.isposinf(scores_stacked))\n",
    "    # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "    # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "    # shouldn't be set to -inf\n",
    "    # Not a 100% test but very likely to identify\n",
    "    idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "    assert len(idx_neginf[idx_neginf[:,2] == pp_tokenizer.eos_token_id, :]) == \\\n",
    "              (pp_model_params[\"min_length\"] -1) * orig_batch_size  \n",
    "    del idx_neginf\n",
    "    \n",
    "    ### Take log softmax of scores and then extract those that correspond \n",
    "    # to the generated sequences    \n",
    "    scores_log_softmax = scores_stacked.log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    ### TESTS \n",
    "    # -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. \n",
    "    assert torch.all(~torch.isnan(   scores_log_softmax))\n",
    "    assert torch.all(~torch.isposinf(scores_log_softmax))\n",
    "    check_scores_log_softmax_sums(scores_log_softmax)\n",
    "    # probs should be 1-1 with the filtered tkns: check shape to confirm\n",
    "    assert seq_token_log_probs.shape == seq_without_first_tkn.shape  \n",
    "    # Check that the last token probability corresponds to a possible end token\n",
    "    # this has to be tested before the attention mask is multiplied with it because if the \n",
    "    # padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)\n",
    "    output_end_ids = get_start_end_special_token_ids(pp_tokenizer)['output_end_id']\n",
    "    assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "    del output_end_ids\n",
    "    ## THIS ONE IS LONG - a test rather than assert \n",
    "    # check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, \n",
    "    #                                             seq_token_log_probs) \n",
    "    \n",
    "    ### Generate attention mask to identify padding tokens. Then apply it to the \n",
    "    # sequence probabilities so that we don't consider probability of padding tokens \n",
    "    # when getting sequence probabilities. \n",
    "    # Also replace the -inf values in seq_token_log_probs with a large negative number because if we \n",
    "    # leave them in we end up with nan's introduced after multiplying with attention_mask, \n",
    "    # since  -inf * 0 = nan \n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs, nan=None, posinf=None, neginf=-10000)\n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    ### TESTS\n",
    "    assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "    # check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "    assert all(seq_without_first_tkn[attention_mask == 0] == pp_tokenizer.pad_token_id)\n",
    "    check_no_nans_or_infs(seq_token_log_probs)\n",
    "    # check that we aren't picking extrememly rare tokens\n",
    "    assert torch.all(seq_token_log_probs  > -10)  \n",
    "    \n",
    "    ### Get sequence probabilities by summing up token log probabilities \n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "    ## TESTS \n",
    "    assert seq_log_prob.shape == torch.Size([pp_batch_size])\n",
    "    check_no_nans_or_infs(seq_log_prob)\n",
    "\n",
    "    \n",
    "    if wandb_log_token_entropy:\n",
    "        with timecode() as time_log_entropy:\n",
    "            ent_d = get_entropy_metrics(scores_stacked, attention_mask)\n",
    "        ent_d['time/log_entropy'] = time_log_entropy.t\n",
    "        if log_times:   # need a better way to handle this. \n",
    "            wandb.log(ent_d, commit=False)\n",
    "    \n",
    "    \n",
    "    if wandb_log_token_probabilities: \n",
    "        with timecode() as time_log_token_probabilities:\n",
    "            token_prob_d = get_token_probability_metrics(scores_log_softmax, attention_mask, k=3)\n",
    "        token_prob_d['time/log_token_probabilities'] = time_log_token_probabilities.t\n",
    "        if log_times: \n",
    "            wandb.log(token_prob_d, commit=False)\n",
    "        \n",
    "    return seq_log_prob\n",
    "\n",
    "def reward_fn(data, raw, pp_l, return_components=False, log_times=True): \n",
    "    \"\"\"orig_l, pp_l are lists of original and paraphrase respectively\"\"\"\n",
    "    # Victim model probability differences between orig and pp\n",
    "    with timecode() as time_vm_scores:\n",
    "        pp_probs = get_vm_probs(pp_l) \n",
    "        pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "        pp_truelabel_probs   = torch.gather(pp_probs, 1, data['label'][:,None]).squeeze()\n",
    "        pp_predclass_probs   = torch.gather(pp_probs, 1, pp_predclass[ :,None]).squeeze()\n",
    "        label_flip = (pp_predclass != data['label']) * 1\n",
    "        vm_scores = (data['orig_truelabel_probs'] - pp_truelabel_probs)\n",
    "    \n",
    "    \n",
    "    # STS scores\n",
    "    with timecode() as time_sts_scores:\n",
    "        pp_embeddings   = sts_model.encode(pp_l,        batch_size=len(raw), convert_to_tensor=True, device=device)\n",
    "        # This returns a cosine similarity matrix, of which we just want the diagonal\n",
    "        sts_scores = pytorch_cos_sim(data['orig_sts_embeddings'], pp_embeddings).diagonal()  \n",
    "    \n",
    "    # Reward calculation \n",
    "    rewards = torch.tensor([-0.5 if sts < 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)],device=device)\n",
    "    \n",
    "    if log_times:\n",
    "        wandb.log({'epoch': epoch, 'global_step': global_step, \n",
    "                   'time/vm_scores': time_vm_scores.t, 'time/sts_scores': time_sts_scores.t }, \n",
    "                   commit=False)\n",
    "    \n",
    "    if return_components: \n",
    "        return {\n",
    "            \"orig_l\": raw['text'],\n",
    "            \"pp_l\": pp_l,  \n",
    "            \"truelabel\": data['label'],\n",
    "            \"orig_truelabel_probs\": data['orig_truelabel_probs'],\n",
    "            \"pp_truelabel_probs\":  pp_truelabel_probs,\n",
    "            \"pp_predclass\": pp_predclass,\n",
    "            \"pp_predclass_probs\": pp_predclass_probs,\n",
    "            \"vm_score\": vm_scores, \n",
    "            \"sts_score\": sts_scores,\n",
    "            \"reward\": rewards,\n",
    "            \"label_flip\": label_flip\n",
    "        }\n",
    "    else:  return {\"reward\": rewards}\n",
    "\n",
    "def pp_model_forward(data): \n",
    "    global orig_batch_size,orig_length,pp_batch_size,pp_length\n",
    "    orig_batch_size     = data['input_ids'].shape[0]\n",
    "    orig_length         = data['input_ids'].shape[1]\n",
    "    pp_output, pp_l = get_paraphrases(data['input_ids'], data['attention_mask']) \n",
    "    pp_batch_size = pp_output.sequences.shape[0]\n",
    "    # for greedy search pp_length is equal to orig_batch_size but this won't be for beam search\n",
    "    pp_length     = pp_output.sequences.shape[1]  \n",
    "    return pp_output, pp_l\n",
    "\n",
    "def loss_fn(data, raw, pp_output, pp_l, return_components=False, log_times=True): \n",
    "    with timecode() as time_reward_fn:\n",
    "        d = reward_fn(data, raw, pp_l, return_components=return_components, log_times=log_times)\n",
    "        \n",
    "    if normalise_rewards: \n",
    "        d['orig_reward'] = copy.deepcopy(d['reward'])\n",
    "        d['reward'] = (d['reward']-torch.mean(d['reward']))/torch.std(d['reward'])\n",
    "        \n",
    "    with timecode() as time_pp_logp:\n",
    "        d['pp_logp'] = get_pp_logp(pp_output,log_times=log_times)\n",
    "    \n",
    "    with timecode() as time_loss_fn_loss_calc:\n",
    "        d['loss'] = -d['reward'] * d['pp_logp']\n",
    "        d['loss_batch'] = torch.mean(d['loss'])\n",
    "        if return_components ==  False: return d['loss_batch'] \n",
    "        \n",
    "    # remove some items from compgraph\n",
    "    with timecode() as time_loss_fn_detach:\n",
    "        d['pp_logp'] = d['pp_logp'].detach()  \n",
    "        d['loss']    = d['loss'].detach()\n",
    "        \n",
    "#     # This was taking a lot of time so removed it. Add it in if needed. \n",
    "#     #gc.collect() \n",
    "#     print(\"\\t### INSIDE loss_fn### \")\n",
    "#     print(\"\\tglobal_step\", global_step)\n",
    "#     print(\"\\treward_fn_time\", time_reward_fn.t)\n",
    "#     print(\"\\t######### \")\n",
    "\n",
    "    if log_times:   # true for training, not eval. \n",
    "        wandb.log({'epoch': epoch, 'global_step': global_step, \n",
    "                   'time/reward_fn': time_reward_fn.t, 'time/pp_logp': time_pp_logp.t, \n",
    "                  'time/loss_fn_loss_calc': time_loss_fn_loss_calc.t, \n",
    "                   'time/pp_logp_detach': time_loss_fn_detach.t, \n",
    "                  }, \n",
    "                 commit=False)\n",
    "    return d\n",
    "\n",
    "def training_step(data, raw, accelerator): \n",
    "    \"\"\"With gradient accumulation\"\"\"\n",
    "    with timecode() as time_generate_pp:\n",
    "        pp_output, pp_l = pp_model_forward(data)\n",
    "    \n",
    "    #logger.info(show_gpu(f'Batch {batch_num}, GPU memory usage after forward pass: '))\n",
    "    \n",
    "    with accelerator.autocast():\n",
    "        with timecode() as time_loss_fn:\n",
    "            if wandb_log_training_step_table: \n",
    "                results_d = loss_fn(data, raw, pp_output, pp_l, return_components=True)\n",
    "                loss_batch = results_d['loss_batch']\n",
    "            else: \n",
    "                loss_batch = loss_fn(data, raw, pp_output, pp_l, return_components=False)\n",
    "        \n",
    "        loss_batch = loss_batch / accumulation_steps  # Normalize our loss for gradient accumulation\n",
    "        \n",
    "    with timecode() as time_backwards:\n",
    "        accelerator.backward(loss_batch) \n",
    "    \n",
    "    #logger.info(show_gpu(f'Batch {batch_num}, GPU memory usage after backwards pass: '))\n",
    "    if (accumulation_num + 1) % accumulation_steps == 0: \n",
    "        with timecode() as time_opt_step:\n",
    "            optimizer.step()\n",
    "        \n",
    "        pp_model.zero_grad(set_to_none=zero_grad_with_none)\n",
    "    if wandb_log_training_step_table: \n",
    "        with timecode() as time_add_to_training_step_table:\n",
    "            results_d = process_results_d1(results_d, raw)\n",
    "            add_preds_to_data_d(results_d, split='training_step') \n",
    "        \n",
    "#     print(\"### INSIDE training_step ####\")    \n",
    "#     print(\"epoch\", epoch)\n",
    "#     print(\"batch_num\", batch_num)\n",
    "#     print(\"global_step\", global_step)\n",
    "#     print(\"model in training mode\", pp_model.training)\n",
    "#     print(\"orig\", raw['text'])\n",
    "#     print(\"pp_l\", pp_l)\n",
    "#     print(\"pp_seq\", pp_output.sequences)\n",
    "#     print(\"pp_length\", pp_output.sequences.shape, pp_length)\n",
    "#     print(\"loss_fn_time\", time_loss_fn.t)\n",
    "#     print(\"### INSIDE training_step ####\")\n",
    "    \n",
    "    wandb.log({'time/generate_pp': time_generate_pp.t, 'time/loss_fn': time_loss_fn.t, \n",
    "               'time/backwards_pass': time_backwards.t, 'time/optimizer_step': time_opt_step.t, \n",
    "               'time/add_to_training_step_table': time_add_to_training_step_table.t, \n",
    "               'epoch': epoch, 'global_step': global_step,'batch_num': batch_num,\n",
    "               'orig_length': orig_length,'orig_batch_size': orig_batch_size,\n",
    "              'pp_length': pp_length, 'pp_batch_size': pp_batch_size}\n",
    "              ,commit=False)\n",
    "\n",
    "def process_results_d1(results_d, raw): \n",
    "    \"\"\"REFACTOR THIS LATER\"\"\"\n",
    "    # wandb logging \n",
    "    results_d['epoch'] = epoch\n",
    "    results_d['idx'] = raw['idx']\n",
    "    for k,v in results_d.items(): \n",
    "        if torch.is_tensor(v): \n",
    "            results_d[k] = v.detach().cpu().tolist()\n",
    "        elif type(v) == int or type(v) == float: \n",
    "            # make into list repeated n times\n",
    "            results_d[k] = [v for o in range(batch_size_train)]\n",
    "    return results_d\n",
    "\n",
    "def save_model(): \n",
    "    path = f\"{path_run}model_{epoch}\"\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'pp_model_state_dict': pp_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, path)\n",
    "    \n",
    "def resume_model(path): \n",
    "    state = torch.load(path)\n",
    "    pp_model.load_state_dict(state['pp_model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval and wandb functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2,
     49,
     68,
     78,
     85,
     95,
     104,
     120
    ]
   },
   "outputs": [],
   "source": [
    "def table2df(table):  return pd.DataFrame(data=table.data, columns=table.columns)  # wandb table to dataframe\n",
    "\n",
    "def process_results_d_for_wandb(results_d): \n",
    "    # Flatten batches for each key, depending on datatype (e.g. lists of lists )\n",
    "    for k,v in results_d.items(): \n",
    "        # v[0] is arbitrary - we are just checking the first item in the list to see the type\n",
    "        if type(v) == float or type(v) == int: \n",
    "            next\n",
    "        elif  torch.is_tensor(v[0]): \n",
    "            # case where we have a list of scalars - the cat function doesn't work here \n",
    "            if  v[0].size() == torch.Size([]): x = torch.stack(v)\n",
    "            else:                              x = torch.cat(v)\n",
    "            results_d[k] = x.detach().cpu().squeeze().tolist()  # convert to list (squeeze is for single scalar list)\n",
    "        elif type(v[0]) == list:  # this is True for tensors also, so it has to go after the is_tensor check\n",
    "            results_d[k] = list(itertools.chain(*v)) \n",
    "        elif type(v) == list: \n",
    "            next\n",
    "        else: \n",
    "            raise Exception(\"shouldn't get here\")\n",
    "    return results_d\n",
    "\n",
    "def eval_dl(dl_tkn, dl_raw, device): \n",
    "    \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "    # Put models in eval mode and do the forward pass \n",
    "    # Current logic: push all batches together into one big list.     \n",
    "    if pp_model.training: pp_model.eval()\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    results_d = defaultdict(list)\n",
    "    with torch.no_grad(): \n",
    "        for eval_batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "          #  logger.info(show_gpu(f'EVAL, batch {i}, GPU memory usage after loading data: '))\n",
    "            for k, v in data.items(): \n",
    "                if data[k].device != device: data[k] = data[k].to(device)\n",
    "           # if data['input_ids'].device != device: data['input_ids'].to(device)\n",
    "                \n",
    "            pp_output, pp_l = pp_model_forward(data)\n",
    "            d = loss_fn(data, raw, pp_output, pp_l, return_components=True, log_times=False)\n",
    "            #logger.info(show_gpu(f'EVAL, batch {eval_batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "            d['idx'] = raw['idx']\n",
    "                      \n",
    "            for k,v in d.items(): \n",
    "                results_d[k].append(v) \n",
    "    del eval_batch_num, data, raw, pp_output, pp_l, d\n",
    "    results_d = process_results_d_for_wandb(results_d)\n",
    "            \n",
    "    # Calculate additional metrics \n",
    "    results_d['epoch'] = epoch\n",
    "    return results_d\n",
    "\n",
    "def add_preds_to_data_d(results_d, split):\n",
    "    if split not in data_d.keys() or split == \"training_summary\": # training summary table logic is elsewhere\n",
    "        raise Exception(\"split not in table keys or split == training_summary \") \n",
    "    #table = table_d[split]\n",
    "        \n",
    "    # Need epoch to be repeated to the same length as the rest of the fields \n",
    "    # (this isn't the batch size because we concat a bunch of stuff)\n",
    "    # we don't want to change the `epoch` key because it screws up logging of the other metrics. \n",
    "    # So we make a new dict.\n",
    "    # d1 = copy.deepcopy(results_d)\n",
    "    d1 = results_d\n",
    "    d1['epoch'] = [epoch for o in range(len(d1['pp_l']))]\n",
    "    dcols = [d1[c] for c in table_columns]  # filter out loss_batch\n",
    "    assert len(set([len(o) for o in dcols])) == 1  # all lists should be of the same length \n",
    "    \n",
    "    for row in zip(*dcols):\n",
    "        d2 = {k:v for k,v in zip(table_columns,row)}\n",
    "        data_d[split].append(d2)\n",
    "\n",
    "def update_training_summary_table(results_d, split):\n",
    "    d = dict()\n",
    "    # key names here have to match those in summary_table_columns\n",
    "    d['epoch'] = epoch\n",
    "    d['split'] = split\n",
    "    for metric in metrics:\n",
    "        d[f'{metric}_avg'] = np.mean(results_d[metric])\n",
    "    #data_d['training_summary'].append(*[d[c] for c in summary_table_columns])\n",
    "    data_d['training_summary'].append(d)\n",
    "\n",
    "def log_wandb_tables(run): \n",
    "    \"\"\"Log wandb tables to the UI\"\"\"\n",
    "    d = dict()\n",
    "    d[\"eval/training_summary_table\"] = table_d['training_summary']\n",
    "  #  print(len(d[\"eval/training_summary_table\"].data))\n",
    "    run.log(d)\n",
    "\n",
    "def plot_examples_chart(split, table, metric, commit):\n",
    "    spec = \"uts_nlp/line_chart_v2\"\n",
    "    fields = {\"x\": \"epoch\",'groupKeys': 'idx'}\n",
    "    fields['y'] = f\"{metric}\"\n",
    "    string_fields = dict()\n",
    "    string_fields['title'] = f\"{split}_{metric} vs epoch (examples)\"\n",
    "    chart = wandb.plot_table(vega_spec_name=spec, data_table=table, \n",
    "                            fields=fields, string_fields=string_fields)\n",
    "    wandb.log({f\"individual_examples/{split}_{metric}_vs_epoch_examples\": chart}, commit=commit)\n",
    "\n",
    "def plot_summary_charts(metric, table, commit):\n",
    "    spec = \"uts_nlp/line_chart_v2\"\n",
    "    fields = {\"x\": \"epoch\",'groupKeys': 'split'}\n",
    "    fields['y']  = f\"{metric}_avg\"\n",
    "    chart = wandb.plot_table(vega_spec_name=spec, data_table=table, \n",
    "                                 fields=fields, string_fields={'title': f\"{metric} vs epoch\"})\n",
    "    \n",
    "    wandb.log({f\"summary_charts/avg_{metric}_vs_epoch\": chart}, commit=commit)\n",
    "\n",
    "def plot_wandb_charts(): \n",
    "    if wandb_plot_examples: \n",
    "        # Examples charts \n",
    "        for split in ['train', 'valid']:\n",
    "            df = pd.DataFrame(data_d[split]) if type(data_d[split]) is list else data_d[split]\n",
    "            df = df.query(\"idx in @plt_idx_d[@split]\").sort_values(['idx', 'epoch'])\n",
    "            for metric in metrics: \n",
    "                plot_examples_chart(split, table=wandb.Table(dataframe=df), metric=metric, commit=False)\n",
    "        \n",
    "    ## Summary charts \n",
    "    for metric in metrics: \n",
    "        commit = True if metric == metrics[len(metrics)-1] else False\n",
    "        df = pd.DataFrame(data_d['training_summary'])\n",
    "        plot_summary_charts(metric, table=wandb.Table(dataframe=df),\n",
    "                            commit=commit)\n",
    "\n",
    "def add_wandb_run_summary_statistics(run):\n",
    "    ## Training summary statistics \n",
    "    df_summary = pd.DataFrame(data_d['training_summary']) \n",
    "    # We calculate the best epoch according to the validation set\n",
    "    best_epoch_idx = df_summary.query(\"split=='valid'\")['loss_avg'].idxmin() \n",
    "    valid_row = df_summary.iloc[best_epoch_idx]\n",
    "    best_epoch = valid_row['epoch'].item()\n",
    "    run.summary['best_epoch'] = best_epoch\n",
    "    # iloc transforms 1row df to series (so it is same as  valid_row)\n",
    "    train_row = df_summary.query(\"split=='train' & epoch==@best_epoch\").iloc[0]  \n",
    "    for metric in metrics: \n",
    "        run.summary[f\"{metric}_avg_train\"] = train_row[f\"{metric}_avg\"].item()\n",
    "        run.summary[f\"{metric}_avg_valid\"] = valid_row[f\"{metric}_avg\"].item()\n",
    "                                 \n",
    "    ## Summary statistics of the test set \n",
    "    # From the last epoch atm because we don't have early stopping \n",
    "    test_metrics = data_d['test'].filter(metrics, axis=1).mean()\n",
    "    for metric, val in zip(test_metrics.index, test_metrics): \n",
    "        run.summary[f\"{metric}_avg_test\"] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests and asserts for `get_pp_logp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     17,
     22,
     39,
     49
    ]
   },
   "outputs": [],
   "source": [
    "def get_start_end_special_token_ids(tokenizer): \n",
    "    \"\"\"The token id's that input/output sequences should start and end with\"\"\"\n",
    "    d = {}\n",
    "    if tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "        d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    elif tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "        d[\"input_start_id\"] =  None\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "        d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    else: \n",
    "        raise Exception(\"unrecognised tokenizer\")\n",
    "    return d\n",
    "\n",
    "def check_no_nans_or_infs(x):\n",
    "    assert torch.all(~torch.isnan(x))\n",
    "    assert torch.all(~torch.isneginf(x))\n",
    "    assert torch.all(~torch.isposinf(x))\n",
    "    \n",
    "def assert_start_and_end_tokens_are_correct(tokenizer, orig_token_ids, pp_token_ids):\n",
    "    \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "    right special tokens (depends on tokenizer)\"\"\"\n",
    "    start_end_token_d = get_start_end_special_token_ids(pp_tokenizer)\n",
    "    \n",
    "    # Input\n",
    "    if start_end_token_d['input_start_id'] is not None: \n",
    "        assert torch.all(orig_token_ids[:,0] == start_end_token_d['input_start_id'])\n",
    "    # can probs rewrite this to make it nicer but it's fine for now\n",
    "    assert torch.all(torch.logical_or(orig_token_ids[:,-1] == start_end_token_d['input_end_id'][0], \n",
    "                                      orig_token_ids[:,-1] == start_end_token_d['input_end_id'][1]))\n",
    "    \n",
    "    # Output\n",
    "    assert torch.all(pp_token_ids[:,0] == start_end_token_d['output_start_id'])\n",
    "    assert torch.all(torch.logical_or(pp_token_ids[:,-1] == start_end_token_d['output_end_id'][0], \n",
    "                                      pp_token_ids[:,-1] == start_end_token_d['output_end_id'][1]))\n",
    "    \n",
    "def check_scores_log_softmax_sums(scores_log_softmax):\n",
    "    sums = scores_log_softmax.exp().sum(2)\n",
    "    # check that the axes is right\n",
    "    # we want to sum over token probabilities at each generation step, so we \n",
    "    # should end up with a shape [orig_batch_size, pp_length]\n",
    "    assert sums.shape[0] == orig_batch_size  \n",
    "    assert sums.shape[1] == pp_length - 1\n",
    "    # check that they sum to 1 along the pp_length axis\n",
    "    assert torch.allclose(sums, torch.ones(sums.size(), device=device), atol = 1e-4)\n",
    "    \n",
    "def check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, seq_token_log_probs): \n",
    "    \"\"\"Just enumerates and checks values\n",
    "    Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i_ex in range(orig_batch_size):\n",
    "        for i_step in range(pp_length - 1):\n",
    "            i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "            l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "    assert all(l)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### Set up models and do layer freezing\n",
    "vm_model.eval()\n",
    "pp_model.train()\n",
    "## Layer freezing \n",
    "# Unfreeze last 2 layers of the base model decoder\n",
    "# Not sure if decoder layer norm should be unfrozen or not, but it appears after the\n",
    "#   other parameters in the module ordering, so let's include it for now\n",
    "# Also unfreeze the linear head.  This isn't stored in the base model but rather tacked on top\n",
    "#   and will be fine-tuned for summarisation. \n",
    "if pp_name == \"tuner007/pegasus_paraphrase\":\n",
    "    layer_list = ['decoder.layers.14', 'decoder.layers.15', 'decoder.layer_norm'] \n",
    "elif pp_name == \"tdopierre/ProtAugment-ParaphraseGenerator\":\n",
    "    layer_list = ['decoder.layers.4','decoder.layers.5', 'decoder.layernorm_embedding']\n",
    "elif pp_name == \"eugenesiow/bart-paraphrase\":\n",
    "    layer_list = ['decoder.layers.10','decoder.layers.11', 'decoder.layernorm_embedding']\n",
    "for i, (name,param) in enumerate(pp_model.base_model.named_parameters()): \n",
    "    if np.any([o in name for o in layer_list]):   param.requires_grad = True\n",
    "    else:                                         param.requires_grad = False\n",
    "for param in pp_model.lm_head.parameters():       param.requires_grad = True\n",
    "# Not sure if to include this or not. this seems to affect lm_head. i might just leave it as it was for now.\n",
    "# this will freeze the embeddings/lm head. \n",
    "# From here: https://github.com/huggingface/transformers/issues/10479#issuecomment-788964822\n",
    "# self.lm_head is tied (the same parameter as) to self.encoder.embed_tokens and self.decoder.embed_tokens.\n",
    "for param in pp_model.base_model.shared.parameters(): param.requires_grad = False \n",
    "### For checking the grad status of the layers\n",
    "# for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)\n",
    "# for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    print(i, name, param.requires_grad)\n",
    "\n",
    "#### Optimizer\n",
    "# For now we just keep this simple\n",
    "optimizer = AdamW(pp_model.parameters(), lr=lr)\n",
    "\n",
    "#### Set up other miscellaneous things\n",
    "#rouge_metric = load_metric(\"rouge\", keep_in_memory=True)\n",
    "n_train_steps = n_train_epochs * len(dld_tkn['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training_function(pp_model, vm_model, dld_tkn, dld_raw, optimizer): \n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    \n",
    "    pp_model,vm_model,optimizer,dld_tkn['train'] = accelerator.prepare(\n",
    "        pp_model,vm_model,optimizer,dld_tkn['train']\n",
    "    )\n",
    "    \n",
    "    #logger.info(show_gpu(f' GPU memory usage after loading models:'))\n",
    "    progress_bar = tqdm(range(n_train_steps))\n",
    "\n",
    "    pp_model.zero_grad(set_to_none=zero_grad_with_none) \n",
    "    global accumulation_num, global_step, epoch, batch_num\n",
    "    accumulation_num = 0 \n",
    "    global_step = 0\n",
    "    for epoch in range(n_train_epochs): \n",
    "        logger.info(f\"Now on epoch {epoch} of {n_train_epochs}\")\n",
    "        if not pp_model.training: pp_model.train()\n",
    "        with timecode() as time_train_one_epoch:\n",
    "            for batch_num, (data, raw) in enumerate(zip(dld_tkn['train'], dld_raw['train'])): \n",
    "                if batch_num % 10 == 0 :   logging.info(f\"Now processing batch {batch_num} out of {len(dld_tkn['train'])}\")\n",
    "                training_step(data, raw, accelerator) \n",
    "                accumulation_num += 1\n",
    "                progress_bar.update(1) \n",
    "                global_step += 1 \n",
    "        \n",
    "        wandb.log({'time/train_one_epoch_time': time_train_one_epoch.t,\n",
    "                   'time/train_one_epoch_thoroughput': len(dsd['train']) / time_train_one_epoch.t,\n",
    "                   'epoch': epoch}, commit=True)\n",
    "        \n",
    "        if wandb_log_grads and epoch % wandb_log_grads_freq == 0: \n",
    "            plt = plot_grad_flow(pp_model.named_parameters())\n",
    "            wandb.log({\"gradient flow\": wandb.Image(plt)})  # doesn't work as a non-image (i.e. plotly)\n",
    "            del plt \n",
    "        gc.collect() \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if save_model_while_training and (epoch + 1) % save_model_freq == 0:  save_model(epoch)\n",
    "        \n",
    "        # Evaluation loop\n",
    "        if epoch % eval_freq == 0: \n",
    "            logging.info(f\"Now doing train eval\")\n",
    "            with timecode() as time_eval_train:\n",
    "                train_set_preds = eval_dl(dl_tkn = dld_tkn['train_eval'], dl_raw=dld_raw['train_eval'], \n",
    "                                          device=device)\n",
    "\n",
    "            logging.info(f\"Now doing valid eval\")\n",
    "            with timecode() as time_eval_valid:\n",
    "                valid_set_preds = eval_dl(dl_tkn = dld_tkn['valid'],      dl_raw=dld_raw['valid'], \n",
    "                                        device=device)\n",
    "            \n",
    "            # update the tables every epoch and log them\n",
    "            with timecode() as time_update_training_summary_table:\n",
    "                update_training_summary_table(train_set_preds, split='train')\n",
    "                update_training_summary_table(valid_set_preds, split='valid')\n",
    "            with timecode() as time_add_eval_preds_to_data_d:    \n",
    "                add_preds_to_data_d(train_set_preds, split='train')\n",
    "                add_preds_to_data_d(valid_set_preds, split='valid')\n",
    "            #log_wandb_tables(run)\n",
    "            plot_wandb_charts()\n",
    "            del train_set_preds\n",
    "            del valid_set_preds\n",
    "            with timecode() as time_eval_gc_collect:\n",
    "                gc.collect() \n",
    "            with timecode() as time_eval_empty_cache:\n",
    "                torch.cuda.empty_cache()\n",
    "            wandb.log({'time/eval_train_time': time_eval_train.t, 'time/eval_valid_time': time_eval_valid.t,\n",
    "                       'time/eval_train_thoroughput': len(dsd['train']) / time_eval_train.t,\n",
    "                       'time/eval_valid_thoroughput': len(dsd['valid']) / time_eval_valid.t,\n",
    "                       'time/eval_update_training_summary_table': time_update_training_summary_table.t, \n",
    "                       'time/eval_add_preds_to_data_d': time_add_eval_preds_to_data_d.t,\n",
    "                       'time/eval_gc_collect': time_eval_gc_collect.t, \n",
    "                       'time/eval_empty_cache': time_eval_empty_cache.t,\n",
    "               'epoch': epoch},\n",
    "                      commit=True)\n",
    "    \n",
    "    logging.info(f\"Now doing test eval\")        \n",
    "    # Eval on test set \n",
    "    test_set_preds = eval_dl(dl_tkn = dld_tkn['test'], dl_raw=dld_raw['test'],  device=device)\n",
    "    add_preds_to_data_d(test_set_preds, split='test')\n",
    "    # Log, plot, and finish up\n",
    "    #log_wandb_tables(run)\n",
    "    \n",
    "    # Data -> df and save dfs to file \n",
    "    for key in data_d.keys():  # splits and sometimes 'training_step' too \n",
    "        data_d[key] = pd.DataFrame(data_d[key]) # dict of list of dict -> dict of dataframe\n",
    "        data_d[key].to_csv(f\"{path_run}{key}.csv\", index=False)\n",
    "    # Save training_summary table to csv too \n",
    "    pd.DataFrame(data_d['training_summary']).to_csv(f\"{path_run}training_summary.csv\", index=False)\n",
    "        \n",
    "    plot_wandb_charts()\n",
    "    add_wandb_run_summary_statistics(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muts_nlp\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/uts_nlp/travis_attack/runs/17fsdayz\" target=\"_blank\">devout-darkness-237</a></strong> to <a href=\"https://wandb.ai/uts_nlp/travis_attack\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb2f1d2b29a4fa8890c0c09971f7bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 0 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 0\n",
      "\treward_fn_time 0.01882677525281906\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 0\n",
      "batch_num 0\n",
      "global_step 0\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.027931813150644302\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 1\n",
      "\treward_fn_time 0.017211955040693283\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 0\n",
      "batch_num 1\n",
      "global_step 1\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['I love this apple.', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,   100,   657,    42, 15162,     4,     2,     1,     1,     1],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02395452931523323\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 2\n",
      "\treward_fn_time 0.026250343769788742\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 2\n",
      "\treward_fn_time 0.02561035379767418\n",
      "\t######### \n",
      "Now on epoch 1 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 2\n",
      "\treward_fn_time 0.017356514930725098\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 1\n",
      "batch_num 0\n",
      "global_step 2\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026498612016439438\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 3\n",
      "\treward_fn_time 0.0171470046043396\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 1\n",
      "batch_num 1\n",
      "global_step 3\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['I love this apple.', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,   100,   657,    42, 15162,     4,     2,     1,     1,     1],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023846711963415146\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 2 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 4\n",
      "\treward_fn_time 0.01719825342297554\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 2\n",
      "batch_num 0\n",
      "global_step 4\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.0243082158267498\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 5\n",
      "\treward_fn_time 0.01674463227391243\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 2\n",
      "batch_num 1\n",
      "global_step 5\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['I love this apple.', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,   100,   657,    42, 15162,     4,     2,     1,     1,     1],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02327025681734085\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 3 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 6\n",
      "\treward_fn_time 0.017290890216827393\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 3\n",
      "batch_num 0\n",
      "global_step 6\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024620506912469864\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 7\n",
      "\treward_fn_time 0.016751497983932495\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 3\n",
      "batch_num 1\n",
      "global_step 7\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['I love this apple.', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,   100,   657,    42, 15162,     4,     2,     1,     1,     1],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02330009639263153\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 4 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 8\n",
      "\treward_fn_time 0.023333482444286346\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 4\n",
      "batch_num 0\n",
      "global_step 8\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.03223222866654396\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 9\n",
      "\treward_fn_time 0.016772788017988205\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 4\n",
      "batch_num 1\n",
      "global_step 9\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02335268259048462\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 5 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 10\n",
      "\treward_fn_time 0.01717313751578331\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 5\n",
      "batch_num 0\n",
      "global_step 10\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024266641587018967\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 11\n",
      "\treward_fn_time 0.017181914299726486\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 5\n",
      "batch_num 1\n",
      "global_step 11\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024287927895784378\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 6 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 12\n",
      "\treward_fn_time 0.017342153936624527\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 6\n",
      "batch_num 0\n",
      "global_step 12\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'I hate this apple.']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,   100,  4157,    42, 15162,     4,     2,     1,     1,     1]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02441668137907982\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 13\n",
      "\treward_fn_time 0.01665966957807541\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 6\n",
      "batch_num 1\n",
      "global_step 13\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you think about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023242231458425522\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 7 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 14\n",
      "\treward_fn_time 0.01734282076358795\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 7\n",
      "batch_num 0\n",
      "global_step 14\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024418119341135025\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 15\n",
      "\treward_fn_time 0.0168038010597229\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 7\n",
      "batch_num 1\n",
      "global_step 15\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023412976413965225\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 8 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 16\n",
      "\treward_fn_time 0.01717330887913704\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 8\n",
      "batch_num 0\n",
      "global_step 16\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024467144161462784\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 17\n",
      "\treward_fn_time 0.016843978315591812\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 8\n",
      "batch_num 1\n",
      "global_step 17\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02366381511092186\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 9 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 18\n",
      "\treward_fn_time 0.01781841367483139\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 9\n",
      "batch_num 0\n",
      "global_step 18\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02501584216952324\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 19\n",
      "\treward_fn_time 0.01710527017712593\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 9\n",
      "batch_num 1\n",
      "global_step 19\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023908812552690506\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 10 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 20\n",
      "\treward_fn_time 0.017388418316841125\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 10\n",
      "batch_num 0\n",
      "global_step 20\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024561498314142227\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 21\n",
      "\treward_fn_time 0.016932625323534012\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 10\n",
      "batch_num 1\n",
      "global_step 21\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023699797689914703\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 22\n",
      "\treward_fn_time 0.026347368955612183\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 22\n",
      "\treward_fn_time 0.02581626921892166\n",
      "\t######### \n",
      "Now on epoch 11 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 22\n",
      "\treward_fn_time 0.01786002516746521\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 11\n",
      "batch_num 0\n",
      "global_step 22\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024990815669298172\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 23\n",
      "\treward_fn_time 0.016665704548358917\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 11\n",
      "batch_num 1\n",
      "global_step 23\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023325379937887192\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 12 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 24\n",
      "\treward_fn_time 0.01933326944708824\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 12\n",
      "batch_num 0\n",
      "global_step 24\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026467103511095047\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 25\n",
      "\treward_fn_time 0.016809608787298203\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 12\n",
      "batch_num 1\n",
      "global_step 25\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023461580276489258\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 13 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 26\n",
      "\treward_fn_time 0.02002878487110138\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 13\n",
      "batch_num 0\n",
      "global_step 26\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02711602672934532\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 27\n",
      "\treward_fn_time 0.017362374812364578\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 13\n",
      "batch_num 1\n",
      "global_step 27\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024181965738534927\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 14 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 28\n",
      "\treward_fn_time 0.017418377101421356\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 14\n",
      "batch_num 0\n",
      "global_step 28\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.0244898684322834\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 29\n",
      "\treward_fn_time 0.01682141050696373\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 14\n",
      "batch_num 1\n",
      "global_step 29\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023480255156755447\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 15 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 30\n",
      "\treward_fn_time 0.01752343401312828\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 15\n",
      "batch_num 0\n",
      "global_step 30\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024734798818826675\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 31\n",
      "\treward_fn_time 0.016805995255708694\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 15\n",
      "batch_num 1\n",
      "global_step 31\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02350780740380287\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 16 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 32\n",
      "\treward_fn_time 0.017364125698804855\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 16\n",
      "batch_num 0\n",
      "global_step 32\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02447991818189621\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 33\n",
      "\treward_fn_time 0.016882196068763733\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 16\n",
      "batch_num 1\n",
      "global_step 33\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023606158792972565\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 17 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 34\n",
      "\treward_fn_time 0.01762380078434944\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 17\n",
      "batch_num 0\n",
      "global_step 34\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024775035679340363\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 35\n",
      "\treward_fn_time 0.016852278262376785\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 17\n",
      "batch_num 1\n",
      "global_step 35\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023509826511144638\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 18 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 36\n",
      "\treward_fn_time 0.01831626147031784\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 18\n",
      "batch_num 0\n",
      "global_step 36\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02618977427482605\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 37\n",
      "\treward_fn_time 0.01722181960940361\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 18\n",
      "batch_num 1\n",
      "global_step 37\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02391893044114113\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 19 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 38\n",
      "\treward_fn_time 0.017688386142253876\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 19\n",
      "batch_num 0\n",
      "global_step 38\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.0250357985496521\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 39\n",
      "\treward_fn_time 0.01731353998184204\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 19\n",
      "batch_num 1\n",
      "global_step 39\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024327021092176437\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 20 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 40\n",
      "\treward_fn_time 0.017641372978687286\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 20\n",
      "batch_num 0\n",
      "global_step 40\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02558046579360962\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 41\n",
      "\treward_fn_time 0.01726333051919937\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 20\n",
      "batch_num 1\n",
      "global_step 41\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024326488375663757\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 42\n",
      "\treward_fn_time 0.026869598776102066\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 42\n",
      "\treward_fn_time 0.02599233388900757\n",
      "\t######### \n",
      "Now on epoch 21 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 42\n",
      "\treward_fn_time 0.022808905690908432\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 21\n",
      "batch_num 0\n",
      "global_step 42\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.03130638971924782\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 43\n",
      "\treward_fn_time 0.0215417742729187\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 21\n",
      "batch_num 1\n",
      "global_step 43\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02880968153476715\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 22 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 44\n",
      "\treward_fn_time 0.017376452684402466\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 22\n",
      "batch_num 0\n",
      "global_step 44\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02461310103535652\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 45\n",
      "\treward_fn_time 0.016715165227651596\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 22\n",
      "batch_num 1\n",
      "global_step 45\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023407842963933945\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 23 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 46\n",
      "\treward_fn_time 0.017179854214191437\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 23\n",
      "batch_num 0\n",
      "global_step 46\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024306032806634903\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 47\n",
      "\treward_fn_time 0.01661590114235878\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 23\n",
      "batch_num 1\n",
      "global_step 47\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02322903648018837\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 24 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 48\n",
      "\treward_fn_time 0.01728081703186035\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 24\n",
      "batch_num 0\n",
      "global_step 48\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024287063628435135\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 49\n",
      "\treward_fn_time 0.016902625560760498\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 24\n",
      "batch_num 1\n",
      "global_step 49\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023594100028276443\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 25 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 50\n",
      "\treward_fn_time 0.01734442636370659\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 25\n",
      "batch_num 0\n",
      "global_step 50\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02437756583094597\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 51\n",
      "\treward_fn_time 0.01647801697254181\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 25\n",
      "batch_num 1\n",
      "global_step 51\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02305106818675995\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 26 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 52\n",
      "\treward_fn_time 0.017331361770629883\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 26\n",
      "batch_num 0\n",
      "global_step 52\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024349216371774673\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 53\n",
      "\treward_fn_time 0.01658231019973755\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 26\n",
      "batch_num 1\n",
      "global_step 53\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023136530071496964\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 27 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 54\n",
      "\treward_fn_time 0.01896532252430916\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 27\n",
      "batch_num 0\n",
      "global_step 54\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026142582297325134\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 55\n",
      "\treward_fn_time 0.01678711175918579\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 27\n",
      "batch_num 1\n",
      "global_step 55\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023510582745075226\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 28 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 56\n",
      "\treward_fn_time 0.017244774848222733\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 28\n",
      "batch_num 0\n",
      "global_step 56\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02448146417737007\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 57\n",
      "\treward_fn_time 0.016735799610614777\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 28\n",
      "batch_num 1\n",
      "global_step 57\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02356477826833725\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 29 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 58\n",
      "\treward_fn_time 0.01738734170794487\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 29\n",
      "batch_num 0\n",
      "global_step 58\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02461617812514305\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 59\n",
      "\treward_fn_time 0.016637898981571198\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 29\n",
      "batch_num 1\n",
      "global_step 59\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023414481431245804\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 30 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 60\n",
      "\treward_fn_time 0.017307743430137634\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 30\n",
      "batch_num 0\n",
      "global_step 60\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024441685527563095\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 61\n",
      "\treward_fn_time 0.020307477563619614\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 30\n",
      "batch_num 1\n",
      "global_step 61\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.0274973027408123\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 62\n",
      "\treward_fn_time 0.027033090591430664\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 62\n",
      "\treward_fn_time 0.025725476443767548\n",
      "\t######### \n",
      "Now on epoch 31 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 62\n",
      "\treward_fn_time 0.017665423452854156\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 31\n",
      "batch_num 0\n",
      "global_step 62\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024826154112815857\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 63\n",
      "\treward_fn_time 0.01681576296687126\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 31\n",
      "batch_num 1\n",
      "global_step 63\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02370459958910942\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 32 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 64\n",
      "\treward_fn_time 0.01761796325445175\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 32\n",
      "batch_num 0\n",
      "global_step 64\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025050874799489975\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 65\n",
      "\treward_fn_time 0.016915779560804367\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 32\n",
      "batch_num 1\n",
      "global_step 65\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023615997284650803\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 33 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 66\n",
      "\treward_fn_time 0.017410527914762497\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 33\n",
      "batch_num 0\n",
      "global_step 66\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024711254984140396\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 67\n",
      "\treward_fn_time 0.016845203936100006\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 33\n",
      "batch_num 1\n",
      "global_step 67\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023494407534599304\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 34 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 68\n",
      "\treward_fn_time 0.018516726791858673\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 34\n",
      "batch_num 0\n",
      "global_step 68\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025831829756498337\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 69\n",
      "\treward_fn_time 0.017307840287685394\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 34\n",
      "batch_num 1\n",
      "global_step 69\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024337027221918106\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 35 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 70\n",
      "\treward_fn_time 0.017292462289333344\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 35\n",
      "batch_num 0\n",
      "global_step 70\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024407055228948593\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 71\n",
      "\treward_fn_time 0.017277825623750687\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 35\n",
      "batch_num 1\n",
      "global_step 71\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024156462401151657\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 36 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 72\n",
      "\treward_fn_time 0.021083693951368332\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 36\n",
      "batch_num 0\n",
      "global_step 72\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.028593212366104126\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 73\n",
      "\treward_fn_time 0.016834352165460587\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 36\n",
      "batch_num 1\n",
      "global_step 73\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023636765778064728\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 37 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 74\n",
      "\treward_fn_time 0.01738252118229866\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 37\n",
      "batch_num 0\n",
      "global_step 74\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024860955774784088\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 75\n",
      "\treward_fn_time 0.01671597734093666\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 37\n",
      "batch_num 1\n",
      "global_step 75\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02379439026117325\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 38 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 76\n",
      "\treward_fn_time 0.017557557672262192\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 38\n",
      "batch_num 0\n",
      "global_step 76\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02526164799928665\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 77\n",
      "\treward_fn_time 0.01714909076690674\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 38\n",
      "batch_num 1\n",
      "global_step 77\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024310819804668427\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 39 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 78\n",
      "\treward_fn_time 0.017491120845079422\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 39\n",
      "batch_num 0\n",
      "global_step 78\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024888336658477783\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 79\n",
      "\treward_fn_time 0.016947560012340546\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 39\n",
      "batch_num 1\n",
      "global_step 79\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023861519992351532\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 40 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 80\n",
      "\treward_fn_time 0.017409205436706543\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 40\n",
      "batch_num 0\n",
      "global_step 80\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024545341730117798\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 81\n",
      "\treward_fn_time 0.018142245709896088\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 40\n",
      "batch_num 1\n",
      "global_step 81\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025161463767290115\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 82\n",
      "\treward_fn_time 0.027051199227571487\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 82\n",
      "\treward_fn_time 0.026211507618427277\n",
      "\t######### \n",
      "Now on epoch 41 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 82\n",
      "\treward_fn_time 0.017322465777397156\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 41\n",
      "batch_num 0\n",
      "global_step 82\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025045257061719894\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 83\n",
      "\treward_fn_time 0.01682364195585251\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 41\n",
      "batch_num 1\n",
      "global_step 83\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023619238287210464\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 42 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 84\n",
      "\treward_fn_time 0.016991782933473587\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 42\n",
      "batch_num 0\n",
      "global_step 84\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025583483278751373\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 85\n",
      "\treward_fn_time 0.016706805676221848\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 42\n",
      "batch_num 1\n",
      "global_step 85\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023480024188756943\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 43 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 86\n",
      "\treward_fn_time 0.01783515140414238\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 43\n",
      "batch_num 0\n",
      "global_step 86\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.027551058679819107\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 87\n",
      "\treward_fn_time 0.01710985228419304\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 43\n",
      "batch_num 1\n",
      "global_step 87\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023893535137176514\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 44 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 88\n",
      "\treward_fn_time 0.017360802739858627\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 44\n",
      "batch_num 0\n",
      "global_step 88\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024685613811016083\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 89\n",
      "\treward_fn_time 0.01679570972919464\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 44\n",
      "batch_num 1\n",
      "global_step 89\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02360077202320099\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 45 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 90\n",
      "\treward_fn_time 0.01738395169377327\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 45\n",
      "batch_num 0\n",
      "global_step 90\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024594414979219437\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 91\n",
      "\treward_fn_time 0.01710861548781395\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 45\n",
      "batch_num 1\n",
      "global_step 91\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024140305817127228\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 46 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 92\n",
      "\treward_fn_time 0.01904015988111496\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 46\n",
      "batch_num 0\n",
      "global_step 92\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026684634387493134\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 93\n",
      "\treward_fn_time 0.01708432286977768\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 46\n",
      "batch_num 1\n",
      "global_step 93\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02387888729572296\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on epoch 47 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 94\n",
      "\treward_fn_time 0.018363341689109802\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 47\n",
      "batch_num 0\n",
      "global_step 94\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02584323287010193\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 95\n",
      "\treward_fn_time 0.01680738478899002\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 47\n",
      "batch_num 1\n",
      "global_step 95\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02354176715016365\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 48 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 96\n",
      "\treward_fn_time 0.019305050373077393\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 48\n",
      "batch_num 0\n",
      "global_step 96\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.03399698808789253\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 97\n",
      "\treward_fn_time 0.017606504261493683\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 48\n",
      "batch_num 1\n",
      "global_step 97\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024536248296499252\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 49 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 98\n",
      "\treward_fn_time 0.017907191067934036\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 49\n",
      "batch_num 0\n",
      "global_step 98\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02524728700518608\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 99\n",
      "\treward_fn_time 0.01714048907160759\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 49\n",
      "batch_num 1\n",
      "global_step 99\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02391994744539261\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 50 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 100\n",
      "\treward_fn_time 0.017334643751382828\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 50\n",
      "batch_num 0\n",
      "global_step 100\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024521268904209137\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 101\n",
      "\treward_fn_time 0.016865171492099762\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 50\n",
      "batch_num 1\n",
      "global_step 101\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023635756224393845\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 102\n",
      "\treward_fn_time 0.02651326358318329\n",
      "\t######### \n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 102\n",
      "\treward_fn_time 0.02596518024802208\n",
      "\t######### \n",
      "Now on epoch 51 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 102\n",
      "\treward_fn_time 0.022552572190761566\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 51\n",
      "batch_num 0\n",
      "global_step 102\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.030582226812839508\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 103\n",
      "\treward_fn_time 0.01767657697200775\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 51\n",
      "batch_num 1\n",
      "global_step 103\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02482769638299942\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 52 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 104\n",
      "\treward_fn_time 0.019488632678985596\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 52\n",
      "batch_num 0\n",
      "global_step 104\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026595953851938248\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 105\n",
      "\treward_fn_time 0.017045192420482635\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 52\n",
      "batch_num 1\n",
      "global_step 105\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023891132324934006\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 53 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 106\n",
      "\treward_fn_time 0.017506036907434464\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 53\n",
      "batch_num 0\n",
      "global_step 106\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024519845843315125\n",
      "### INSIDE training_step ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 107\n",
      "\treward_fn_time 0.016950063407421112\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 53\n",
      "batch_num 1\n",
      "global_step 107\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023600667715072632\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 54 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 108\n",
      "\treward_fn_time 0.01966610550880432\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 54\n",
      "batch_num 0\n",
      "global_step 108\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026709921658039093\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 109\n",
      "\treward_fn_time 0.0168977789580822\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 54\n",
      "batch_num 1\n",
      "global_step 109\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023598410189151764\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 55 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 110\n",
      "\treward_fn_time 0.018898557871580124\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 55\n",
      "batch_num 0\n",
      "global_step 110\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.026172209531068802\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 111\n",
      "\treward_fn_time 0.01689336821436882\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 55\n",
      "batch_num 1\n",
      "global_step 111\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023523904383182526\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 56 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 112\n",
      "\treward_fn_time 0.01740844175219536\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 56\n",
      "batch_num 0\n",
      "global_step 112\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.03177477419376373\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 113\n",
      "\treward_fn_time 0.016766738146543503\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 56\n",
      "batch_num 1\n",
      "global_step 113\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.023528769612312317\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 57 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 114\n",
      "\treward_fn_time 0.017407990992069244\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 57\n",
      "batch_num 0\n",
      "global_step 114\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.024504080414772034\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 115\n",
      "\treward_fn_time 0.018107596784830093\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 57\n",
      "batch_num 1\n",
      "global_step 115\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025019798427820206\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 58 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 116\n",
      "\treward_fn_time 0.01795317605137825\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 58\n",
      "batch_num 0\n",
      "global_step 116\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025144465267658234\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 117\n",
      "\treward_fn_time 0.01725694164633751\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 58\n",
      "batch_num 1\n",
      "global_step 117\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02403811365365982\n",
      "### INSIDE training_step ####\n",
      "Now on epoch 59 of 60\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 118\n",
      "\treward_fn_time 0.018260445445775986\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 59\n",
      "batch_num 0\n",
      "global_step 118\n",
      "model in training mode True\n",
      "orig ['I do not like this movie', 'I hate this apple']\n",
      "pp_l ['What do you think about this movie?', 'What do you hate about this apple?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2],\n",
      "        [    2,  2264,   109,    47,  4157,    59,    42, 15162,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.025665469467639923\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 119\n",
      "\treward_fn_time 0.017329897731542587\n",
      "\t######### \n",
      "### INSIDE training_step ####\n",
      "epoch 59\n",
      "batch_num 1\n",
      "global_step 119\n",
      "model in training mode True\n",
      "orig ['I love this apple', 'I like this movie']\n",
      "pp_l ['What do you love about this apple?', 'What do you think about this movie?']\n",
      "pp_seq tensor([[    2,  2264,   109,    47,   657,    59,    42, 15162,   116,     2],\n",
      "        [    2,  2264,   109,    47,   206,    59,    42,  1569,   116,     2]],\n",
      "       device='cuda:0')\n",
      "pp_length torch.Size([2, 10]) 10\n",
      "loss_fn_time 0.02431391179561615\n",
      "### INSIDE training_step ####\n",
      "\t### INSIDE Reward_fn### \n",
      "\tglobal_step 120\n",
      "\treward_fn_time 0.027331508696079254\n",
      "\t######### \n"
     ]
    }
   ],
   "source": [
    "#resume_model(f\"{path_checkpoints}devout-durian-172_39\")\n",
    "## Launch run and configure what it is tracking\n",
    "#wandb_mode='disabled'\n",
    "#wandb_mode='online'\n",
    "run = wandb.init(project=\"travis_attack\", entity=\"uts_nlp\", config=config_d,\n",
    "                 mode=wandb_mode, notes=run_notes)\n",
    "if wandb_log_grads: wandb.watch(pp_model, log='gradients', log_freq=wandb_log_grads_freq)\n",
    "\n",
    "path_run = f\"{path_checkpoints}{run.name}/\"\n",
    "if not os.path.exists(path_run): os.makedirs(path_run, exist_ok=True)\n",
    "### Set up tables \n",
    "\n",
    "\n",
    "##### TO REFACTOR ######\n",
    "# These have to be in the keys of the output from eval_dl\n",
    "# table_columns = ['idx', 'orig_l',  'truelabel', 'orig_truelabel_probs', 'epoch', 'pp_l',\n",
    "#              'pp_truelabel_probs', \"pp_predclass\", \"pp_predclass_probs\"] + metrics\n",
    "# def make_table(cols): return wandb.Table(columns=cols)\n",
    "# for key in splits:                table_d[key]             = make_table(table_columns) \n",
    "# if wandb_log_training_step_table: table_d['training_step'] = make_table(table_columns) \n",
    "# summary_table_columns = ['epoch','split'] + [f'{m}_avg' for m in metrics]\n",
    "# table_d['training_summary'] = make_table(summary_table_columns)\n",
    "########################\n",
    "\n",
    "\n",
    "#### NEW ####\n",
    "## Raw observation data (lists of dicts, later becomes pandas df)\n",
    "# These have to be in the keys of the output from eval_dl\n",
    "table_columns = ['idx', 'orig_l',  'truelabel', 'orig_truelabel_probs', 'epoch', 'pp_l',\n",
    "             'pp_truelabel_probs', \"pp_predclass\", \"pp_predclass_probs\"] + metrics\n",
    "summary_table_columns = ['epoch','split'] + [f'{m}_avg' for m in metrics]\n",
    "\n",
    "data_d = dict()\n",
    "for key in splits + ['training_summary']:  data_d[key]             = [] \n",
    "if wandb_log_training_step_table:          data_d['training_step'] = []\n",
    "\n",
    "## Training summary table (W&B table)\n",
    "# Future W&B tables here\n",
    "#table_d = dict()\n",
    "#def make_table(cols): return wandb.Table(columns=cols)\n",
    "#table_d['training_summary'] = make_table(summary_table_columns)\n",
    "#############\n",
    "\n",
    "## Get indices for the examples plots\n",
    "if wandb_plot_examples:\n",
    "    def get_examples_plot_idxs(ds): \n",
    "        return np.random.choice(ds['idx'], size=wandb_n_examples_plot, replace=False).tolist()\n",
    "    plt_idx_d = dict()\n",
    "    for split in splits: plt_idx_d[split] = get_examples_plot_idxs(dsd[split])\n",
    "\n",
    "#%lprun -f training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)\n",
    "notebook_launcher(training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12927... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.06MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.97223922208…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>%_of_tokens_above_prob_0.0001</td><td>█▇▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>%_of_tokens_above_prob_0.001</td><td>█▇▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>%_of_tokens_above_prob_0.01</td><td>█▇▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>%_of_tokens_above_prob_0.1</td><td>█▅▂▃▃▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>%_of_tokens_above_prob_1/vocab_size</td><td>█▇▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>%_of_tokens_above_prob_1e-05</td><td>█▇▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_num</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_lower_quartile</td><td>█▇▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_max</td><td>▇█▆▆▃▄▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_mean</td><td>█▆▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_median</td><td>█▆▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_min</td><td>█▇▄▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ent_upper_quartile</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>orig_batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>orig_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>pp_batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>pp_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rank_1_token_prob_0.25_quantile</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>rank_1_token_prob_0.75_quantile</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>rank_1_token_prob_mean</td><td>▁▄▆▆▇▇██████████████████████████████████</td></tr><tr><td>rank_1_token_prob_median</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>rank_2_token_prob_0.25_quantile</td><td>█▇▆▆▅▅▅▄▄▃▄▄▂▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▁▂▁▂▁▁▂▂▂▂▁▂</td></tr><tr><td>rank_2_token_prob_0.75_quantile</td><td>█▇▆▆▅▅▄▅▄▃▄▃▃▃▃▃▂▃▂▃▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂</td></tr><tr><td>rank_2_token_prob_mean</td><td>█▇▆▆▆▅▅▅▄▃▄▃▂▃▃▃▂▂▂▃▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁</td></tr><tr><td>rank_2_token_prob_median</td><td>█▇▆▆▅▅▄▄▃▃▄▃▂▂▃▃▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>rank_3_token_prob_0.25_quantile</td><td>██▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▁▂▁▂▁▁▂▂▂▂▁▁</td></tr><tr><td>rank_3_token_prob_0.75_quantile</td><td>██▇▅▆▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>rank_3_token_prob_mean</td><td>██▇▆▆▆▅▅▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>rank_3_token_prob_median</td><td>▇█▇▆▅▅▅▅▄▃▃▄▃▃▃▃▃▃▃▃▁▃▂▂▂▁▂▂▂▁▁▂▂▂▂▂▁▁▁▁</td></tr><tr><td>time/add_to_training_step_table</td><td>▇▇███████▇█▇▇▇▇▁████▇██████▇███████▇▇█▇▇</td></tr><tr><td>time/backwards_pass</td><td>▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▃▇█▇▇▇▇▇▇▇▇▇▇▇▇▁▂▁▁▁▂▁▁</td></tr><tr><td>time/eval_add_preds_to_data_d</td><td>█▁▁▂▁█</td></tr><tr><td>time/eval_empty_cache</td><td>▁▃▂▆██</td></tr><tr><td>time/eval_gc_collect</td><td>▁█▁▄▃▃</td></tr><tr><td>time/eval_train_thoroughput</td><td>██▆▁▁▅</td></tr><tr><td>time/eval_train_time</td><td>▁▁▃██▄</td></tr><tr><td>time/eval_update_training_summary_table</td><td>▄▅▁▆█▂</td></tr><tr><td>time/eval_valid_thoroughput</td><td>▁█▆▇▆▂</td></tr><tr><td>time/eval_valid_time</td><td>█▁▃▂▃▇</td></tr><tr><td>time/generate_pp</td><td>▅▃▄▂▄▂▅▃▅▄▄▂█▂▅▁▁▁▄▅▄▂▂▅▂▃▃▆▃▅█▄▆▇▆▂▃▂▂▃</td></tr><tr><td>time/log_entropy</td><td>▄▃▁▁▂▂▂▃▂▄▂▃▃▄█▁▂▁▂▃▅▃▁▅▁▃▃▅▂▂▃▄▅▃▅▂▃▂▅▄</td></tr><tr><td>time/log_token_probabilities</td><td>▂▂▁▃▂▃▄▃▃▄▅▄▃▇▆▅▄▃▃▄▅▄▄█▅▅▆▆▄▆█▅▅▅▅▄▄▄▅▄</td></tr><tr><td>time/loss_fn</td><td>▂▂▁▁▁▁▂▂▁▂▂▂▂▃█▁▂▁▂▂▆▂▂▃▂▂▂▄▂▂▂▂▃▂▃▂▂▂▃▃</td></tr><tr><td>time/loss_fn_loss_calc</td><td>▁▂▁▁▁▁▁▁▂▂▃▃▁▂▂▂▂▁▁▁█▂▁▁▂▆▁▂▂▃▂▂▄▂▁▂▁▂▂▂</td></tr><tr><td>time/optimizer_step</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁█▁▁▁▂</td></tr><tr><td>time/pp_logp</td><td>▃▃▁▁▁▂▂▂▂▄▃▃▃▅█▃▂▁▂▃▆▃▂▆▂▅▃▅▂▄▅▄▅▃▅▂▃▂▅▄</td></tr><tr><td>time/pp_logp_detach</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁█</td></tr><tr><td>time/reward_fn</td><td>▂▂▁▁▁▁▂▂▁▂▁▂▂▂█▁▂▁▁▁▆▁▂▂▁▁▂▃▁▂▂▂▃▂▃▂▂▂▃▂</td></tr><tr><td>time/sts_scores</td><td>▂▂▁▁▁▁▂▁▁▂▁▁▂▂█▁▁▁▁▁▇▁▁▂▁▁▂▃▁▂▂▁▂▂▂▁▁▁▃▂</td></tr><tr><td>time/train_one_epoch_thoroughput</td><td>▅▇█▇█▇▇▇▇▇▇▇▅▇▅██▇▂▁▂▁▂▁▂▂▂▂▂▁▁▁▄▆▅▇▇█▇▆</td></tr><tr><td>time/train_one_epoch_time</td><td>▃▂▁▂▁▂▂▂▂▂▂▂▄▂▃▁▁▂▇█▇█▇█▇▇▇▇▇██▇▄▃▃▂▂▁▂▃</td></tr><tr><td>time/vm_scores</td><td>▄▄▂▂▂▃▄▃▂▄▂▃▄▆█▂▃▁▄▃▂▂▃▅▄▂▃▇▂▄▄▄▆▄▇▄▃▃▇▄</td></tr><tr><td>top_token_prob_over_0.1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top_token_prob_over_0.3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top_token_prob_over_0.5</td><td>▁▅▅▅████████████████████████████████████</td></tr><tr><td>top_token_prob_over_0.75</td><td>▁▅▇▆▇▇██████████████████████████████████</td></tr><tr><td>top_token_prob_over_0.9</td><td>▁▃▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>top_token_prob_over_0.95</td><td>▁▂▆▇▇▇██████████████████████████████████</td></tr><tr><td>top_token_prob_over_0.975</td><td>▁▂▄▆▆▇██████████████████████████████████</td></tr><tr><td>top_token_prob_over_0.99</td><td>▁▁▂▃▅▅▇▆████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>%_of_tokens_above_prob_0.0001</td><td>3e-05</td></tr><tr><td>%_of_tokens_above_prob_0.001</td><td>2e-05</td></tr><tr><td>%_of_tokens_above_prob_0.01</td><td>2e-05</td></tr><tr><td>%_of_tokens_above_prob_0.1</td><td>2e-05</td></tr><tr><td>%_of_tokens_above_prob_1/vocab_size</td><td>7e-05</td></tr><tr><td>%_of_tokens_above_prob_1e-05</td><td>9e-05</td></tr><tr><td>batch_num</td><td>1</td></tr><tr><td>best_epoch</td><td>30</td></tr><tr><td>ent_lower_quartile</td><td>0.00094</td></tr><tr><td>ent_max</td><td>0.00939</td></tr><tr><td>ent_mean</td><td>0.00297</td></tr><tr><td>ent_median</td><td>0.00196</td></tr><tr><td>ent_min</td><td>0.00013</td></tr><tr><td>ent_upper_quartile</td><td>0.0044</td></tr><tr><td>epoch</td><td>59</td></tr><tr><td>global_step</td><td>119</td></tr><tr><td>label_flip_avg_test</td><td>0.25</td></tr><tr><td>label_flip_avg_train</td><td>0.25</td></tr><tr><td>label_flip_avg_valid</td><td>0.5</td></tr><tr><td>loss_avg_test</td><td>0.02906</td></tr><tr><td>loss_avg_train</td><td>0.00164</td></tr><tr><td>loss_avg_valid</td><td>0.02561</td></tr><tr><td>orig_batch_size</td><td>2</td></tr><tr><td>orig_length</td><td>8</td></tr><tr><td>pp_batch_size</td><td>2</td></tr><tr><td>pp_length</td><td>10</td></tr><tr><td>pp_logp_avg_test</td><td>-0.05225</td></tr><tr><td>pp_logp_avg_train</td><td>-0.00251</td></tr><tr><td>pp_logp_avg_valid</td><td>-0.03685</td></tr><tr><td>rank_1_token_prob_0.25_quantile</td><td>-0.00042</td></tr><tr><td>rank_1_token_prob_0.75_quantile</td><td>-8e-05</td></tr><tr><td>rank_1_token_prob_mean</td><td>-0.00027</td></tr><tr><td>rank_1_token_prob_median</td><td>-0.00019</td></tr><tr><td>rank_2_token_prob_0.25_quantile</td><td>-10.17591</td></tr><tr><td>rank_2_token_prob_0.75_quantile</td><td>-8.42242</td></tr><tr><td>rank_2_token_prob_mean</td><td>-9.58895</td></tr><tr><td>rank_2_token_prob_median</td><td>-9.51012</td></tr><tr><td>rank_3_token_prob_0.25_quantile</td><td>-11.84684</td></tr><tr><td>rank_3_token_prob_0.75_quantile</td><td>-9.51237</td></tr><tr><td>rank_3_token_prob_mean</td><td>-10.68438</td></tr><tr><td>rank_3_token_prob_median</td><td>-10.65129</td></tr><tr><td>reward_avg_test</td><td>0.65605</td></tr><tr><td>reward_avg_train</td><td>0.67814</td></tr><tr><td>reward_avg_valid</td><td>0.46689</td></tr><tr><td>sts_score_avg_test</td><td>0.6997</td></tr><tr><td>sts_score_avg_train</td><td>0.80357</td></tr><tr><td>sts_score_avg_valid</td><td>0.70964</td></tr><tr><td>time/add_to_training_step_table</td><td>0.00256</td></tr><tr><td>time/backwards_pass</td><td>0.09797</td></tr><tr><td>time/eval_add_preds_to_data_d</td><td>5e-05</td></tr><tr><td>time/eval_empty_cache</td><td>0.0023</td></tr><tr><td>time/eval_gc_collect</td><td>0.14546</td></tr><tr><td>time/eval_train_thoroughput</td><td>26.90078</td></tr><tr><td>time/eval_train_time</td><td>0.14869</td></tr><tr><td>time/eval_update_training_summary_table</td><td>0.00021</td></tr><tr><td>time/eval_valid_thoroughput</td><td>28.15443</td></tr><tr><td>time/eval_valid_time</td><td>0.14207</td></tr><tr><td>time/generate_pp</td><td>0.13191</td></tr><tr><td>time/log_entropy</td><td>0.00155</td></tr><tr><td>time/log_token_probabilities</td><td>0.0033</td></tr><tr><td>time/loss_fn</td><td>0.02431</td></tr><tr><td>time/loss_fn_loss_calc</td><td>5e-05</td></tr><tr><td>time/optimizer_step</td><td>0.00365</td></tr><tr><td>time/pp_logp</td><td>0.00656</td></tr><tr><td>time/pp_logp_detach</td><td>2e-05</td></tr><tr><td>time/reward_fn</td><td>0.01733</td></tr><tr><td>time/sts_scores</td><td>0.01179</td></tr><tr><td>time/train_one_epoch_thoroughput</td><td>7.05384</td></tr><tr><td>time/train_one_epoch_time</td><td>0.56707</td></tr><tr><td>time/vm_scores</td><td>0.00532</td></tr><tr><td>top_token_prob_over_0.1</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.3</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.5</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.75</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.9</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.95</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.975</td><td>1.0</td></tr><tr><td>top_token_prob_over_0.99</td><td>1.0</td></tr><tr><td>vm_score_avg_test</td><td>0.20987</td></tr><tr><td>vm_score_avg_train</td><td>0.22599</td></tr><tr><td>vm_score_avg_valid</td><td>0.29956</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 42 media file(s), 6 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devout-darkness-237</strong>: <a href=\"https://wandb.ai/uts_nlp/travis_attack/runs/17fsdayz\" target=\"_blank\">https://wandb.ai/uts_nlp/travis_attack/runs/17fsdayz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220211_171931-17fsdayz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-16-6aaf1f276005>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datasets import load_metric, Dataset\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.debugger import set_trace\n",
    "import plotly.express as px\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "import functools\n",
    "import string\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import operator\n",
    "import spacy\n",
    "import textstat\n",
    "import difflib as dl\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "run_name = run.name\n",
    "run_id = run.id\n",
    "# TODO: merge this with the other path \n",
    "path_run_results = f\"../model_checkpoints/travis_attack/{run_name}/\"\n",
    "train            = pd.read_csv(path_run_results + \"train.csv\")\n",
    "valid            = pd.read_csv(path_run_results + \"valid.csv\")\n",
    "#test             = pd.read_csv(path_run_results + \"test.csv\")\n",
    "training_step    = pd.read_csv(path_run_results + \"training_step.csv\")\n",
    "#training_summary = pd.read_csv(path_run_results + \"training_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 2\n",
    "df_training_step = postprocess_df(training_step, filter_idx=None, num_proc=num_proc)\n",
    "df_train = postprocess_df(train, filter_idx=None, num_proc=num_proc)\n",
    "df_valid = postprocess_df(valid, filter_idx=None,  num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_step.to_pickle(path_run_results + \"df_temp_training_step.pkl\")\n",
    "df_train.to_pickle(path_run_results + \"df_temp_train.pkl\")\n",
    "df_valid.to_pickle(path_run_results + \"df_temp_valid.pkl\")\n",
    "# df_training_step = pd.read_pickle(path_run_results + \"df_temp_training_step.pkl\")\n",
    "# df_train = pd.read_pickle(path_run_results + \"df_temp_train.pkl\")\n",
    "# df_valid = pd.read_pickle(path_run_results + \"df_temp_valid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_step['data_split'] = 'training_step'\n",
    "df_train['data_split'] = 'eval_train'\n",
    "df_valid['data_split'] = 'eval_valid'\n",
    "df_concat = pd.concat([df_training_step,df_train,df_valid])\n",
    "df_concat = df_concat.reset_index(drop=True)\n",
    "# Otherwise we get a big spike at 0\n",
    "df_concat.loc[df_concat.epoch_of_first_label_flip == 0, 'epoch_of_first_label_flip'] = None\n",
    "fig_l = []\n",
    "\n",
    "hist_config_dicts = [\n",
    "    {\n",
    "        'colname': 'epoch_of_first_label_flip', \n",
    "        'xlabel': \"Epoch of first label flip\", \n",
    "        'desc': \"Cumulative prob Epoch of first label flip for each original example\",\n",
    "        'cumulative': True,\n",
    "    },\n",
    "    {\n",
    "        'colname': 'idx_n_unique_pp', \n",
    "        'xlabel': \"Unique paraphrases per original example\", \n",
    "        \"desc\": \"Number of generated unique paraphrases per original example during training\", \n",
    "        'cumulative': False,\n",
    "    },\n",
    "    {\n",
    "        'colname': 'idx_n_pp_changes', \n",
    "        'xlabel': \"Paraphrase changes per original example\", \n",
    "        \"desc\": \"Number of paraphrase changes per original example during training\", \n",
    "        'cumulative': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "fig_l = []\n",
    "line_colnames = [o for o in df_concat.columns if \"_diff\" in o] + [\n",
    "    'rouge_score', \"is_truncation\", 'any_phrase_capitalised', 'any_phrase_decapitalised', \n",
    "'n_segments_inserted', 'n_segments_removed', 'n_tokens_inserted', 'n_tokens_removed','edit_distance_token_level']\n",
    "for colname in line_colnames: \n",
    "    fig = plot_epoch_line_charts(df_concat, colname)\n",
    "    fig_l.append({f\"pp_metrics/{colname}\":fig })\n",
    "    \n",
    "for d in hist_config_dicts: \n",
    "    fig = plot_idx_hist(df_concat, d['colname'],d['xlabel'],d['cumulative'])\n",
    "    fig_l.append({f\"pp_metrics/{d['colname']}\":fig })\n",
    "\n",
    "d1 = {k: v for d in fig_l for k, v in d.items()}\n",
    "\n",
    "run.log(d1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and debugging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Verifying that the weights update each training step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check_parameters_update(dl)  # from utils script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code scraps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiments around plotting average parameter updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_parameter_group_dict(): \n",
    "#     \"\"\"Function to create \"groups\" of parameters. This is useful to check how much a group of \n",
    "#     parameters updates at an epoch. \n",
    "#     Parameter groups are hardcoded into this code for now. \n",
    "#     \"\"\"\n",
    "#     # Identify which parameters should be grouped together\n",
    "#     isolates = ['model.shared.weight',\"model.encoder.embed_positions.weight\", \"model.encoder.layer_norm\",\n",
    "#                 \"model.decoder.embed_positions.weight\", \"model.decoder.layer_norm\"]\n",
    "#     layers_base = [\"model.encoder.layers\", \"model.decoder.layers\"]\n",
    "#     def flatten_list(l): return list(np.concatenate(l).flat)\n",
    "#     layers = flatten_list([[lyr + \".\" + str(o) +\".\" for o in list(range(16))] for lyr in layers_base])\n",
    "#     parameter_groups = layers + isolates\n",
    "#     # Sort the parameter groups by the order they appear in the model \n",
    "#     all_params = [name for name,_ in pp_model.named_parameters()]\n",
    "#     ordering = [np.min(np.where([pg in o for o in all_params])) for pg in parameter_groups]\n",
    "#     parameter_groups = [o for _,o in sorted(zip(ordering, parameter_groups))]\n",
    "#     # Assign each model parameter a parameter group \n",
    "#     group_d = dict()\n",
    "#     for pg in parameter_groups: \n",
    "#         name = pg[:-1] if pg in layers else pg  # remove the \".\" from the end of the name for the numeric layers\n",
    "#         group_d[name] = [o for o in all_params if pg in o]\n",
    "#     return group_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_parameter_update_amount(): \n",
    "#     group_d = get_parameter_group_dict()\n",
    "#     params_all_initial_d = dict(params_all_initial)\n",
    "#     params_all_d = dict(params_all)\n",
    "#     group_d = get_parameter_group_dict()\n",
    "#     df_d = dict()\n",
    "#     for k,param_l in group_d.items(): \n",
    "#         l = list()\n",
    "#         for p in param_l: \n",
    "#             l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "#         l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "#         df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "#     df = pd.DataFrame(df_d)\n",
    "#     df.index = pd.DataFrame([1,2,3]).describe().index\n",
    "#     return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Random code snippets\n",
    "\n",
    "# initial_params = [(name, p.detach().clone()) for (name, p) in pp_model.named_parameters()]\n",
    "# loss, reward, pp_logp = training_step(data) \n",
    "# update_d =  dict()\n",
    "# for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#     update_d[name] = torch.abs(old_p - new_p).detach().flatten()     \n",
    "    \n",
    "#             update_d =  dict()\n",
    "#             for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#                 update_d[name] = torch.abs(old_p - new_p).flatten() \n",
    "#                 print (name, torch.norm(new_p - old_p).item())  \n",
    "            \n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             initial_params_d,current_params_d = dict(initial_params),dict()\n",
    "#             params_all_d = dict(params_all)\n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             df_d = dict()\n",
    "#             for k,param_l in group_d.items(): \n",
    "#                 l = list()\n",
    "#                 for p in param_l: \n",
    "#                     l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "#                 l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "#                 df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "#             df = pd.DataFrame(df_d)\n",
    "#             df.index = pd.DataFrame([1,2,3]).describe().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generating a paraphrase dataset and getting VM predictions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def create_paraphrase_dataset(batch, cname_input, cname_output, num_beams=32,\n",
    "#                               num_return_sequences=32): \n",
    "#     \"\"\"Create paraphrases for each example in the batch. Then repeat the other fields \n",
    "#         so that the resulting datase has the same length as the number of paraphrases. \n",
    "#         Key assumption is \n",
    "#         that the same number of paraphrases is created for each example.\n",
    "#         batch: a dict of examples used by the `map` function from the dataset\n",
    "#         cname_input: What column to create paraphrases of \n",
    "#         cname_output: What to call the column of paraphrases\n",
    "#         other parameters - passed to get_paraphrases. \"\"\"\n",
    "    \n",
    "#     # Generate paraphrases. \n",
    "#     # This can be later extended to add diversity or so on. \n",
    "#     #set_trace()\n",
    "#     pp_l,probs = get_paraphrases(batch[cname_input], num_beams=num_beams,\n",
    "#         num_return_sequences=num_return_sequences)\n",
    "    \n",
    "#     # To return paraphrases as a list of lists for batch input (not done here but might need later)\n",
    "#     #     split_into_sublists = lambda l,n: [l[i:i + n] for i in range(0, len(l), n)]\n",
    "#     #     pp_l = split_into_sublists(pp_l, n_seed_seqs)\n",
    "#     batch[cname_output] = pp_l \n",
    "#     batch[\"probs\"] = probs.to('cpu').numpy()\n",
    "    \n",
    "#     # Repeat each entry in all other columns `num_return_sequences` times so they are the same length\n",
    "#     # as the paraphrase column\n",
    "#     # Only works if the same number of paraphrases is generated for each phrase. \n",
    "#     # Else try something like \n",
    "#         # for o in zip(*batch.values()):\n",
    "#         #     d = dict(zip(batch.keys(), o))\n",
    "#         #     get_paraphrases(batch[cname_input],num_return_sequences=n_seed_seqs,num_beams=n_seed_seqs)\n",
    "#         #     for k,v in d.items(): \n",
    "#         #       return_d[k] += v if k == 'text' else [v for o in range(n_paraphrases)]\n",
    "#         # return return_d\n",
    "#     return_d = defaultdict(list) \n",
    "#     repeat_each_item_n_times = lambda l,n: [o for o in l for i in range(n)]\n",
    "#     for k in batch.keys(): \n",
    "#         if   k == cname_output: return_d[k] = batch[cname_output]\n",
    "#         elif k == \"probs\"     : return_d[k] = batch[\"probs\"]\n",
    "#         else:                   return_d[k] = repeat_each_item_n_times(batch[k], num_return_sequences)\n",
    "#     return return_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_vm_scores(ds_pp, cname_orig, cname_pp, cname_label='label', \n",
    "#                   use_metric=False, monitor=False): \n",
    "#     \"\"\"Get victim model preds+probs for the paraphrase dataset.\n",
    "#     \"\"\"\n",
    "#     assert vm_model.training == False  # checks that model is in eval mode \n",
    "#     if use_metric: \n",
    "#         metric_d = {}\n",
    "#         metric_d['orig'],metric_d['pp'] = load_metric('accuracy'),load_metric('accuracy')\n",
    "#     orig_probs_l,pp_probs_l = [],[]\n",
    "#     if monitor: monitor = Monitor(2)  # track GPU usage and memory\n",
    "    \n",
    "#     def get_vm_preds(x): \n",
    "#         \"\"\"Get predictions for a vector x (here a vector of documents/text). \n",
    "#         Works for a sentiment-analysis dataset (needs to be adjusted for NLI tasks)\"\"\"\n",
    "#         inputs = vm_tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         inputs.to(device)\n",
    "#         outputs = vm_model(**inputs, labels=labels)\n",
    "#         probs = outputs.logits.softmax(1).cpu()\n",
    "#         preds = probs.argmax(1)\n",
    "#         return probs, preds\n",
    "       \n",
    "#     print(\"Getting victim model predictions for both original and paraphrased text.\")\n",
    "#     dl = DataLoader(ds_pp, batch_size=batch_size, shuffle=False, \n",
    "#                     num_workers=n_wkrs, pin_memory=True)\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(dl): \n",
    "#             if i % 50 == 0 : print(\"Now processing batch\", i, \"out of\", len(dl))\n",
    "#             labels,orig,pp = data['label'].to(device),data[cname_orig],data[cname_pp]\n",
    "#             orig_probs, orig_preds = get_vm_preds(orig)            \n",
    "#             pp_probs,   pp_preds   = get_vm_preds(pp)    \n",
    "#             orig_probs_l.append(orig_probs); pp_probs_l.append(pp_probs)\n",
    "#             if use_metric: \n",
    "#                 metric_d['orig'].add_batch(predictions=orig_preds, references=labels)\n",
    "#                 metric_d['pp'].add_batch(  predictions=pp_preds,   references=labels)\n",
    "#     if monitor: monitor.stop()\n",
    "#     def list2tensor(l): return torch.cat(l)\n",
    "#     orig_probs_t,pp_probs_t = list2tensor(orig_probs_l),list2tensor(pp_probs_l)\n",
    "#     if use_metric: return orig_probs_t, pp_probs_t, metric_d\n",
    "#     else:          return orig_probs_t, pp_probs_t, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Generate paraphrase dataset\n",
    "# num_beams = 10\n",
    "# num_return_sequences = 3\n",
    "# cname_input = 'text' # which text column to paraphrase\n",
    "# cname_output= cname_input + '_pp'\n",
    "# date = '20210825'\n",
    "# fname = path_cache + '_rt_train'+ date + '_' + str(num_return_sequences)\n",
    "# if os.path.exists(fname):  \n",
    "#     ds_pp = datasets.load_from_disk(fname)\n",
    "# else:\n",
    "#     ds_pp = train.shard(200, 0, contiguous=True)\n",
    "#     # Have to call with batched=True\n",
    "#     # Need to set a batch size otherwise will run out of memory on the GPU card. \n",
    "#     # 64 seems to work well \n",
    "#     ds_pp = ds_pp.map(\n",
    "#         lambda x: create_paraphrase_dataset(x, \n",
    "#             num_beams=num_beams, num_return_sequences=num_return_sequences,\n",
    "#             cname_input=cname_input, cname_output=cname_output),\n",
    "#         batched=True, batch_size=4) \n",
    "#     ds_pp.save_to_disk(fname)\n",
    "#     gc.collect(); torch.cuda.empty_cache() # free up most of the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Get predictions\n",
    "# cname_orig = cname_input\n",
    "# cname_pp = cname_output\n",
    "# cname_label = 'label'\n",
    "# print_metric = True\n",
    "# fname = path_cache + 'results_df_'+ date + \"_\" + str(num_return_sequences) + \".csv\"\n",
    "# if os.path.exists(fname):    results_df = pd.read_csv(fname)\n",
    "# else: \n",
    "#     #sim_score_t = generate_sim_scores()\n",
    "#     orig_probs_t,pp_probs_t,metric_d = get_vm_scores(ds_pp, cname_orig, \n",
    "#                                                      cname_pp, cname_label,\n",
    "#                                                      monitor=True, use_metric=print_metric)\n",
    "#     if print_metric: \n",
    "#         print(\"orig vm accuracy:\",       metric_d['orig'].compute())\n",
    "#         print(\"paraphrase vm accuracy:\", metric_d['pp'].compute())\n",
    "#     vm_orig_scores  = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], orig_probs_t)])\n",
    "#     vm_pp_scores    = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], pp_probs_t)])\n",
    "#     results_df = pd.DataFrame({\n",
    "#                   cname_orig: ds_pp[cname_orig],\n",
    "#                   cname_pp: ds_pp[cname_pp],\n",
    "#    #               'sim_score': sim_score_t,\n",
    "#                   'label_true': ds_pp[cname_label], \n",
    "#                   'label_vm_orig': orig_probs_t.argmax(1),\n",
    "#                   'label_vm_pp': pp_probs_t.argmax(1),\n",
    "#                   'vm_orig_truelabel': vm_orig_scores,             \n",
    "#                   'vm_pp_truelabel': vm_pp_scores,\n",
    "#                   'vm_truelabel_change': vm_orig_scores - vm_pp_scores,\n",
    "#                   'vm_orig_class0': orig_probs_t[:,0], \n",
    "#                   'vm_orig_class1': orig_probs_t[:,1], \n",
    "#                   'vm_pp_class0': pp_probs_t[:,0], \n",
    "#                   'vm_pp_class1': pp_probs_t[:,1], \n",
    "#                   })\n",
    "# #    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "#     results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing how to keep gradients with `generate` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ### Testing the `generate_with_grad` function\n",
    "\n",
    "# input_text=\"hello my name is Tom\"\n",
    "# num_return_sequences=1\n",
    "# num_beams=2\n",
    "# return_probs=True\n",
    "# batch = pp_tokenizer(input_text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "# generated = pp_model.generate_with_grad(**batch, return_dict_in_generate=True, output_scores=True,\n",
    "#                               num_return_sequences=num_return_sequences,\n",
    "#                                 num_beams=num_beams,\n",
    "#                                 num_beam_groups=1,\n",
    "#                                 diversity_penalty=0,\n",
    "#                                 temperature=1.5, \n",
    "#                               length_penalty=1)\n",
    "# print(generated)\n",
    "\n",
    "# tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)\n",
    "# print(pp_tokenizer.tokenize(tgt_text[0]))\n",
    "# print(pp_tokenizer.encode(tgt_text[0]))\n",
    "\n",
    "# # Score: score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "# # gradient gets removed (i think) by the line \n",
    "# # beam_hyp.add(\n",
    "# #   input_ids[batch_beam_idx].clone(),\n",
    "# #   next_score.item())\n",
    "\n",
    "\n",
    "# x=generated['scores'][5]\n",
    "# print(x.max(1))\n",
    "# x.max(1).values / (len(generated['scores']) ** 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## An example of how to use greedy_search\n",
    "\n",
    "# from transformers import (\n",
    "# AutoTokenizer,\n",
    "# AutoModelForCausalLM,\n",
    "# LogitsProcessorList,\n",
    "# MinLengthLogitsProcessor,\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# input_prompt = \"Today is a beautiful day, and\"\n",
    "# input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # instantiate logits processors\n",
    "# logits_processor = LogitsProcessorList([\n",
    "#     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "# ])\n",
    "\n",
    "# outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
    "\n",
    "# print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tensorboard setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import datetime \n",
    "# # Create writer and track to run directory \n",
    "# path_runs = './runs/'\n",
    "# log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = SummaryWriter(log_dir = log_dir)\n",
    "# # stuff here logging to tensorboard\n",
    "# #writer.close() # important otherwise Tensorboard eventually shuts down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### WandB artifact tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# log predictions table to wandb, giving it a name\n",
    "#  train_table_artifact = wandb.Artifact(\"train_samples_\" + str(wandb.run.id), type=\"predictions\")\n",
    "#  valid_table_artifact = wandb.Artifact(\"test_samples_\"   + str(wandb.run.id), type=\"predictions\")\n",
    "#train_table_artifact.add(train_table, \"predictions\")\n",
    "#valid_table_artifact.add(valid_table, \"predictions\")\n",
    "#wandb.run.log_artifact(train_table_artifact) \n",
    "#wandb.run.log_artifact(valid_table_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A no_grad version of `model.generate()` adapted from transformers v4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Union\n",
    "# from transformers.generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "# import torch\n",
    "\n",
    "\n",
    "# def generate_with_grad(\n",
    "#     self,\n",
    "#     input_ids: Optional[torch.LongTensor] = None,\n",
    "#     max_length: Optional[int] = None,\n",
    "#     min_length: Optional[int] = None,\n",
    "#     do_sample: Optional[bool] = None,\n",
    "#     early_stopping: Optional[bool] = None,\n",
    "#     num_beams: Optional[int] = None,\n",
    "#     temperature: Optional[float] = None,\n",
    "#     top_k: Optional[int] = None,\n",
    "#     top_p: Optional[float] = None,\n",
    "#     repetition_penalty: Optional[float] = None,\n",
    "#     bad_words_ids: Optional[Iterable[int]] = None,\n",
    "#     bos_token_id: Optional[int] = None,\n",
    "#     pad_token_id: Optional[int] = None,\n",
    "#     eos_token_id: Optional[int] = None,\n",
    "#     length_penalty: Optional[float] = None,\n",
    "#     no_repeat_ngram_size: Optional[int] = None,\n",
    "#     encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "#     num_return_sequences: Optional[int] = None,\n",
    "#     max_time: Optional[float] = None,\n",
    "#     decoder_start_token_id: Optional[int] = None,\n",
    "#     use_cache: Optional[bool] = None,\n",
    "#     num_beam_groups: Optional[int] = None,\n",
    "#     diversity_penalty: Optional[float] = None,\n",
    "#     prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "#     output_attentions: Optional[bool] = None,\n",
    "#     output_hidden_states: Optional[bool] = None,\n",
    "#     output_scores: Optional[bool] = None,\n",
    "#     return_dict_in_generate: Optional[bool] = None,\n",
    "#     forced_bos_token_id: Optional[int] = None,\n",
    "#     forced_eos_token_id: Optional[int] = None,\n",
    "#     remove_invalid_values: Optional[bool] = None,\n",
    "#     **model_kwargs):\n",
    "#     # set init values\n",
    "#     num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "#     num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
    "#     max_length = max_length if max_length is not None else self.config.max_length\n",
    "#     do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "#     num_return_sequences = (\n",
    "#         num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "#     )\n",
    "\n",
    "#     pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "#     bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "#     eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "#     output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "#     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "#     output_hidden_states = (\n",
    "#         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "#     )\n",
    "#     return_dict_in_generate = (\n",
    "#         return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "#     )\n",
    "\n",
    "#     model_kwargs[\"output_attentions\"] = output_attentions\n",
    "#     model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "#     if input_ids is None:\n",
    "#         # init `input_ids` with bos_token_id\n",
    "#         input_ids = self._prepare_input_ids_for_generation(bos_token_id, model_kwargs.get(\"encoder_outputs\"))\n",
    "\n",
    "#     if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "#         # init `attention_mask` depending on `pad_token_id`\n",
    "#         model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "#             input_ids, pad_token_id, eos_token_id\n",
    "#         )\n",
    "\n",
    "#     # special case if pad_token_id is not defined\n",
    "#     if pad_token_id is None and eos_token_id is not None:\n",
    "#         logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "#         pad_token_id = eos_token_id\n",
    "\n",
    "#     # Storing encoder_input_ids for logits_processor that could use them\n",
    "#     encoder_input_ids = input_ids if self.config.is_encoder_decoder else None\n",
    "\n",
    "#     if self.config.is_encoder_decoder:\n",
    "#         # add encoder_outputs to model_kwargs\n",
    "#         model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "#         # set input_ids as decoder_input_ids\n",
    "#         if \"decoder_input_ids\" in model_kwargs:\n",
    "#             input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "#         else:\n",
    "#             input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "#                 input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "#             )\n",
    "\n",
    "# #         if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "# #             raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "#     if input_ids.shape[-1] >= max_length:\n",
    "#         input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "#         logger.warning(\n",
    "#             f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "#             \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "#         )\n",
    "\n",
    "#     # determine generation mode\n",
    "#     is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#     is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#     is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "#     is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "#     is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "#     if num_beam_groups > num_beams:\n",
    "#         raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "#     if is_group_beam_gen_mode and do_sample is True:\n",
    "#         raise ValueError(\n",
    "#             \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "#         )\n",
    "\n",
    "#     # set model_kwargs\n",
    "#     model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "#     # get distribution pre_processing samplers\n",
    "#     logits_processor = self._get_logits_processor(\n",
    "#         repetition_penalty=repetition_penalty,\n",
    "#         no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "#         encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "#         encoder_input_ids=encoder_input_ids,\n",
    "#         bad_words_ids=bad_words_ids,\n",
    "#         min_length=min_length,\n",
    "#         max_length=max_length,\n",
    "#         eos_token_id=eos_token_id,\n",
    "#         forced_bos_token_id=forced_bos_token_id,\n",
    "#         forced_eos_token_id=forced_eos_token_id,\n",
    "#         prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "#         num_beams=num_beams,\n",
    "#         num_beam_groups=num_beam_groups,\n",
    "#         diversity_penalty=diversity_penalty,\n",
    "#         remove_invalid_values=remove_invalid_values,\n",
    "#     )\n",
    "\n",
    "#     stopping_criteria = self._get_stopping_criteria(\n",
    "#         max_length=max_length,\n",
    "#         max_time=max_time,\n",
    "#     )\n",
    "\n",
    "#     if is_greedy_gen_mode:\n",
    "#         if num_return_sequences > 1:\n",
    "#             raise ValueError(\n",
    "#                 f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "#             )\n",
    "\n",
    "#         # greedy search\n",
    "#         return self.greedy_search(\n",
    "#             input_ids,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_sample_gen_mode:\n",
    "#         # get probability distribution warper\n",
    "#         logits_warper = self._get_logits_warper(\n",
    "#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#         )\n",
    "\n",
    "#         # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids,\n",
    "#             expand_size=num_return_sequences,\n",
    "#             is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#         # sample\n",
    "#         return self.sample(\n",
    "#             input_ids,\n",
    "#             logits_processor=logits_processor,\n",
    "#             logits_warper=logits_warper,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_beam_gen_mode:\n",
    "#         batch_size = input_ids.shape[0]\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "#         if num_return_sequences > num_beams:\n",
    "#             raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#         beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#             num_beam_hyps_to_keep=num_return_sequences,\n",
    "#         )\n",
    "#         # interleave with `num_beams`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "#         )\n",
    "#         #with torchsnooper.snoop(depth=4, max_variable_length=200, normalize=True):\n",
    "#         return self.beam_search(\n",
    "#             input_ids,\n",
    "#             beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_beam_sample_gen_mode:\n",
    "#         logits_warper = self._get_logits_warper(\n",
    "#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "#         )\n",
    "\n",
    "#         batch_size = input_ids.shape[0] * num_return_sequences\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#         )\n",
    "\n",
    "#         # interleave with `num_beams * num_return_sequences`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids,\n",
    "#             expand_size=num_beams * num_return_sequences,\n",
    "#             is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#         return self.beam_sample(\n",
    "#             input_ids,\n",
    "#             beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             logits_warper=logits_warper,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n",
    "\n",
    "#     elif is_group_beam_gen_mode:\n",
    "#         batch_size = input_ids.shape[0]\n",
    "\n",
    "#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "#         if num_return_sequences > num_beams:\n",
    "#             raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "#         if num_beams % num_beam_groups != 0:\n",
    "#             raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "#         diverse_beam_scorer = BeamSearchScorer(\n",
    "#             batch_size=batch_size,\n",
    "#             max_length=max_length,\n",
    "#             num_beams=num_beams,\n",
    "#             device=self.device,\n",
    "#             length_penalty=length_penalty,\n",
    "#             do_early_stopping=early_stopping,\n",
    "#             num_beam_hyps_to_keep=num_return_sequences,\n",
    "#             num_beam_groups=num_beam_groups,\n",
    "#         )\n",
    "#         # interleave with `num_beams`\n",
    "#         input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "#         )\n",
    "#         return self.group_beam_search(\n",
    "#             input_ids,\n",
    "#             diverse_beam_scorer,\n",
    "#             logits_processor=logits_processor,\n",
    "#             stopping_criteria=stopping_criteria,\n",
    "#             max_length=max_length,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             output_scores=output_scores,\n",
    "#             return_dict_in_generate=return_dict_in_generate,\n",
    "#             **model_kwargs,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old eval/wandb functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_eval_metrics(preds, split):\n",
    "#     ### Might be obselete ###\n",
    "#     print(f\"{split} paraphrases:\", preds['pp_l'])\n",
    "#     print(f\"{split} VM scores:\",    np.round(preds['vm_score'],3))\n",
    "#     print(f\"{split} ROUGE scores:\", np.round(preds['rouge_score'],3))\n",
    "#     if normalise_rewards: print(f\"{split} unnormalised rewards:\", preds['orig_reward'])\n",
    "#     print(f\"{split} rewards:\", round_t(preds['reward'], 3))\n",
    "#     print(f\"{split} avg reward:\", torch.mean(preds['reward']).item())\n",
    "#     print(f\"{split} logp:\", round_t(preds['pp_logp'], 3))\n",
    "#     print(f\"{split} avg logp:\", torch.mean(preds['pp_logp']).item())\n",
    "#     print(f\"{split} loss:\", train_set_preds['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_wandb_metrics(results_d, split): \n",
    "#     \"\"\"flattens lists of metrics to wandb acceptable form. obselete now but might be useful later\"\"\"\n",
    "#     #### MIGHT BE OBSELETE #####\n",
    "#     # Log numeric data \n",
    "#     # Convert all lists of values to wandb format. Scalars are unchanged\n",
    "#     d = dict()\n",
    "#     orig_keys = results_d.keys()\n",
    "#     for k,v in results_d.items(): \n",
    "#         if type(v) is list: \n",
    "#             if type(v[0]) == int or type(v[0]) == float:  # we handle strings differently\n",
    "#                 d1 = {f\"eval/{split}/examples/{k}/{i}\": o for i,o in enumerate(v)}  # list -> dict of len(v) scalars\n",
    "#                 d = {**d, **d1}  # merge dicts\n",
    "#         else: \n",
    "#             d[f\"{split}/{k}\"] = v\n",
    "#     d['epoch'] = epoch\n",
    "#     wandb.log(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "### An attempt to rename wandb columns of a table. This seemed to break something internally inside the table.\n",
    "#\n",
    "#     def rename_wandb_column(table, old, new): \n",
    "#         if old not in table.columns: \n",
    "#             warnings.warn(f\"{old} not in columns of table. Skipping. Columns of table are {table.columns}\")\n",
    "#         else: \n",
    "#             idx = [i for i,o in enumerate(table.columns) if o == old][0]\n",
    "#             table.columns[idx] = new\n",
    "        \n",
    "    # Can't just originally name these 'orig' and 'pp' because they don't match key names used in `eval_dl`\n",
    "    # NOTE: this seems to break the internals of wandb. might have to just live with the names. \n",
    "#     rename_wandb_column(train_table, old='orig_l', new='orig')\n",
    "#     rename_wandb_column(train_table, old='pp_l',   new='pp')\n",
    "#     rename_wandb_column(valid_table,  old='orig_l', new='orig')\n",
    "#     rename_wandb_column(valid_table,  old='pp_l',   new='pp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
