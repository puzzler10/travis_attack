{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we are trying to adjust parameters of a paraphrase model to generate adversarial examples. \n",
    "### Policy gradients \n",
    "The key parameter update equation is $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)$, where $\\alpha$ is a step size parameter, the parameter vector $\\theta$ is for a model (here a paraphrase model), and $J$ is a loss function. The time step $t$ depends on the problem specification and we will get to it later. \n",
    "\n",
    "Now in my review I have defined the loss function $J(\\theta) = E_\\pi[r(\\tau)]$. Here: \n",
    "* $\\pi$ is the policy, a probability distribution for the next action in a given state; essentially $p(a_t|s_t)$\n",
    "* $\\tau$ is a trajectory, a specific sequence $s_0, a_0, r_1, s_1, a_1, \\ldots$ of the agent in the game. This starts at time $t=0$ and finishes at time $t=T$. \n",
    "* $r(\\tau)$ is the sum of rewards for a trajectory $\\tau$, or in other words, the total reward for the trajectory. \n",
    "\n",
    "For this loss function higher values are better (which might make it a reward function) and so we might have to invert it at some point. \n",
    "\n",
    "To update parameters we must find the gradient $\\nabla_\\theta J(\\theta)$, which measures how $J(\\theta)$ changes when we adjust the parameters of the paraphrase model. The gradient is simplified through some maths to get the policy gradient theorem $$ \\nabla_\\theta J(\\theta) =  \\nabla_\\theta E_\\pi [r(\\tau)]  = E_\\pi \\left[r(\\tau) \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a_t|s_t)  \\right] $$ \n",
    "\n",
    "To calculate this you need to calculate the expectation term, which in turn means evaluating every possible trajectory $\\tau$ and its expected return. Generally this is not possible and instead we turn to estimators.  \n",
    "\n",
    "One of these is REINFORCE. It gives us  $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ where \n",
    "* $G_t$ is the discounted return and is given by $G_t = r_t + \\beta r_{t-1} + \\beta^2 r_{t-2} + \\dots$. It's a rough estimate of $r(\\tau)$. Rewards obtained later in the episode are weighted much higher than rewards obtained earlier. I guess it assumes that the parameters update every timestep. \n",
    "* $S$ is some number of samples.\n",
    "\n",
    "The implementation of REINFORCE and similar estimators depends on how we formulate the problem. Below we present some possible formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation One: Document-level  \n",
    "This is the first implementation we will try. \n",
    "\n",
    "Here we generate a list of paraphrases at each time point. The idea is that there is one paraphrase amongst them that is a good adversarial example. We try to tune the model to produce the best one. \n",
    "\n",
    "This interpretation sees forming the complete paraphrase as one time step. So it isn't token-level but document-level. \n",
    "\n",
    "* Starting state: $s0 = x$, the original example  \n",
    "* Actions: each action is \"choosing\" a paraphrase (or of choosing $n$ paraphrases). The set of all possible paraphrases and their probabilities is the policy. So $\\pi(a|s) = p(x'| x;\\theta)$ where $x'$ is the paraphrase (or list of paraphrases). \n",
    "    * To approximate this probability, what we can do is generate a large list of paraphrases, and for each, the probabilities of generating each token in turn for that paraphrase. This gives a rough \"probability\" of how likely that sequence was. This number is kind of like a weight for how good that paraphrase is, according to the model.  We can then turn the weights into probabilities to get a \"probability\" of the paraphrase. This is dependent on the number of paraphrases generated, so generating a large list is likely to be better for this task. \n",
    "* Reward: The paraphrase moves through the reward function $R(x, x')$) to get the reward $r$. \n",
    "* Time steps: We only have one time step in the game ($T=1$ and $G_t=r$)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are a few variations to this scenario that we can do. For each of these we will formulate the policy and the reward function $R$. Below, $x'$ means paraphrase, $f(x)_y$ means the model confidence of x for the class of the true label $y$, $SS(a,b)$ is the result of a semantic similarity model run over $a$ and $b$, and $\\lambda$ is a hyperparameter.  \n",
    "\n",
    "\n",
    "#### One-paraphrase \n",
    "Here we only generate one paraphrase. This scenario also has a few options. First we generate a list of paraphrases with the probabilities of selecting one. Then we either sample probabilistically from the list or pick the most probable option. \n",
    "\n",
    "In this case the policy $p(x'|x,\\theta)$ is the chance of obtaining a specific paraphrase. For the sampling option this is equal to its sample probability. For the top option this is just the probability of selecting that option. \n",
    "\n",
    "The reward function might look like $R(x,x') = f(x)_y - f(x')_y + \\lambda SS(x, x')$. We could also make the $SS$ factor a step-function above some threshold. \n",
    "\n",
    "The REINFORCE equation $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S \\sum_{t=1}^T G_t \\nabla \\log \\pi(a_t|s_t)$$ becomes $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$ We repeat the process $S$ times where $S$ is ideally as large as possible. We can start with something simple (e.g. $S=10$ or $S=100$) and go from there.  \n",
    "\n",
    "The gradient term $\\nabla \\log p(x'_s|x,\\theta)$ can hopefully be found with autodiff. \n",
    "\n",
    "#### Set of paraphrases\n",
    "In this scenario the paraphrase model is evaluated on performance over a set of paraphrases, which we call $X'$ here. The policy becomes $p(X'|x, \\theta)$, the probability of obtaining that list. We can get this probability by multipling together the \"probability\" of each individual paraphrase, multiplying also by nCr (for r paraphrases out of n total) to account for the lack of order in the list. \n",
    "\n",
    "We can make a number of sub-scenarios here. \n",
    "\n",
    "For the **top-paraphrase in set** condition the paraphrase generator is only measured on the best reward for a paraphrase in its set. The idea is the generator will learn to produce a diverse set of examples, any of which could plausibly be a good adversarial example. Here we only look at best performing paraphrase $x'_m$, which we can find by $x'_m = \\max_i [f(x)_y - f(x'_i)_y]$, then return $R(x,x'_m) = [f(x)_y - f(x'_m)_y] + \\lambda SS(x,x'_m)$ \n",
    "\n",
    "For the **average-paraphrase in set** condition the paraphrase generator is measured on the average reward of the paraphrases in its set. This encourages the generator to consider performance of all examples more-or-less equally. The reward function could be something like $\\frac{1}{k} \\sum_{i=1}^k \\left[ f(x)_y - f(x'_i)_y + \\lambda SS(x, x'_i) \\right]$ \n",
    "\n",
    "A combination of these scenarios is the **top-k/top-p\\% paraphrases in set**. Here we only use the top-$k$ paraphrases, or more generally, the top $p$ percentage of paraphrases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interpretation 2: Token-level\n",
    "This interpretation is at token-level; it sees choosing the next word as the next time step. \n",
    "\n",
    "* Starting state: $s0 = x$, the initial state. But you also have a \"blank slate\" for the paraphrase. So maybe it's a tuple (x, pp) where pp is a paraphrase with no words. Here x is used as the reference for the paraphrase generator.  \n",
    "* Actions: Choose the next word of p. I guess this starts with the \\<START\\> token (or something similar). Then you have the policy $\\pi(a|s)$ which is the same as $p(w_{next}|pp, x; \\theta)$ where $\\theta$ is the paraphrase model parameters, $pp$ is the so-far constructed sentence, and $w_{next}$ is the next token (I say token because I don't know if this model is on the subword or word basis). \n",
    "* Time steps: every token is generated one-by-one and each of these is allocated a time step. This means probably that you also update the parameters after each token generated too. \n",
    "* Reward. The reward is allocated every token. There are many reward functions (see papers on token-level loss functions). Some also incorporate document-level rewards too. \n",
    "* Next state. $s_1$ is again the tuple $(x, pp)$ but now $pp$ has the first word in it. \n",
    "\n",
    "On *teacher forcing*. This is when you have a ground-truth paraphrase and you can use it when generating tokens. This is useful because if the model makes a mistake it doesn't continue down that track but is adjusted back. This stops big divergences (but also might limit the diversity of generated paraphrases). This is used when training a paraphrase model. You have a set of reference paraphrases that are human provided. Here though we only have the original sentence and no references. We could generate adversarial examples and use that to do teacher forcing. Generating them using textattack recipes might work. This is only really used on the token-level rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Updating the paraphrase model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is a choice here. We can either directly update the parameters of the paraphrase model. Or we can fix the parameters and add a new dense layer to the end of the model. We could then train this dense layer to convert paraphrases to adversarial paraphrases. \n",
    "\n",
    "Before trying this out, I am worried that we will destroy the capabilities of the paraphrase generator a bit. We might get semantically invalid or ungrammatical or gibberish text. If so we could try and mitigate it a bit by shaping our reward function to maintain grammatical components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Experiment order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plan is to try the following order: \n",
    "\n",
    "1. One-paraphrase (most probable option). I'll start with this one because it is probably the most simple case. Within this category: \n",
    "    1a. tune existing parameters only (see if the text is recognisable) \n",
    "    1b. add dense layer onto end and try again \n",
    "2. One-paraphrase (sampled). This seems like a logical extension on the first one. \n",
    "3. Paraphrase-set options. (Decide after finishing 1, 2) \n",
    "4. Token-level tuning. (Decide after 1,2,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Layer Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I am uncertain on if to do this or not. \n",
    "\n",
    "* This [paper](https://arxiv.org/abs/1911.03090) indicates that you can get pretty good results by freezing all layers except the last few \n",
    "* Conversely I saw in the transformers documentation that transformers train better if you don't do layer freezing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, load models + datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, os, gc, logging\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from collections import defaultdict\n",
    "from types import MethodType\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper\n",
    "\n",
    "# Paths\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "\n",
    "# Seeds\n",
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Devices and GPU settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size_dl = 64\n",
    "\n",
    "# Config\n",
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "\n",
    "# Logging \n",
    "logging.basicConfig(format='%(message)s')\n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "### Parameters and training settings\n",
    "# Paraphrase parameters  \n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "\n",
    "# REINFORCE parameters \n",
    "S = 1\n",
    "\n",
    "# Model training parameters\n",
    "batch_size = 4\n",
    "plot_grads = False\n",
    "lr = 1e-4 # Initial learning rate (after the potential warmup period) to use\n",
    "weight_decay = 0\n",
    "n_train_epochs = 15000\n",
    "#lr_scheduler_type = 'none'\n",
    "n_warmup_steps = 30 \n",
    "plot_grads = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Paraphrase (pp) model \n",
    "pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "pp_tokenizer = AutoTokenizer.from_pretrained(pp_name)\n",
    "# takes about 3GB memory space up on the GPU\n",
    "pp_model = AutoModelForSeq2SeqLM.from_pretrained(pp_name).to(device)\n",
    "# If need a no_grad version of generate:\n",
    "pp_model.generate_with_grad = MethodType(utils.generate_with_grad, pp_model)\n",
    "\n",
    "## Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-uncased-rotten-tomatoes\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load raw datasets and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9c411f7ecd9f3045389de0d9ce984061a1056507703d2e3183b1ac1a90816e4d)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "label_cname = 'label'\n",
    "## For snli\n",
    "# remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "# train = train.filter(remove_minus1_labels)\n",
    "# valid = valid.filter(remove_minus1_labels)\n",
    "# test = test.filter(remove_minus1_labels)\n",
    "\n",
    "# make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "assert train.features[label_cname].num_classes == vm_num_labels\n",
    "assert valid.features[label_cname].num_classes == vm_num_labels\n",
    "assert test.features[ label_cname].num_classes == vm_num_labels\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size_dl, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size_dl, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size_dl, shuffle=True, num_workers=n_wkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a2b91d51da8a7742\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-a2b91d51da8a7742/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-73f997ff87f1fac5\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-73f997ff87f1fac5/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# For testing, we'll just use a simple dataset\n",
    "simple_dataset = load_dataset('csv',data_files=\"simple_dataset.csv\")['train']\n",
    "simple_dataset_test = load_dataset('csv',data_files=\"simple_dataset_test.csv\")['train']\n",
    "simple_dl = DataLoader(simple_dataset, batch_size=batch_size, \n",
    "                            shuffle=False, num_workers=n_wkrs)\n",
    "simple_dl_test = DataLoader(simple_dataset_test, batch_size=batch_size, \n",
    "                            shuffle=False, num_workers=n_wkrs)\n",
    "dl = simple_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop pseudocode\n",
    "\n",
    "The REINFORCE estimator is $$ \\nabla_\\theta J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\nabla \\log p(x'_s|x,\\theta)$$\n",
    "\n",
    "**Non-batched version (one example), stochastic gradient descent**  \n",
    "Inputs: train, n_pp=1, vm, ppm, $\\alpha = 5e^{-5}$ (saw this rate for $\\alpha$ somewhere  \n",
    "Set eval_mode=true for vm, eval_mode = false for ppm  \n",
    "Freeze all layers of ppm except last 6  \n",
    "Shuffle traning dataset  \n",
    "\n",
    "Loop: take one row $x$ from train\n",
    "* tokenize\n",
    "* do greedy search to get paraphrase pp\n",
    "* get reward using `reward_fn(x, pp)`. $r=R(x,x'_s) = f(x)_y - f(x'_s)_y + \\lambda SS(x, x'_s)$ \n",
    "* update model parameters \n",
    "\n",
    "\n",
    "* generate large UNIVERSE list of paraphrases `pp_l` (e.g. 128) from 'text' column using ppm\n",
    "* extract sequence scores from this list to get a vector of probabilities `pp_probs`\n",
    "* take `log` of `pp_probs` and store in `pp_logprobs`\n",
    "* pick S paraphrases from `pp_l` to get `pp_s`. \n",
    "* Take the corresponding entries from `pp_logprobs`. Get gradient of each entry by looking at .grad attribute. Sum them up and store in a variable `gradsum` \n",
    "* for each `pp` (i.e. $x'_s$) in `pp_s`:\n",
    "    * \n",
    "* Sum up these rewards to get `rewardsum` and add to `gradsum` to get `nablaJ`\n",
    "* Update parameters of paraphrase model with $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta) \\approx \\sum_{s=1}^S  R(x,x'_s) \\log p(x'_s|x,\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(text):\n",
    "    \"\"\"Wrapper for generating paraphrases (pp's). Most keywords are passed on to pp_model.generate function, \n",
    "    so see docs for that function. \"\"\"\n",
    "    batch = pp_tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    # Only greedy search supported at the moment\n",
    "    generated = pp_model.generate_with_grad(**batch, \n",
    "                                         **pp_model_params,\n",
    "                                         do_sample=False, \n",
    "                                         return_dict_in_generate=True,\n",
    "                                         output_scores=True,\n",
    "                                         pad_token_id = pp_tokenizer.pad_token_id,\n",
    "                                         eos_token_id = pp_tokenizer.eos_token_id)\n",
    "    tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)\n",
    "    return generated, tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    scores_stacked = torch.stack(translated.scores, 1)\n",
    "    scores_log_softmax = torch.log_softmax(scores_stacked, 2)\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "    return seq_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_vm_probs(text): \n",
    "    \"\"\"Used in the reward_fn to get vm_score\"\"\"\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    tkns = vm_tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    logits = vm_model(**tkns).logits\n",
    "    probs = torch.softmax(logits,1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reward_fn(orig_l, pp_l, truelabel, return_components=False): \n",
    "    \"\"\"orig_l, pp_l are lists of original and paraphrase respectively\"\"\"\n",
    "    # Victim model probability differences between orig and pp\n",
    "    orig_probs,pp_probs = get_vm_probs(orig_l),get_vm_probs(pp_l)\n",
    "    orig_truelabel_probs = torch.gather(orig_probs,1,truelabel[:,None]).squeeze()\n",
    "    pp_truelabel_probs   = torch.gather(pp_probs,1,truelabel[:,None]).squeeze()\n",
    "    vm_scores = (orig_truelabel_probs - pp_truelabel_probs).detach().cpu().tolist()\n",
    "    \n",
    "    # ROUGE scores\n",
    "    def get_rouge_score(ref, pred):\n",
    "        return rouge_metric.compute(rouge_types=[\"rougeL\"],\n",
    "            predictions=[pred], references=[ref])['rougeL'].mid.fmeasure \n",
    "    rouge_scores = [get_rouge_score(ref=orig,pred=pp) for orig,pp in zip(orig_l, pp_l)]\n",
    "\n",
    "    # Reward calculation \n",
    "    rewards = torch.tensor([-9999 if r < 0.15 else v*r for v,r in zip(vm_scores, rouge_scores)],device=device)\n",
    "    \n",
    "#     print(\"orig_l\", orig_l)\n",
    "#     print(\"pp_l\", pp_l)\n",
    "#     print(\"VM score: \", vm_scores)\n",
    "#     print(\"ROUGE score:\", rouge_scores)\n",
    "#     print(\"Reward:\", rewards)\n",
    "    \n",
    "    if return_components: \n",
    "        return {\n",
    "            \"orig_l\": orig_l,\n",
    "            \"pp_l\": pp_l,  \n",
    "            \"truelabel\": truelabel,\n",
    "            \"orig_truelabel_probs\":orig_truelabel_probs,\n",
    "            \"pp_truelabel_probs\":  pp_truelabel_probs,\n",
    "            \"vm_score\": vm_scores, \n",
    "            \"rouge_score\": rouge_scores,\n",
    "            \"reward\": rewards\n",
    "        }\n",
    "    else:  return {\"reward\": rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pp_model_forward(text): \n",
    "    generated, pp_text = get_paraphrases(text)\n",
    "    return generated, pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loss_fn(text, label, generated,  pp_text, return_components=False): \n",
    "    d = reward_fn(orig_l=text, pp_l=pp_text, truelabel=label, return_components=return_components)\n",
    "    d['pp_logp'] = get_pp_logp(generated)\n",
    "    d['loss'] = torch.sum(-d['reward'] * d['pp_logp'])  # don't know if summing is the right approach here\n",
    "    if return_components:  return d\n",
    "    else:                  return d['loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training_step(data): \n",
    "    optimizer.zero_grad()\n",
    "    label,text = data['label'].to(device),data[\"text\"]\n",
    "    generated, pp_text = pp_model_forward(text)\n",
    "    loss = loss_fn(text, label, generated, pp_text, return_components=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #  lr_scheduler.step()\n",
    "    #return loss, reward, pp_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_dl(dl): \n",
    "    \"\"\"Get evaluation metrics for a dataloader\"\"\"\n",
    "    # Put models in eval mode and do the forward pass \n",
    "    if pp_model.training: pp_model.eval()\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    results_d = defaultdict(list)\n",
    "    for i, data in enumerate(dl):\n",
    "        label,text = data['label'].to(device),data[\"text\"]\n",
    "        generated, pp_text = pp_model_forward(text)\n",
    "        d = loss_fn(text, label, generated,  pp_text, return_components=True)\n",
    "        for k,v in d.items(): \n",
    "            results_d[k].append(v) \n",
    "            \n",
    "    # Flatten batches for each key, depending on datatype (e.g. lists of lists )\n",
    "    for k,v in results_d.items(): \n",
    "        # v[0] is arbitrary - we are just checking the first item in the list to see the type\n",
    "        if  torch.is_tensor(v[0]): \n",
    "            # case where we have a list of scalars - the cat function doesn't work here \n",
    "            if  v[0].size() == torch.Size([]): results_d[k] = torch.stack(v)\n",
    "            else:                              results_d[k] = torch.cat(v)\n",
    "        elif type(v[0]) == list:  # this is True for tensors also, so it has to go after the is_tensor check\n",
    "            results_d[k] = list(itertools.chain(*v)) \n",
    "        else: \n",
    "            raise Exception(\"shouldn't get here\")\n",
    "    return results_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Set up models and do layer freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Setup\n",
    "vm_model.eval()\n",
    "pp_model.train()\n",
    "\n",
    "## Layer freezing \n",
    "# Unfreeze last 2 layers of the base model decoder\n",
    "# Not sure if decoder layer norm should be unfrozen or not, but it appears after the\n",
    "#   other parameters in the module ordering, so let's include it for now\n",
    "# Also unfreeze the linear head.  This isn't stored in the base model but rather tacked on top\n",
    "#   and will be fine-tuned for summarisation. \n",
    "layer_list = ['decoder.layers.14', 'decoder.layers.15', 'decoder.layer_norm'] \n",
    "for i, (name,param) in enumerate(pp_model.base_model.named_parameters()): \n",
    "    if np.any([o in name for o in layer_list]):   param.requires_grad = True\n",
    "    else:                                         param.requires_grad = False\n",
    "for param in pp_model.lm_head.parameters():       param.requires_grad = True\n",
    "# For some reason this seems to be excluded\n",
    "for param in pp_model.base_model.shared.parameters(): param.requires_grad = False \n",
    "### For checking the grad status of the layers\n",
    "# for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)\n",
    "# for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    print(i, name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create small dataset (dev step for quicker development, delete later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_small = train.shard(10000, 4, contiguous=False)  # small training set for testing purposes\n",
    "# train_small_dl = DataLoader(train_small, batch_size=batch_size, \n",
    "#                             shuffle=True, num_workers=n_wkrs)\n",
    "# dl = train_small_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up optimiser and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Code below taken from https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue_no_trainer.py#L363\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "# no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [p for n, p in pp_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": weight_decay,\n",
    "#     },\n",
    "#     {\n",
    "#         \"params\": [p for n, p in pp_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#         \"weight_decay\": 0.0,\n",
    "#     },\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# For now we just keep this simple\n",
    "optimizer = AdamW(pp_model.parameters(), lr=lr)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=lr_scheduler_type,\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=n_warmup_steps,\n",
    "#     num_training_steps=n_train_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up other miscellaneous things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")\n",
    "n_train_steps = n_train_epochs * len(dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd33d0120394437a843a66e6f06a883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 0 of 15000\n",
      "Now on epoch 1 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I like the movie.', \"I don't like this movie.\", 'I love apples.', \"I don't like apples.\"]\n",
      "Train rewards: tensor([-0.0542, -0.0211, -0.0141, -0.0173], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.026666367101290868\n",
      "Test paraphrases: ['I like the taste of this banana.', \"I don't like bananas.\", 'I like this film.', \"I don't like this film.\"]\n",
      "Test rewards: tensor([-0.2036, -0.0140,  0.0250, -0.0423], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -0.05870265100489963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 2 of 15000\n",
      "Now on epoch 3 of 15000\n",
      "Now on epoch 4 of 15000\n",
      "Now on epoch 5 of 15000\n",
      "Now on epoch 6 of 15000\n",
      "Now on epoch 7 of 15000\n",
      "Now on epoch 8 of 15000\n",
      "Now on epoch 9 of 15000\n",
      "Now on epoch 10 of 15000\n",
      "Now on epoch 11 of 15000\n",
      "Now on epoch 12 of 15000\n",
      "Now on epoch 13 of 15000\n",
      "Now on epoch 14 of 15000\n",
      "Now on epoch 15 of 15000\n",
      "Now on epoch 16 of 15000\n",
      "Now on epoch 17 of 15000\n",
      "Now on epoch 18 of 15000\n",
      "Now on epoch 19 of 15000\n",
      "Now on epoch 20 of 15000\n",
      "Now on epoch 21 of 15000\n",
      "Now on epoch 22 of 15000\n",
      "Now on epoch 23 of 15000\n",
      "Now on epoch 24 of 15000\n",
      "Now on epoch 25 of 15000\n",
      "Now on epoch 26 of 15000\n",
      "Now on epoch 27 of 15000\n",
      "Now on epoch 28 of 15000\n",
      "Now on epoch 29 of 15000\n",
      "Now on epoch 30 of 15000\n",
      "Now on epoch 31 of 15000\n",
      "Now on epoch 32 of 15000\n",
      "Now on epoch 33 of 15000\n",
      "Now on epoch 34 of 15000\n",
      "Now on epoch 35 of 15000\n",
      "Now on epoch 36 of 15000\n",
      "Now on epoch 37 of 15000\n",
      "Now on epoch 38 of 15000\n",
      "Now on epoch 39 of 15000\n",
      "Now on epoch 40 of 15000\n",
      "Now on epoch 41 of 15000\n",
      "Now on epoch 42 of 15000\n",
      "Now on epoch 43 of 15000\n",
      "Now on epoch 44 of 15000\n",
      "Now on epoch 45 of 15000\n",
      "Now on epoch 46 of 15000\n",
      "Now on epoch 47 of 15000\n",
      "Now on epoch 48 of 15000\n",
      "Now on epoch 49 of 15000\n",
      "Now on epoch 50 of 15000\n",
      "Now on epoch 51 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I like the movie.', \"I don't like this movie.\", 'I love apples.', \"I don't like apples.\"]\n",
      "Train rewards: tensor([-0.0542, -0.0211, -0.0141, -0.0173], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.026666367101290868\n",
      "Test paraphrases: ['I like the taste of this banana.', \"I don't like bananas.\", 'I like this film.', \"I don't like this film.\"]\n",
      "Test rewards: tensor([-0.2036, -0.0140,  0.0250, -0.0423], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -0.05870265100489963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 52 of 15000\n",
      "Now on epoch 53 of 15000\n",
      "Now on epoch 54 of 15000\n",
      "Now on epoch 55 of 15000\n",
      "Now on epoch 56 of 15000\n",
      "Now on epoch 57 of 15000\n",
      "Now on epoch 58 of 15000\n",
      "Now on epoch 59 of 15000\n",
      "Now on epoch 60 of 15000\n",
      "Now on epoch 61 of 15000\n",
      "Now on epoch 62 of 15000\n",
      "Now on epoch 63 of 15000\n",
      "Now on epoch 64 of 15000\n",
      "Now on epoch 65 of 15000\n",
      "Now on epoch 66 of 15000\n",
      "Now on epoch 67 of 15000\n",
      "Now on epoch 68 of 15000\n",
      "Now on epoch 69 of 15000\n",
      "Now on epoch 70 of 15000\n",
      "Now on epoch 71 of 15000\n",
      "Now on epoch 72 of 15000\n",
      "Now on epoch 73 of 15000\n",
      "Now on epoch 74 of 15000\n",
      "Now on epoch 75 of 15000\n",
      "Now on epoch 76 of 15000\n",
      "Now on epoch 77 of 15000\n",
      "Now on epoch 78 of 15000\n",
      "Now on epoch 79 of 15000\n",
      "Now on epoch 80 of 15000\n",
      "Now on epoch 81 of 15000\n",
      "Now on epoch 82 of 15000\n",
      "Now on epoch 83 of 15000\n",
      "Now on epoch 84 of 15000\n",
      "Now on epoch 85 of 15000\n",
      "Now on epoch 86 of 15000\n",
      "Now on epoch 87 of 15000\n",
      "Now on epoch 88 of 15000\n",
      "Now on epoch 89 of 15000\n",
      "Now on epoch 90 of 15000\n",
      "Now on epoch 91 of 15000\n",
      "Now on epoch 92 of 15000\n",
      "Now on epoch 93 of 15000\n",
      "Now on epoch 94 of 15000\n",
      "Now on epoch 95 of 15000\n",
      "Now on epoch 96 of 15000\n",
      "Now on epoch 97 of 15000\n",
      "Now on epoch 98 of 15000\n",
      "Now on epoch 99 of 15000\n",
      "Now on epoch 100 of 15000\n",
      "Now on epoch 101 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I enjoy this movie.', \"I don't like this movie.\", 'I love apples.', \"I don't like apples.\"]\n",
      "Train rewards: tensor([-0.1631, -0.0211, -0.0141, -0.0173], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.05388592709861104\n",
      "Test paraphrases: ['I like bananas.', \"I don't like bananas.\", 'I am a fan of this film.', \"I don't like this film.\"]\n",
      "Test rewards: tensor([ 0.0195, -0.0140,  0.0365, -0.0423], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -4.878376985525132e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 102 of 15000\n",
      "Now on epoch 103 of 15000\n",
      "Now on epoch 104 of 15000\n",
      "Now on epoch 105 of 15000\n",
      "Now on epoch 106 of 15000\n",
      "Now on epoch 107 of 15000\n",
      "Now on epoch 108 of 15000\n",
      "Now on epoch 109 of 15000\n",
      "Now on epoch 110 of 15000\n",
      "Now on epoch 111 of 15000\n",
      "Now on epoch 112 of 15000\n",
      "Now on epoch 113 of 15000\n",
      "Now on epoch 114 of 15000\n",
      "Now on epoch 115 of 15000\n",
      "Now on epoch 116 of 15000\n",
      "Now on epoch 117 of 15000\n",
      "Now on epoch 118 of 15000\n",
      "Now on epoch 119 of 15000\n",
      "Now on epoch 120 of 15000\n",
      "Now on epoch 121 of 15000\n",
      "Now on epoch 122 of 15000\n",
      "Now on epoch 123 of 15000\n",
      "Now on epoch 124 of 15000\n",
      "Now on epoch 125 of 15000\n",
      "Now on epoch 126 of 15000\n",
      "Now on epoch 127 of 15000\n",
      "Now on epoch 128 of 15000\n",
      "Now on epoch 129 of 15000\n",
      "Now on epoch 130 of 15000\n",
      "Now on epoch 131 of 15000\n",
      "Now on epoch 132 of 15000\n",
      "Now on epoch 133 of 15000\n",
      "Now on epoch 134 of 15000\n",
      "Now on epoch 135 of 15000\n",
      "Now on epoch 136 of 15000\n",
      "Now on epoch 137 of 15000\n",
      "Now on epoch 138 of 15000\n",
      "Now on epoch 139 of 15000\n",
      "Now on epoch 140 of 15000\n",
      "Now on epoch 141 of 15000\n",
      "Now on epoch 142 of 15000\n",
      "Now on epoch 143 of 15000\n",
      "Now on epoch 144 of 15000\n",
      "Now on epoch 145 of 15000\n",
      "Now on epoch 146 of 15000\n",
      "Now on epoch 147 of 15000\n",
      "Now on epoch 148 of 15000\n",
      "Now on epoch 149 of 15000\n",
      "Now on epoch 150 of 15000\n",
      "Now on epoch 151 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I like the movie.', \"I don't like this movie.\", 'This apple is very attractive to me.', 'I hate apples.']\n",
      "Train rewards: tensor([-0.0542, -0.0211, -0.0277, -0.0324], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.033826937987690875\n",
      "Test paraphrases: ['I like banana.', \"I don't like banana.\", 'I love the film.', 'I hate this film.']\n",
      "Test rewards: tensor([ 0.0286, -0.0219, -0.0169, -0.0312], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -0.01038515746786997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 152 of 15000\n",
      "Now on epoch 153 of 15000\n",
      "Now on epoch 154 of 15000\n",
      "Now on epoch 155 of 15000\n",
      "Now on epoch 156 of 15000\n",
      "Now on epoch 157 of 15000\n",
      "Now on epoch 158 of 15000\n",
      "Now on epoch 159 of 15000\n",
      "Now on epoch 160 of 15000\n",
      "Now on epoch 161 of 15000\n",
      "Now on epoch 162 of 15000\n",
      "Now on epoch 163 of 15000\n",
      "Now on epoch 164 of 15000\n",
      "Now on epoch 165 of 15000\n",
      "Now on epoch 166 of 15000\n",
      "Now on epoch 167 of 15000\n",
      "Now on epoch 168 of 15000\n",
      "Now on epoch 169 of 15000\n",
      "Now on epoch 170 of 15000\n",
      "Now on epoch 171 of 15000\n",
      "Now on epoch 172 of 15000\n",
      "Now on epoch 173 of 15000\n",
      "Now on epoch 174 of 15000\n",
      "Now on epoch 175 of 15000\n",
      "Now on epoch 176 of 15000\n",
      "Now on epoch 177 of 15000\n",
      "Now on epoch 178 of 15000\n",
      "Now on epoch 179 of 15000\n",
      "Now on epoch 180 of 15000\n",
      "Now on epoch 181 of 15000\n",
      "Now on epoch 182 of 15000\n",
      "Now on epoch 183 of 15000\n",
      "Now on epoch 184 of 15000\n",
      "Now on epoch 185 of 15000\n",
      "Now on epoch 186 of 15000\n",
      "Now on epoch 187 of 15000\n",
      "Now on epoch 188 of 15000\n",
      "Now on epoch 189 of 15000\n",
      "Now on epoch 190 of 15000\n",
      "Now on epoch 191 of 15000\n",
      "Now on epoch 192 of 15000\n",
      "Now on epoch 193 of 15000\n",
      "Now on epoch 194 of 15000\n",
      "Now on epoch 195 of 15000\n",
      "Now on epoch 196 of 15000\n",
      "Now on epoch 197 of 15000\n",
      "Now on epoch 198 of 15000\n",
      "Now on epoch 199 of 15000\n",
      "Now on epoch 200 of 15000\n",
      "Now on epoch 201 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I like the movie.', \"I don't like this movie.\", 'This apple is very attractive to me.', 'I hate apples.']\n",
      "Train rewards: tensor([-0.0542, -0.0211, -0.0277, -0.0324], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.033826937987690875\n",
      "Test paraphrases: ['I like banana.', \"I don't like banana.\", 'I love the film.', 'I hate the film.']\n",
      "Test rewards: tensor([ 0.0286, -0.0219, -0.0169, -0.0359], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -0.011551910938767644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 202 of 15000\n",
      "Now on epoch 203 of 15000\n",
      "Now on epoch 204 of 15000\n",
      "Now on epoch 205 of 15000\n",
      "Now on epoch 206 of 15000\n",
      "Now on epoch 207 of 15000\n",
      "Now on epoch 208 of 15000\n",
      "Now on epoch 209 of 15000\n",
      "Now on epoch 210 of 15000\n",
      "Now on epoch 211 of 15000\n",
      "Now on epoch 212 of 15000\n",
      "Now on epoch 213 of 15000\n",
      "Now on epoch 214 of 15000\n",
      "Now on epoch 215 of 15000\n",
      "Now on epoch 216 of 15000\n",
      "Now on epoch 217 of 15000\n",
      "Now on epoch 218 of 15000\n",
      "Now on epoch 219 of 15000\n",
      "Now on epoch 220 of 15000\n",
      "Now on epoch 221 of 15000\n",
      "Now on epoch 222 of 15000\n",
      "Now on epoch 223 of 15000\n",
      "Now on epoch 224 of 15000\n",
      "Now on epoch 225 of 15000\n",
      "Now on epoch 226 of 15000\n",
      "Now on epoch 227 of 15000\n",
      "Now on epoch 228 of 15000\n",
      "Now on epoch 229 of 15000\n",
      "Now on epoch 230 of 15000\n",
      "Now on epoch 231 of 15000\n",
      "Now on epoch 232 of 15000\n",
      "Now on epoch 233 of 15000\n",
      "Now on epoch 234 of 15000\n",
      "Now on epoch 235 of 15000\n",
      "Now on epoch 236 of 15000\n",
      "Now on epoch 237 of 15000\n",
      "Now on epoch 238 of 15000\n",
      "Now on epoch 239 of 15000\n",
      "Now on epoch 240 of 15000\n",
      "Now on epoch 241 of 15000\n",
      "Now on epoch 242 of 15000\n",
      "Now on epoch 243 of 15000\n",
      "Now on epoch 244 of 15000\n",
      "Now on epoch 245 of 15000\n",
      "Now on epoch 246 of 15000\n",
      "Now on epoch 247 of 15000\n",
      "Now on epoch 248 of 15000\n",
      "Now on epoch 249 of 15000\n",
      "Now on epoch 250 of 15000\n",
      "Now on epoch 251 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paraphrases: ['I like the movie.', \"I do not think it's a good movie.\", 'I love apples.', 'I hate apples.']\n",
      "Train rewards: tensor([-0.0542, -0.0169, -0.0141, -0.0324], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Train avg reward: -0.029388998377890815\n",
      "Test paraphrases: ['I like banana.', 'I do not like bananas.', 'I love the film.', 'I hate the film.']\n",
      "Test rewards: tensor([ 0.0286, -0.0202, -0.0169, -0.0359], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Test avg reward: -0.011122849032089308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on epoch 252 of 15000\n",
      "Now on epoch 253 of 15000\n",
      "Now on epoch 254 of 15000\n",
      "Now on epoch 255 of 15000\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(n_train_steps))\n",
    "for epoch in range(n_train_epochs): \n",
    "    logger.info(f\"Now on epoch {epoch} of {n_train_epochs}\")\n",
    "    if not pp_model.training: pp_model.train()\n",
    "    for i, data in enumerate(dl): \n",
    "        if i % 10 == 0 :   logging.info(f\"Now processing batch {i} out of {len(dl)}\")\n",
    "        training_step(data) \n",
    "        if plot_grads: plot_grad_flow(pp_model.named_parameters())\n",
    "            \n",
    "        # For debugging\n",
    "        # print_info_on_generated_text()\n",
    "        progress_bar.update(1)            \n",
    "    \n",
    "    # Evaluation loop every 5 epochs\n",
    "    if epoch % 50 == 0: \n",
    "        train_set_preds = eval_dl(dl = simple_dl)\n",
    "        test_set_preds  = eval_dl(dl = simple_dl_test)\n",
    "        print(\"Train paraphrases:\", train_set_preds['pp_l'])\n",
    "        print(\"Train rewards:\", train_set_preds['reward'])\n",
    "        print(\"Train avg reward:\", torch.mean(train_set_preds['reward']).item())\n",
    "        print(\"Test paraphrases:\",  test_set_preds['pp_l'])\n",
    "        print(\"Test rewards:\", test_set_preds['reward'])\n",
    "        print(\"Test avg reward:\",  torch.mean(test_set_preds['reward']).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_preds['reward']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and debugging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the weights update each training step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#check_parameters_update(dl)  # from utils script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code scraps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiments around plotting average parameter updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_parameter_group_dict(): \n",
    "    \"\"\"Function to create \"groups\" of parameters. This is useful to check how much a group of \n",
    "    parameters updates at an epoch. \n",
    "    Parameter groups are hardcoded into this code for now. \n",
    "    \"\"\"\n",
    "    # Identify which parameters should be grouped together\n",
    "    isolates = ['model.shared.weight',\"model.encoder.embed_positions.weight\", \"model.encoder.layer_norm\",\n",
    "                \"model.decoder.embed_positions.weight\", \"model.decoder.layer_norm\"]\n",
    "    layers_base = [\"model.encoder.layers\", \"model.decoder.layers\"]\n",
    "    def flatten_list(l): return list(np.concatenate(l).flat)\n",
    "    layers = flatten_list([[lyr + \".\" + str(o) +\".\" for o in list(range(16))] for lyr in layers_base])\n",
    "    parameter_groups = layers + isolates\n",
    "    # Sort the parameter groups by the order they appear in the model \n",
    "    all_params = [name for name,_ in pp_model.named_parameters()]\n",
    "    ordering = [np.min(np.where([pg in o for o in all_params])) for pg in parameter_groups]\n",
    "    parameter_groups = [o for _,o in sorted(zip(ordering, parameter_groups))]\n",
    "    # Assign each model parameter a parameter group \n",
    "    group_d = dict()\n",
    "    for pg in parameter_groups: \n",
    "        name = pg[:-1] if pg in layers else pg  # remove the \".\" from the end of the name for the numeric layers\n",
    "        group_d[name] = [o for o in all_params if pg in o]\n",
    "    return group_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_parameter_update_amount(): \n",
    "    group_d = get_parameter_group_dict()\n",
    "    params_all_initial_d = dict(params_all_initial)\n",
    "    params_all_d = dict(params_all)\n",
    "    group_d = get_parameter_group_dict()\n",
    "    df_d = dict()\n",
    "    for k,param_l in group_d.items(): \n",
    "        l = list()\n",
    "        for p in param_l: \n",
    "            l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "        l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "        df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "    df = pd.DataFrame(df_d)\n",
    "    df.index = pd.DataFrame([1,2,3]).describe().index\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Random code snippets\n",
    "\n",
    "# initial_params = [(name, p.detach().clone()) for (name, p) in pp_model.named_parameters()]\n",
    "# loss, reward, pp_logp = training_step(data) \n",
    "# update_d =  dict()\n",
    "# for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#     update_d[name] = torch.abs(old_p - new_p).detach().flatten()     \n",
    "    \n",
    "#             update_d =  dict()\n",
    "#             for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): \n",
    "#                 update_d[name] = torch.abs(old_p - new_p).flatten() \n",
    "#                 print (name, torch.norm(new_p - old_p).item())  \n",
    "            \n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             initial_params_d,current_params_d = dict(initial_params),dict()\n",
    "#             params_all_d = dict(params_all)\n",
    "#             group_d = get_parameter_group_dict()\n",
    "#             df_d = dict()\n",
    "#             for k,param_l in group_d.items(): \n",
    "#                 l = list()\n",
    "#                 for p in param_l: \n",
    "#                     l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())\n",
    "#                 l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy\n",
    "#                 df_d[k] = pd.DataFrame(l).describe().values.flatten()\n",
    "#             df = pd.DataFrame(df_d)\n",
    "#             df.index = pd.DataFrame([1,2,3]).describe().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generating a paraphrase dataset and getting VM predictions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_paraphrase_dataset(batch, cname_input, cname_output, num_beams=32,\n",
    "                              num_return_sequences=32): \n",
    "    \"\"\"Create paraphrases for each example in the batch. Then repeat the other fields \n",
    "        so that the resulting datase has the same length as the number of paraphrases. \n",
    "        Key assumption is \n",
    "        that the same number of paraphrases is created for each example.\n",
    "        batch: a dict of examples used by the `map` function from the dataset\n",
    "        cname_input: What column to create paraphrases of \n",
    "        cname_output: What to call the column of paraphrases\n",
    "        other parameters - passed to get_paraphrases. \"\"\"\n",
    "    \n",
    "    # Generate paraphrases. \n",
    "    # This can be later extended to add diversity or so on. \n",
    "    #set_trace()\n",
    "    pp_l,probs = get_paraphrases(batch[cname_input], num_beams=num_beams,\n",
    "        num_return_sequences=num_return_sequences)\n",
    "    \n",
    "    # To return paraphrases as a list of lists for batch input (not done here but might need later)\n",
    "    #     split_into_sublists = lambda l,n: [l[i:i + n] for i in range(0, len(l), n)]\n",
    "    #     pp_l = split_into_sublists(pp_l, n_seed_seqs)\n",
    "    batch[cname_output] = pp_l \n",
    "    batch[\"probs\"] = probs.to('cpu').numpy()\n",
    "    \n",
    "    # Repeat each entry in all other columns `num_return_sequences` times so they are the same length\n",
    "    # as the paraphrase column\n",
    "    # Only works if the same number of paraphrases is generated for each phrase. \n",
    "    # Else try something like \n",
    "        # for o in zip(*batch.values()):\n",
    "        #     d = dict(zip(batch.keys(), o))\n",
    "        #     get_paraphrases(batch[cname_input],num_return_sequences=n_seed_seqs,num_beams=n_seed_seqs)\n",
    "        #     for k,v in d.items(): \n",
    "        #       return_d[k] += v if k == 'text' else [v for o in range(n_paraphrases)]\n",
    "        # return return_d\n",
    "    return_d = defaultdict(list) \n",
    "    repeat_each_item_n_times = lambda l,n: [o for o in l for i in range(n)]\n",
    "    for k in batch.keys(): \n",
    "        if   k == cname_output: return_d[k] = batch[cname_output]\n",
    "        elif k == \"probs\"     : return_d[k] = batch[\"probs\"]\n",
    "        else:                   return_d[k] = repeat_each_item_n_times(batch[k], num_return_sequences)\n",
    "    return return_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_vm_scores(ds_pp, cname_orig, cname_pp, cname_label='label', \n",
    "                  use_metric=False, monitor=False): \n",
    "    \"\"\"Get victim model preds+probs for the paraphrase dataset.\n",
    "    \"\"\"\n",
    "    assert vm_model.training == False  # checks that model is in eval mode \n",
    "    if use_metric: \n",
    "        metric_d = {}\n",
    "        metric_d['orig'],metric_d['pp'] = load_metric('accuracy'),load_metric('accuracy')\n",
    "    orig_probs_l,pp_probs_l = [],[]\n",
    "    if monitor: monitor = Monitor(2)  # track GPU usage and memory\n",
    "    \n",
    "    def get_vm_preds(x): \n",
    "        \"\"\"Get predictions for a vector x (here a vector of documents/text). \n",
    "        Works for a sentiment-analysis dataset (needs to be adjusted for NLI tasks)\"\"\"\n",
    "        inputs = vm_tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        outputs = vm_model(**inputs, labels=labels)\n",
    "        probs = outputs.logits.softmax(1).cpu()\n",
    "        preds = probs.argmax(1)\n",
    "        return probs, preds\n",
    "       \n",
    "    print(\"Getting victim model predictions for both original and paraphrased text.\")\n",
    "    dl = DataLoader(ds_pp, batch_size=batch_size, shuffle=False, \n",
    "                    num_workers=n_wkrs, pin_memory=True)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dl): \n",
    "            if i % 50 == 0 : print(\"Now processing batch\", i, \"out of\", len(dl))\n",
    "            labels,orig,pp = data['label'].to(device),data[cname_orig],data[cname_pp]\n",
    "            orig_probs, orig_preds = get_vm_preds(orig)            \n",
    "            pp_probs,   pp_preds   = get_vm_preds(pp)    \n",
    "            orig_probs_l.append(orig_probs); pp_probs_l.append(pp_probs)\n",
    "            if use_metric: \n",
    "                metric_d['orig'].add_batch(predictions=orig_preds, references=labels)\n",
    "                metric_d['pp'].add_batch(  predictions=pp_preds,   references=labels)\n",
    "    if monitor: monitor.stop()\n",
    "    def list2tensor(l): return torch.cat(l)\n",
    "    orig_probs_t,pp_probs_t = list2tensor(orig_probs_l),list2tensor(pp_probs_l)\n",
    "    if use_metric: return orig_probs_t, pp_probs_t, metric_d\n",
    "    else:          return orig_probs_t, pp_probs_t, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Generate paraphrase dataset\n",
    "num_beams = 10\n",
    "num_return_sequences = 3\n",
    "cname_input = 'text' # which text column to paraphrase\n",
    "cname_output= cname_input + '_pp'\n",
    "date = '20210825'\n",
    "fname = path_cache + '_rt_train'+ date + '_' + str(num_return_sequences)\n",
    "if os.path.exists(fname):  \n",
    "    ds_pp = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    ds_pp = train.shard(200, 0, contiguous=True)\n",
    "    # Have to call with batched=True\n",
    "    # Need to set a batch size otherwise will run out of memory on the GPU card. \n",
    "    # 64 seems to work well \n",
    "    ds_pp = ds_pp.map(\n",
    "        lambda x: create_paraphrase_dataset(x, \n",
    "            num_beams=num_beams, num_return_sequences=num_return_sequences,\n",
    "            cname_input=cname_input, cname_output=cname_output),\n",
    "        batched=True, batch_size=4) \n",
    "    ds_pp.save_to_disk(fname)\n",
    "    gc.collect(); torch.cuda.empty_cache() # free up most of the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Get predictions\n",
    "cname_orig = cname_input\n",
    "cname_pp = cname_output\n",
    "cname_label = 'label'\n",
    "print_metric = True\n",
    "fname = path_cache + 'results_df_'+ date + \"_\" + str(num_return_sequences) + \".csv\"\n",
    "if os.path.exists(fname):    results_df = pd.read_csv(fname)\n",
    "else: \n",
    "    #sim_score_t = generate_sim_scores()\n",
    "    orig_probs_t,pp_probs_t,metric_d = get_vm_scores(ds_pp, cname_orig, \n",
    "                                                     cname_pp, cname_label,\n",
    "                                                     monitor=True, use_metric=print_metric)\n",
    "    if print_metric: \n",
    "        print(\"orig vm accuracy:\",       metric_d['orig'].compute())\n",
    "        print(\"paraphrase vm accuracy:\", metric_d['pp'].compute())\n",
    "    vm_orig_scores  = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], orig_probs_t)])\n",
    "    vm_pp_scores    = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], pp_probs_t)])\n",
    "    results_df = pd.DataFrame({\n",
    "                  cname_orig: ds_pp[cname_orig],\n",
    "                  cname_pp: ds_pp[cname_pp],\n",
    "   #               'sim_score': sim_score_t,\n",
    "                  'label_true': ds_pp[cname_label], \n",
    "                  'label_vm_orig': orig_probs_t.argmax(1),\n",
    "                  'label_vm_pp': pp_probs_t.argmax(1),\n",
    "                  'vm_orig_truelabel': vm_orig_scores,             \n",
    "                  'vm_pp_truelabel': vm_pp_scores,\n",
    "                  'vm_truelabel_change': vm_orig_scores - vm_pp_scores,\n",
    "                  'vm_orig_class0': orig_probs_t[:,0], \n",
    "                  'vm_orig_class1': orig_probs_t[:,1], \n",
    "                  'vm_pp_class0': pp_probs_t[:,0], \n",
    "                  'vm_pp_class1': pp_probs_t[:,1], \n",
    "                  })\n",
    "#    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "    results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing how to keep gradients with `generate` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Testing the `generate_with_grad` function\n",
    "\n",
    "input_text=\"hello my name is Tom\"\n",
    "num_return_sequences=1\n",
    "num_beams=2\n",
    "return_probs=True\n",
    "batch = pp_tokenizer(input_text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "generated = pp_model.generate_with_grad(**batch, return_dict_in_generate=True, output_scores=True,\n",
    "                              num_return_sequences=num_return_sequences,\n",
    "                                num_beams=num_beams,\n",
    "                                num_beam_groups=1,\n",
    "                                diversity_penalty=0,\n",
    "                                temperature=1.5, \n",
    "                              length_penalty=1)\n",
    "print(generated)\n",
    "\n",
    "tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)\n",
    "print(pp_tokenizer.tokenize(tgt_text[0]))\n",
    "print(pp_tokenizer.encode(tgt_text[0]))\n",
    "\n",
    "# Score: score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "# gradient gets removed (i think) by the line \n",
    "# beam_hyp.add(\n",
    "#   input_ids[batch_beam_idx].clone(),\n",
    "#   next_score.item())\n",
    "\n",
    "\n",
    "x=generated['scores'][5]\n",
    "print(x.max(1))\n",
    "x.max(1).values / (len(generated['scores']) ** 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## An example of how to use greedy_search\n",
    "\n",
    "# from transformers import (\n",
    "# AutoTokenizer,\n",
    "# AutoModelForCausalLM,\n",
    "# LogitsProcessorList,\n",
    "# MinLengthLogitsProcessor,\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# input_prompt = \"Today is a beautiful day, and\"\n",
    "# input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # instantiate logits processors\n",
    "# logits_processor = LogitsProcessorList([\n",
    "#     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "# ])\n",
    "\n",
    "# outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
    "\n",
    "# print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import datetime \n",
    "# # Create writer and track to run directory \n",
    "# path_runs = './runs/'\n",
    "# log_dir = path_runs + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = SummaryWriter(log_dir = log_dir)\n",
    "# # stuff here logging to tensorboard\n",
    "# #writer.close() # important otherwise Tensorboard eventually shuts down\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
