---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "run.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: run.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">load_ext</span> line_profiler
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TOKENIZERS_PARALLELISM&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="s2">&quot;true&quot;</span>  <span class="c1"># set to false if not working</span>


<span class="kn">from</span> <span class="nn">travis_attack.config</span> <span class="kn">import</span> <span class="n">Config</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cfg</span> <span class="o">=</span> <span class="n">Config</span><span class="p">()</span><span class="o">.</span><span class="n">simple_dataset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">vars</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;pp_name&#39;: &#39;eugenesiow/bart-paraphrase&#39;,
 &#39;vm_name&#39;: &#39;textattack/distilbert-base-uncased-rotten-tomatoes&#39;,
 &#39;sts_name&#39;: &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;,
 &#39;dataset_name&#39;: &#39;simple_dataset&#39;,
 &#39;seed&#39;: 420,
 &#39;use_fp16&#39;: True,
 &#39;lr&#39;: 1e-05,
 &#39;normalise_rewards&#39;: False,
 &#39;metrics&#39;: [&#39;loss&#39;,
  &#39;pp_logp&#39;,
  &#39;reward&#39;,
  &#39;vm_score&#39;,
  &#39;sts_score&#39;,
  &#39;label_flip&#39;],
 &#39;pin_memory&#39;: True,
 &#39;zero_grad_with_none&#39;: False,
 &#39;pad_token_embeddings&#39;: True,
 &#39;padding_multiple&#39;: 8,
 &#39;bucket_by_length&#39;: True,
 &#39;shuffle_train&#39;: False,
 &#39;remove_misclassified_examples&#39;: True,
 &#39;pp&#39;: {&#39;num_beams&#39;: 1,
  &#39;num_return_sequences&#39;: 1,
  &#39;num_beam_groups&#39;: 1,
  &#39;diversity_penalty&#39;: 0.0,
  &#39;temperature&#39;: 1.5,
  &#39;length_penalty&#39;: 1,
  &#39;min_length&#39;: 5,
  &#39;max_length&#39;: 20},
 &#39;use_small_ds&#39;: False,
 &#39;n_shards&#39;: None,
 &#39;shard_contiguous&#39;: None,
 &#39;save_model_while_training&#39;: False,
 &#39;save_model_freq&#39;: 10,
 &#39;wandb&#39;: {&#39;mode&#39;: &#39;online&#39;,
  &#39;log_grads&#39;: False,
  &#39;log_grads_freq&#39;: 1,
  &#39;plot_examples&#39;: False,
  &#39;n_examples_plot&#39;: 4,
  &#39;log_training_step_table&#39;: True,
  &#39;log_token_entropy&#39;: True,
  &#39;log_token_probabilities&#39;: True},
 &#39;sampling_strategy&#39;: &#39;greedy&#39;,
 &#39;reward_strategy&#39;: &#39;[-0.5 if sts &lt; 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]&#39;,
 &#39;n_layers_frozen&#39;: &#39;2&#39;,
 &#39;orig_max_length&#39;: 20,
 &#39;batch_size_train&#39;: 2,
 &#39;batch_size_eval&#39;: 4,
 &#39;accumulation_steps&#39;: 1,
 &#39;eval_freq&#39;: 10,
 &#39;n_train_epochs&#39;: 60}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

