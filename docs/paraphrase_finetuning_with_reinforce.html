---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "paraphrase_finetuning_with_reinforce.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: paraphrase_finetuning_with_reinforce.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we are trying to adjust parameters of a paraphrase model to generate adversarial examples.</p>
<h3 id="Policy-gradients">Policy gradients<a class="anchor-link" href="#Policy-gradients"> </a></h3><p>The key parameter update equation is $\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta)$, where $\alpha$ is a step size parameter, the parameter vector $\theta$ is for a model (here a paraphrase model), and $J$ is a loss function. The time step $t$ depends on the problem specification and we will get to it later.</p>
<p>Now in my review I have defined the loss function $J(\theta) = E_\pi[r(\tau)]$. Here:</p>
<ul>
<li>$\pi$ is the policy, a probability distribution for the next action in a given state; essentially $p(a_t|s_t)$</li>
<li>$\tau$ is a trajectory, a specific sequence $s_0, a_0, r_1, s_1, a_1, \ldots$ of the agent in the game. This starts at time $t=0$ and finishes at time $t=T$. </li>
<li>$r(\tau)$ is the sum of rewards for a trajectory $\tau$, or in other words, the total reward for the trajectory. </li>
</ul>
<p>For this loss function higher values are better (which might make it a reward function) and so we might have to invert it at some point.</p>
<p>To update parameters we must find the gradient $\nabla_\theta J(\theta)$, which measures how $J(\theta)$ changes when we adjust the parameters of the paraphrase model. The gradient is simplified through some maths to get the policy gradient theorem $$ \nabla_\theta J(\theta) =  \nabla_\theta E_\pi [r(\tau)]  = E_\pi \left[r(\tau) \sum_{t=1}^T \nabla_\theta \log \pi (a_t|s_t)  \right] $$</p>
<p>To calculate this you need to calculate the expectation term, which in turn means evaluating every possible trajectory $\tau$ and its expected return. Generally this is not possible and instead we turn to estimators.</p>
<p>One of these is REINFORCE. It gives us  $$ \nabla_\theta J(\theta) \approx \sum_{s=1}^S \sum_{t=1}^T G_t \nabla \log \pi(a_t|s_t)$$ where</p>
<ul>
<li>$G_t$ is the discounted return and is given by $G_t = r_t + \beta r_{t-1} + \beta^2 r_{t-2} + \dots$. It's a rough estimate of $r(\tau)$. Rewards obtained later in the episode are weighted much higher than rewards obtained earlier. I guess it assumes that the parameters update every timestep. </li>
<li>$S$ is some number of samples.</li>
</ul>
<p>The implementation of REINFORCE and similar estimators depends on how we formulate the problem. Below we present some possible formulations</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Interpretation-One:-Document-level">Interpretation One: Document-level<a class="anchor-link" href="#Interpretation-One:-Document-level"> </a></h3><p>This is the first implementation we will try.</p>
<p>Here we generate a list of paraphrases at each time point. The idea is that there is one paraphrase amongst them that is a good adversarial example. We try to tune the model to produce the best one.</p>
<p>This interpretation sees forming the complete paraphrase as one time step. So it isn't token-level but document-level.</p>
<ul>
<li>Starting state: $s0 = x$, the original example  </li>
<li>Actions: each action is "choosing" a paraphrase (or of choosing $n$ paraphrases). The set of all possible paraphrases and their probabilities is the policy. So $\pi(a|s) = p(x'| x;\theta)$ where $x'$ is the paraphrase (or list of paraphrases). <ul>
<li>To approximate this probability, what we can do is generate a large list of paraphrases, and for each, the probabilities of generating each token in turn for that paraphrase. This gives a rough "probability" of how likely that sequence was. This number is kind of like a weight for how good that paraphrase is, according to the model.  We can then turn the weights into probabilities to get a "probability" of the paraphrase. This is dependent on the number of paraphrases generated, so generating a large list is likely to be better for this task. </li>
</ul>
</li>
<li>Reward: The paraphrase moves through the reward function $R(x, x')$) to get the reward $r$. </li>
<li>Time steps: We only have one time step in the game ($T=1$ and $G_t=r$)  </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are a few variations to this scenario that we can do. For each of these we will formulate the policy and the reward function $R$. Below, $x'$ means paraphrase, $f(x)_y$ means the model confidence of x for the class of the true label $y$, $SS(a,b)$ is the result of a semantic similarity model run over $a$ and $b$, and $\lambda$ is a hyperparameter.</p>
<h4 id="One-paraphrase">One-paraphrase<a class="anchor-link" href="#One-paraphrase"> </a></h4><p>Here we only generate one paraphrase. This scenario also has a few options. First we generate a list of paraphrases with the probabilities of selecting one. Then we either sample probabilistically from the list or pick the most probable option.</p>
<p>In this case the policy $p(x'|x,\theta)$ is the chance of obtaining a specific paraphrase. For the sampling option this is equal to its sample probability. For the top option this is just the probability of selecting that option.</p>
<p>The reward function might look like $R(x,x') = f(x)_y - f(x')_y + \lambda SS(x, x')$. We could also make the $SS$ factor a step-function above some threshold.</p>
<p>The REINFORCE equation $$ \nabla_\theta J(\theta) \approx \sum_{s=1}^S \sum_{t=1}^T G_t \nabla \log \pi(a_t|s_t)$$ becomes $$ \nabla_\theta J(\theta) \approx \sum_{s=1}^S  R(x,x'_s) \nabla \log p(x'_s|x,\theta)$$ We repeat the process $S$ times where $S$ is ideally as large as possible. We can start with something simple (e.g. $S=10$ or $S=100$) and go from there.</p>
<p>The gradient term $\nabla \log p(x'_s|x,\theta)$ can hopefully be found with autodiff.</p>
<h4 id="Set-of-paraphrases">Set of paraphrases<a class="anchor-link" href="#Set-of-paraphrases"> </a></h4><p>In this scenario the paraphrase model is evaluated on performance over a set of paraphrases, which we call $X'$ here. The policy becomes $p(X'|x, \theta)$, the probability of obtaining that list. We can get this probability by multipling together the "probability" of each individual paraphrase, multiplying also by nCr (for r paraphrases out of n total) to account for the lack of order in the list.</p>
<p>We can make a number of sub-scenarios here.</p>
<p>For the <strong>top-paraphrase in set</strong> condition the paraphrase generator is only measured on the best reward for a paraphrase in its set. The idea is the generator will learn to produce a diverse set of examples, any of which could plausibly be a good adversarial example. Here we only look at best performing paraphrase $x'_m$, which we can find by $x'_m = \max_i [f(x)_y - f(x'_i)_y]$, then return $R(x,x'_m) = [f(x)_y - f(x'_m)_y] + \lambda SS(x,x'_m)$</p>
<p>For the <strong>average-paraphrase in set</strong> condition the paraphrase generator is measured on the average reward of the paraphrases in its set. This encourages the generator to consider performance of all examples more-or-less equally. The reward function could be something like $\frac{1}{k} \sum_{i=1}^k \left[ f(x)_y - f(x'_i)_y + \lambda SS(x, x'_i) \right]$</p>
<p>A combination of these scenarios is the <strong>top-k/top-p\% paraphrases in set</strong>. Here we only use the top-$k$ paraphrases, or more generally, the top $p$ percentage of paraphrases.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Interpretation-2:-Token-level">Interpretation 2: Token-level<a class="anchor-link" href="#Interpretation-2:-Token-level"> </a></h3><p>This interpretation is at token-level; it sees choosing the next word as the next time step.</p>
<ul>
<li>Starting state: $s0 = x$, the initial state. But you also have a "blank slate" for the paraphrase. So maybe it's a tuple (x, pp) where pp is a paraphrase with no words. Here x is used as the reference for the paraphrase generator.  </li>
<li>Actions: Choose the next word of p. I guess this starts with the \&lt;START&gt; token (or something similar). Then you have the policy $\pi(a|s)$ which is the same as $p(w_{next}|pp, x; \theta)$ where $\theta$ is the paraphrase model parameters, $pp$ is the so-far constructed sentence, and $w_{next}$ is the next token (I say token because I don't know if this model is on the subword or word basis). </li>
<li>Time steps: every token is generated one-by-one and each of these is allocated a time step. This means probably that you also update the parameters after each token generated too. </li>
<li>Reward. The reward is allocated every token. There are many reward functions (see papers on token-level loss functions). Some also incorporate document-level rewards too. </li>
<li>Next state. $s_1$ is again the tuple $(x, pp)$ but now $pp$ has the first word in it. </li>
</ul>
<p>On <em>teacher forcing</em>. This is when you have a ground-truth paraphrase and you can use it when generating tokens. This is useful because if the model makes a mistake it doesn't continue down that track but is adjusted back. This stops big divergences (but also might limit the diversity of generated paraphrases). This is used when training a paraphrase model. You have a set of reference paraphrases that are human provided. Here though we only have the original sentence and no references. We could generate adversarial examples and use that to do teacher forcing. Generating them using textattack recipes might work. This is only really used on the token-level rewards.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Updating-the-paraphrase-model-parameters.">Updating the paraphrase model parameters.<a class="anchor-link" href="#Updating-the-paraphrase-model-parameters."> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is a choice here. We can either directly update the parameters of the paraphrase model. Or we can fix the parameters and add a new dense layer to the end of the model. We could then train this dense layer to convert paraphrases to adversarial paraphrases.</p>
<p>Before trying this out, I am worried that we will destroy the capabilities of the paraphrase generator a bit. We might get semantically invalid or ungrammatical or gibberish text. If so we could try and mitigate it a bit by shaping our reward function to maintain grammatical components.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experiment-order">Experiment order<a class="anchor-link" href="#Experiment-order"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Plan is to try the following order:</p>
<ol>
<li>One-paraphrase (most probable option). I'll start with this one because it is probably the most simple case. Within this category: 
 1a. tune existing parameters only (see if the text is recognisable) 
 1b. add dense layer onto end and try again </li>
<li>One-paraphrase (sampled). This seems like a logical extension on the first one. </li>
<li>Paraphrase-set options. (Decide after finishing 1, 2) </li>
<li>Token-level tuning. (Decide after 1,2,3)</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Layer-Freezing">Layer Freezing<a class="anchor-link" href="#Layer-Freezing"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I am uncertain on if to do this or not.</p>
<ul>
<li>This <a href="https://arxiv.org/abs/1911.03090">paper</a> indicates that you can get pretty good results by freezing all layers except the last few </li>
<li>Conversely I saw in the transformers documentation that transformers train better if you don't do layer freezing </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup,-load-models-+-datasets">Setup, load models + datasets<a class="anchor-link" href="#Setup,-load-models-+-datasets"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">load_ext</span> line_profiler
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TOKENIZERS_PARALLELISM&quot;</span><span class="p">]</span>  <span class="o">=</span> <span class="s2">&quot;true&quot;</span>  <span class="c1"># set to false if not working</span>

<span class="c1"># Core imports </span>
<span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span><span class="o">,</span> <span class="nn">gc</span><span class="o">,</span><span class="nn">sys</span><span class="o">,</span> <span class="nn">logging</span><span class="o">,</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">load_metric</span><span class="p">,</span> <span class="n">load_from_disk</span><span class="p">,</span> <span class="n">DatasetDict</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> 
                          <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">SchedulerType</span><span class="p">,</span> <span class="n">get_scheduler</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.util</span> <span class="kn">import</span> <span class="n">pytorch_cos_sim</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span><span class="p">,</span> <span class="n">notebook_launcher</span>
<span class="kn">from</span> <span class="nn">cachetools</span> <span class="kn">import</span> <span class="n">cached</span><span class="p">,</span> <span class="n">LRUCache</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">MethodType</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="kn">import</span> <span class="nn">utils</span><span class="p">;</span> <span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>   <span class="c1"># local script</span>
<span class="kn">from</span> <span class="nn">tests</span> <span class="kn">import</span> <span class="o">*</span>   <span class="c1"># local script</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">copy</span> 
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">from</span> <span class="nn">undecorated</span> <span class="kn">import</span> <span class="n">undecorated</span>


<span class="c1"># Dev imports (not needed for final script)</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">IPython.core.debugger</span> <span class="kn">import</span> <span class="n">set_trace</span>
<span class="kn">from</span> <span class="nn">GPUtil</span> <span class="kn">import</span> <span class="n">showUtilization</span>
<span class="kn">import</span> <span class="nn">torchsnooper</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">### These parameters mostly don&#39;t do anything but are more notes (for the wanb.init function)</span>
<span class="n">debug_run</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>   <span class="c1"># doesn&#39;t do anything</span>
<span class="n">sampling_strategy</span> <span class="o">=</span> <span class="s2">&quot;greedy&quot;</span>  <span class="c1"># doesn&#39;t do anything</span>
<span class="c1"># copy-paste this from reward function</span>
<span class="n">reward_strategy</span> <span class="o">=</span> <span class="s2">&quot;[-0.5 if sts &lt; 0.5 else 0.5+v*sts for v,sts in zip(vm_scores, sts_scores)]&quot;</span> <span class="c1"># doesn&#39;t do anything</span>
<span class="c1"># options for the pp_model </span>
<span class="c1"># 1. tuner007/pegasus_paraphrase</span>
<span class="c1"># 2. tdopierre/ProtAugment-ParaphraseGenerator</span>
<span class="c1"># 3. eugenesiow/bart-paraphrase</span>
<span class="c1">#pp_name = &quot;eugenesiow/bart-paraphrase&quot;</span>
<span class="n">pp_name</span> <span class="o">=</span> <span class="s2">&quot;eugenesiow/bart-paraphrase&quot;</span>
<span class="n">vm_name</span> <span class="o">=</span> <span class="s2">&quot;textattack/distilbert-base-uncased-rotten-tomatoes&quot;</span>
<span class="n">sts_name</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;</span>
<span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;rotten_tomatoes&quot;</span>
<span class="n">n_layers_frozen</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>  <span class="c1"># counting from the back (doesn&#39;t do anything yet)</span>
<span class="n">use_fp16</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">save_model_while_training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">save_model_freq</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># These parameters only have effect if small_ds = True</span>

<span class="c1">### Paraphrase parameters  </span>
<span class="n">pp_model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
    <span class="s2">&quot;num_return_sequences&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
    <span class="s2">&quot;num_beam_groups&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
    <span class="s2">&quot;diversity_penalty&quot;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,</span>   <span class="c1"># must be a float</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>
    <span class="s2">&quot;length_penalty&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;min_length&quot;</span> <span class="p">:</span> <span class="mi">5</span>
<span class="p">}</span>
<span class="k">if</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s2">&quot;rotten_tomatoes&quot;</span><span class="p">:</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">pp_model_params</span><span class="p">[</span><span class="s1">&#39;max_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="c1">#batch_size_train = 32</span>
    <span class="c1">#batch_size_eval = 128</span>
    <span class="n">batch_size_train</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">batch_size_eval</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">n_train_epochs</span> <span class="o">=</span> <span class="mi">250</span>
    <span class="n">eval_freq</span> <span class="o">=</span> <span class="mi">1</span> 
    <span class="n">use_small_ds</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># for testing purposes</span>
    <span class="n">n_shards</span>         <span class="o">=</span> <span class="mi">60</span>    <span class="k">if</span> <span class="n">use_small_ds</span> <span class="k">else</span> <span class="kc">None</span> 
    <span class="n">shard_contiguous</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">use_small_ds</span> <span class="k">else</span> <span class="kc">None</span> 
<span class="k">elif</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s2">&quot;simple&quot;</span><span class="p">:</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">pp_model_params</span><span class="p">[</span><span class="s1">&#39;max_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="n">batch_size_train</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">batch_size_eval</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">eval_freq</span> <span class="o">=</span> <span class="mi">10</span> 
    <span class="n">n_train_epochs</span> <span class="o">=</span> <span class="mi">60</span>
    <span class="n">use_small_ds</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># for testing purposes</span>
    <span class="n">n_shards</span>         <span class="o">=</span>  <span class="kc">None</span> 
    <span class="n">shard_contiguous</span> <span class="o">=</span>  <span class="kc">None</span> 
    <span class="k">if</span> <span class="n">use_small_ds</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span> 
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Don&#39;t shard when using the simple dataset (no need)&quot;</span><span class="p">)</span>


<span class="c1">### Training parameters</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">420</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-5</span> <span class="c1"># Initial learning rate (after the potential warmup period) to use</span>
<span class="n">normalise_rewards</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;pp_logp&#39;</span><span class="p">,</span> <span class="s1">&#39;reward&#39;</span><span class="p">,</span> <span class="s1">&#39;vm_score&#39;</span><span class="p">,</span> <span class="s2">&quot;sts_score&quot;</span><span class="p">,</span> <span class="s1">&#39;label_flip&#39;</span><span class="p">]</span>
<span class="n">pin_memory</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">zero_grad_with_none</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">pad_token_embeddings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">padding_multiple</span> <span class="o">=</span> <span class="mi">8</span> 
<span class="n">bucket_by_length</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">shuffle_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">remove_misclassified_examples</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1">#weight_decay = 0</span>
<span class="c1">#lr_scheduler_type = &#39;none&#39;</span>
<span class="c1">#n_warmup_steps = 30 </span>

<span class="c1">### W&amp;B parameters</span>
<span class="n">wandb_mode</span> <span class="o">=</span> <span class="s2">&quot;online&quot;</span>  <span class="c1"># set to &quot;disabled&quot; to turn off wandb, &quot;online&quot; to enable it</span>
<span class="n">wandb_log_grads</span> <span class="o">=</span> <span class="kc">False</span>   
<span class="n">wandb_log_grads_freq</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># no effect if wandb_log_grads is False</span>
<span class="n">wandb_plot_examples</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">wandb_n_examples_plot</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># number of individual examples to plot curves for</span>
<span class="c1"># log a table to wandb with the examples and rewards the model sees while training. Useful for debugging </span>
<span class="c1"># and seeing what is going on, but slows down training time. </span>
<span class="n">wandb_log_training_step_table</span> <span class="o">=</span> <span class="kc">True</span>  
<span class="n">wandb_log_token_entropy</span><span class="o">=</span><span class="kc">True</span>
<span class="n">wandb_log_token_probabilities</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Parameter dict</span>
<span class="n">config_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">debug_run</span> <span class="o">=</span> <span class="n">debug_run</span><span class="p">,</span>
    <span class="n">sampling_strategy</span> <span class="o">=</span> <span class="n">sampling_strategy</span><span class="p">,</span>
    <span class="n">reward_strategy</span> <span class="o">=</span> <span class="n">reward_strategy</span><span class="p">,</span>
    <span class="n">pp_name</span> <span class="o">=</span> <span class="n">pp_name</span><span class="p">,</span>
    <span class="n">vm_name</span> <span class="o">=</span> <span class="n">vm_name</span><span class="p">,</span>
    <span class="n">sts_name</span><span class="o">=</span><span class="n">sts_name</span><span class="p">,</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="n">dataset_name</span><span class="p">,</span> 
    <span class="n">use_small_ds</span> <span class="o">=</span> <span class="n">use_small_ds</span><span class="p">,</span>
    <span class="n">shard_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">n_shards</span> <span class="o">=</span> <span class="n">n_shards</span><span class="p">,</span>
        <span class="n">shard_contiguous</span> <span class="o">=</span> <span class="n">shard_contiguous</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">n_layers_frozen</span> <span class="o">=</span> <span class="n">n_layers_frozen</span><span class="p">,</span>
    <span class="n">pp_model_params</span> <span class="o">=</span> <span class="n">pp_model_params</span><span class="p">,</span> 
    <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">,</span>
    <span class="n">batch_size_train</span> <span class="o">=</span> <span class="n">batch_size_train</span><span class="p">,</span>
    <span class="n">batch_size_eval</span> <span class="o">=</span> <span class="n">batch_size_eval</span><span class="p">,</span>
    <span class="n">fp16</span> <span class="o">=</span> <span class="n">use_fp16</span><span class="p">,</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span><span class="p">,</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">,</span> 
    <span class="n">accumulation_steps</span><span class="o">=</span><span class="n">accumulation_steps</span><span class="p">,</span>
    <span class="n">n_train_epochs</span> <span class="o">=</span> <span class="n">n_train_epochs</span><span class="p">,</span>
    <span class="n">eval_freq</span> <span class="o">=</span> <span class="n">eval_freq</span><span class="p">,</span>
    <span class="n">normalise_rewards</span> <span class="o">=</span> <span class="n">normalise_rewards</span><span class="p">,</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">,</span>
    <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span><span class="p">,</span>
    <span class="n">zero_grad_with_none</span> <span class="o">=</span> <span class="n">zero_grad_with_none</span><span class="p">,</span>
    <span class="n">pad_token_embeddings</span> <span class="o">=</span> <span class="n">pad_token_embeddings</span><span class="p">,</span>
    <span class="n">padding_multiple</span> <span class="o">=</span> <span class="n">padding_multiple</span><span class="p">,</span>
    <span class="n">bucket_by_length</span> <span class="o">=</span> <span class="n">bucket_by_length</span><span class="p">,</span>
    <span class="n">shuffle_train</span> <span class="o">=</span> <span class="n">shuffle_train</span><span class="p">,</span>
    <span class="n">remove_misclassified_examples</span> <span class="o">=</span> <span class="n">remove_misclassified_examples</span><span class="p">,</span> 
    <span class="n">wandb_params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">log_grads</span> <span class="o">=</span> <span class="n">wandb_log_grads</span><span class="p">,</span>
        <span class="n">log_grads_freq</span> <span class="o">=</span> <span class="n">wandb_log_grads_freq</span><span class="p">,</span> 
        <span class="n">plot_examples</span> <span class="o">=</span> <span class="n">wandb_plot_examples</span><span class="p">,</span> 
        <span class="n">n_examples_plot</span> <span class="o">=</span> <span class="n">wandb_n_examples_plot</span><span class="p">,</span> 
        <span class="n">log_training_step_table</span> <span class="o">=</span> <span class="n">wandb_log_training_step_table</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Paths</span>
<span class="n">path_data</span> <span class="o">=</span> <span class="s2">&quot;./data/&quot;</span>
<span class="n">path_checkpoints</span> <span class="o">=</span> <span class="s2">&quot;../model_checkpoints/travis_attack/&quot;</span>

<span class="c1"># Seeds</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Devices and GPU settings</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span> 
<span class="c1">#device = accelerator.device</span>
<span class="n">devicenum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
<span class="c1"># When not using Accelerator</span>
<span class="c1">#n_wkrs = 4 * torch.cuda.device_count()</span>
<span class="c1"># When using Accelerator </span>
<span class="n">n_wkrs</span> <span class="o">=</span> <span class="mi">0</span> 

<span class="c1"># Configs</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">MAX_ARTIFACT_ROWS</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">MAX_ROWS</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">run_notes</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Debug run:</span><span class="si">{</span><span class="n">debug_run</span><span class="si">}</span><span class="se">\n</span><span class="s2">Reward: </span><span class="si">{</span><span class="n">reward_strategy</span><span class="si">}</span><span class="se">\n</span><span class="s2">Dataset: </span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="se">\</span>
<span class="se">\n</span><span class="s2">Sampling strategy: </span><span class="si">{</span><span class="n">sampling_strategy</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Logging </span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">)</span> <span class="c1"># stdout while we are doing stdout to file piping</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;main_logger&quot;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

<span class="c1"># Other </span>
<span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]</span>

<span class="c1">### Dev code</span>
<span class="c1"># used to know what to set max_pp_length to. </span>
<span class="n">track_pp_sizes</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">track_pp_sizes</span><span class="p">:</span> 
    <span class="n">n_train_epochs</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">use_small_ds</span> <span class="o">=</span> <span class="kc">True</span> 
    <span class="n">n_shards</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># half the train dataset gives us a good understanding of this</span>
    <span class="n">orig_max_l</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pp_max_l</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># After running the code  </span>
    <span class="c1"># sns.distplot(orig_max_l)</span>
    <span class="c1"># sns.distplot(pp_max_l)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-models">Load models<a class="anchor-link" href="#Load-models"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pp_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pp_name</span><span class="p">)</span>
<span class="c1"># takes about 3GB memory space up on the GPU</span>
<span class="c1"># change the `local_files_only` argument if changing the model name </span>
<span class="n">pp_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pp_name</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># The no_grad version of generate</span>
<span class="n">generate_with_grad</span> <span class="o">=</span> <span class="n">undecorated</span><span class="p">(</span><span class="n">pp_model</span><span class="o">.</span><span class="n">generate</span><span class="p">)</span>
<span class="n">pp_model</span><span class="o">.</span><span class="n">generate_with_grad</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">generate_with_grad</span><span class="p">,</span> <span class="n">pp_model</span><span class="p">)</span>

<span class="c1">## Victim Model (VM)</span>
<span class="n">vm_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">vm_name</span><span class="p">)</span>
<span class="n">vm_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">vm_name</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vm_idx2lbl</span> <span class="o">=</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>
<span class="n">vm_lbl2idx</span> <span class="o">=</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">label2id</span>
<span class="n">vm_num_labels</span> <span class="o">=</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">num_labels</span>

<span class="c1">## Semantic Textual Similarity (STS) model</span>
<span class="n">sts_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">sts_name</span><span class="p">)</span>

<span class="c1"># Pad vocab to multiple of 8 (for better tensor core efficiency in fp16)</span>
<span class="k">if</span> <span class="n">pad_token_embeddings</span><span class="p">:</span> 
    <span class="k">def</span> <span class="nf">pad_token_embeddings_to_multiple_of_n</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">get_new_vocab_size</span><span class="p">(</span><span class="n">model</span><span class="p">):</span> <span class="k">return</span> <span class="nb">int</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="n">get_new_vocab_size</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
    <span class="n">pad_token_embeddings_to_multiple_of_n</span><span class="p">(</span><span class="n">pp_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">pad_token_embeddings_to_multiple_of_n</span><span class="p">(</span><span class="n">vm_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="c1"># sts_model is from SentenceTransformers so needs a bit of unwrapping to access the base huggingface model </span>
    <span class="n">pad_token_embeddings_to_multiple_of_n</span><span class="p">(</span><span class="n">sts_model</span><span class="o">.</span><span class="n">_first_module</span><span class="p">()</span><span class="o">.</span><span class="n">auto_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> 
<span class="c1"># unlike pp_tokenizer.vocab_size this includes the padding </span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span><span class="o">.</span><span class="n">num_embeddings</span>  
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-raw-datasets-and-create-dataloaders">Load raw datasets and create dataloaders<a class="anchor-link" href="#Load-raw-datasets-and-create-dataloaders"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Functions">Functions<a class="anchor-link" href="#Functions"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">add_idx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">;</span> <span class="k">return</span> <span class="n">x</span>   <span class="c1"># add row numbers</span>

<span class="k">def</span> <span class="nf">add_n_tokens</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;n_tokens&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]];</span> <span class="k">return</span> <span class="n">x</span>  <span class="c1"># number of tokens of &quot;text&quot; field </span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="k">return</span> <span class="n">pp_tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">prep_small_ds</span><span class="p">(</span><span class="n">ds_dict</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">ds_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="n">ds_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">n_shards</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">contiguous</span><span class="o">=</span><span class="n">shard_contiguous</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ds_dict</span>

<span class="k">def</span> <span class="nf">get_vm_probs</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_predclass</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;Used also by the reward_fn to get vm_score&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">tkns</span> <span class="o">=</span> <span class="n">vm_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">padding_multiple</span><span class="p">,</span>
                            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">vm_model</span><span class="p">(</span><span class="o">**</span><span class="n">tkns</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_predclass</span><span class="p">:</span>    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>                   <span class="k">return</span> <span class="n">probs</span>

<span class="k">def</span> <span class="nf">get_vm_orig_score</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span> 
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">orig_probs</span><span class="p">,</span><span class="n">orig_predclass</span> <span class="o">=</span> <span class="n">get_vm_probs</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">return_predclass</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;orig_truelabel_probs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">orig_probs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:,</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;orig_vm_predclass&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">orig_predclass</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="k">def</span> <span class="nf">get_sampler</span><span class="p">(</span><span class="n">ds</span><span class="p">):</span> 
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_sts_orig_embeddings</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span> 
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;orig_sts_embeddings&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sts_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="k">def</span> <span class="nf">collate_fn_tkn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;orig_truelabel_probs&#39;</span><span class="p">,</span> <span class="s1">&#39;orig_sts_embeddings&#39;</span><span class="p">]:</span> 
        <span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">padding_multiple</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">collate_fn_raw</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">]:</span> 
        <span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">d</span> 

<span class="k">def</span> <span class="nf">get_dataloaders_dict</span><span class="p">(</span><span class="n">ds_dict</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">):</span> 
    <span class="k">if</span> <span class="n">bucket_by_length</span> <span class="ow">and</span> <span class="n">shuffle_train</span><span class="p">:</span>  <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Can only do one of bucket by length or shuffle&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">split</span><span class="p">,</span> <span class="n">ds</span> <span class="ow">in</span> <span class="n">ds_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
        <span class="k">if</span> <span class="n">shuffle_train</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span> 
                <span class="n">sampler</span> <span class="o">=</span> <span class="n">get_sampler</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span>
                <span class="n">d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span>  <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_train</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
                                       <span class="n">num_workers</span><span class="o">=</span><span class="n">n_wkrs</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">)</span> 
            <span class="k">else</span><span class="p">:</span> 
                <span class="n">d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span>  <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_eval</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
                                       <span class="n">num_workers</span><span class="o">=</span><span class="n">n_wkrs</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">)</span> 
        <span class="k">if</span> <span class="n">bucket_by_length</span><span class="p">:</span> 
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_train</span> <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span> <span class="k">else</span> <span class="n">batch_size_eval</span>
            <span class="n">d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span>  <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
                                   <span class="n">num_workers</span><span class="o">=</span><span class="n">n_wkrs</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">)</span> 
    
    <span class="c1"># Add eval dataloader for train </span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;train_eval&#39;</span><span class="p">]</span> <span class="o">=</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">ds_dict</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_eval</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">n_wkrs</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">d</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Load,-tokenise,-preprocess-datasets">Load, tokenise, preprocess datasets<a class="anchor-link" href="#Load,-tokenise,-preprocess-datasets"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s2">&quot;rotten_tomatoes&quot;</span><span class="p">:</span> 
    <span class="n">dsd_raw</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;rotten_tomatoes&quot;</span><span class="p">)</span>
    <span class="n">dsd_raw</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dsd_raw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;validation&#39;</span><span class="p">)</span>  <span class="c1"># &quot;valid&quot; is easier than &quot;validation&quot;</span>
    <span class="n">label_cname</span> <span class="o">=</span> <span class="s1">&#39;label&#39;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">ds</span> <span class="ow">in</span> <span class="n">dsd_raw</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># make sure that all datasets have the same number of labels as what the victim model predicts</span>
        <span class="k">assert</span> <span class="n">ds</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">label_cname</span><span class="p">]</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="n">vm_num_labels</span>    
<span class="k">elif</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="s2">&quot;simple&quot;</span><span class="p">:</span> 
    <span class="n">dsd_raw</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>  <span class="n">dsd_raw</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;csv&#39;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path_data</span><span class="si">}</span><span class="s2">simple_dataset_</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">.csv&quot;</span><span class="p">)[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span>
<span class="k">elif</span> <span class="n">dataset_name</span><span class="o">==</span><span class="s2">&quot;snli&quot;</span><span class="p">:</span> 
    <span class="nb">next</span>
    <span class="c1">## For snli</span>
    <span class="c1"># remove_minus1_labels = lambda x: x[label_cname] != -1</span>
    <span class="c1"># ds_train = ds_train.filter(remove_minus1_labels)</span>
    <span class="c1"># valid = valid.filter(remove_minus1_labels)</span>
    <span class="c1"># test = test.filter(remove_minus1_labels)</span>
<span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd_raw</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">add_idx</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">use_small_ds</span><span class="p">:</span> <span class="n">dsd</span> <span class="o">=</span> <span class="n">prep_small_ds</span><span class="p">(</span><span class="n">dsd</span><span class="p">)</span>  <span class="c1"># do after adding idx so it&#39;s consistent across runs</span>
<span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">get_vm_orig_score</span><span class="p">,</span>       <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">remove_misclassified_examples</span><span class="p">:</span> <span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;orig_vm_predclass&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
<span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">get_sts_orig_embeddings</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">add_n_tokens</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">bucket_by_length</span><span class="p">:</span> <span class="n">dsd</span> <span class="o">=</span> <span class="n">dsd</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;n_tokens&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dld_raw</span> <span class="o">=</span> <span class="n">get_dataloaders_dict</span><span class="p">(</span><span class="n">dsd</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn_raw</span><span class="p">)</span> 
<span class="n">dld_tkn</span> <span class="o">=</span> <span class="n">get_dataloaders_dict</span><span class="p">(</span><span class="n">dsd</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn_tkn</span><span class="p">)</span>    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using custom data configuration default
Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5)
Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-cc6f195b68a8dc00.arrow
Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-250225f8dcb802d9.arrow
Loading cached processed dataset at /data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5/cache-1d69e4abfa135e2d.arrow
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training">Training<a class="anchor-link" href="#Training"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Description">Description<a class="anchor-link" href="#Description"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training loop pseudocode</p>
<p>The REINFORCE estimator is $$ \nabla_\theta J(\theta) \approx \sum_{s=1}^S  R(x,x'_s) \nabla \log p(x'_s|x,\theta)$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% raw %}
$$ J(\theta) \approx \sum_{s=1}^S  R(x,x'_s) \log p(x'_s|x,\theta)$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preprocessing-and-setup">Preprocessing and setup<a class="anchor-link" href="#Preprocessing-and-setup"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Define-functions">Define functions<a class="anchor-link" href="#Define-functions"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Training-functions">Training functions<a class="anchor-link" href="#Training-functions"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_entropy_metrics</span><span class="p">(</span><span class="n">scores_stacked</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span> 
    <span class="n">ent</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span> <span class="o">=</span> <span class="n">scores_stacked</span><span class="p">)</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">ent</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">pp_batch_size</span><span class="p">,</span> <span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

    <span class="n">ent</span> <span class="o">=</span> <span class="n">ent</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># stop values after eos token from contributing to ent score </span>
    <span class="c1"># first remove structure (otherwise we have ragged arrays)</span>
    <span class="c1"># then remove corresponding attention mask values</span>
    <span class="c1"># we can&#39;t just filter by ent[ent != 0] because we might have zero tokens during the sequence</span>
    <span class="n">att_flat</span><span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">att_flat</span><span class="p">)</span>
    <span class="n">ent_flat</span> <span class="o">=</span> <span class="n">ent</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[</span><span class="n">indices</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">att_flat</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="c1"># check everything we filter out is zero </span>
    <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">ent</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[</span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="o">~</span><span class="p">(</span><span class="n">att_flat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span> <span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">ent_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">ent_min</span>             <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="n">ent_lower_quartile</span>  <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
        <span class="n">ent_median</span>          <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">median</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
        <span class="n">ent_mean</span>            <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
        <span class="n">ent_upper_quartile</span>  <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
        <span class="n">ent_max</span>             <span class="o">=</span> <span class="n">ent_flat</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> 
        <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">ent_d</span>

<span class="k">def</span> <span class="nf">get_token_probability_metrics</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span> 
        <span class="n">token_prob_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">tkn_kmaxprob</span><span class="p">,</span> <span class="n">tkn_kmaxidx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">,</span><span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">tkn_kmaxprob</span> <span class="o">=</span> <span class="n">tkn_kmaxprob</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># log these </span>
        <span class="k">assert</span> <span class="n">tkn_kmaxprob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">pp_batch_size</span><span class="p">,</span> <span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>

        <span class="c1"># % of first prob over 0.9, 0.75, 0.5, 0.3, 0.1</span>
        <span class="n">top_probs</span> <span class="o">=</span> <span class="n">tkn_kmaxprob</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="n">top_probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">top_probs</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">top_probs</span> <span class="o">=</span> <span class="n">top_probs</span><span class="p">[</span><span class="n">top_probs</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">prob_threshold_l</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prob_threshold_l</span><span class="p">:</span> 
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;top_token_prob_over_</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">top_probs</span> <span class="o">&gt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">top_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># avg + median + lower + upper quartile of first, second, third choice probs</span>
        <span class="n">tkn_kmaxprob_mask</span> <span class="o">=</span> <span class="n">tkn_kmaxprob</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">]</span>  <span class="c1"># broadcasting over kth dim</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> 
            <span class="n">probs</span> <span class="o">=</span> <span class="n">tkn_kmaxprob_mask</span><span class="p">[:,:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">probs</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rank_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_token_prob_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rank_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_token_prob_median&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">median</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rank_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_token_prob_0.25_quantile&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rank_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_token_prob_0.75_quantile&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># tokens over probs above 0.1, 0.01, 0.001, 0.0001, 1/vocab_size prob </span>
        <span class="n">allprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores_log_softmax</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">allprobs</span> <span class="o">=</span> <span class="n">allprobs</span><span class="p">[</span><span class="n">allprobs</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.00001</span><span class="p">]:</span> 
            <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;%_of_tokens_above_prob_</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">allprobs</span> <span class="o">&gt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">allprobs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">token_prob_d</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;%_of_tokens_above_prob_1/vocab_size&quot;</span><span class="p">]</span> <span class="o">=</span> \
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">allprobs</span> <span class="o">&gt;</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">vocab_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">allprobs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
        <span class="n">token_prob_d</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="n">token_prob_d</span><span class="p">[</span><span class="s1">&#39;global_step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_step</span>
        <span class="k">return</span> <span class="n">token_prob_d</span>

<span class="k">def</span> <span class="nf">get_paraphrases</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span><span class="n">attention_mask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper for generating paraphrases (pp&#39;s). Most keywords are passed on to pp_model.generate function, </span>
<span class="sd">    so see docs for that function. &quot;&quot;&quot;</span>
    <span class="c1"># Only greedy search supported at the moment</span>
    <span class="n">pp_output</span> <span class="o">=</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">generate_with_grad</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> 
                                            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> 
                                             <span class="o">**</span><span class="n">pp_model_params</span><span class="p">,</span>
                                             <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                             <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                             <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                <span class="n">remove_invalid_values</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                             <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
                                             <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
    <span class="n">pp_l</span> <span class="o">=</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">pp_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">track_pp_sizes</span><span class="p">:</span>  <span class="c1"># DEV CODE (can delete later)</span>
        <span class="n">orig_max_l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">pp_max_l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pp_output</span><span class="o">.</span><span class="n">sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span>

<span class="k">def</span> <span class="nf">get_pp_logp</span><span class="p">(</span><span class="n">pp_output</span><span class="p">,</span><span class="n">log_times</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;log(p(pp|orig)) basically.</span>
<span class="sd">    works for greedy search, will need tweaking for other types probably&quot;&quot;&quot;</span>
    <span class="c1">### TODO: this looks like logp to me, not plogp. Find out if this is right and if so rename, if not, fix</span>
    <span class="c1">### We want to align tokens with token probabilities. The first token is given at the start </span>
    <span class="c1"># and has no probability attached to it, so we remove it. </span>
    <span class="n">seq_without_first_tkn</span> <span class="o">=</span> <span class="n">pp_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">assert</span> <span class="n">seq_without_first_tkn</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">orig_batch_size</span><span class="p">,</span> <span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
    
    <span class="c1">### Convert from tuple of scores to one big tensor of scores </span>
    <span class="n">scores_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pp_output</span><span class="o">.</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">### TESTS </span>
    <span class="c1"># We check shape and that there is no +inf or nan in scores. </span>
    <span class="c1"># Scores can have -inf in them - see explanation in `exploring_generation`.  </span>
    <span class="k">assert</span> <span class="n">scores_stacked</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">orig_batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">vocab_size</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">scores_stacked</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isposinf</span><span class="p">(</span><span class="n">scores_stacked</span><span class="p">))</span>
    <span class="c1"># Rough check that all idx before min_length are -inf for all elements in batch</span>
    <span class="c1"># We do min_length - 1 because sequences are allowed to have length min_length so that idx </span>
    <span class="c1"># shouldn&#39;t be set to -inf</span>
    <span class="c1"># Not a 100% test but very likely to identify</span>
    <span class="n">idx_neginf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isneginf</span><span class="p">(</span><span class="n">scores_stacked</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_neginf</span><span class="p">[</span><span class="n">idx_neginf</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="p">:])</span> <span class="o">==</span> \
              <span class="p">(</span><span class="n">pp_model_params</span><span class="p">[</span><span class="s2">&quot;min_length&quot;</span><span class="p">]</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">orig_batch_size</span>  
    <span class="k">del</span> <span class="n">idx_neginf</span>
    
    <span class="c1">### Take log softmax of scores and then extract those that correspond </span>
    <span class="c1"># to the generated sequences    </span>
    <span class="n">scores_log_softmax</span> <span class="o">=</span> <span class="n">scores_stacked</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">seq_token_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">seq_without_first_tkn</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">### TESTS </span>
    <span class="c1"># -inf is possible in scores_log_softmax and seq_token_log_probs before the attention mask is added. </span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span>   <span class="n">scores_log_softmax</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isposinf</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">))</span>
    <span class="n">check_scores_log_softmax_sums</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">)</span>
    <span class="c1"># probs should be 1-1 with the filtered tkns: check shape to confirm</span>
    <span class="k">assert</span> <span class="n">seq_token_log_probs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">seq_without_first_tkn</span><span class="o">.</span><span class="n">shape</span>  
    <span class="c1"># Check that the last token probability corresponds to a possible end token</span>
    <span class="c1"># this has to be tested before the attention mask is multiplied with it because if the </span>
    <span class="c1"># padding token is 0 then this will be 0 too (and not the same as scores_log_softmax)</span>
    <span class="n">output_end_ids</span> <span class="o">=</span> <span class="n">get_start_end_special_token_ids</span><span class="p">(</span><span class="n">pp_tokenizer</span><span class="p">)[</span><span class="s1">&#39;output_end_id&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">o</span> <span class="ow">in</span> <span class="n">scores_log_softmax</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_end_ids</span><span class="p">]</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">seq_token_log_probs</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="k">del</span> <span class="n">output_end_ids</span>
    <span class="c1">## THIS ONE IS LONG - a test rather than assert </span>
    <span class="c1"># check_seq_token_log_prob_values_are_correct(seq_without_first_tkn, scores_log_softmax, </span>
    <span class="c1">#                                             seq_token_log_probs) </span>
    
    <span class="c1">### Generate attention mask to identify padding tokens. Then apply it to the </span>
    <span class="c1"># sequence probabilities so that we don&#39;t consider probability of padding tokens </span>
    <span class="c1"># when getting sequence probabilities. </span>
    <span class="c1"># Also replace the -inf values in seq_token_log_probs with a large negative number because if we </span>
    <span class="c1"># leave them in we end up with nan&#39;s introduced after multiplying with attention_mask, </span>
    <span class="c1"># since  -inf * 0 = nan </span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">_prepare_attention_mask_for_generation</span><span class="p">(</span>
        <span class="n">seq_without_first_tkn</span><span class="p">,</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>
    <span class="n">seq_token_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">seq_token_log_probs</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=-</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">seq_token_log_probs</span> <span class="o">=</span> <span class="n">seq_token_log_probs</span> <span class="o">*</span> <span class="n">attention_mask</span>
    <span class="c1">### TESTS</span>
    <span class="k">assert</span> <span class="n">seq_token_log_probs</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">seq_token_log_probs</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># check attention mask only has 0 for padding tokens and not eos tokens or anything else</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">seq_without_first_tkn</span><span class="p">[</span><span class="n">attention_mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pp_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
    <span class="n">check_no_nans_or_infs</span><span class="p">(</span><span class="n">seq_token_log_probs</span><span class="p">)</span>
    <span class="c1"># check that we aren&#39;t picking extrememly rare tokens</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">seq_token_log_probs</span>  <span class="o">&gt;</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span>  
    
    <span class="c1">### Get sequence probabilities by summing up token log probabilities </span>
    <span class="n">seq_log_prob</span> <span class="o">=</span> <span class="n">seq_token_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">## TESTS </span>
    <span class="k">assert</span> <span class="n">seq_log_prob</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">pp_batch_size</span><span class="p">])</span>
    <span class="n">check_no_nans_or_infs</span><span class="p">(</span><span class="n">seq_log_prob</span><span class="p">)</span>

    
    <span class="k">if</span> <span class="n">wandb_log_token_entropy</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_log_entropy</span><span class="p">:</span>
            <span class="n">ent_d</span> <span class="o">=</span> <span class="n">get_entropy_metrics</span><span class="p">(</span><span class="n">scores_stacked</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">ent_d</span><span class="p">[</span><span class="s1">&#39;time/log_entropy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time_log_entropy</span><span class="o">.</span><span class="n">t</span>
        <span class="k">if</span> <span class="n">log_times</span><span class="p">:</span>   <span class="c1"># need a better way to handle this. </span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ent_d</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    
    <span class="k">if</span> <span class="n">wandb_log_token_probabilities</span><span class="p">:</span> 
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_log_token_probabilities</span><span class="p">:</span>
            <span class="n">token_prob_d</span> <span class="o">=</span> <span class="n">get_token_probability_metrics</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">token_prob_d</span><span class="p">[</span><span class="s1">&#39;time/log_token_probabilities&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time_log_token_probabilities</span><span class="o">.</span><span class="n">t</span>
        <span class="k">if</span> <span class="n">log_times</span><span class="p">:</span> 
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">token_prob_d</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">seq_log_prob</span>

<span class="k">def</span> <span class="nf">reward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_times</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;orig_l, pp_l are lists of original and paraphrase respectively&quot;&quot;&quot;</span>
    <span class="c1"># Victim model probability differences between orig and pp</span>
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_vm_scores</span><span class="p">:</span>
        <span class="n">pp_probs</span> <span class="o">=</span> <span class="n">get_vm_probs</span><span class="p">(</span><span class="n">pp_l</span><span class="p">)</span> 
        <span class="n">pp_predclass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pp_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pp_truelabel_probs</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">pp_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:,</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">pp_predclass_probs</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">pp_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pp_predclass</span><span class="p">[</span> <span class="p">:,</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">label_flip</span> <span class="o">=</span> <span class="p">(</span><span class="n">pp_predclass</span> <span class="o">!=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="n">vm_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;orig_truelabel_probs&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pp_truelabel_probs</span><span class="p">)</span>
    
    
    <span class="c1"># STS scores</span>
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_sts_scores</span><span class="p">:</span>
        <span class="n">pp_embeddings</span>   <span class="o">=</span> <span class="n">sts_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">pp_l</span><span class="p">,</span>        <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">raw</span><span class="p">),</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># This returns a cosine similarity matrix, of which we just want the diagonal</span>
        <span class="n">sts_scores</span> <span class="o">=</span> <span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;orig_sts_embeddings&#39;</span><span class="p">],</span> <span class="n">pp_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span>  
    
    <span class="c1"># Reward calculation </span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">sts</span> <span class="o">&lt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mf">0.5</span><span class="o">+</span><span class="n">v</span><span class="o">*</span><span class="n">sts</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span><span class="n">sts</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vm_scores</span><span class="p">,</span> <span class="n">sts_scores</span><span class="p">)],</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">log_times</span><span class="p">:</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;global_step&#39;</span><span class="p">:</span> <span class="n">global_step</span><span class="p">,</span> 
                   <span class="s1">&#39;time/vm_scores&#39;</span><span class="p">:</span> <span class="n">time_vm_scores</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;time/sts_scores&#39;</span><span class="p">:</span> <span class="n">time_sts_scores</span><span class="o">.</span><span class="n">t</span> <span class="p">},</span> 
                   <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">return_components</span><span class="p">:</span> 
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;orig_l&quot;</span><span class="p">:</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span>
            <span class="s2">&quot;pp_l&quot;</span><span class="p">:</span> <span class="n">pp_l</span><span class="p">,</span>  
            <span class="s2">&quot;truelabel&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span>
            <span class="s2">&quot;orig_truelabel_probs&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;orig_truelabel_probs&#39;</span><span class="p">],</span>
            <span class="s2">&quot;pp_truelabel_probs&quot;</span><span class="p">:</span>  <span class="n">pp_truelabel_probs</span><span class="p">,</span>
            <span class="s2">&quot;pp_predclass&quot;</span><span class="p">:</span> <span class="n">pp_predclass</span><span class="p">,</span>
            <span class="s2">&quot;pp_predclass_probs&quot;</span><span class="p">:</span> <span class="n">pp_predclass_probs</span><span class="p">,</span>
            <span class="s2">&quot;vm_score&quot;</span><span class="p">:</span> <span class="n">vm_scores</span><span class="p">,</span> 
            <span class="s2">&quot;sts_score&quot;</span><span class="p">:</span> <span class="n">sts_scores</span><span class="p">,</span>
            <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="n">rewards</span><span class="p">,</span>
            <span class="s2">&quot;label_flip&quot;</span><span class="p">:</span> <span class="n">label_flip</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>  <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="n">rewards</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">pp_model_forward</span><span class="p">(</span><span class="n">data</span><span class="p">):</span> 
    <span class="k">global</span> <span class="n">orig_batch_size</span><span class="p">,</span><span class="n">orig_length</span><span class="p">,</span><span class="n">pp_batch_size</span><span class="p">,</span><span class="n">pp_length</span>
    <span class="n">orig_batch_size</span>     <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">orig_length</span>         <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span> <span class="o">=</span> <span class="n">get_paraphrases</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">])</span> 
    <span class="n">pp_batch_size</span> <span class="o">=</span> <span class="n">pp_output</span><span class="o">.</span><span class="n">sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># for greedy search pp_length is equal to orig_batch_size but this won&#39;t be for beam search</span>
    <span class="n">pp_length</span>     <span class="o">=</span> <span class="n">pp_output</span><span class="o">.</span><span class="n">sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  
    <span class="k">return</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_times</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> 
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_reward_fn</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">reward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="n">return_components</span><span class="p">,</span> <span class="n">log_times</span><span class="o">=</span><span class="n">log_times</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">normalise_rewards</span><span class="p">:</span> 
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;orig_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">])</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]))</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">])</span>
        
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_pp_logp</span><span class="p">:</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;pp_logp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_pp_logp</span><span class="p">(</span><span class="n">pp_output</span><span class="p">,</span><span class="n">log_times</span><span class="o">=</span><span class="n">log_times</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_loss_fn_loss_calc</span><span class="p">:</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;pp_logp&#39;</span><span class="p">]</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss_batch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">return_components</span> <span class="o">==</span>  <span class="kc">False</span><span class="p">:</span> <span class="k">return</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss_batch&#39;</span><span class="p">]</span> 
        
    <span class="c1"># remove some items from compgraph</span>
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_loss_fn_detach</span><span class="p">:</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;pp_logp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;pp_logp&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>    <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        
<span class="c1">#     # This was taking a lot of time so removed it. Add it in if needed. </span>
<span class="c1">#     #gc.collect() </span>
<span class="c1">#     print(&quot;\t### INSIDE loss_fn### &quot;)</span>
<span class="c1">#     print(&quot;\tglobal_step&quot;, global_step)</span>
<span class="c1">#     print(&quot;\treward_fn_time&quot;, time_reward_fn.t)</span>
<span class="c1">#     print(&quot;\t######### &quot;)</span>

    <span class="k">if</span> <span class="n">log_times</span><span class="p">:</span>   <span class="c1"># true for training, not eval. </span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;global_step&#39;</span><span class="p">:</span> <span class="n">global_step</span><span class="p">,</span> 
                   <span class="s1">&#39;time/reward_fn&#39;</span><span class="p">:</span> <span class="n">time_reward_fn</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;time/pp_logp&#39;</span><span class="p">:</span> <span class="n">time_pp_logp</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
                  <span class="s1">&#39;time/loss_fn_loss_calc&#39;</span><span class="p">:</span> <span class="n">time_loss_fn_loss_calc</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
                   <span class="s1">&#39;time/pp_logp_detach&#39;</span><span class="p">:</span> <span class="n">time_loss_fn_detach</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
                  <span class="p">},</span> 
                 <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">accelerator</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;With gradient accumulation&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_generate_pp</span><span class="p">:</span>
        <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span> <span class="o">=</span> <span class="n">pp_model_forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
    <span class="c1">#logger.info(show_gpu(f&#39;Batch {batch_num}, GPU memory usage after forward pass: &#39;))</span>
    
    <span class="k">with</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_loss_fn</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">wandb_log_training_step_table</span><span class="p">:</span> 
                <span class="n">results_d</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">loss_batch</span> <span class="o">=</span> <span class="n">results_d</span><span class="p">[</span><span class="s1">&#39;loss_batch&#39;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span> 
                <span class="n">loss_batch</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="n">loss_batch</span> <span class="o">=</span> <span class="n">loss_batch</span> <span class="o">/</span> <span class="n">accumulation_steps</span>  <span class="c1"># Normalize our loss for gradient accumulation</span>
        
    <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_backwards</span><span class="p">:</span>
        <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss_batch</span><span class="p">)</span> 
    
    <span class="c1">#logger.info(show_gpu(f&#39;Batch {batch_num}, GPU memory usage after backwards pass: &#39;))</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">accumulation_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_opt_step</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">pp_model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="n">zero_grad_with_none</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wandb_log_training_step_table</span><span class="p">:</span> 
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_add_to_training_step_table</span><span class="p">:</span>
            <span class="n">results_d</span> <span class="o">=</span> <span class="n">process_results_d1</span><span class="p">(</span><span class="n">results_d</span><span class="p">,</span> <span class="n">raw</span><span class="p">)</span>
            <span class="n">add_preds_to_data_d</span><span class="p">(</span><span class="n">results_d</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;training_step&#39;</span><span class="p">)</span> 
        
<span class="c1">#     print(&quot;### INSIDE training_step ####&quot;)    </span>
<span class="c1">#     print(&quot;epoch&quot;, epoch)</span>
<span class="c1">#     print(&quot;batch_num&quot;, batch_num)</span>
<span class="c1">#     print(&quot;global_step&quot;, global_step)</span>
<span class="c1">#     print(&quot;model in training mode&quot;, pp_model.training)</span>
<span class="c1">#     print(&quot;orig&quot;, raw[&#39;text&#39;])</span>
<span class="c1">#     print(&quot;pp_l&quot;, pp_l)</span>
<span class="c1">#     print(&quot;pp_seq&quot;, pp_output.sequences)</span>
<span class="c1">#     print(&quot;pp_length&quot;, pp_output.sequences.shape, pp_length)</span>
<span class="c1">#     print(&quot;loss_fn_time&quot;, time_loss_fn.t)</span>
<span class="c1">#     print(&quot;### INSIDE training_step ####&quot;)</span>
    
    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s1">&#39;time/generate_pp&#39;</span><span class="p">:</span> <span class="n">time_generate_pp</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;time/loss_fn&#39;</span><span class="p">:</span> <span class="n">time_loss_fn</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
               <span class="s1">&#39;time/backwards_pass&#39;</span><span class="p">:</span> <span class="n">time_backwards</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;time/optimizer_step&#39;</span><span class="p">:</span> <span class="n">time_opt_step</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
               <span class="s1">&#39;time/add_to_training_step_table&#39;</span><span class="p">:</span> <span class="n">time_add_to_training_step_table</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
               <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;global_step&#39;</span><span class="p">:</span> <span class="n">global_step</span><span class="p">,</span><span class="s1">&#39;batch_num&#39;</span><span class="p">:</span> <span class="n">batch_num</span><span class="p">,</span>
               <span class="s1">&#39;orig_length&#39;</span><span class="p">:</span> <span class="n">orig_length</span><span class="p">,</span><span class="s1">&#39;orig_batch_size&#39;</span><span class="p">:</span> <span class="n">orig_batch_size</span><span class="p">,</span>
              <span class="s1">&#39;pp_length&#39;</span><span class="p">:</span> <span class="n">pp_length</span><span class="p">,</span> <span class="s1">&#39;pp_batch_size&#39;</span><span class="p">:</span> <span class="n">pp_batch_size</span><span class="p">}</span>
              <span class="p">,</span><span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">process_results_d1</span><span class="p">(</span><span class="n">results_d</span><span class="p">,</span> <span class="n">raw</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;REFACTOR THIS LATER&quot;&quot;&quot;</span>
    <span class="c1"># wandb logging </span>
    <span class="n">results_d</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch</span>
    <span class="n">results_d</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">results_d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">):</span> 
            <span class="n">results_d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span><span class="p">:</span> 
            <span class="c1"># make into list repeated n times</span>
            <span class="n">results_d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size_train</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">results_d</span>

<span class="k">def</span> <span class="nf">save_model</span><span class="p">():</span> 
    <span class="n">path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path_run</span><span class="si">}</span><span class="s2">model_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
                <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s1">&#39;pp_model_state_dict&#39;</span><span class="p">:</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="p">},</span> <span class="n">path</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">resume_model</span><span class="p">(</span><span class="n">path</span><span class="p">):</span> 
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">pp_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;pp_model_state_dict&#39;</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">])</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Eval-and-wandb-functions">Eval and wandb functions<a class="anchor-link" href="#Eval-and-wandb-functions"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">table2df</span><span class="p">(</span><span class="n">table</span><span class="p">):</span>  <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">table</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">table</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>  <span class="c1"># wandb table to dataframe</span>

<span class="k">def</span> <span class="nf">process_results_d_for_wandb</span><span class="p">(</span><span class="n">results_d</span><span class="p">):</span> 
    <span class="c1"># Flatten batches for each key, depending on datatype (e.g. lists of lists )</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">results_d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
        <span class="c1"># v[0] is arbitrary - we are just checking the first item in the list to see the type</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span> 
            <span class="nb">next</span>
        <span class="k">elif</span>  <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span> 
            <span class="c1"># case where we have a list of scalars - the cat function doesn&#39;t work here </span>
            <span class="k">if</span>  <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]):</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>                              <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="n">results_d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># convert to list (squeeze is for single scalar list)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>  <span class="c1"># this is True for tensors also, so it has to go after the is_tensor check</span>
            <span class="n">results_d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">v</span><span class="p">))</span> 
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span> 
            <span class="nb">next</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;shouldn&#39;t get here&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results_d</span>

<span class="k">def</span> <span class="nf">eval_dl</span><span class="p">(</span><span class="n">dl_tkn</span><span class="p">,</span> <span class="n">dl_raw</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;Get evaluation metrics for a dataloader&quot;&quot;&quot;</span>
    <span class="c1"># Put models in eval mode and do the forward pass </span>
    <span class="c1"># Current logic: push all batches together into one big list.     </span>
    <span class="k">if</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span> <span class="n">vm_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">results_d</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="k">for</span> <span class="n">eval_batch_num</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dl_tkn</span><span class="p">,</span> <span class="n">dl_raw</span><span class="p">)):</span>
          <span class="c1">#  logger.info(show_gpu(f&#39;EVAL, batch {i}, GPU memory usage after loading data: &#39;))</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
                <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">device</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
           <span class="c1"># if data[&#39;input_ids&#39;].device != device: data[&#39;input_ids&#39;].to(device)</span>
                
            <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span> <span class="o">=</span> <span class="n">pp_model_forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">return_components</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">log_times</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1">#logger.info(show_gpu(f&#39;EVAL, batch {eval_batch_num}, GPU memory usage after loss_fn pass: &#39;))</span>
            <span class="n">d</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">]</span>
                      
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
                <span class="n">results_d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> 
    <span class="k">del</span> <span class="n">eval_batch_num</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">pp_output</span><span class="p">,</span> <span class="n">pp_l</span><span class="p">,</span> <span class="n">d</span>
    <span class="n">results_d</span> <span class="o">=</span> <span class="n">process_results_d_for_wandb</span><span class="p">(</span><span class="n">results_d</span><span class="p">)</span>
            
    <span class="c1"># Calculate additional metrics </span>
    <span class="n">results_d</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch</span>
    <span class="k">return</span> <span class="n">results_d</span>

<span class="k">def</span> <span class="nf">add_preds_to_data_d</span><span class="p">(</span><span class="n">results_d</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">split</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">data_d</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">or</span> <span class="n">split</span> <span class="o">==</span> <span class="s2">&quot;training_summary&quot;</span><span class="p">:</span> <span class="c1"># training summary table logic is elsewhere</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;split not in table keys or split == training_summary &quot;</span><span class="p">)</span> 
    <span class="c1">#table = table_d[split]</span>
        
    <span class="c1"># Need epoch to be repeated to the same length as the rest of the fields </span>
    <span class="c1"># (this isn&#39;t the batch size because we concat a bunch of stuff)</span>
    <span class="c1"># we don&#39;t want to change the `epoch` key because it screws up logging of the other metrics. </span>
    <span class="c1"># So we make a new dict.</span>
    <span class="c1"># d1 = copy.deepcopy(results_d)</span>
    <span class="n">d1</span> <span class="o">=</span> <span class="n">results_d</span>
    <span class="n">d1</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">epoch</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d1</span><span class="p">[</span><span class="s1">&#39;pp_l&#39;</span><span class="p">]))]</span>
    <span class="n">dcols</span> <span class="o">=</span> <span class="p">[</span><span class="n">d1</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">table_columns</span><span class="p">]</span>  <span class="c1"># filter out loss_batch</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">dcols</span><span class="p">]))</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># all lists should be of the same length </span>
    
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">dcols</span><span class="p">):</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">table_columns</span><span class="p">,</span><span class="n">row</span><span class="p">)}</span>
        <span class="n">data_d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update_training_summary_table</span><span class="p">(</span><span class="n">results_d</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="c1"># key names here have to match those in summary_table_columns</span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch</span>
    <span class="n">d</span><span class="p">[</span><span class="s1">&#39;split&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">split</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
        <span class="n">d</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s1">_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results_d</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>
    <span class="c1">#data_d[&#39;training_summary&#39;].append(*[d[c] for c in summary_table_columns])</span>
    <span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log_wandb_tables</span><span class="p">(</span><span class="n">run</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;Log wandb tables to the UI&quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">d</span><span class="p">[</span><span class="s2">&quot;eval/training_summary_table&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">table_d</span><span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">]</span>
  <span class="c1">#  print(len(d[&quot;eval/training_summary_table&quot;].data))</span>
    <span class="n">run</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_examples_chart</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">metric</span><span class="p">,</span> <span class="n">commit</span><span class="p">):</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="s2">&quot;uts_nlp/line_chart_v2&quot;</span>
    <span class="n">fields</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span><span class="s1">&#39;groupKeys&#39;</span><span class="p">:</span> <span class="s1">&#39;idx&#39;</span><span class="p">}</span>
    <span class="n">fields</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">string_fields</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">string_fields</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> vs epoch (examples)&quot;</span>
    <span class="n">chart</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">plot_table</span><span class="p">(</span><span class="n">vega_spec_name</span><span class="o">=</span><span class="n">spec</span><span class="p">,</span> <span class="n">data_table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> 
                            <span class="n">fields</span><span class="o">=</span><span class="n">fields</span><span class="p">,</span> <span class="n">string_fields</span><span class="o">=</span><span class="n">string_fields</span><span class="p">)</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;individual_examples/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_vs_epoch_examples&quot;</span><span class="p">:</span> <span class="n">chart</span><span class="p">},</span> <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_summary_charts</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">commit</span><span class="p">):</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="s2">&quot;uts_nlp/line_chart_v2&quot;</span>
    <span class="n">fields</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span><span class="s1">&#39;groupKeys&#39;</span><span class="p">:</span> <span class="s1">&#39;split&#39;</span><span class="p">}</span>
    <span class="n">fields</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>  <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg&quot;</span>
    <span class="n">chart</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">plot_table</span><span class="p">(</span><span class="n">vega_spec_name</span><span class="o">=</span><span class="n">spec</span><span class="p">,</span> <span class="n">data_table</span><span class="o">=</span><span class="n">table</span><span class="p">,</span> 
                                 <span class="n">fields</span><span class="o">=</span><span class="n">fields</span><span class="p">,</span> <span class="n">string_fields</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> vs epoch&quot;</span><span class="p">})</span>
    
    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;summary_charts/avg_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_vs_epoch&quot;</span><span class="p">:</span> <span class="n">chart</span><span class="p">},</span> <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_wandb_charts</span><span class="p">():</span> 
    <span class="k">if</span> <span class="n">wandb_plot_examples</span><span class="p">:</span> 
        <span class="c1"># Examples charts </span>
        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">]:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="n">split</span><span class="p">])</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="n">split</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">list</span> <span class="k">else</span> <span class="n">data_d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;idx in @plt_idx_d[@split]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s1">&#39;idx&#39;</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span> 
                <span class="n">plot_examples_chart</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">table</span><span class="o">=</span><span class="n">wandb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">dataframe</span><span class="o">=</span><span class="n">df</span><span class="p">),</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="c1">## Summary charts </span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span> 
        <span class="n">commit</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="n">metrics</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">])</span>
        <span class="n">plot_summary_charts</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">table</span><span class="o">=</span><span class="n">wandb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">dataframe</span><span class="o">=</span><span class="n">df</span><span class="p">),</span>
                            <span class="n">commit</span><span class="o">=</span><span class="n">commit</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_wandb_run_summary_statistics</span><span class="p">(</span><span class="n">run</span><span class="p">):</span>
    <span class="c1">## Training summary statistics </span>
    <span class="n">df_summary</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">])</span> 
    <span class="c1"># We calculate the best epoch according to the validation set</span>
    <span class="n">best_epoch_idx</span> <span class="o">=</span> <span class="n">df_summary</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;split==&#39;valid&#39;&quot;</span><span class="p">)[</span><span class="s1">&#39;loss_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> 
    <span class="n">valid_row</span> <span class="o">=</span> <span class="n">df_summary</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">best_epoch_idx</span><span class="p">]</span>
    <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">valid_row</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s1">&#39;best_epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_epoch</span>
    <span class="c1"># iloc transforms 1row df to series (so it is same as  valid_row)</span>
    <span class="n">train_row</span> <span class="o">=</span> <span class="n">df_summary</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;split==&#39;train&#39; &amp; epoch==@best_epoch&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span> 
        <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_row</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg_valid&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_row</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                                 
    <span class="c1">## Summary statistics of the test set </span>
    <span class="c1"># From the last epoch atm because we don&#39;t have early stopping </span>
    <span class="n">test_metrics</span> <span class="o">=</span> <span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">test_metrics</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">test_metrics</span><span class="p">):</span> 
        <span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">_avg_test&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Tests-and-asserts-for-get_pp_logp">Tests and asserts for <code>get_pp_logp</code><a class="anchor-link" href="#Tests-and-asserts-for-get_pp_logp"> </a></h5>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_start_end_special_token_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;The token id&#39;s that input/output sequences should start and end with&quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">name_or_path</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;eugenesiow/bart-paraphrase&#39;</span><span class="p">,</span> <span class="s1">&#39;tdopierre/ProtAugment-ParaphraseGenerator&#39;</span><span class="p">]:</span> 
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;input_start_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;input_end_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;output_start_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> 
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;output_end_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">==</span> <span class="s2">&quot;tuner007/pegasus_paraphrase&quot;</span><span class="p">:</span>
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;input_start_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="kc">None</span>
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;input_end_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> 
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;output_start_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">d</span><span class="p">[</span><span class="s2">&quot;output_end_id&quot;</span><span class="p">]</span> <span class="o">=</span>  <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;unrecognised tokenizer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">check_no_nans_or_infs</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isneginf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">isposinf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">assert_start_and_end_tokens_are_correct</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">orig_token_ids</span><span class="p">,</span> <span class="n">pp_token_ids</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make sure input sequences (orig) and output sequences (pp) start and end with the </span>
<span class="sd">    right special tokens (depends on tokenizer)&quot;&quot;&quot;</span>
    <span class="n">start_end_token_d</span> <span class="o">=</span> <span class="n">get_start_end_special_token_ids</span><span class="p">(</span><span class="n">pp_tokenizer</span><span class="p">)</span>
    
    <span class="c1"># Input</span>
    <span class="k">if</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;input_start_id&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">orig_token_ids</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;input_start_id&#39;</span><span class="p">])</span>
    <span class="c1"># can probs rewrite this to make it nicer but it&#39;s fine for now</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">orig_token_ids</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;input_end_id&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
                                      <span class="n">orig_token_ids</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;input_end_id&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># Output</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">pp_token_ids</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;output_start_id&#39;</span><span class="p">])</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">pp_token_ids</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;output_end_id&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> 
                                      <span class="n">pp_token_ids</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_end_token_d</span><span class="p">[</span><span class="s1">&#39;output_end_id&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
    
<span class="k">def</span> <span class="nf">check_scores_log_softmax_sums</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">):</span>
    <span class="n">sums</span> <span class="o">=</span> <span class="n">scores_log_softmax</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># check that the axes is right</span>
    <span class="c1"># we want to sum over token probabilities at each generation step, so we </span>
    <span class="c1"># should end up with a shape [orig_batch_size, pp_length]</span>
    <span class="k">assert</span> <span class="n">sums</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">orig_batch_size</span>  
    <span class="k">assert</span> <span class="n">sums</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="c1"># check that they sum to 1 along the pp_length axis</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sums</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sums</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">atol</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">check_seq_token_log_prob_values_are_correct</span><span class="p">(</span><span class="n">seq_without_first_tkn</span><span class="p">,</span> <span class="n">scores_log_softmax</span><span class="p">,</span> <span class="n">seq_token_log_probs</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;Just enumerates and checks values</span>
<span class="sd">    Quite slow for large batches so run as a test rather than an assert in every batch. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i_ex</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">orig_batch_size</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pp_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">i_tkn</span> <span class="o">=</span> <span class="n">seq_without_first_tkn</span><span class="p">[</span><span class="n">i_ex</span><span class="p">][</span><span class="n">i_step</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores_log_softmax</span><span class="p">[</span><span class="n">i_ex</span><span class="p">,</span><span class="n">i_step</span><span class="p">,</span> <span class="n">i_tkn</span><span class="p">]</span> <span class="o">==</span> <span class="n">seq_token_log_probs</span><span class="p">[</span><span class="n">i_ex</span><span class="p">,</span><span class="n">i_step</span><span class="p">])</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vm_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pp_model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1">## Layer freezing </span>
<span class="c1"># Unfreeze last 2 layers of the base model decoder</span>
<span class="c1"># Not sure if decoder layer norm should be unfrozen or not, but it appears after the</span>
<span class="c1">#   other parameters in the module ordering, so let&#39;s include it for now</span>
<span class="c1"># Also unfreeze the linear head.  This isn&#39;t stored in the base model but rather tacked on top</span>
<span class="c1">#   and will be fine-tuned for summarisation. </span>
<span class="k">if</span> <span class="n">pp_name</span> <span class="o">==</span> <span class="s2">&quot;tuner007/pegasus_paraphrase&quot;</span><span class="p">:</span>
    <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;decoder.layers.14&#39;</span><span class="p">,</span> <span class="s1">&#39;decoder.layers.15&#39;</span><span class="p">,</span> <span class="s1">&#39;decoder.layer_norm&#39;</span><span class="p">]</span> 
<span class="k">elif</span> <span class="n">pp_name</span> <span class="o">==</span> <span class="s2">&quot;tdopierre/ProtAugment-ParaphraseGenerator&quot;</span><span class="p">:</span>
    <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;decoder.layers.4&#39;</span><span class="p">,</span><span class="s1">&#39;decoder.layers.5&#39;</span><span class="p">,</span> <span class="s1">&#39;decoder.layernorm_embedding&#39;</span><span class="p">]</span>
<span class="k">elif</span> <span class="n">pp_name</span> <span class="o">==</span> <span class="s2">&quot;eugenesiow/bart-paraphrase&quot;</span><span class="p">:</span>
    <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;decoder.layers.10&#39;</span><span class="p">,</span><span class="s1">&#39;decoder.layers.11&#39;</span><span class="p">,</span> <span class="s1">&#39;decoder.layernorm_embedding&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pp_model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()):</span> 
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">([</span><span class="n">o</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">layer_list</span><span class="p">]):</span>   <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>                                         <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>       <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># Not sure if to include this or not. this seems to affect lm_head. i might just leave it as it was for now.</span>
<span class="c1"># this will freeze the embeddings/lm head. </span>
<span class="c1"># From here: https://github.com/huggingface/transformers/issues/10479#issuecomment-788964822</span>
<span class="c1"># self.lm_head is tied (the same parameter as) to self.encoder.embed_tokens and self.decoder.embed_tokens.</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span> 
<span class="c1">### For checking the grad status of the layers</span>
<span class="c1"># for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)</span>
<span class="c1"># for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    print(i, name, param.requires_grad)</span>

<span class="c1">#### Optimizer</span>
<span class="c1"># For now we just keep this simple</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">pp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1">#### Set up other miscellaneous things</span>
<span class="c1">#rouge_metric = load_metric(&quot;rouge&quot;, keep_in_memory=True)</span>
<span class="n">n_train_steps</span> <span class="o">=</span> <span class="n">n_train_epochs</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">training_function</span><span class="p">(</span><span class="n">pp_model</span><span class="p">,</span> <span class="n">vm_model</span><span class="p">,</span> <span class="n">dld_tkn</span><span class="p">,</span> <span class="n">dld_raw</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span> 
    <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">device</span>
    
    <span class="n">pp_model</span><span class="p">,</span><span class="n">vm_model</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
        <span class="n">pp_model</span><span class="p">,</span><span class="n">vm_model</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span>
    <span class="p">)</span>
    
    <span class="c1">#logger.info(show_gpu(f&#39; GPU memory usage after loading models:&#39;))</span>
    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_train_steps</span><span class="p">))</span>

    <span class="n">pp_model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="n">zero_grad_with_none</span><span class="p">)</span> 
    <span class="k">global</span> <span class="n">accumulation_num</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_num</span>
    <span class="n">accumulation_num</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train_epochs</span><span class="p">):</span> 
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Now on epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">n_train_epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span> <span class="n">pp_model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_train_one_epoch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">batch_num</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">dld_raw</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])):</span> 
                <span class="k">if</span> <span class="n">batch_num</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>   <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Now processing batch </span><span class="si">{</span><span class="n">batch_num</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">training_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="p">,</span> <span class="n">accelerator</span><span class="p">)</span> 
                <span class="n">accumulation_num</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> 
                <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span> 
        
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s1">&#39;time/train_one_epoch_time&#39;</span><span class="p">:</span> <span class="n">time_train_one_epoch</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                   <span class="s1">&#39;time/train_one_epoch_thoroughput&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">dsd</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="n">time_train_one_epoch</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                   <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">},</span> <span class="n">commit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">wandb_log_grads</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">wandb_log_grads_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
            <span class="n">plt</span> <span class="o">=</span> <span class="n">plot_grad_flow</span><span class="p">(</span><span class="n">pp_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">&quot;gradient flow&quot;</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">plt</span><span class="p">)})</span>  <span class="c1"># doesn&#39;t work as a non-image (i.e. plotly)</span>
            <span class="k">del</span> <span class="n">plt</span> 
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">save_model_while_training</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">save_model_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="n">save_model</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        
        <span class="c1"># Evaluation loop</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Now doing train eval&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_eval_train</span><span class="p">:</span>
                <span class="n">train_set_preds</span> <span class="o">=</span> <span class="n">eval_dl</span><span class="p">(</span><span class="n">dl_tkn</span> <span class="o">=</span> <span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;train_eval&#39;</span><span class="p">],</span> <span class="n">dl_raw</span><span class="o">=</span><span class="n">dld_raw</span><span class="p">[</span><span class="s1">&#39;train_eval&#39;</span><span class="p">],</span> 
                                          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Now doing valid eval&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_eval_valid</span><span class="p">:</span>
                <span class="n">valid_set_preds</span> <span class="o">=</span> <span class="n">eval_dl</span><span class="p">(</span><span class="n">dl_tkn</span> <span class="o">=</span> <span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">],</span>      <span class="n">dl_raw</span><span class="o">=</span><span class="n">dld_raw</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">],</span> 
                                        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1"># update the tables every epoch and log them</span>
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_update_training_summary_table</span><span class="p">:</span>
                <span class="n">update_training_summary_table</span><span class="p">(</span><span class="n">train_set_preds</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
                <span class="n">update_training_summary_table</span><span class="p">(</span><span class="n">valid_set_preds</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_add_eval_preds_to_data_d</span><span class="p">:</span>    
                <span class="n">add_preds_to_data_d</span><span class="p">(</span><span class="n">train_set_preds</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
                <span class="n">add_preds_to_data_d</span><span class="p">(</span><span class="n">valid_set_preds</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
            <span class="c1">#log_wandb_tables(run)</span>
            <span class="n">plot_wandb_charts</span><span class="p">()</span>
            <span class="k">del</span> <span class="n">train_set_preds</span>
            <span class="k">del</span> <span class="n">valid_set_preds</span>
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_eval_gc_collect</span><span class="p">:</span>
                <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> 
            <span class="k">with</span> <span class="n">timecode</span><span class="p">()</span> <span class="k">as</span> <span class="n">time_eval_empty_cache</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s1">&#39;time/eval_train_time&#39;</span><span class="p">:</span> <span class="n">time_eval_train</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;time/eval_valid_time&#39;</span><span class="p">:</span> <span class="n">time_eval_valid</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                       <span class="s1">&#39;time/eval_train_thoroughput&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">dsd</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="n">time_eval_train</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                       <span class="s1">&#39;time/eval_valid_thoroughput&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">dsd</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="n">time_eval_valid</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                       <span class="s1">&#39;time/eval_update_training_summary_table&#39;</span><span class="p">:</span> <span class="n">time_update_training_summary_table</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
                       <span class="s1">&#39;time/eval_add_preds_to_data_d&#39;</span><span class="p">:</span> <span class="n">time_add_eval_preds_to_data_d</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
                       <span class="s1">&#39;time/eval_gc_collect&#39;</span><span class="p">:</span> <span class="n">time_eval_gc_collect</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> 
                       <span class="s1">&#39;time/eval_empty_cache&#39;</span><span class="p">:</span> <span class="n">time_eval_empty_cache</span><span class="o">.</span><span class="n">t</span><span class="p">,</span>
               <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">},</span>
                      <span class="n">commit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Now doing test eval&quot;</span><span class="p">)</span>        
    <span class="c1"># Eval on test set </span>
    <span class="n">test_set_preds</span> <span class="o">=</span> <span class="n">eval_dl</span><span class="p">(</span><span class="n">dl_tkn</span> <span class="o">=</span> <span class="n">dld_tkn</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">dl_raw</span><span class="o">=</span><span class="n">dld_raw</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span>  <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">add_preds_to_data_d</span><span class="p">(</span><span class="n">test_set_preds</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
    <span class="c1"># Log, plot, and finish up</span>
    <span class="c1">#log_wandb_tables(run)</span>
    
    <span class="c1"># Data -&gt; df and save dfs to file </span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data_d</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>  <span class="c1"># splits and sometimes &#39;training_step&#39; too </span>
        <span class="n">data_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="c1"># dict of list of dict -&gt; dict of dataframe</span>
        <span class="n">data_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path_run</span><span class="si">}{</span><span class="n">key</span><span class="si">}</span><span class="s2">.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Save training_summary table to csv too </span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path_run</span><span class="si">}</span><span class="s2">training_summary.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="n">plot_wandb_charts</span><span class="p">()</span>
    <span class="n">add_wandb_run_summary_statistics</span><span class="p">(</span><span class="n">run</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Run">Run<a class="anchor-link" href="#Run"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Launch run and configure what it is tracking</span>
<span class="c1">#wandb_mode=&#39;disabled&#39;</span>
<span class="c1">#wandb_mode=&#39;online&#39;</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s2">&quot;travis_attack&quot;</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="s2">&quot;uts_nlp&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_d</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="n">wandb_mode</span><span class="p">,</span> <span class="n">notes</span><span class="o">=</span><span class="n">run_notes</span><span class="p">)</span>
<span class="k">if</span> <span class="n">wandb_log_grads</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">pp_model</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s1">&#39;gradients&#39;</span><span class="p">,</span> <span class="n">log_freq</span><span class="o">=</span><span class="n">wandb_log_grads_freq</span><span class="p">)</span>

<span class="n">path_run</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path_checkpoints</span><span class="si">}{</span><span class="n">run</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">/&quot;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_run</span><span class="p">):</span> <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path_run</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">### Set up tables </span>


<span class="c1">##### TO REFACTOR ######</span>
<span class="c1"># These have to be in the keys of the output from eval_dl</span>
<span class="c1"># table_columns = [&#39;idx&#39;, &#39;orig_l&#39;,  &#39;truelabel&#39;, &#39;orig_truelabel_probs&#39;, &#39;epoch&#39;, &#39;pp_l&#39;,</span>
<span class="c1">#              &#39;pp_truelabel_probs&#39;, &quot;pp_predclass&quot;, &quot;pp_predclass_probs&quot;] + metrics</span>
<span class="c1"># def make_table(cols): return wandb.Table(columns=cols)</span>
<span class="c1"># for key in splits:                table_d[key]             = make_table(table_columns) </span>
<span class="c1"># if wandb_log_training_step_table: table_d[&#39;training_step&#39;] = make_table(table_columns) </span>
<span class="c1"># summary_table_columns = [&#39;epoch&#39;,&#39;split&#39;] + [f&#39;{m}_avg&#39; for m in metrics]</span>
<span class="c1"># table_d[&#39;training_summary&#39;] = make_table(summary_table_columns)</span>
<span class="c1">########################</span>


<span class="c1">#### NEW ####</span>
<span class="c1">## Raw observation data (lists of dicts, later becomes pandas df)</span>
<span class="c1"># These have to be in the keys of the output from eval_dl</span>
<span class="n">table_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">,</span> <span class="s1">&#39;orig_l&#39;</span><span class="p">,</span>  <span class="s1">&#39;truelabel&#39;</span><span class="p">,</span> <span class="s1">&#39;orig_truelabel_probs&#39;</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="s1">&#39;pp_l&#39;</span><span class="p">,</span>
             <span class="s1">&#39;pp_truelabel_probs&#39;</span><span class="p">,</span> <span class="s2">&quot;pp_predclass&quot;</span><span class="p">,</span> <span class="s2">&quot;pp_predclass_probs&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">metrics</span>
<span class="n">summary_table_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span><span class="s1">&#39;split&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">_avg&#39;</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">]</span>

<span class="n">data_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">splits</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;training_summary&#39;</span><span class="p">]:</span>  <span class="n">data_d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>             <span class="o">=</span> <span class="p">[]</span> 
<span class="k">if</span> <span class="n">wandb_log_training_step_table</span><span class="p">:</span>          <span class="n">data_d</span><span class="p">[</span><span class="s1">&#39;training_step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">## Training summary table (W&amp;B table)</span>
<span class="c1"># Future W&amp;B tables here</span>
<span class="c1">#table_d = dict()</span>
<span class="c1">#def make_table(cols): return wandb.Table(columns=cols)</span>
<span class="c1">#table_d[&#39;training_summary&#39;] = make_table(summary_table_columns)</span>
<span class="c1">#############</span>

<span class="c1">## Get indices for the examples plots</span>
<span class="k">if</span> <span class="n">wandb_plot_examples</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">get_examples_plot_idxs</span><span class="p">(</span><span class="n">ds</span><span class="p">):</span> 
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">wandb_n_examples_plot</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">plt_idx_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span> <span class="n">plt_idx_d</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_examples_plot_idxs</span><span class="p">(</span><span class="n">dsd</span><span class="p">[</span><span class="n">split</span><span class="p">])</span>

<span class="c1">#%lprun -f training_function -f  get_pp_logp -f training_step -f  reward_fn -f  loss_fn -f eval_dl  notebook_launcher(training_function, args=(pp_model, vm_model, dld_tkn, dld_raw, optimizer), num_processes=1, use_fp16=use_fp16)</span>
<span class="n">notebook_launcher</span><span class="p">(</span><span class="n">training_function</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">pp_model</span><span class="p">,</span> <span class="n">vm_model</span><span class="p">,</span> <span class="n">dld_tkn</span><span class="p">,</span> <span class="n">dld_raw</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">),</span> <span class="n">num_processes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_fp16</span><span class="o">=</span><span class="n">use_fp16</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre><span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">uts_nlp</span> (use `wandb login --relogin` to force relogin)
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: wandb version 0.12.10 is available!  To upgrade, please run:
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>:  $ pip install wandb --upgrade
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
                    Syncing run <strong><a href="https://wandb.ai/uts_nlp/travis_attack/runs/2h1yz104" target="_blank">enchanting-date-244</a></strong> to <a href="https://wandb.ai/uts_nlp/travis_attack" target="_blank">Weights & Biases</a> (<a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">docs</a>).<br/>

                </div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Launching training on one GPU.
Now on epoch 0 of 250
Now on epoch 1 of 250
Now on epoch 2 of 250
Now on epoch 3 of 250
Now on epoch 4 of 250
Now on epoch 5 of 250
Now on epoch 6 of 250
Now on epoch 7 of 250
Now on epoch 8 of 250
Now on epoch 9 of 250
Now on epoch 10 of 250
Now on epoch 11 of 250
Now on epoch 12 of 250
Now on epoch 13 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 33 of 250
Now on epoch 34 of 250
Now on epoch 35 of 250
Now on epoch 36 of 250
Now on epoch 37 of 250
Now on epoch 38 of 250
Now on epoch 39 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 59 of 250
Now on epoch 60 of 250
Now on epoch 61 of 250
Now on epoch 62 of 250
Now on epoch 63 of 250
Now on epoch 64 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 85 of 250
Now on epoch 86 of 250
Now on epoch 87 of 250
Now on epoch 88 of 250
Now on epoch 89 of 250
Now on epoch 90 of 250
Now on epoch 91 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 111 of 250
Now on epoch 112 of 250
Now on epoch 113 of 250
Now on epoch 114 of 250
Now on epoch 115 of 250
Now on epoch 116 of 250
Now on epoch 117 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 138 of 250
Now on epoch 139 of 250
Now on epoch 140 of 250
Now on epoch 141 of 250
Now on epoch 142 of 250
Now on epoch 143 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 163 of 250
Now on epoch 164 of 250
Now on epoch 165 of 250
Now on epoch 166 of 250
Now on epoch 167 of 250
Now on epoch 168 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 188 of 250
Now on epoch 189 of 250
Now on epoch 190 of 250
Now on epoch 191 of 250
Now on epoch 192 of 250
Now on epoch 193 of 250
Now on epoch 194 of 250
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Now on epoch 198 of 250
Now on epoch 199 of 250
Now on epoch 200 of 250
Now on epoch 201 of 250
Now on epoch 202 of 250
Now on epoch 203 of 250
Now on epoch 204 of 250
Now on epoch 205 of 250
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span><span class="o">,</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_metric</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">IPython.core.debugger</span> <span class="kn">import</span> <span class="n">set_trace</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">from</span> <span class="nn">lexicalrichness</span> <span class="kn">import</span> <span class="n">LexicalRichness</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">psutil</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">spacy_wordnet.wordnet_annotator</span> <span class="kn">import</span> <span class="n">WordnetAnnotator</span> 
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">textstat</span>
<span class="kn">import</span> <span class="nn">difflib</span> <span class="k">as</span> <span class="nn">dl</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">groupby</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">analysis_functions</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s2">&quot;travis_attack&quot;</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="s2">&quot;uts_nlp&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_d</span><span class="p">,</span>
                 <span class="n">resume</span><span class="o">=</span><span class="s1">&#39;must&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;2h1yz104&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">wandb_mode</span><span class="p">,</span> <span class="n">notes</span><span class="o">=</span><span class="n">run_notes</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre><span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">uts_nlp</span> (use `wandb login --relogin` to force relogin)
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: wandb version 0.12.10 is available!  To upgrade, please run:
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>:  $ pip install wandb --upgrade
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
                    Resuming run <strong><a href="https://wandb.ai/uts_nlp/travis_attack/runs/2h1yz104" target="_blank">enchanting-date-244</a></strong> to <a href="https://wandb.ai/uts_nlp/travis_attack" target="_blank">Weights & Biases</a> (<a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">docs</a>).<br/>

                </div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># default=&#39;warn&#39;</span>
<span class="n">run_name</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">name</span>
<span class="n">run_id</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">id</span>
<span class="c1"># TODO: merge this with the other path </span>
<span class="n">path_run_results</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;../model_checkpoints/travis_attack/</span><span class="si">{</span><span class="n">run_name</span><span class="si">}</span><span class="s2">/&quot;</span>
<span class="n">train</span>            <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;train.csv&quot;</span><span class="p">)</span>
<span class="n">valid</span>            <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;valid.csv&quot;</span><span class="p">)</span>
<span class="c1">#test             = pd.read_csv(path_run_results + &quot;test.csv&quot;)</span>
<span class="n">training_step</span>    <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;training_step.csv&quot;</span><span class="p">)</span>
<span class="c1">#training_summary = pd.read_csv(path_run_results + &quot;training_summary.csv&quot;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_proc</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">df_training_step</span> <span class="o">=</span> <span class="n">postprocess_df</span><span class="p">(</span><span class="n">training_step</span><span class="p">,</span> <span class="n">filter_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">postprocess_df</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">filter_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">)</span>
<span class="n">df_valid</span> <span class="o">=</span> <span class="n">postprocess_df</span><span class="p">(</span><span class="n">valid</span><span class="p">,</span> <span class="n">filter_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
#### Calculating text statistics for the original examples. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text statistics for paraphrases. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text pair statistics ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text statistics for the original examples. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text statistics for paraphrases. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text pair statistics ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text statistics for the original examples. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text statistics for paraphrases. ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.

#### Calculating text pair statistics ####

Setting TOKENIZERS_PARALLELISM=false for forked processes.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_training_step</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;df_temp_training_step.pkl&quot;</span><span class="p">)</span>
<span class="n">df_train</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;df_temp_train.pkl&quot;</span><span class="p">)</span>
<span class="n">df_valid</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">path_run_results</span> <span class="o">+</span> <span class="s2">&quot;df_temp_valid.pkl&quot;</span><span class="p">)</span>
<span class="c1"># df_training_step = pd.read_pickle(path_run_results + &quot;df_temp_training_step.pkl&quot;)</span>
<span class="c1"># df_train = pd.read_pickle(path_run_results + &quot;df_temp_train.pkl&quot;)</span>
<span class="c1"># df_valid = pd.read_pickle(path_run_results + &quot;df_temp_valid.pkl&quot;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_training_step</span><span class="p">[</span><span class="s1">&#39;data_split&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;training_step&#39;</span>
<span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;data_split&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;eval_train&#39;</span>
<span class="n">df_valid</span><span class="p">[</span><span class="s1">&#39;data_split&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;eval_valid&#39;</span>
<span class="n">df_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_training_step</span><span class="p">,</span><span class="n">df_train</span><span class="p">,</span><span class="n">df_valid</span><span class="p">])</span>
<span class="n">df_concat</span> <span class="o">=</span> <span class="n">df_concat</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Otherwise we get a big spike at 0</span>
<span class="n">df_concat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_concat</span><span class="o">.</span><span class="n">epoch_of_first_label_flip</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;epoch_of_first_label_flip&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">fig_l</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">hist_config_dicts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;colname&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch_of_first_label_flip&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;xlabel&#39;</span><span class="p">:</span> <span class="s2">&quot;Epoch of first label flip&quot;</span><span class="p">,</span> 
        <span class="s1">&#39;desc&#39;</span><span class="p">:</span> <span class="s2">&quot;Cumulative prob Epoch of first label flip for each original example&quot;</span><span class="p">,</span>
        <span class="s1">&#39;cumulative&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;colname&#39;</span><span class="p">:</span> <span class="s1">&#39;idx_n_unique_pp&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;xlabel&#39;</span><span class="p">:</span> <span class="s2">&quot;Unique paraphrases per original example&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;desc&quot;</span><span class="p">:</span> <span class="s2">&quot;Number of generated unique paraphrases per original example during training&quot;</span><span class="p">,</span> 
        <span class="s1">&#39;cumulative&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;colname&#39;</span><span class="p">:</span> <span class="s1">&#39;idx_n_pp_changes&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;xlabel&#39;</span><span class="p">:</span> <span class="s2">&quot;Paraphrase changes per original example&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;desc&quot;</span><span class="p">:</span> <span class="s2">&quot;Number of paraphrase changes per original example during training&quot;</span><span class="p">,</span> 
        <span class="s1">&#39;cumulative&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>


<span class="n">fig_l</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">line_colnames</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">df_concat</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s2">&quot;_diff&quot;</span> <span class="ow">in</span> <span class="n">o</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
     <span class="s2">&quot;is_truncation&quot;</span><span class="p">,</span> <span class="s1">&#39;any_phrase_capitalised&#39;</span><span class="p">,</span> <span class="s1">&#39;any_phrase_decapitalised&#39;</span><span class="p">,</span> 
<span class="s1">&#39;n_segments_inserted&#39;</span><span class="p">,</span> <span class="s1">&#39;n_segments_removed&#39;</span><span class="p">,</span> <span class="s1">&#39;n_tokens_inserted&#39;</span><span class="p">,</span> <span class="s1">&#39;n_tokens_removed&#39;</span><span class="p">,</span><span class="s1">&#39;edit_distance_token_level&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">colname</span> <span class="ow">in</span> <span class="n">line_colnames</span><span class="p">:</span> 
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_epoch_line_charts</span><span class="p">(</span><span class="n">df_concat</span><span class="p">,</span> <span class="n">colname</span><span class="p">)</span>
    <span class="n">fig_l</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;pp_metrics/</span><span class="si">{</span><span class="n">colname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span><span class="n">fig</span> <span class="p">})</span>
    
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">hist_config_dicts</span><span class="p">:</span> 
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plot_idx_hist</span><span class="p">(</span><span class="n">df_concat</span><span class="p">,</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;colname&#39;</span><span class="p">],</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;xlabel&#39;</span><span class="p">],</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;cumulative&#39;</span><span class="p">])</span>
    <span class="n">fig_l</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;pp_metrics/</span><span class="si">{</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;colname&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span><span class="n">fig</span> <span class="p">})</span>

<span class="n">d1</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">fig_l</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">run</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">d1</span><span class="p">)</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea "><br/>Waiting for W&B process to finish, PID 22157... <strong style="color:green">(success).</strong></div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Testing-and-debugging">Testing and debugging<a class="anchor-link" href="#Testing-and-debugging"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Verifying-that-the-weights-update-each-training-step">Verifying that the weights update each training step<a class="anchor-link" href="#Verifying-that-the-weights-update-each-training-step"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Code-scraps">Code scraps<a class="anchor-link" href="#Code-scraps"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experiments-around-plotting-average-parameter-updates">Experiments around plotting average parameter updates<a class="anchor-link" href="#Experiments-around-plotting-average-parameter-updates"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     &quot;&quot;&quot;Function to create &quot;groups&quot; of parameters. This is useful to check how much a group of </span>
<span class="c1">#     parameters updates at an epoch. </span>
<span class="c1">#     Parameter groups are hardcoded into this code for now. </span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     # Identify which parameters should be grouped together</span>
<span class="c1">#     isolates = [&#39;model.shared.weight&#39;,&quot;model.encoder.embed_positions.weight&quot;, &quot;model.encoder.layer_norm&quot;,</span>
<span class="c1">#                 &quot;model.decoder.embed_positions.weight&quot;, &quot;model.decoder.layer_norm&quot;]</span>
<span class="c1">#     layers_base = [&quot;model.encoder.layers&quot;, &quot;model.decoder.layers&quot;]</span>
<span class="c1">#     def flatten_list(l): return list(np.concatenate(l).flat)</span>
<span class="c1">#     layers = flatten_list([[lyr + &quot;.&quot; + str(o) +&quot;.&quot; for o in list(range(16))] for lyr in layers_base])</span>
<span class="c1">#     parameter_groups = layers + isolates</span>
<span class="c1">#     # Sort the parameter groups by the order they appear in the model </span>
<span class="c1">#     all_params = [name for name,_ in pp_model.named_parameters()]</span>
<span class="c1">#     ordering = [np.min(np.where([pg in o for o in all_params])) for pg in parameter_groups]</span>
<span class="c1">#     parameter_groups = [o for _,o in sorted(zip(ordering, parameter_groups))]</span>
<span class="c1">#     # Assign each model parameter a parameter group </span>
<span class="c1">#     group_d = dict()</span>
<span class="c1">#     for pg in parameter_groups: </span>
<span class="c1">#         name = pg[:-1] if pg in layers else pg  # remove the &quot;.&quot; from the end of the name for the numeric layers</span>
<span class="c1">#         group_d[name] = [o for o in all_params if pg in o]</span>
<span class="c1">#     return group_d</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     group_d = get_parameter_group_dict()</span>
<span class="c1">#     params_all_initial_d = dict(params_all_initial)</span>
<span class="c1">#     params_all_d = dict(params_all)</span>
<span class="c1">#     group_d = get_parameter_group_dict()</span>
<span class="c1">#     df_d = dict()</span>
<span class="c1">#     for k,param_l in group_d.items(): </span>
<span class="c1">#         l = list()</span>
<span class="c1">#         for p in param_l: </span>
<span class="c1">#             l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())</span>
<span class="c1">#         l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy</span>
<span class="c1">#         df_d[k] = pd.DataFrame(l).describe().values.flatten()</span>
<span class="c1">#     df = pd.DataFrame(df_d)</span>
<span class="c1">#     df.index = pd.DataFrame([1,2,3]).describe().index</span>
<span class="c1">#     return df </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># initial_params = [(name, p.detach().clone()) for (name, p) in pp_model.named_parameters()]</span>
<span class="c1"># loss, reward, pp_logp = training_step(data) </span>
<span class="c1"># update_d =  dict()</span>
<span class="c1"># for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): </span>
<span class="c1">#     update_d[name] = torch.abs(old_p - new_p).detach().flatten()     </span>
    
<span class="c1">#             update_d =  dict()</span>
<span class="c1">#             for (_,old_p), (name, new_p) in zip(initial_params, pp_model.named_parameters()): </span>
<span class="c1">#                 update_d[name] = torch.abs(old_p - new_p).flatten() </span>
<span class="c1">#                 print (name, torch.norm(new_p - old_p).item())  </span>
            
<span class="c1">#             group_d = get_parameter_group_dict()</span>
<span class="c1">#             initial_params_d,current_params_d = dict(initial_params),dict()</span>
<span class="c1">#             params_all_d = dict(params_all)</span>
<span class="c1">#             group_d = get_parameter_group_dict()</span>
<span class="c1">#             df_d = dict()</span>
<span class="c1">#             for k,param_l in group_d.items(): </span>
<span class="c1">#                 l = list()</span>
<span class="c1">#                 for p in param_l: </span>
<span class="c1">#                     l.append((params_all_initial_d[p] - params_all_d[p]).abs().flatten())</span>
<span class="c1">#                 l = torch.cat(l).cpu().detach().numpy()  # list of 1-d tensors to tensor and then to numpy</span>
<span class="c1">#                 df_d[k] = pd.DataFrame(l).describe().values.flatten()</span>
<span class="c1">#             df = pd.DataFrame(df_d)</span>
<span class="c1">#             df.index = pd.DataFrame([1,2,3]).describe().index</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generating-a-paraphrase-dataset-and-getting-VM-predictions-for-it">Generating a paraphrase dataset and getting VM predictions for it<a class="anchor-link" href="#Generating-a-paraphrase-dataset-and-getting-VM-predictions-for-it"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#                               num_return_sequences=32): </span>
<span class="c1">#     &quot;&quot;&quot;Create paraphrases for each example in the batch. Then repeat the other fields </span>
<span class="c1">#         so that the resulting datase has the same length as the number of paraphrases. </span>
<span class="c1">#         Key assumption is </span>
<span class="c1">#         that the same number of paraphrases is created for each example.</span>
<span class="c1">#         batch: a dict of examples used by the `map` function from the dataset</span>
<span class="c1">#         cname_input: What column to create paraphrases of </span>
<span class="c1">#         cname_output: What to call the column of paraphrases</span>
<span class="c1">#         other parameters - passed to get_paraphrases. &quot;&quot;&quot;</span>
    
<span class="c1">#     # Generate paraphrases. </span>
<span class="c1">#     # This can be later extended to add diversity or so on. </span>
<span class="c1">#     #set_trace()</span>
<span class="c1">#     pp_l,probs = get_paraphrases(batch[cname_input], num_beams=num_beams,</span>
<span class="c1">#         num_return_sequences=num_return_sequences)</span>
    
<span class="c1">#     # To return paraphrases as a list of lists for batch input (not done here but might need later)</span>
<span class="c1">#     #     split_into_sublists = lambda l,n: [l[i:i + n] for i in range(0, len(l), n)]</span>
<span class="c1">#     #     pp_l = split_into_sublists(pp_l, n_seed_seqs)</span>
<span class="c1">#     batch[cname_output] = pp_l </span>
<span class="c1">#     batch[&quot;probs&quot;] = probs.to(&#39;cpu&#39;).numpy()</span>
    
<span class="c1">#     # Repeat each entry in all other columns `num_return_sequences` times so they are the same length</span>
<span class="c1">#     # as the paraphrase column</span>
<span class="c1">#     # Only works if the same number of paraphrases is generated for each phrase. </span>
<span class="c1">#     # Else try something like </span>
<span class="c1">#         # for o in zip(*batch.values()):</span>
<span class="c1">#         #     d = dict(zip(batch.keys(), o))</span>
<span class="c1">#         #     get_paraphrases(batch[cname_input],num_return_sequences=n_seed_seqs,num_beams=n_seed_seqs)</span>
<span class="c1">#         #     for k,v in d.items(): </span>
<span class="c1">#         #       return_d[k] += v if k == &#39;text&#39; else [v for o in range(n_paraphrases)]</span>
<span class="c1">#         # return return_d</span>
<span class="c1">#     return_d = defaultdict(list) </span>
<span class="c1">#     repeat_each_item_n_times = lambda l,n: [o for o in l for i in range(n)]</span>
<span class="c1">#     for k in batch.keys(): </span>
<span class="c1">#         if   k == cname_output: return_d[k] = batch[cname_output]</span>
<span class="c1">#         elif k == &quot;probs&quot;     : return_d[k] = batch[&quot;probs&quot;]</span>
<span class="c1">#         else:                   return_d[k] = repeat_each_item_n_times(batch[k], num_return_sequences)</span>
<span class="c1">#     return return_d </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#                   use_metric=False, monitor=False): </span>
<span class="c1">#     &quot;&quot;&quot;Get victim model preds+probs for the paraphrase dataset.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     assert vm_model.training == False  # checks that model is in eval mode </span>
<span class="c1">#     if use_metric: </span>
<span class="c1">#         metric_d = {}</span>
<span class="c1">#         metric_d[&#39;orig&#39;],metric_d[&#39;pp&#39;] = load_metric(&#39;accuracy&#39;),load_metric(&#39;accuracy&#39;)</span>
<span class="c1">#     orig_probs_l,pp_probs_l = [],[]</span>
<span class="c1">#     if monitor: monitor = Monitor(2)  # track GPU usage and memory</span>
    
<span class="c1">#     def get_vm_preds(x): </span>
<span class="c1">#         &quot;&quot;&quot;Get predictions for a vector x (here a vector of documents/text). </span>
<span class="c1">#         Works for a sentiment-analysis dataset (needs to be adjusted for NLI tasks)&quot;&quot;&quot;</span>
<span class="c1">#         inputs = vm_tokenizer(x, padding=True, truncation=True, return_tensors=&quot;pt&quot;)</span>
<span class="c1">#         inputs.to(device)</span>
<span class="c1">#         outputs = vm_model(**inputs, labels=labels)</span>
<span class="c1">#         probs = outputs.logits.softmax(1).cpu()</span>
<span class="c1">#         preds = probs.argmax(1)</span>
<span class="c1">#         return probs, preds</span>
       
<span class="c1">#     print(&quot;Getting victim model predictions for both original and paraphrased text.&quot;)</span>
<span class="c1">#     dl = DataLoader(ds_pp, batch_size=batch_size, shuffle=False, </span>
<span class="c1">#                     num_workers=n_wkrs, pin_memory=True)</span>
<span class="c1">#     with torch.no_grad():</span>
<span class="c1">#         for i, data in enumerate(dl): </span>
<span class="c1">#             if i % 50 == 0 : print(&quot;Now processing batch&quot;, i, &quot;out of&quot;, len(dl))</span>
<span class="c1">#             labels,orig,pp = data[&#39;label&#39;].to(device),data[cname_orig],data[cname_pp]</span>
<span class="c1">#             orig_probs, orig_preds = get_vm_preds(orig)            </span>
<span class="c1">#             pp_probs,   pp_preds   = get_vm_preds(pp)    </span>
<span class="c1">#             orig_probs_l.append(orig_probs); pp_probs_l.append(pp_probs)</span>
<span class="c1">#             if use_metric: </span>
<span class="c1">#                 metric_d[&#39;orig&#39;].add_batch(predictions=orig_preds, references=labels)</span>
<span class="c1">#                 metric_d[&#39;pp&#39;].add_batch(  predictions=pp_preds,   references=labels)</span>
<span class="c1">#     if monitor: monitor.stop()</span>
<span class="c1">#     def list2tensor(l): return torch.cat(l)</span>
<span class="c1">#     orig_probs_t,pp_probs_t = list2tensor(orig_probs_l),list2tensor(pp_probs_l)</span>
<span class="c1">#     if use_metric: return orig_probs_t, pp_probs_t, metric_d</span>
<span class="c1">#     else:          return orig_probs_t, pp_probs_t, None</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># num_beams = 10</span>
<span class="c1"># num_return_sequences = 3</span>
<span class="c1"># cname_input = &#39;text&#39; # which text column to paraphrase</span>
<span class="c1"># cname_output= cname_input + &#39;_pp&#39;</span>
<span class="c1"># date = &#39;20210825&#39;</span>
<span class="c1"># fname = path_cache + &#39;_rt_train&#39;+ date + &#39;_&#39; + str(num_return_sequences)</span>
<span class="c1"># if os.path.exists(fname):  </span>
<span class="c1">#     ds_pp = datasets.load_from_disk(fname)</span>
<span class="c1"># else:</span>
<span class="c1">#     ds_pp = train.shard(200, 0, contiguous=True)</span>
<span class="c1">#     # Have to call with batched=True</span>
<span class="c1">#     # Need to set a batch size otherwise will run out of memory on the GPU card. </span>
<span class="c1">#     # 64 seems to work well </span>
<span class="c1">#     ds_pp = ds_pp.map(</span>
<span class="c1">#         lambda x: create_paraphrase_dataset(x, </span>
<span class="c1">#             num_beams=num_beams, num_return_sequences=num_return_sequences,</span>
<span class="c1">#             cname_input=cname_input, cname_output=cname_output),</span>
<span class="c1">#         batched=True, batch_size=4) </span>
<span class="c1">#     ds_pp.save_to_disk(fname)</span>
<span class="c1">#     gc.collect(); torch.cuda.empty_cache() # free up most of the GPU memory</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># cname_orig = cname_input</span>
<span class="c1"># cname_pp = cname_output</span>
<span class="c1"># cname_label = &#39;label&#39;</span>
<span class="c1"># print_metric = True</span>
<span class="c1"># fname = path_cache + &#39;results_df_&#39;+ date + &quot;_&quot; + str(num_return_sequences) + &quot;.csv&quot;</span>
<span class="c1"># if os.path.exists(fname):    results_df = pd.read_csv(fname)</span>
<span class="c1"># else: </span>
<span class="c1">#     #sim_score_t = generate_sim_scores()</span>
<span class="c1">#     orig_probs_t,pp_probs_t,metric_d = get_vm_scores(ds_pp, cname_orig, </span>
<span class="c1">#                                                      cname_pp, cname_label,</span>
<span class="c1">#                                                      monitor=True, use_metric=print_metric)</span>
<span class="c1">#     if print_metric: </span>
<span class="c1">#         print(&quot;orig vm accuracy:&quot;,       metric_d[&#39;orig&#39;].compute())</span>
<span class="c1">#         print(&quot;paraphrase vm accuracy:&quot;, metric_d[&#39;pp&#39;].compute())</span>
<span class="c1">#     vm_orig_scores  = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], orig_probs_t)])</span>
<span class="c1">#     vm_pp_scores    = torch.tensor([r[idx] for idx,r in zip(ds_pp[cname_label], pp_probs_t)])</span>
<span class="c1">#     results_df = pd.DataFrame({</span>
<span class="c1">#                   cname_orig: ds_pp[cname_orig],</span>
<span class="c1">#                   cname_pp: ds_pp[cname_pp],</span>
<span class="c1">#    #               &#39;sim_score&#39;: sim_score_t,</span>
<span class="c1">#                   &#39;label_true&#39;: ds_pp[cname_label], </span>
<span class="c1">#                   &#39;label_vm_orig&#39;: orig_probs_t.argmax(1),</span>
<span class="c1">#                   &#39;label_vm_pp&#39;: pp_probs_t.argmax(1),</span>
<span class="c1">#                   &#39;vm_orig_truelabel&#39;: vm_orig_scores,             </span>
<span class="c1">#                   &#39;vm_pp_truelabel&#39;: vm_pp_scores,</span>
<span class="c1">#                   &#39;vm_truelabel_change&#39;: vm_orig_scores - vm_pp_scores,</span>
<span class="c1">#                   &#39;vm_orig_class0&#39;: orig_probs_t[:,0], </span>
<span class="c1">#                   &#39;vm_orig_class1&#39;: orig_probs_t[:,1], </span>
<span class="c1">#                   &#39;vm_pp_class0&#39;: pp_probs_t[:,0], </span>
<span class="c1">#                   &#39;vm_pp_class1&#39;: pp_probs_t[:,1], </span>
<span class="c1">#                   })</span>
<span class="c1"># #    results_df[&#39;vm_truelabel_change_X_sim_score&#39;] = results_df[&#39;vm_truelabel_change&#39;] * results_df[&#39;sim_score&#39;]</span>
<span class="c1">#     results_df.to_csv(fname, index_label = &#39;idx&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Testing-how-to-keep-gradients-with-generate-functions">Testing how to keep gradients with <code>generate</code> functions<a class="anchor-link" href="#Testing-how-to-keep-gradients-with-generate-functions"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># input_text=&quot;hello my name is Tom&quot;</span>
<span class="c1"># num_return_sequences=1</span>
<span class="c1"># num_beams=2</span>
<span class="c1"># return_probs=True</span>
<span class="c1"># batch = pp_tokenizer(input_text, truncation=True, padding=&#39;longest&#39;, return_tensors=&quot;pt&quot;).to(device)</span>
<span class="c1"># generated = pp_model.generate_with_grad(**batch, return_dict_in_generate=True, output_scores=True,</span>
<span class="c1">#                               num_return_sequences=num_return_sequences,</span>
<span class="c1">#                                 num_beams=num_beams,</span>
<span class="c1">#                                 num_beam_groups=1,</span>
<span class="c1">#                                 diversity_penalty=0,</span>
<span class="c1">#                                 temperature=1.5, </span>
<span class="c1">#                               length_penalty=1)</span>
<span class="c1"># print(generated)</span>

<span class="c1"># tgt_text = pp_tokenizer.batch_decode(generated.sequences, skip_special_tokens=True)</span>
<span class="c1"># print(pp_tokenizer.tokenize(tgt_text[0]))</span>
<span class="c1"># print(pp_tokenizer.encode(tgt_text[0]))</span>

<span class="c1"># # Score: score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)</span>
<span class="c1"># # gradient gets removed (i think) by the line </span>
<span class="c1"># # beam_hyp.add(</span>
<span class="c1"># #   input_ids[batch_beam_idx].clone(),</span>
<span class="c1"># #   next_score.item())</span>


<span class="c1"># x=generated[&#39;scores&#39;][5]</span>
<span class="c1"># print(x.max(1))</span>
<span class="c1"># x.max(1).values / (len(generated[&#39;scores&#39;]) ** 0.8)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from transformers import (</span>
<span class="c1"># AutoTokenizer,</span>
<span class="c1"># AutoModelForCausalLM,</span>
<span class="c1"># LogitsProcessorList,</span>
<span class="c1"># MinLengthLogitsProcessor,</span>
<span class="c1"># )</span>

<span class="c1"># tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="c1"># model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span>

<span class="c1"># # set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="c1"># model.config.pad_token_id = model.config.eos_token_id</span>

<span class="c1"># input_prompt = &quot;Today is a beautiful day, and&quot;</span>
<span class="c1"># input_ids = tokenizer(input_prompt, return_tensors=&quot;pt&quot;).input_ids</span>

<span class="c1"># # instantiate logits processors</span>
<span class="c1"># logits_processor = LogitsProcessorList([</span>
<span class="c1">#     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),</span>
<span class="c1"># ])</span>

<span class="c1"># outputs = model.greedy_search(input_ids, logits_processor=logits_processor)</span>

<span class="c1"># print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tensorboard-setup">Tensorboard setup<a class="anchor-link" href="#Tensorboard-setup"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from torch.utils.tensorboard import SummaryWriter</span>
<span class="c1"># import datetime </span>
<span class="c1"># # Create writer and track to run directory </span>
<span class="c1"># path_runs = &#39;./runs/&#39;</span>
<span class="c1"># log_dir = path_runs + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) + &quot;/&quot;</span>
<span class="c1"># writer = SummaryWriter(log_dir = log_dir)</span>
<span class="c1"># # stuff here logging to tensorboard</span>
<span class="c1"># #writer.close() # important otherwise Tensorboard eventually shuts down</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="WandB-artifact-tables">WandB artifact tables<a class="anchor-link" href="#WandB-artifact-tables"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#  train_table_artifact = wandb.Artifact(&quot;train_samples_&quot; + str(wandb.run.id), type=&quot;predictions&quot;)</span>
<span class="c1">#  valid_table_artifact = wandb.Artifact(&quot;test_samples_&quot;   + str(wandb.run.id), type=&quot;predictions&quot;)</span>
<span class="c1">#train_table_artifact.add(train_table, &quot;predictions&quot;)</span>
<span class="c1">#valid_table_artifact.add(valid_table, &quot;predictions&quot;)</span>
<span class="c1">#wandb.run.log_artifact(train_table_artifact) </span>
<span class="c1">#wandb.run.log_artifact(valid_table_artifact)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-no_grad-version-of-model.generate()-adapted-from-transformers-v4.5.0">A no_grad version of <code>model.generate()</code> adapted from transformers v4.5.0<a class="anchor-link" href="#A-no_grad-version-of-model.generate()-adapted-from-transformers-v4.5.0"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from transformers.generation_beam_search import BeamScorer, BeamSearchScorer</span>
<span class="c1"># import torch</span>


<span class="c1"># def generate_with_grad(</span>
<span class="c1">#     self,</span>
<span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
<span class="c1">#     max_length: Optional[int] = None,</span>
<span class="c1">#     min_length: Optional[int] = None,</span>
<span class="c1">#     do_sample: Optional[bool] = None,</span>
<span class="c1">#     early_stopping: Optional[bool] = None,</span>
<span class="c1">#     num_beams: Optional[int] = None,</span>
<span class="c1">#     temperature: Optional[float] = None,</span>
<span class="c1">#     top_k: Optional[int] = None,</span>
<span class="c1">#     top_p: Optional[float] = None,</span>
<span class="c1">#     repetition_penalty: Optional[float] = None,</span>
<span class="c1">#     bad_words_ids: Optional[Iterable[int]] = None,</span>
<span class="c1">#     bos_token_id: Optional[int] = None,</span>
<span class="c1">#     pad_token_id: Optional[int] = None,</span>
<span class="c1">#     eos_token_id: Optional[int] = None,</span>
<span class="c1">#     length_penalty: Optional[float] = None,</span>
<span class="c1">#     no_repeat_ngram_size: Optional[int] = None,</span>
<span class="c1">#     encoder_no_repeat_ngram_size: Optional[int] = None,</span>
<span class="c1">#     num_return_sequences: Optional[int] = None,</span>
<span class="c1">#     max_time: Optional[float] = None,</span>
<span class="c1">#     decoder_start_token_id: Optional[int] = None,</span>
<span class="c1">#     use_cache: Optional[bool] = None,</span>
<span class="c1">#     num_beam_groups: Optional[int] = None,</span>
<span class="c1">#     diversity_penalty: Optional[float] = None,</span>
<span class="c1">#     prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,</span>
<span class="c1">#     output_attentions: Optional[bool] = None,</span>
<span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
<span class="c1">#     output_scores: Optional[bool] = None,</span>
<span class="c1">#     return_dict_in_generate: Optional[bool] = None,</span>
<span class="c1">#     forced_bos_token_id: Optional[int] = None,</span>
<span class="c1">#     forced_eos_token_id: Optional[int] = None,</span>
<span class="c1">#     remove_invalid_values: Optional[bool] = None,</span>
<span class="c1">#     **model_kwargs):</span>
<span class="c1">#     # set init values</span>
<span class="c1">#     num_beams = num_beams if num_beams is not None else self.config.num_beams</span>
<span class="c1">#     num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups</span>
<span class="c1">#     max_length = max_length if max_length is not None else self.config.max_length</span>
<span class="c1">#     do_sample = do_sample if do_sample is not None else self.config.do_sample</span>
<span class="c1">#     num_return_sequences = (</span>
<span class="c1">#         num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences</span>
<span class="c1">#     )</span>

<span class="c1">#     pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id</span>
<span class="c1">#     bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id</span>
<span class="c1">#     eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id</span>

<span class="c1">#     output_scores = output_scores if output_scores is not None else self.config.output_scores</span>
<span class="c1">#     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions</span>
<span class="c1">#     output_hidden_states = (</span>
<span class="c1">#         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states</span>
<span class="c1">#     )</span>
<span class="c1">#     return_dict_in_generate = (</span>
<span class="c1">#         return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate</span>
<span class="c1">#     )</span>

<span class="c1">#     model_kwargs[&quot;output_attentions&quot;] = output_attentions</span>
<span class="c1">#     model_kwargs[&quot;output_hidden_states&quot;] = output_hidden_states</span>

<span class="c1">#     if input_ids is None:</span>
<span class="c1">#         # init `input_ids` with bos_token_id</span>
<span class="c1">#         input_ids = self._prepare_input_ids_for_generation(bos_token_id, model_kwargs.get(&quot;encoder_outputs&quot;))</span>

<span class="c1">#     if model_kwargs.get(&quot;attention_mask&quot;, None) is None:</span>
<span class="c1">#         # init `attention_mask` depending on `pad_token_id`</span>
<span class="c1">#         model_kwargs[&quot;attention_mask&quot;] = self._prepare_attention_mask_for_generation(</span>
<span class="c1">#             input_ids, pad_token_id, eos_token_id</span>
<span class="c1">#         )</span>

<span class="c1">#     # special case if pad_token_id is not defined</span>
<span class="c1">#     if pad_token_id is None and eos_token_id is not None:</span>
<span class="c1">#         logger.warning(f&quot;Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.&quot;)</span>
<span class="c1">#         pad_token_id = eos_token_id</span>

<span class="c1">#     # Storing encoder_input_ids for logits_processor that could use them</span>
<span class="c1">#     encoder_input_ids = input_ids if self.config.is_encoder_decoder else None</span>

<span class="c1">#     if self.config.is_encoder_decoder:</span>
<span class="c1">#         # add encoder_outputs to model_kwargs</span>
<span class="c1">#         model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)</span>

<span class="c1">#         # set input_ids as decoder_input_ids</span>
<span class="c1">#         if &quot;decoder_input_ids&quot; in model_kwargs:</span>
<span class="c1">#             input_ids = model_kwargs.pop(&quot;decoder_input_ids&quot;)</span>
<span class="c1">#         else:</span>
<span class="c1">#             input_ids = self._prepare_decoder_input_ids_for_generation(</span>
<span class="c1">#                 input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id</span>
<span class="c1">#             )</span>

<span class="c1"># #         if &quot;encoder_outputs&quot; not in model_kwargs or not isinstance(model_kwargs[&quot;encoder_outputs&quot;], ModelOutput):</span>
<span class="c1"># #             raise ValueError(&quot;Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.&quot;)</span>
<span class="c1">#     if input_ids.shape[-1] &gt;= max_length:</span>
<span class="c1">#         input_ids_string = &quot;decoder_input_ids&quot; if self.config.is_encoder_decoder else &quot;input_ids&quot;</span>
<span class="c1">#         logger.warning(</span>
<span class="c1">#             f&quot;Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.&quot;</span>
<span class="c1">#             &quot;This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.&quot;</span>
<span class="c1">#         )</span>

<span class="c1">#     # determine generation mode</span>
<span class="c1">#     is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False</span>
<span class="c1">#     is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True</span>
<span class="c1">#     is_beam_gen_mode = (num_beams &gt; 1) and (num_beam_groups == 1) and do_sample is False</span>
<span class="c1">#     is_beam_sample_gen_mode = (num_beams &gt; 1) and (num_beam_groups == 1) and do_sample is True</span>
<span class="c1">#     is_group_beam_gen_mode = (num_beams &gt; 1) and (num_beam_groups &gt; 1)</span>
<span class="c1">#     if num_beam_groups &gt; num_beams:</span>
<span class="c1">#         raise ValueError(&quot;`num_beam_groups` has to be smaller or equal to `num_beams`&quot;)</span>
<span class="c1">#     if is_group_beam_gen_mode and do_sample is True:</span>
<span class="c1">#         raise ValueError(</span>
<span class="c1">#             &quot;Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.&quot;</span>
<span class="c1">#         )</span>

<span class="c1">#     # set model_kwargs</span>
<span class="c1">#     model_kwargs[&quot;use_cache&quot;] = use_cache</span>

<span class="c1">#     # get distribution pre_processing samplers</span>
<span class="c1">#     logits_processor = self._get_logits_processor(</span>
<span class="c1">#         repetition_penalty=repetition_penalty,</span>
<span class="c1">#         no_repeat_ngram_size=no_repeat_ngram_size,</span>
<span class="c1">#         encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,</span>
<span class="c1">#         encoder_input_ids=encoder_input_ids,</span>
<span class="c1">#         bad_words_ids=bad_words_ids,</span>
<span class="c1">#         min_length=min_length,</span>
<span class="c1">#         max_length=max_length,</span>
<span class="c1">#         eos_token_id=eos_token_id,</span>
<span class="c1">#         forced_bos_token_id=forced_bos_token_id,</span>
<span class="c1">#         forced_eos_token_id=forced_eos_token_id,</span>
<span class="c1">#         prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,</span>
<span class="c1">#         num_beams=num_beams,</span>
<span class="c1">#         num_beam_groups=num_beam_groups,</span>
<span class="c1">#         diversity_penalty=diversity_penalty,</span>
<span class="c1">#         remove_invalid_values=remove_invalid_values,</span>
<span class="c1">#     )</span>

<span class="c1">#     stopping_criteria = self._get_stopping_criteria(</span>
<span class="c1">#         max_length=max_length,</span>
<span class="c1">#         max_time=max_time,</span>
<span class="c1">#     )</span>

<span class="c1">#     if is_greedy_gen_mode:</span>
<span class="c1">#         if num_return_sequences &gt; 1:</span>
<span class="c1">#             raise ValueError(</span>
<span class="c1">#                 f&quot;num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.&quot;</span>
<span class="c1">#             )</span>

<span class="c1">#         # greedy search</span>
<span class="c1">#         return self.greedy_search(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             logits_processor=logits_processor,</span>
<span class="c1">#             stopping_criteria=stopping_criteria,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             pad_token_id=pad_token_id,</span>
<span class="c1">#             eos_token_id=eos_token_id,</span>
<span class="c1">#             output_scores=output_scores,</span>
<span class="c1">#             return_dict_in_generate=return_dict_in_generate,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#     elif is_sample_gen_mode:</span>
<span class="c1">#         # get probability distribution warper</span>
<span class="c1">#         logits_warper = self._get_logits_warper(</span>
<span class="c1">#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams</span>
<span class="c1">#         )</span>

<span class="c1">#         # expand input_ids with `num_return_sequences` additional sequences per batch</span>
<span class="c1">#         input_ids, model_kwargs = self._expand_inputs_for_generation(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             expand_size=num_return_sequences,</span>
<span class="c1">#             is_encoder_decoder=self.config.is_encoder_decoder,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#         # sample</span>
<span class="c1">#         return self.sample(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             logits_processor=logits_processor,</span>
<span class="c1">#             logits_warper=logits_warper,</span>
<span class="c1">#             stopping_criteria=stopping_criteria,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             pad_token_id=pad_token_id,</span>
<span class="c1">#             eos_token_id=eos_token_id,</span>
<span class="c1">#             output_scores=output_scores,</span>
<span class="c1">#             return_dict_in_generate=return_dict_in_generate,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#     elif is_beam_gen_mode:</span>
<span class="c1">#         batch_size = input_ids.shape[0]</span>

<span class="c1">#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty</span>
<span class="c1">#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping</span>

<span class="c1">#         if num_return_sequences &gt; num_beams:</span>
<span class="c1">#             raise ValueError(&quot;`num_return_sequences` has to be smaller or equal to `num_beams`.&quot;)</span>

<span class="c1">#         beam_scorer = BeamSearchScorer(</span>
<span class="c1">#             batch_size=batch_size,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             num_beams=num_beams,</span>
<span class="c1">#             device=self.device,</span>
<span class="c1">#             length_penalty=length_penalty,</span>
<span class="c1">#             do_early_stopping=early_stopping,</span>
<span class="c1">#             num_beam_hyps_to_keep=num_return_sequences,</span>
<span class="c1">#         )</span>
<span class="c1">#         # interleave with `num_beams`</span>
<span class="c1">#         input_ids, model_kwargs = self._expand_inputs_for_generation(</span>
<span class="c1">#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs</span>
<span class="c1">#         )</span>
<span class="c1">#         #with torchsnooper.snoop(depth=4, max_variable_length=200, normalize=True):</span>
<span class="c1">#         return self.beam_search(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             beam_scorer,</span>
<span class="c1">#             logits_processor=logits_processor,</span>
<span class="c1">#             stopping_criteria=stopping_criteria,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             pad_token_id=pad_token_id,</span>
<span class="c1">#             eos_token_id=eos_token_id,</span>
<span class="c1">#             output_scores=output_scores,</span>
<span class="c1">#             return_dict_in_generate=return_dict_in_generate,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#     elif is_beam_sample_gen_mode:</span>
<span class="c1">#         logits_warper = self._get_logits_warper(</span>
<span class="c1">#             top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams</span>
<span class="c1">#         )</span>

<span class="c1">#         batch_size = input_ids.shape[0] * num_return_sequences</span>

<span class="c1">#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty</span>
<span class="c1">#         beam_scorer = BeamSearchScorer(</span>
<span class="c1">#             batch_size=batch_size,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             num_beams=num_beams,</span>
<span class="c1">#             device=self.device,</span>
<span class="c1">#             length_penalty=length_penalty,</span>
<span class="c1">#             do_early_stopping=early_stopping,</span>
<span class="c1">#         )</span>

<span class="c1">#         # interleave with `num_beams * num_return_sequences`</span>
<span class="c1">#         input_ids, model_kwargs = self._expand_inputs_for_generation(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             expand_size=num_beams * num_return_sequences,</span>
<span class="c1">#             is_encoder_decoder=self.config.is_encoder_decoder,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#         return self.beam_sample(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             beam_scorer,</span>
<span class="c1">#             logits_processor=logits_processor,</span>
<span class="c1">#             logits_warper=logits_warper,</span>
<span class="c1">#             stopping_criteria=stopping_criteria,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             pad_token_id=pad_token_id,</span>
<span class="c1">#             eos_token_id=eos_token_id,</span>
<span class="c1">#             output_scores=output_scores,</span>
<span class="c1">#             return_dict_in_generate=return_dict_in_generate,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>

<span class="c1">#     elif is_group_beam_gen_mode:</span>
<span class="c1">#         batch_size = input_ids.shape[0]</span>

<span class="c1">#         length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty</span>
<span class="c1">#         early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping</span>

<span class="c1">#         if num_return_sequences &gt; num_beams:</span>
<span class="c1">#             raise ValueError(&quot;`num_return_sequences` has to be smaller or equal to `num_beams`.&quot;)</span>

<span class="c1">#         if num_beams % num_beam_groups != 0:</span>
<span class="c1">#             raise ValueError(&quot;`num_beams` should be divisible by `num_beam_groups` for group beam search.&quot;)</span>

<span class="c1">#         diverse_beam_scorer = BeamSearchScorer(</span>
<span class="c1">#             batch_size=batch_size,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             num_beams=num_beams,</span>
<span class="c1">#             device=self.device,</span>
<span class="c1">#             length_penalty=length_penalty,</span>
<span class="c1">#             do_early_stopping=early_stopping,</span>
<span class="c1">#             num_beam_hyps_to_keep=num_return_sequences,</span>
<span class="c1">#             num_beam_groups=num_beam_groups,</span>
<span class="c1">#         )</span>
<span class="c1">#         # interleave with `num_beams`</span>
<span class="c1">#         input_ids, model_kwargs = self._expand_inputs_for_generation(</span>
<span class="c1">#             input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs</span>
<span class="c1">#         )</span>
<span class="c1">#         return self.group_beam_search(</span>
<span class="c1">#             input_ids,</span>
<span class="c1">#             diverse_beam_scorer,</span>
<span class="c1">#             logits_processor=logits_processor,</span>
<span class="c1">#             stopping_criteria=stopping_criteria,</span>
<span class="c1">#             max_length=max_length,</span>
<span class="c1">#             pad_token_id=pad_token_id,</span>
<span class="c1">#             eos_token_id=eos_token_id,</span>
<span class="c1">#             output_scores=output_scores,</span>
<span class="c1">#             return_dict_in_generate=return_dict_in_generate,</span>
<span class="c1">#             **model_kwargs,</span>
<span class="c1">#         )</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Old-eval/wandb-functions">Old eval/wandb functions<a class="anchor-link" href="#Old-eval/wandb-functions"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     ### Might be obselete ###</span>
<span class="c1">#     print(f&quot;{split} paraphrases:&quot;, preds[&#39;pp_l&#39;])</span>
<span class="c1">#     print(f&quot;{split} VM scores:&quot;,    np.round(preds[&#39;vm_score&#39;],3))</span>
<span class="c1">#     print(f&quot;{split} ROUGE scores:&quot;, np.round(preds[&#39;rouge_score&#39;],3))</span>
<span class="c1">#     if normalise_rewards: print(f&quot;{split} unnormalised rewards:&quot;, preds[&#39;orig_reward&#39;])</span>
<span class="c1">#     print(f&quot;{split} rewards:&quot;, round_t(preds[&#39;reward&#39;], 3))</span>
<span class="c1">#     print(f&quot;{split} avg reward:&quot;, torch.mean(preds[&#39;reward&#39;]).item())</span>
<span class="c1">#     print(f&quot;{split} logp:&quot;, round_t(preds[&#39;pp_logp&#39;], 3))</span>
<span class="c1">#     print(f&quot;{split} avg logp:&quot;, torch.mean(preds[&#39;pp_logp&#39;]).item())</span>
<span class="c1">#     print(f&quot;{split} loss:&quot;, train_set_preds[&#39;loss&#39;].item())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     &quot;&quot;&quot;flattens lists of metrics to wandb acceptable form. obselete now but might be useful later&quot;&quot;&quot;</span>
<span class="c1">#     #### MIGHT BE OBSELETE #####</span>
<span class="c1">#     # Log numeric data </span>
<span class="c1">#     # Convert all lists of values to wandb format. Scalars are unchanged</span>
<span class="c1">#     d = dict()</span>
<span class="c1">#     orig_keys = results_d.keys()</span>
<span class="c1">#     for k,v in results_d.items(): </span>
<span class="c1">#         if type(v) is list: </span>
<span class="c1">#             if type(v[0]) == int or type(v[0]) == float:  # we handle strings differently</span>
<span class="c1">#                 d1 = {f&quot;eval/{split}/examples/{k}/{i}&quot;: o for i,o in enumerate(v)}  # list -&gt; dict of len(v) scalars</span>
<span class="c1">#                 d = {**d, **d1}  # merge dicts</span>
<span class="c1">#         else: </span>
<span class="c1">#             d[f&quot;{split}/{k}&quot;] = v</span>
<span class="c1">#     d[&#39;epoch&#39;] = epoch</span>
<span class="c1">#     wandb.log(d) </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#</span>
<span class="c1">#     def rename_wandb_column(table, old, new): </span>
<span class="c1">#         if old not in table.columns: </span>
<span class="c1">#             warnings.warn(f&quot;{old} not in columns of table. Skipping. Columns of table are {table.columns}&quot;)</span>
<span class="c1">#         else: </span>
<span class="c1">#             idx = [i for i,o in enumerate(table.columns) if o == old][0]</span>
<span class="c1">#             table.columns[idx] = new</span>
        
    <span class="c1"># Can&#39;t just originally name these &#39;orig&#39; and &#39;pp&#39; because they don&#39;t match key names used in `eval_dl`</span>
    <span class="c1"># NOTE: this seems to break the internals of wandb. might have to just live with the names. </span>
<span class="c1">#     rename_wandb_column(train_table, old=&#39;orig_l&#39;, new=&#39;orig&#39;)</span>
<span class="c1">#     rename_wandb_column(train_table, old=&#39;pp_l&#39;,   new=&#39;pp&#39;)</span>
<span class="c1">#     rename_wandb_column(valid_table,  old=&#39;orig_l&#39;, new=&#39;orig&#39;)</span>
<span class="c1">#     rename_wandb_column(valid_table,  old=&#39;pp_l&#39;,   new=&#39;pp&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

