{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE sets: model will be trained on eval set, so you shouldn't also test on the eval set. The problem is that the labels are withheld for the test set. \n",
    "Start with SNLI. MultiNLI is a later option too. As is rotten_tomatoes. \n",
    "* Victim model performance on dataset train, valid, test set. (done, written code to measure it)\n",
    "* Create new paraphrased valid + test datasets (done a preliminary version on the valid set) \n",
    "* Measure victim model performance on paraphrased datasets (done. on vanilla valid set is about 87% accuracy. generating 16 paraphrases (i.e. not many) and evaluating performance on all of them, we get ~75% accuracy)\n",
    "* Get document embeddings of original and paraphrased and compare (done)\n",
    "  * https://github.com/UKPLab/sentence-transformers\n",
    "* Write a simple way to measure paraphrase quality (done) \n",
    "* Construct reward function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets, transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pprint import pprint\n",
    "import numpy as np, pandas as pd\n",
    "from utils import *   # local script \n",
    "import pyarrow\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import seaborn as sns\n",
    "\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "\n",
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size = 64\n",
    "pd.set_option(\"display.max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase model (para)\n",
    "para_name = \"tuner007/pegasus_paraphrase\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-cased-snli\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity model \n",
    "embedding_model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/data/tproth/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf1efbe71974b58a7a780b00a4b16bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb575b92c8d242ddac865e6661de6dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf1ebde46d64884b143d84103261723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"snli\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "\n",
    "label_cname = 'label'\n",
    "remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "train = train.filter(remove_minus1_labels)\n",
    "valid = valid.filter(remove_minus1_labels)\n",
    "test = test.filter(remove_minus1_labels)\n",
    "\n",
    "# make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "assert train.features[label_cname].num_classes == vm_num_labels\n",
    "assert valid.features[label_cname].num_classes == vm_num_labels\n",
    "assert test.features[ label_cname].num_classes == vm_num_labels\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size, shuffle=True, num_workers=n_wkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "    batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                                   temperature=1.5, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty)\n",
    "    tgt_text = para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def gen_hypothesis_paraphrases(x, n_seed_seqs=32): \n",
    "    \"\"\"keep n_seed_seqs at 4,8,16,32,64 etc\"\"\"\n",
    "    # TODO: figure out how to batch this. \n",
    "    if n_seed_seqs % 4 != 0: raise ValueError(\"keep n_seed_seqs divisible by 4 for now\")\n",
    "    n = n_seed_seqs/2\n",
    "    #low diversity (ld) paraphrases \n",
    "    ld_l = get_paraphrases(x['hypothesis'],num_return_sequences=int(n),\n",
    "                            num_beams=int(n))\n",
    "    #high diversity (hd) paraphrases. We can use num_beam_groups and diversity_penalty as hyperparameters. \n",
    "    hd_l =  get_paraphrases(x['hypothesis'],num_return_sequences=int(n),\n",
    "                            num_beams=int(n), num_beam_groups=int(n),diversity_penalty=50002.5)\n",
    "    l = ld_l + hd_l \n",
    "    x['hypothesis_paraphrases'] = l #TODO: change to list(set(l))             \n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seed_seqs = 48\n",
    "fname = path_cache + 'valid_small_'+ str(n_seed_seqs)\n",
    "if os.path.exists(fname):  # simple caching\n",
    "    valid_small = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    valid_small = valid.shard(20, 0, contiguous=True)\n",
    "    valid_small = valid_small.map(lambda x: gen_hypothesis_paraphrases(x, n_seed_seqs=n_seed_seqs),\n",
    "                  batched=False)\n",
    "    valid_small.save_to_disk(fname)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a new dataset by repeating all other fields to be same length as number of paraphrases. \n",
    "def create_paraphrase_dataset(batch): \n",
    "    \"\"\"Repeat the other fields to be the same length as the number of paraphrases.\"\"\"    \n",
    "    n_premises = len(batch['premise'])\n",
    "    paraphrases,hyp,prem,labels=[],[],[],[]\n",
    "    d1 = dict()\n",
    "    def rep_entry(x): return [x for o in range(n_paraphrases)]\n",
    "    for p,h,l,hp in zip(batch['premise'], batch['hypothesis'], batch['label'], batch['hypothesis_paraphrases']):\n",
    "        n_paraphrases = len(hp)\n",
    "        paraphrases +=    hp\n",
    "        hyp         +=    rep_entry(h)\n",
    "        prem        +=    rep_entry(p)\n",
    "        labels      +=    rep_entry(l)\n",
    "    return {\n",
    "        'hypothesis': hyp,\n",
    "        'hypothesis_paraphrases': paraphrases,\n",
    "        'premise':prem,\n",
    "        'label':labels \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = path_cache + 'valid_small_paraphrases_' + str(n_seed_seqs)\n",
    "if os.path.exists(fname):     \n",
    "    valid_small_paraphrases = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    # Need to call this with batched=True to work. \n",
    "    valid_small_paraphrases = valid_small.map(create_paraphrase_dataset,batched=True)\n",
    "    valid_small_paraphrases.save_to_disk(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     42
    ]
   },
   "outputs": [],
   "source": [
    "def get_vm_scores(): \n",
    "    \"\"\"very hacky procedure to generate victim model scores \"\"\"\n",
    "    # Get preds and accuracy on the paraphrase dataset\n",
    "    print(\"Getting victim model scores.\")\n",
    "    some_dl = DataLoader(valid_small_paraphrases, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=n_wkrs, pin_memory=True)\n",
    "    dl = some_dl\n",
    "    metric = load_metric('accuracy')\n",
    "    para_probs_l,orig_probs_l = [], []\n",
    "    assert vm_model.training == False  # checks that model is in eval mode \n",
    "    #monitor = Monitor(2)  # track GPU usage and memory\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dl): \n",
    "            if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "            labels,premise = data['label'].to(device),data[\"premise\"]\n",
    "            paraphrases,orig = data[\"hypothesis_paraphrases\"],data[\"hypothesis\"]\n",
    "\n",
    "            # predictions for original\n",
    "            inputs = vm_tokenizer(premise,orig,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            orig_probs_l.append(probs.cpu())  \n",
    "\n",
    "            # predictions for paraphrases\n",
    "            inputs = vm_tokenizer(premise,paraphrases, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            para_probs_l.append(probs.cpu())\n",
    "            metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    orig_probs_t, para_probs_t = torch.cat(orig_probs_l),torch.cat(para_probs_l)\n",
    "    #monitor.stop()\n",
    "\n",
    "    # bit of a hack, i'm sure there's a native pytorch function for this but I couldn't find it\n",
    "    vm_para_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],para_probs_t)])\n",
    "    vm_orig_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],orig_probs_t)])\n",
    "    return para_probs_t, orig_probs_t\n",
    "    \n",
    "\n",
    "def generate_sim_scores(): \n",
    "    \"\"\"Function to just loop and generate sim scores for each input\"\"\"\n",
    "    print(\"Getting similarity scores\")\n",
    "    sim_score_l = []\n",
    "    for i, data in enumerate(valid_small): \n",
    "        if i % 50 == 0 : print(i, \"out of\", len(valid_small))\n",
    "        orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "        orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "        cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "        sim_score_l.append(cos_sim)\n",
    "    sim_score_t = torch.cat(sim_score_l)\n",
    "    return sim_score_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results dataframe \n",
    "fname = path_cache + 'results_df' + str(n_seed_seqs) + \"_1.csv\"\n",
    "if os.path.exists(fname):\n",
    "    results_df = pd.read_csv(fname)\n",
    "else: \n",
    "    sim_score_t = generate_sim_scores()\n",
    "    para_probs_t, orig_probs_t = get_vm_scores()\n",
    "    vm_para_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],para_probs_t)])\n",
    "    vm_orig_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],orig_probs_t)])\n",
    "    \n",
    "    results_df = pd.DataFrame({'premise': valid_small_paraphrases['premise'],\n",
    "                  'orig': valid_small_paraphrases['hypothesis'],\n",
    "                  'para': valid_small_paraphrases['hypothesis_paraphrases'],\n",
    "                  'sim_score': sim_score_t,\n",
    "                  'label_true': valid_small_paraphrases['label'], \n",
    "                  'label_vm_orig': orig_probs_t.argmax(1),\n",
    "                  'label_vm_para': para_probs_t.argmax(1),\n",
    "                  'vm_orig_truelabel': vm_orig_scores,             \n",
    "                  'vm_para_truelabel': vm_para_scores,\n",
    "                  'vm_truelabel_change': vm_orig_scores - vm_para_scores,\n",
    "                  'vm_orig_class0': orig_probs_t[:,0], \n",
    "                  'vm_orig_class1': orig_probs_t[:,1], \n",
    "                  'vm_orig_class2': orig_probs_t[:,2],  \n",
    "                  'vm_para_class0': para_probs_t[:,0], \n",
    "                  'vm_para_class1': para_probs_t[:,1], \n",
    "                  'vm_para_class2': para_probs_t[:,2]     \n",
    "                  })\n",
    "    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "    results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>premise</th>\n",
       "      <th>orig</th>\n",
       "      <th>para</th>\n",
       "      <th>sim_score</th>\n",
       "      <th>label_true</th>\n",
       "      <th>label_vm_orig</th>\n",
       "      <th>label_vm_para</th>\n",
       "      <th>vm_orig_truelabel</th>\n",
       "      <th>vm_para_truelabel</th>\n",
       "      <th>vm_truelabel_change</th>\n",
       "      <th>vm_orig_class0</th>\n",
       "      <th>vm_orig_class1</th>\n",
       "      <th>vm_orig_class2</th>\n",
       "      <th>vm_para_class0</th>\n",
       "      <th>vm_para_class1</th>\n",
       "      <th>vm_para_class2</th>\n",
       "      <th>vm_truelabel_change_X_sim_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Two women are embracing while holding to go packages.</td>\n",
       "      <td>The sisters are hugging goodbye while holding to go packages after just eating lunch.</td>\n",
       "      <td>The sisters are hugging goodbye after eating lunch.</td>\n",
       "      <td>0.916945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.994097</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.994097</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.001790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Two women are embracing while holding to go packages.</td>\n",
       "      <td>The sisters are hugging goodbye while holding to go packages after just eating lunch.</td>\n",
       "      <td>The sisters are hugging goodbye as they hold to go packages.</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.991314</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.991314</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Two women are embracing while holding to go packages.</td>\n",
       "      <td>The sisters are hugging goodbye while holding to go packages after just eating lunch.</td>\n",
       "      <td>The sisters are holding onto packages after eating lunch.</td>\n",
       "      <td>0.856539</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.001113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Two women are embracing while holding to go packages.</td>\n",
       "      <td>The sisters are hugging goodbye while holding to go packages after just eating lunch.</td>\n",
       "      <td>The sisters are hugging and holding onto packages after eating lunch.</td>\n",
       "      <td>0.905042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.995355</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.995355</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Two women are embracing while holding to go packages.</td>\n",
       "      <td>The sisters are hugging goodbye while holding to go packages after just eating lunch.</td>\n",
       "      <td>The sisters are holding packages after eating lunch.</td>\n",
       "      <td>0.858632</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.995044</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.995044</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23659</th>\n",
       "      <td>23659</td>\n",
       "      <td>Nine women in white robes with hoods walk on plush, green grass.</td>\n",
       "      <td>The women are wearing flip flops.</td>\n",
       "      <td>These Women have flipped flop shoes on</td>\n",
       "      <td>0.726975</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.222727</td>\n",
       "      <td>-0.122628</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.899398</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.222727</td>\n",
       "      <td>0.775645</td>\n",
       "      <td>-0.089148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23660</th>\n",
       "      <td>23660</td>\n",
       "      <td>Nine women in white robes with hoods walk on plush, green grass.</td>\n",
       "      <td>The women are wearing flip flops.</td>\n",
       "      <td>Woman withflops</td>\n",
       "      <td>0.678554</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.899398</td>\n",
       "      <td>0.393144</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>-0.008410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23661</th>\n",
       "      <td>23661</td>\n",
       "      <td>Nine women in white robes with hoods walk on plush, green grass.</td>\n",
       "      <td>The women are wearing flip flops.</td>\n",
       "      <td>Woman withflops on them</td>\n",
       "      <td>0.691026</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.168825</td>\n",
       "      <td>-0.068726</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.899398</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.168825</td>\n",
       "      <td>0.785646</td>\n",
       "      <td>-0.047492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23662</th>\n",
       "      <td>23662</td>\n",
       "      <td>Nine women in white robes with hoods walk on plush, green grass.</td>\n",
       "      <td>The women are wearing flip flops.</td>\n",
       "      <td>Three girls were dressed inflip flops</td>\n",
       "      <td>0.525657</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.147149</td>\n",
       "      <td>-0.047050</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.899398</td>\n",
       "      <td>0.032746</td>\n",
       "      <td>0.147149</td>\n",
       "      <td>0.820106</td>\n",
       "      <td>-0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23663</th>\n",
       "      <td>23663</td>\n",
       "      <td>Nine women in white robes with hoods walk on plush, green grass.</td>\n",
       "      <td>The women are wearing flip flops.</td>\n",
       "      <td>ladies woreflip flops to work</td>\n",
       "      <td>0.514502</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.285750</td>\n",
       "      <td>-0.185651</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.100099</td>\n",
       "      <td>0.899398</td>\n",
       "      <td>0.007574</td>\n",
       "      <td>0.285750</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>-0.095518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23664 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx  \\\n",
       "0          0   \n",
       "1          1   \n",
       "2          2   \n",
       "3          3   \n",
       "4          4   \n",
       "...      ...   \n",
       "23659  23659   \n",
       "23660  23660   \n",
       "23661  23661   \n",
       "23662  23662   \n",
       "23663  23663   \n",
       "\n",
       "                                                                premise  \\\n",
       "0                 Two women are embracing while holding to go packages.   \n",
       "1                 Two women are embracing while holding to go packages.   \n",
       "2                 Two women are embracing while holding to go packages.   \n",
       "3                 Two women are embracing while holding to go packages.   \n",
       "4                 Two women are embracing while holding to go packages.   \n",
       "...                                                                 ...   \n",
       "23659  Nine women in white robes with hoods walk on plush, green grass.   \n",
       "23660  Nine women in white robes with hoods walk on plush, green grass.   \n",
       "23661  Nine women in white robes with hoods walk on plush, green grass.   \n",
       "23662  Nine women in white robes with hoods walk on plush, green grass.   \n",
       "23663  Nine women in white robes with hoods walk on plush, green grass.   \n",
       "\n",
       "                                                                                        orig  \\\n",
       "0      The sisters are hugging goodbye while holding to go packages after just eating lunch.   \n",
       "1      The sisters are hugging goodbye while holding to go packages after just eating lunch.   \n",
       "2      The sisters are hugging goodbye while holding to go packages after just eating lunch.   \n",
       "3      The sisters are hugging goodbye while holding to go packages after just eating lunch.   \n",
       "4      The sisters are hugging goodbye while holding to go packages after just eating lunch.   \n",
       "...                                                                                      ...   \n",
       "23659                                                      The women are wearing flip flops.   \n",
       "23660                                                      The women are wearing flip flops.   \n",
       "23661                                                      The women are wearing flip flops.   \n",
       "23662                                                      The women are wearing flip flops.   \n",
       "23663                                                      The women are wearing flip flops.   \n",
       "\n",
       "                                                                        para  \\\n",
       "0                        The sisters are hugging goodbye after eating lunch.   \n",
       "1               The sisters are hugging goodbye as they hold to go packages.   \n",
       "2                  The sisters are holding onto packages after eating lunch.   \n",
       "3      The sisters are hugging and holding onto packages after eating lunch.   \n",
       "4                       The sisters are holding packages after eating lunch.   \n",
       "...                                                                      ...   \n",
       "23659                                 These Women have flipped flop shoes on   \n",
       "23660                                                        Woman withflops   \n",
       "23661                                                Woman withflops on them   \n",
       "23662                                  Three girls were dressed inflip flops   \n",
       "23663                                          ladies woreflip flops to work   \n",
       "\n",
       "       sim_score  label_true  label_vm_orig  label_vm_para  vm_orig_truelabel  \\\n",
       "0       0.916945           1              1              1           0.996048   \n",
       "1       0.820021           1              1              1           0.996048   \n",
       "2       0.856539           1              1              1           0.996048   \n",
       "3       0.905042           1              1              1           0.996048   \n",
       "4       0.858632           1              1              1           0.996048   \n",
       "...          ...         ...            ...            ...                ...   \n",
       "23659   0.726975           1              2              2           0.100099   \n",
       "23660   0.678554           1              2              2           0.100099   \n",
       "23661   0.691026           1              2              2           0.100099   \n",
       "23662   0.525657           1              2              2           0.100099   \n",
       "23663   0.514502           1              2              2           0.100099   \n",
       "\n",
       "       vm_para_truelabel  vm_truelabel_change  vm_orig_class0  vm_orig_class1  \\\n",
       "0               0.994097             0.001952        0.002556        0.996048   \n",
       "1               0.991314             0.004734        0.002556        0.996048   \n",
       "2               0.994749             0.001299        0.002556        0.996048   \n",
       "3               0.995355             0.000693        0.002556        0.996048   \n",
       "4               0.995044             0.001004        0.002556        0.996048   \n",
       "...                  ...                  ...             ...             ...   \n",
       "23659           0.222727            -0.122628        0.000503        0.100099   \n",
       "23660           0.112493            -0.012394        0.000503        0.100099   \n",
       "23661           0.168825            -0.068726        0.000503        0.100099   \n",
       "23662           0.147149            -0.047050        0.000503        0.100099   \n",
       "23663           0.285750            -0.185651        0.000503        0.100099   \n",
       "\n",
       "       vm_orig_class2  vm_para_class0  vm_para_class1  vm_para_class2  \\\n",
       "0            0.001395        0.002258        0.994097        0.003646   \n",
       "1            0.001395        0.006501        0.991314        0.002185   \n",
       "2            0.001395        0.002427        0.994749        0.002824   \n",
       "3            0.001395        0.002345        0.995355        0.002300   \n",
       "4            0.001395        0.002618        0.995044        0.002338   \n",
       "...               ...             ...             ...             ...   \n",
       "23659        0.899398        0.001628        0.222727        0.775645   \n",
       "23660        0.899398        0.393144        0.112493        0.494363   \n",
       "23661        0.899398        0.045529        0.168825        0.785646   \n",
       "23662        0.899398        0.032746        0.147149        0.820106   \n",
       "23663        0.899398        0.007574        0.285750        0.706676   \n",
       "\n",
       "       vm_truelabel_change_X_sim_score  \n",
       "0                             0.001790  \n",
       "1                             0.003882  \n",
       "2                             0.001113  \n",
       "3                             0.000627  \n",
       "4                             0.000862  \n",
       "...                                ...  \n",
       "23659                        -0.089148  \n",
       "23660                        -0.008410  \n",
       "23661                        -0.047492  \n",
       "23662                        -0.024732  \n",
       "23663                        -0.095518  \n",
       "\n",
       "[23664 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vm_change_X_sim_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6b26fade12d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vm_change_X_sim_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   5285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1558\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vm_change_X_sim_score'"
     ]
    }
   ],
   "source": [
    "tmp = results_df.sort_values('vm_change_X_sim_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[22563:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = results_df.groupby(['premise', 'orig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df.iloc[2000:2040,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculates performance of victim model on a dataloader \n",
    "# dl = valid_dl\n",
    "# metric = load_metric('accuracy')\n",
    "# for i, data in enumerate(dl): \n",
    "#     if i % 10 == 0 : print(i, \"out of\", len(dl)) \n",
    "#     labels,premise,hypothesis = data['label'].to(device),data[\"premise\"],data[\"hypothesis\"]\n",
    "#     inputs = vm_tokenizer(premise,hypothesis, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "#     inputs.to(device)\n",
    "#     outputs = vm_model(**inputs, labels=labels)\n",
    "#     probs = outputs.logits.softmax(1)\n",
    "#     preds = probs.argmax(1)\n",
    "#     metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "# metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score semantic similarity with cross encoders\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "# cross_encoder= CrossEncoder('cross-encoder/quora-distilroberta-base')\n",
    "# i =11\n",
    "# data = valid_small[i]\n",
    "# orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "# orig_rep = [orig for i in range(len(para))]\n",
    "# pairs = list(zip(orig_rep,para))\n",
    "# scores = cross_encoder.predict(pairs)\n",
    "# results_df = pd.DataFrame({'pairs':pairs, 'para': para,'score': cos_sim})\n",
    "# print(orig)\n",
    "# results_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with sentence transformers\n",
    "# valid_small_dl = DataLoader(valid_small, batch_size=4, shuffle=False, \n",
    "#                      num_workers=n_wkrs, pin_memory=True)\n",
    "# sim_score_l = []\n",
    "# for i, data in enumerate(valid_small_dl): \n",
    "#     pass\n",
    "#     orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "#     orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "# #     cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "# #     results_df = pd.DataFrame({'para': para,'score': cos_sim})\n",
    "# #     print(orig)\n",
    "# #     results_df.sort_values('score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
