{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE sets: model will be trained on eval set, so you shouldn't also test on the eval set. The problem is that the labels are withheld for the test set. \n",
    "Start with SNLI. MultiNLI is a later option too. As is rotten_tomatoes. \n",
    "* Victim model performance on dataset train, valid, test set. (done, written code to measure it)\n",
    "* Create new paraphrased valid + test datasets (done a preliminary version on the valid set) \n",
    "* Measure victim model performance on paraphrased datasets (done. on vanilla valid set is about 87% accuracy. generating 16 paraphrases (i.e. not many) and evaluating performance on all of them, we get ~75% accuracy)\n",
    "* Get document embeddings of original and paraphrased and compare \n",
    "  * https://github.com/UKPLab/sentence-transformers\n",
    "* Write a simple way to measure paraphrase quality\n",
    "* Construct reward function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets, transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pprint import pprint\n",
    "import numpy as np, pandas as pd\n",
    "from utils import *   # local script \n",
    "import pyarrow\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_name = \"tuner007/pegasus_paraphrase\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-cased-snli\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/data/tproth/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b963a6d215bf4bb3b26fc9ede1c77060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3225875f9b814c39867b152b24480b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff991543072149959cfa2bab6aa39e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"snli\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "\n",
    "remove_minus1_labels = lambda x: x['label'] != -1\n",
    "train = train.filter(remove_minus1_labels)\n",
    "valid = valid.filter(remove_minus1_labels)\n",
    "test = test.filter(remove_minus1_labels)\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "  batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "  translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                                   temperature=1.5, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty)\n",
    "  tgt_text = para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "def gen_hypothesis_paraphrases(x, n_seed_seqs=32): \n",
    "  \"\"\"keep n_seed_seqs at 4,8,16,32,64 etc\"\"\"\n",
    "  # TODO: figure out how to batch this. \n",
    "  if n_seed_seqs % 4 != 0: raise ValueError(\"keep n_seed_seqs divisible by 4 for now\")\n",
    "  n = n_seed_seqs/2\n",
    "  #low diversity (ld) paraphrases \n",
    "  ld_l = get_paraphrases(x['hypothesis'],num_return_sequences=int(n),\n",
    "                            num_beams=int(n))\n",
    "  #high diversity (hd) paraphrases. We can use num_beam_groups and diversity_penalty as hyperparameters. \n",
    "  hd_l =  get_paraphrases(x['hypothesis'],num_return_sequences=int(n),\n",
    "                            num_beams=int(n), num_beam_groups=int(n),diversity_penalty=50002.5)\n",
    "  l = ld_l + hd_l \n",
    "  x['hypothesis_paraphrases'] = l#  list(set(l))             \n",
    "  return x \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccc6f2c40f54f68a6b8f2141ccc22cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_small = valid.shard(20,0,contiguous=True)\n",
    "valid_small = valid_small.map(lambda x: gen_hypothesis_paraphrases(x, n_seed_seqs=16),\n",
    "                  batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce715613f35a40dd8cf2d9d2b3c93e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset by repeating all other fields to be same length as number of paraphrases. \n",
    "# \n",
    "def create_paraphrase_dataset(batch): \n",
    "    \"\"\"Repeat the other fields to be the same length as the number of paraphrases.\"\"\"    \n",
    "    n_premises = len(batch['premise'])\n",
    "    paraphrases,hyp,prem,labels=[],[],[],[]\n",
    "    d1 = dict()\n",
    "    def rep_entry(x): return [x for o in range(n_paraphrases)]\n",
    "    for p,h,l,hp in zip(batch['premise'], batch['hypothesis'], batch['label'], batch['hypothesis_paraphrases']):\n",
    "        n_paraphrases = len(hp)\n",
    "        paraphrases +=    hp\n",
    "        hyp         +=    rep_entry(h)\n",
    "        prem        +=    rep_entry(p)\n",
    "        labels      +=    rep_entry(l)\n",
    "    return {\n",
    "        'hypothesis': hyp,\n",
    "        'hypothesis_paraphrases': paraphrases,\n",
    "        'premise':prem,\n",
    "        'label':labels \n",
    "       }\n",
    "# Need to call this with batched=True to work. \n",
    "valid_small_paraphrases = valid_small.map(create_paraphrase_dataset,batched=True)\n",
    "some_dl = DataLoader(valid_small_paraphrases, batch_size=16, shuffle=False, num_workers=n_wkrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hypothesis', 'hypothesis_paraphrases', 'label', 'premise'],\n",
       "    num_rows: 7888\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_small_paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7888"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_small_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 493\n",
      "10 out of 493\n",
      "20 out of 493\n",
      "30 out of 493\n",
      "40 out of 493\n",
      "50 out of 493\n",
      "60 out of 493\n",
      "70 out of 493\n",
      "80 out of 493\n",
      "90 out of 493\n",
      "100 out of 493\n",
      "110 out of 493\n",
      "120 out of 493\n",
      "130 out of 493\n",
      "140 out of 493\n",
      "150 out of 493\n",
      "160 out of 493\n",
      "170 out of 493\n",
      "180 out of 493\n",
      "190 out of 493\n",
      "200 out of 493\n",
      "210 out of 493\n",
      "220 out of 493\n",
      "230 out of 493\n",
      "240 out of 493\n",
      "250 out of 493\n",
      "260 out of 493\n",
      "270 out of 493\n",
      "280 out of 493\n",
      "290 out of 493\n",
      "300 out of 493\n",
      "310 out of 493\n",
      "320 out of 493\n",
      "330 out of 493\n",
      "340 out of 493\n",
      "350 out of 493\n",
      "360 out of 493\n",
      "370 out of 493\n",
      "380 out of 493\n",
      "390 out of 493\n",
      "400 out of 493\n",
      "410 out of 493\n",
      "420 out of 493\n",
      "430 out of 493\n",
      "440 out of 493\n",
      "450 out of 493\n",
      "460 out of 493\n",
      "470 out of 493\n",
      "480 out of 493\n",
      "490 out of 493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7560851926977687}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = some_dl\n",
    "metric = load_metric('accuracy')\n",
    "for i, data in enumerate(dl): \n",
    "    if i % 10 == 0 : print(i, \"out of\", len(dl))\n",
    "    labels,premise,paraphrases = data['label'].to(device),data[\"premise\"],data[\"hypothesis_paraphrases\"]\n",
    "    inputs = vm_tokenizer(premise,paraphrases, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = vm_model(**inputs, labels=labels)\n",
    "    probs = outputs.logits.softmax(1)\n",
    "    preds = probs.argmax(1)\n",
    "    metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 154\n",
      "10 out of 154\n",
      "20 out of 154\n",
      "30 out of 154\n",
      "40 out of 154\n",
      "50 out of 154\n",
      "60 out of 154\n",
      "70 out of 154\n",
      "80 out of 154\n",
      "90 out of 154\n",
      "100 out of 154\n",
      "110 out of 154\n",
      "120 out of 154\n",
      "130 out of 154\n",
      "140 out of 154\n",
      "150 out of 154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8768542979069295}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates performance of victim model on a dataloader \n",
    "dl = valid_dl\n",
    "metric = load_metric('accuracy')\n",
    "for i, data in enumerate(dl): \n",
    "    if i % 10 == 0 : print(i, \"out of\", len(dl)) \n",
    "    labels,premise,hypothesis = data['label'].to(device),data[\"premise\"],data[\"hypothesis\"]\n",
    "    inputs = vm_tokenizer(premise,hypothesis, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = vm_model(**inputs, labels=labels)\n",
    "    probs = outputs.logits.softmax(1)\n",
    "    preds = probs.argmax(1)\n",
    "    metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013c80a404f4212b430c820ddfa467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=305584576.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 most similar pairs:\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.8289\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7977\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6842\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.3762\n",
      "A man is eating food. \t A man is riding a horse. \t 0.3545\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t Score: 0.8289\n",
      "A man is eating food. \t A man is eating a piece of bread. \t Score: 0.7977\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t Score: 0.6842\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t Score: 0.3762\n",
      "A man is eating food. \t A man is riding a horse. \t Score: 0.3545\n",
      "A man is eating food. \t A man is riding a white horse on an enclosed ground. \t Score: 0.3529\n",
      "A woman is playing violin. \t A monkey is playing drums. \t Score: 0.3399\n",
      "A man is riding a horse. \t Someone in a gorilla costume is playing a set of drums. \t Score: 0.3079\n",
      "A man is riding a horse. \t A monkey is playing drums. \t Score: 0.2860\n",
      "A man is eating a piece of bread. \t A man is riding a white horse on an enclosed ground. \t Score: 0.2665\n"
     ]
    }
   ],
   "source": [
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t {} \\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
