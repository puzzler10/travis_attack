{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE sets: model will be trained on eval set, so you shouldn't also test on the eval set. The problem is that the labels are withheld for the test set. \n",
    "Start with SNLI. MultiNLI is a later option too. As is rotten_tomatoes. \n",
    "* Victim model performance on dataset train, valid, test set. (done, written code to measure it)\n",
    "* Create new paraphrased valid + test datasets (done a preliminary version on the valid set) \n",
    "* Measure victim model performance on paraphrased datasets (done. on vanilla valid set is about 87% accuracy. generating 16 paraphrases (i.e. not many) and evaluating performance on all of them, we get ~75% accuracy)\n",
    "* Get document embeddings of original and paraphrased and compare (done)\n",
    "  * https://github.com/UKPLab/sentence-transformers\n",
    "* Write a simple way to measure paraphrase quality (done) \n",
    "* Construct reward function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets, transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pprint import pprint\n",
    "import numpy as np, pandas as pd\n",
    "import scipy\n",
    "from utils import *   # local script \n",
    "import pyarrow\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import seaborn as sns\n",
    "from itertools import repeat\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "\n",
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size = 64\n",
    "pd.set_option(\"display.max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase model (para)\n",
    "para_name = \"tuner007/pegasus_paraphrase\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-cased-snli\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity model \n",
    "embedding_model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/data/tproth/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170df7adbe94bafa77a8cf18f34ba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21198aacdb0b4d1ca0732691c7e06d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fc80e9a46a4c25a8730105b8116a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"snli\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "\n",
    "label_cname = 'label'\n",
    "remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "train = train.filter(remove_minus1_labels)\n",
    "valid = valid.filter(remove_minus1_labels)\n",
    "test = test.filter(remove_minus1_labels)\n",
    "\n",
    "# make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "assert train.features[label_cname].num_classes == vm_num_labels\n",
    "assert valid.features[label_cname].num_classes == vm_num_labels\n",
    "assert test.features[ label_cname].num_classes == vm_num_labels\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size, shuffle=True, num_workers=n_wkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "    batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                                   temperature=1.5, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty)\n",
    "    tgt_text = para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def gen_dataset_paraphrases(x, cname_input, cname_output, n_seed_seqs=32): \n",
    "    \"\"\" x: one row of a dataset. \n",
    "    cname_input: column to generate paraphrases for \n",
    "    cname_output: column name to give output of paraphrases \n",
    "    n_seed_seqs: rough indicator of how many paraphrases to return. \n",
    "            For now, keep at 4,8,16,32,64 etc\"\"\"\n",
    "    # TODO: figure out how to batch this. \n",
    "    if n_seed_seqs % 4 != 0: raise ValueError(\"keep n_seed_seqs divisible by 4 for now\")\n",
    "    n = n_seed_seqs/2\n",
    "    #low diversity (ld) paraphrases \n",
    "    ld_l = get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "                            num_beams=int(n))\n",
    "    #high diversity (hd) paraphrases. We can use num_beam_groups and diversity_penalty as hyperparameters. \n",
    "    hd_l =  get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "                            num_beams=int(n), num_beam_groups=int(n),diversity_penalty=50002.5)\n",
    "    l = ld_l + hd_l \n",
    "    x[cname_output] = l #TODO: change to list(set(l))             \n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate paraphrase dataset\n",
    "n_seed_seqs = 48\n",
    "date = '20210629'\n",
    "fname = path_cache + 'valid_small_'+ date + '_' + str(n_seed_seqs)\n",
    "if os.path.exists(fname):  # simple caching\n",
    "    valid_small = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    valid_small = valid.shard(20, 0, contiguous=True)\n",
    "    valid_small = valid_small.map(lambda x: gen_dataset_paraphrases(x, n_seed_seqs=n_seed_seqs,\n",
    "                        cname_input='hypothesis', cname_output='hypothesis_paraphrases'),\n",
    "                    batched=False)\n",
    "    valid_small.save_to_disk(fname)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a new version of paraphrase dataset by repeating all other fields to be same \n",
    "# length as number of paraphrases. \n",
    "def create_paraphrase_dataset(batch, l_cname): \n",
    "    \"\"\"Repeat the other fields to be the same length as the number of paraphrases.\n",
    "    l_cname: column name that contains the list of paraphrases\"\"\"    \n",
    "    return_d = defaultdict(list) \n",
    "    for o in zip(*batch.values()):\n",
    "        d = dict(zip(batch.keys(), o))\n",
    "        n_paraphrases = len(d[l_cname])\n",
    "        for k,v in d.items(): \n",
    "            return_d[k] += v if k == l_cname else [v for o in range(n_paraphrases)]\n",
    "    return return_d      \n",
    "\n",
    "fname = path_cache + 'valid_small_paraphrases_' + date + '_'+ str(n_seed_seqs)\n",
    "if os.path.exists(fname):     \n",
    "    valid_small_paraphrases = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    # Need to call this with batched=True to work. \n",
    "    valid_small_paraphrases = valid_small.map(lambda x: create_paraphrase_dataset(x,\n",
    "                                                             l_cname='hypothesis_paraphrases'), \n",
    "                                              batched=True)\n",
    "    valid_small_paraphrases.save_to_disk(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     1,
     43
    ]
   },
   "outputs": [],
   "source": [
    "# Generate results dataframe \n",
    "def get_vm_scores(): \n",
    "    \"\"\"very hacky procedure to generate victim model scores \"\"\"\n",
    "    # Get preds and accuracy on the paraphrase dataset\n",
    "    print(\"Getting victim model scores.\")\n",
    "    some_dl = DataLoader(valid_small_paraphrases, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=n_wkrs, pin_memory=True)\n",
    "    dl = some_dl\n",
    "    metric = load_metric('accuracy')\n",
    "    para_probs_l,orig_probs_l = [], []\n",
    "    assert vm_model.training == False  # checks that model is in eval mode \n",
    "    #monitor = Monitor(2)  # track GPU usage and memory\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dl): \n",
    "            if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "            labels,premise = data['label'].to(device),data[\"premise\"]\n",
    "            paraphrases,orig = data[\"hypothesis_paraphrases\"],data[\"hypothesis\"]\n",
    "\n",
    "            # predictions for original\n",
    "            inputs = vm_tokenizer(premise,orig,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            orig_probs_l.append(probs.cpu())  \n",
    "\n",
    "            # predictions for paraphrases\n",
    "            inputs = vm_tokenizer(premise,paraphrases, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            para_probs_l.append(probs.cpu())\n",
    "            metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    orig_probs_t, para_probs_t = torch.cat(orig_probs_l),torch.cat(para_probs_l)\n",
    "    #monitor.stop()\n",
    "\n",
    "    # bit of a hack, i'm sure there's a native pytorch function for this but I couldn't find it\n",
    "    vm_para_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],para_probs_t)])\n",
    "    vm_orig_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],orig_probs_t)])\n",
    "    return para_probs_t, orig_probs_t\n",
    "\n",
    "def generate_sim_scores(): \n",
    "    \"\"\"Function to just loop and generate sim scores for each input\"\"\"\n",
    "    print(\"Getting similarity scores\")\n",
    "    sim_score_l = []\n",
    "    for i, data in enumerate(valid_small): \n",
    "        if i % 50 == 0 : print(i, \"out of\", len(valid_small))\n",
    "        orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "        orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "        cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "        sim_score_l.append(cos_sim)\n",
    "    sim_score_t = torch.cat(sim_score_l)\n",
    "    return sim_score_t\n",
    "\n",
    "fname = path_cache + 'results_df_'+ date + \"_\" + str(n_seed_seqs) + \".csv\"\n",
    "if os.path.exists(fname):\n",
    "    results_df = pd.read_csv(fname)\n",
    "else: \n",
    "    sim_score_t = generate_sim_scores()\n",
    "    para_probs_t, orig_probs_t = get_vm_scores()\n",
    "    vm_para_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],para_probs_t)])\n",
    "    vm_orig_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],orig_probs_t)])\n",
    "    \n",
    "    results_df = pd.DataFrame({'premise': valid_small_paraphrases['premise'],\n",
    "                  'orig': valid_small_paraphrases['hypothesis'],\n",
    "                  'para': valid_small_paraphrases['hypothesis_paraphrases'],\n",
    "                  'sim_score': sim_score_t,\n",
    "                  'label_true': valid_small_paraphrases['label'], \n",
    "                  'label_vm_orig': orig_probs_t.argmax(1),\n",
    "                  'label_vm_para': para_probs_t.argmax(1),\n",
    "                  'vm_orig_truelabel': vm_orig_scores,             \n",
    "                  'vm_para_truelabel': vm_para_scores,\n",
    "                  'vm_truelabel_change': vm_orig_scores - vm_para_scores,\n",
    "                  'vm_orig_class0': orig_probs_t[:,0], \n",
    "                  'vm_orig_class1': orig_probs_t[:,1], \n",
    "                  'vm_orig_class2': orig_probs_t[:,2],  \n",
    "                  'vm_para_class0': para_probs_t[:,0], \n",
    "                  'vm_para_class1': para_probs_t[:,1], \n",
    "                  'vm_para_class2': para_probs_t[:,2]     \n",
    "                  })\n",
    "    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "    results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation method to detect label flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take each example $Ex$ in the filtered set and generate paraphrases (e.g. 16) of it (or it might work better with a simple token-replacement strategy). Run each through the victim model (might be better with a different model, but still trained on dataset) and record predictions. Then tally up the label predictions (or maybe take average of the probabilities). Each prediction is a vote for the true label. \n",
    "\n",
    "Idea is that if $Ex$ changes ground truth label to class 4, then most of the paraphrases of $Ex$ will be of class 4 too. If $Ex$ is truly adversarial, then most of the paraphrases of $Ex$ are likely to be of the original class (or at least of other classes). So in other words: \n",
    "* if `is_adversarial = 1` then we expect most votes to be for other classes to `label_vm_para`. This means we expect more variance in the voting. If we take model confidence for the class of `label_vm_para` and work out entropy/variance, we expect it to be high. \n",
    "* if `is_adversarial = 0` then we expect most votes to be for the same class as `label_vm_para`. This means we expect less variance in the voting. If we take model confidence for the class of `label_vm_para` and work out entropy/variance, we expect it to be low. \n",
    "\n",
    "Variations \n",
    "\n",
    "* Instead of generating further paraphrases for all label flippers, try the checklist tests on the input. e.g. replace number/proper noun\n",
    "* Try systematic perturbations\n",
    "* Record probability of the true class or the predicted class and put it into a distribution. Calculate entropy of it (STRIP style). The idea is that there is some reliable difference in these probabilities between ground-truth flips and otherwise and that entropy can be used as a rough measurement to distinguish between it. \n",
    "* Can try the above while keeping track of sentence embeddings + attention layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ebc62bd8d2fb84e0\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-ebc62bd8d2fb84e0/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# Read in manually labelled data. This is to track results. \n",
    "fname = path_cache + 'results_df_48_20210514_labelled_subset.csv'\n",
    "dset_advlbl = load_dataset('csv', data_files=fname)['train'].train_test_split(test_size=0.25)\n",
    "train_advlbl,test_advlbl = dset_advlbl['train'],dset_advlbl['test']\n",
    "\n",
    "# # as pandas df\n",
    "# df_advlbl = pd.read_csv(fname)\n",
    "# train_advlbl,_,test_advlbl = create_train_valid_test(df_advlbl, frac_train=0.75, frac_valid = 0.001)\n",
    "# # To join with the original. (might be some issues with the idx/row-number col)\n",
    "# # x = pd.merge(results_df, df_advlbl, on =['idx', 'premise','orig', 'para'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paraphrases of paraphrases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp dataset -> gen_paraphrases (returns dataset) -> create_paraphrase_dataset -> get vm labels -> save in data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a4e01fd9f3444ba1168f697ddf38dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a675c66f3e2b4c7792fa6ec0782dc917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 16 \n",
    "cols_to_drop = ['is_adversarial','label_true','label_vm_orig','orig','sim_score']\n",
    "def paraphrase_and_return_dict(x, n_seed_seqs=16): \n",
    "    x['perms'] = get_paraphrases(x['para'], num_return_sequences=n, num_beams=n)\n",
    "    return x \n",
    "train_advlbl_perms = train_advlbl.map(lambda x: paraphrase_and_return_dict(x, n_seed_seqs=n),\n",
    "                  batched=False, remove_columns = cols_to_drop)\n",
    "train_advlbl_expanded = train_advlbl_perms.map(lambda x: create_paraphrase_dataset(x, l_cname='perms'),\n",
    "                           batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'label_vm_para', 'para', 'perms', 'premise'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_advlbl_perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 50\n"
     ]
    }
   ],
   "source": [
    "# Get victim model predictions for each prediction  \n",
    "advlbl_expanded_dl = DataLoader(train_advlbl_expanded, batch_size=batch_size, shuffle=False, \n",
    "                                num_workers=n_wkrs, pin_memory=True)\n",
    "dl = advlbl_expanded_dl\n",
    "probs_l = []\n",
    "assert vm_model.training == False  # checks that model is in eval mode \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dl): \n",
    "        if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "        premise,perms = data[\"premise\"],data[\"perms\"]\n",
    "        # predictions for original\n",
    "        inputs = vm_tokenizer(premise,perms,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        outputs = vm_model(**inputs)\n",
    "        probs = outputs.logits.softmax(1)\n",
    "        # preds = probs.argmax(1)\n",
    "        probs_l.append(probs.cpu()) \n",
    "\n",
    "probs_t = torch.cat(probs_l)\n",
    "preds_t = torch.argmax(probs_t,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back to original\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_label', preds_t.tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob0', probs_t[:,0].tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob1', probs_t[:,1].tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob2', probs_t[:,2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make into pandas_df \n",
    "advlbl_df = pd.DataFrame(train_advlbl_expanded) \n",
    "advlbl_df.vm_label = advlbl_df.vm_label.astype('category')\n",
    "\n",
    "# Count \"votes\" of each set of permutations \n",
    "votes_df = advlbl_df.groupby(['idx'])['vm_label'].describe()\n",
    "votes_df = votes_df.rename(columns={'count':'votes','unique': \"n_cats_with_votes\",\n",
    "                                     \"top\": 'top_cat', 'freq': 'top_cat_votes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label_vm_para</th>\n",
       "      <th>para</th>\n",
       "      <th>perms</th>\n",
       "      <th>premise</th>\n",
       "      <th>vm_label</th>\n",
       "      <th>vm_prob0</th>\n",
       "      <th>vm_prob1</th>\n",
       "      <th>vm_prob2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969092</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.012004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman isn't awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928273</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.034710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>The woman is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963114</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.013559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is asleep.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.997646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A person is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.007448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is sleeping.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.998818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>There is a woman not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.887519</td>\n",
       "      <td>0.014687</td>\n",
       "      <td>0.097794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not awake</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945859</td>\n",
       "      <td>0.032418</td>\n",
       "      <td>0.021723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not sleeping.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947776</td>\n",
       "      <td>0.047572</td>\n",
       "      <td>0.004652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not sleepy.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.872165</td>\n",
       "      <td>0.121609</td>\n",
       "      <td>0.006226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>a woman is not awake</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950812</td>\n",
       "      <td>0.031679</td>\n",
       "      <td>0.017508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not asleep.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958537</td>\n",
       "      <td>0.038354</td>\n",
       "      <td>0.003109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A lady is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969498</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.012826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is sleepy.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>0.927589</td>\n",
       "      <td>0.041699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not tired.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672921</td>\n",
       "      <td>0.321957</td>\n",
       "      <td>0.005122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is tired.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011820</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.003512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx  label_vm_para                   para                        perms  \\\n",
       "1296  7299              0  A woman is not awake.        A woman is not awake.   \n",
       "1297  7299              0  A woman is not awake.         A woman isn't awake.   \n",
       "1298  7299              0  A woman is not awake.      The woman is not awake.   \n",
       "1299  7299              0  A woman is not awake.           A woman is asleep.   \n",
       "1300  7299              0  A woman is not awake.       A person is not awake.   \n",
       "1301  7299              0  A woman is not awake.         A woman is sleeping.   \n",
       "1302  7299              0  A woman is not awake.  There is a woman not awake.   \n",
       "1303  7299              0  A woman is not awake.         A woman is not awake   \n",
       "1304  7299              0  A woman is not awake.     A woman is not sleeping.   \n",
       "1305  7299              0  A woman is not awake.       A woman is not sleepy.   \n",
       "1306  7299              0  A woman is not awake.         a woman is not awake   \n",
       "1307  7299              0  A woman is not awake.       A woman is not asleep.   \n",
       "1308  7299              0  A woman is not awake.         A lady is not awake.   \n",
       "1309  7299              0  A woman is not awake.           A woman is sleepy.   \n",
       "1310  7299              0  A woman is not awake.        A woman is not tired.   \n",
       "1311  7299              0  A woman is not awake.            A woman is tired.   \n",
       "\n",
       "                                                            premise vm_label  \\\n",
       "1296  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1297  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1298  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1299  A woman is talking on the phone while standing next to a dog.        2   \n",
       "1300  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1301  A woman is talking on the phone while standing next to a dog.        2   \n",
       "1302  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1303  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1304  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1305  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1306  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1307  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1308  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1309  A woman is talking on the phone while standing next to a dog.        1   \n",
       "1310  A woman is talking on the phone while standing next to a dog.        0   \n",
       "1311  A woman is talking on the phone while standing next to a dog.        1   \n",
       "\n",
       "      vm_prob0  vm_prob1  vm_prob2  \n",
       "1296  0.969092  0.018904  0.012004  \n",
       "1297  0.928273  0.037017  0.034710  \n",
       "1298  0.963114  0.023327  0.013559  \n",
       "1299  0.000436  0.001917  0.997646  \n",
       "1300  0.977200  0.015352  0.007448  \n",
       "1301  0.000251  0.000932  0.998818  \n",
       "1302  0.887519  0.014687  0.097794  \n",
       "1303  0.945859  0.032418  0.021723  \n",
       "1304  0.947776  0.047572  0.004652  \n",
       "1305  0.872165  0.121609  0.006226  \n",
       "1306  0.950812  0.031679  0.017508  \n",
       "1307  0.958537  0.038354  0.003109  \n",
       "1308  0.969498  0.017676  0.012826  \n",
       "1309  0.030712  0.927589  0.041699  \n",
       "1310  0.672921  0.321957  0.005122  \n",
       "1311  0.011820  0.984667  0.003512  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advlbl_df.query(\"idx==7299\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     2,
     15
    ]
   },
   "outputs": [],
   "source": [
    "# Get entropy and variance from each set of permutations, then choose only the values\n",
    "# that correspond to the predicted label of the paraphrase\n",
    "def get_entropy(x, bins=10): \n",
    "    \"\"\"Return shannon entropy of a vector. Used in pandas summary functions\"\"\"\n",
    "    # the bins parameters affects the entropy quite a bit (it introduces zeros)\n",
    "    hist,_ = np.histogram(x, bins=bins)  \n",
    "    hist = hist/sum(hist)  # turn into PMF (not strictly required for scipy entropy, but easier to interpret)\n",
    "    return scipy.stats.entropy(hist)\n",
    "grp = advlbl_df.groupby(['idx'])[['vm_prob0','vm_prob1','vm_prob2']]\n",
    "entropy_df = grp.agg(func = get_entropy)\n",
    "var_df     = grp.agg(func = 'var')\n",
    "entropy_df.columns = [o + \"_entropy\" for o in entropy_df.columns]\n",
    "var_df.columns     = [o + \"_var\"     for o in var_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     2,
     15
    ]
   },
   "outputs": [],
   "source": [
    "label_df =  advlbl_df[['idx','label_vm_para']].drop_duplicates()\n",
    "def choose_col_of_df_from_label_column(df, labeldf, name='entropy'): \n",
    "    \"\"\"Picks columns of df corresponding to the predicted vm label of the paraphrase. \n",
    "    Works only if probs of classes are the first columns of df in order.\"\"\"\n",
    "    df = df.merge(labeldf,left_index=True, right_on='idx')\n",
    "    v = df['label_vm_para'].values\n",
    "    # See https://stackoverflow.com/a/61234228/5381490\n",
    "    df[name+'_label_vm_para'] = np.take_along_axis(df.values, v[:,None] ,axis=1)\n",
    "    return df \n",
    "entropy_df = choose_col_of_df_from_label_column(entropy_df, label_df, name='entropy')\n",
    "var_df     = choose_col_of_df_from_label_column(var_df,     label_df, name='var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Change original labelled set to a pandas data frame and merge it in \n",
    "train_advlbl_df,test_advlbl_df = pd.DataFrame(dset_advlbl['train']),pd.DataFrame(dset_advlbl['test'])\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, votes_df, left_on ='idx', right_index=True)\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, entropy_df[['idx','entropy_label_vm_para']], \n",
    "                           left_on ='idx', right_on='idx')\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, var_df[['idx', 'var_label_vm_para']], \n",
    "                           left_on ='idx', right_on='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_adversarial</th>\n",
       "      <th>-2</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permutation_success</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To be determined</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>106</td>\n",
       "      <td>67</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_adversarial       -2  -1    0   1  All\n",
       "permutation_success                      \n",
       "False                 0   0   19  45   64\n",
       "True                  0   0   87  22  109\n",
       "To be determined      3  22    0   0   25\n",
       "All                   3  22  106  67  198"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label flip percentage and measure success\n",
    "train_advlbl_df['label_flip'] = train_advlbl_df['top_cat'] != train_advlbl_df['label_vm_para'] \n",
    "def permutation_success(x,y): \n",
    "    result = None\n",
    "    if   x == 1 and y == True:   result = True\n",
    "    elif x == 0 and y == False:  result = True\n",
    "    elif x == -1 or x == -2:     result = \"To be determined\"\n",
    "    else:                        result = False\n",
    "    return result\n",
    "v1,v2 = train_advlbl_df['is_adversarial'].values, train_advlbl_df['label_flip'].values\n",
    "train_advlbl_df['permutation_success'] = list(map(permutation_success, v1,v2))\n",
    "\n",
    "pd.crosstab(index=train_advlbl_df['permutation_success'], \n",
    "                             columns=train_advlbl_df['is_adversarial'],\n",
    "                             margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Example with idx **12649**   \n",
       "\n",
       "|    |   idx | premise                                                                                        | orig                         | para                |   sim_score |   label_true |   label_vm_orig |   label_vm_para |   is_adversarial |   votes |   n_cats_with_votes |   top_cat |   top_cat_votes |   entropy_label_vm_para |   var_label_vm_para | label_flip   | permutation_success   |\n",
       "|---:|------:|:-----------------------------------------------------------------------------------------------|:-----------------------------|:--------------------|------------:|-------------:|----------------:|----------------:|-----------------:|--------:|--------------------:|----------:|----------------:|------------------------:|--------------------:|:-------------|:----------------------|\n",
       "| 46 | 12649 | A man is a safety suit walking outside while another man in a dark suit walks into a building. | Men are sitting in the park. | The men are outside |    0.577053 |            2 |               2 |               0 |                0 |      16 |                   1 |         0 |              16 |                0.702919 |          0.00771518 | False        | True                  |   \n",
       "\n",
       "\n",
       "* **Premise**: `A man is a safety suit walking outside while another man in a dark suit walks into a building.`  \n",
       "* **Hypothesis (original)**: `Men are sitting in the park.` (True label **2**, Victim Model (VM) label **2**)    \n",
       "* **Hypothesis paraphrase**: `The men are outside` (VM label **0**)     \n",
       "\n",
       "This example is **unsuccessful**: it flips the true label.    \n",
       "\n",
       "We generate 16 further *permutations* of the hypothesis paraphrase and get VM votes and confidence for \n",
       "each of them. The label of the hypothesis paraphrase was **0**. \n",
       "Here are five of these permutations (randomly chosen):  \n",
       "\n",
       "|     |   idx |   label_vm_para | para                | perms                         | premise                                                                                        |   vm_label |   vm_prob0 |   vm_prob1 |    vm_prob2 |\n",
       "|----:|------:|----------------:|:--------------------|:------------------------------|:-----------------------------------------------------------------------------------------------|-----------:|-----------:|-----------:|------------:|\n",
       "| 751 | 12649 |               0 | The men are outside | They are outside.             | A man is a safety suit walking outside while another man in a dark suit walks into a building. |          0 |   0.95497  |  0.0420003 | 0.00302959  |\n",
       "| 747 | 12649 |               0 | The men are outside | The men are outdoors          | A man is a safety suit walking outside while another man in a dark suit walks into a building. |          0 |   0.956289 |  0.0411819 | 0.00252904  |\n",
       "| 743 | 12649 |               0 | The men are outside | There are men outdoors.       | A man is a safety suit walking outside while another man in a dark suit walks into a building. |          0 |   0.994275 |  0.0051475 | 0.000577882 |\n",
       "| 748 | 12649 |               0 | The men are outside | The men are standing outside. | A man is a safety suit walking outside while another man in a dark suit walks into a building. |          0 |   0.630127 |  0.135395  | 0.234478    |\n",
       "| 737 | 12649 |               0 | The men are outside | The men are outdoors.         | A man is a safety suit walking outside while another man in a dark suit walks into a building. |          0 |   0.983828 |  0.01457   | 0.00160161  |\n",
       "\n",
       "**Voting strategy results** \n",
       "\n",
       "We get 1 categories with votes. The most voted for category is **label 0** with 16\n",
       "votes. The paraphrase initially had label **0**.\n",
       "\n",
       "This does not flip the predicted label. \n",
       "\n",
       "If the theory is correct we expect the label does not flip for an unadversarial example.\n",
       " The label flip did not occur, so this was **successful**.\n",
       "\n",
       "\n",
       "\n",
       "Now we look at the variance and entropy of the predicted probabilities of each class. \n",
       "We are interested in class **0** as it is the label of the hypothesis paraphrase. \n",
       "\n",
       "*Entropy*  \n",
       "\n",
       "|     |   vm_prob0_entropy |   vm_prob1_entropy |   vm_prob2_entropy |   idx |   label_vm_para |   entropy_label_vm_para |\n",
       "|----:|-------------------:|-------------------:|-------------------:|------:|----------------:|------------------------:|\n",
       "| 736 |                0.7 |                1.3 |               0.23 | 12649 |               0 |                     0.7 |\n",
       "\n",
       "*Variance*   \n",
       "\n",
       "|     |   vm_prob0_var |   vm_prob1_var |   vm_prob2_var |   idx |   label_vm_para |   var_label_vm_para |\n",
       "|----:|---------------:|---------------:|---------------:|------:|----------------:|--------------------:|\n",
       "| 736 |           0.01 |              0 |              0 | 12649 |               0 |                0.01 |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Exploring the method via reporting ####\n",
    "\n",
    "## Set up parameters \n",
    "idx = train_advlbl_df.sample()[['idx']].values[0][0] #sample an index randomly from the table\n",
    "main_tbl = train_advlbl_df.query(\"idx==@idx\")\n",
    "def getval(cname): return  main_tbl.loc[:,cname].values[0]\n",
    "prem,hyp,para,sim_score = getval('premise'),getval('orig'),getval('para'),getval('sim_score')  \n",
    "label_true,label_vm_orig,label_vm_para = getval('label_true'),getval('label_vm_orig'),getval('label_vm_para')\n",
    "advlbl = getval('is_adversarial')\n",
    "d_advlbl2str = {\n",
    "    1: \"is a **successful** adversarial example\",\n",
    "    0: \"is **unsuccessful**: it flips the true label\",\n",
    "    -1: \"contains a hypothesis paraphrase that **doesn't make sense** or is nonsensical.\", \n",
    "    -2: \"is **excluded**: the original label might be wrong\"\n",
    "}\n",
    "advstr = d_advlbl2str[advlbl]\n",
    "perm_samples = advlbl_df.query(\"idx==@idx\").sample(5).to_markdown()\n",
    "ncats,top_cat,top_cat_votes = getval('n_cats_with_votes'),getval('top_cat'),getval('top_cat_votes')\n",
    "\n",
    "label_flip               = top_cat != label_vm_para\n",
    "label_flip_to_orig_label = top_cat == label_vm_orig\n",
    "label_flip_to_diff_label = top_cat != label_vm_para and top_cat != label_vm_orig\n",
    "\n",
    "results_msg = \"\"\n",
    "if not label_flip:           results_msg += \"This does not flip the predicted label. \\n\"\n",
    "if label_flip_to_orig_label: results_msg += \"This flips the label to the vm predicted label (\" +\\\n",
    "    str(label_vm_orig) + \") of the original hypothesis. \\n\"\n",
    "if label_flip_to_diff_label: results_msg += \"This flips the predicted label but to a different class to the vm prediction of the original hypothesis.\\n\"\n",
    "\n",
    "results_msg += \"\\n\"\n",
    "if  advlbl == 1:  \n",
    "    results_msg += \"If the theory is correct we expected a label flip for an adversarial example.\\n \"\n",
    "    if label_flip: results_msg +=  \"The label flip occured, so this was **successful**.\\n\"\n",
    "    else:          results_msg +=  \"The label flip did not occur, so this was **unsuccessful**.\\n\"   \n",
    "elif advlbl == 0:  \n",
    "    results_msg += \"If the theory is correct we expect the label does not flip for an unadversarial example.\\n \"\n",
    "    if label_flip: results_msg +=  \"The label flip occured, so this was **unsuccessful**.\\n\"\n",
    "    else:          results_msg +=  \"The label flip did not occur, so this was **successful**.\\n\"  \n",
    "elif advlbl == -1: \n",
    "    results_msg += \"The original paraphrase didn't make sense, so we should figure out how to detect this.\\n \"\n",
    "else: \n",
    "    results_msg += \"The SNLI example was wrong or strange: disregard this example.\\n\"\n",
    "\n",
    "## Insert into template \n",
    "Markdown(f\"\"\"\n",
    "Example with idx **{idx}**   \n",
    "\n",
    "{main_tbl.to_markdown(index=True)}   \n",
    "\n",
    "\n",
    "* **Premise**: `{prem}`  \n",
    "* **Hypothesis (original)**: `{hyp}` (True label **{label_true}**, Victim Model (VM) label **{label_vm_orig}**)    \n",
    "* **Hypothesis paraphrase**: `{para}` (VM label **{label_vm_para}**)     \n",
    "\n",
    "This example {advstr}.    \n",
    "\n",
    "We generate {n} further *permutations* of the hypothesis paraphrase and get VM votes and confidence for \n",
    "each of them. The label of the hypothesis paraphrase was **{label_vm_para}**. \n",
    "Here are five of these permutations (randomly chosen):  \n",
    "\n",
    "{perm_samples}\n",
    "\n",
    "**Voting strategy results** \n",
    "\n",
    "We get {ncats} categories with votes. The most voted for category is **label {top_cat}** with {top_cat_votes}\n",
    "votes. The paraphrase initially had label **{label_vm_para}**.\n",
    "\n",
    "{results_msg}\n",
    "\n",
    "\n",
    "Now we look at the variance and entropy of the predicted probabilities of each class. \n",
    "We are interested in class **{label_vm_para}** as it is the label of the hypothesis paraphrase. \n",
    "\n",
    "*Entropy*  \n",
    "\n",
    "{entropy_df.query(\"idx==@idx\").round(2).to_markdown(index=True)}\n",
    "\n",
    "*Variance*   \n",
    "\n",
    "{var_df.query(\"idx==@idx\").round(2).to_markdown(index=True)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculates performance of victim model on a dataloader\n",
    "\n",
    "# dl = valid_dl\n",
    "# metric = load_metric('accuracy')\n",
    "# for i, data in enumerate(dl): \n",
    "#     if i % 10 == 0 : print(i, \"out of\", len(dl)) \n",
    "#     labels,premise,hypothesis = data['label'].to(device),data[\"premise\"],data[\"hypothesis\"]\n",
    "#     inputs = vm_tokenizer(premise,hypothesis, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "#     inputs.to(device)\n",
    "#     outputs = vm_model(**inputs, labels=labels)\n",
    "#     probs = outputs.logits.softmax(1)\n",
    "#     preds = probs.argmax(1)\n",
    "#     metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "# metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score semantic similarity with cross encoders\n",
    "\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "# cross_encoder= CrossEncoder('cross-encoder/quora-distilroberta-base')\n",
    "# i =11\n",
    "# data = valid_small[i]\n",
    "# orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "# orig_rep = [orig for i in range(len(para))]\n",
    "# pairs = list(zip(orig_rep,para))\n",
    "# scores = cross_encoder.predict(pairs)\n",
    "# results_df = pd.DataFrame({'pairs':pairs, 'para': para,'score': cos_sim})\n",
    "# print(orig)\n",
    "# results_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with sentence transformers\n",
    "\n",
    "# valid_small_dl = DataLoader(valid_small, batch_size=4, shuffle=False, \n",
    "#                      num_workers=n_wkrs, pin_memory=True)\n",
    "# sim_score_l = []\n",
    "# for i, data in enumerate(valid_small_dl): \n",
    "#     pass\n",
    "#     orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "#     orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "# #     cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "# #     results_df = pd.DataFrame({'para': para,'score': cos_sim})\n",
    "# #     print(orig)\n",
    "# #     results_df.sort_values('score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
