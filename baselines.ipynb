{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, nltk, pandas as pd, torch\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import functools\n",
    "\n",
    "\n",
    "from textattack import Attack, AttackArgs,Attacker\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_recipes import AttackRecipe\n",
    "from textattack.search_methods import BeamSearch\n",
    "from textattack.constraints import Constraint\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.transformations import WordSwapEmbedding, WordSwapMaskedLM\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.metrics.attack_metrics.attack_success_rate import AttackSuccessRate\n",
    "from textattack.metrics.attack_metrics.words_perturbed import WordsPerturbed\n",
    "from textattack.metrics.attack_metrics.attack_queries import AttackQueries\n",
    "from textattack.metrics.quality_metrics.perplexity import Perplexity\n",
    "from textattack.metrics.quality_metrics.use import USEMetric\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "\n",
    "from travis_attack.utils import display_all, merge_dicts, append_df_to_csv\n",
    "from travis_attack.data import prep_dsd_rotten_tomatoes,prep_dsd_simple,prep_dsd_financial\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import _prepare_vm_tokenizer_and_model, get_vm_probs, prepare_models, get_nli_probs\n",
    "from fastcore.basics import in_jupyter\n",
    "\n",
    "path_baselines = \"./baselines/\"\n",
    "datetime_now = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook baselines.ipynb to python\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert \\\n",
    "    --TagRemovePreprocessor.enabled=True \\\n",
    "    --TagRemovePreprocessor.remove_cell_tags=\"['hide']\" \\\n",
    "    --TemplateExporter.exclude_markdown=True \\\n",
    "    --to python \"baselines.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_baselines_parser(): \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ds_name\")\n",
    "    parser.add_argument(\"--split\")\n",
    "    parser.add_argument(\"--attack_name\")\n",
    "    parser.add_argument(\"--num_examples\", type=int)\n",
    "    parser.add_argument(\"--beam_sz\", type=int)\n",
    "    parser.add_argument(\"--max_candidates\", type=int)\n",
    "    parser.add_argument(\"--sts_threshold\", type=float)\n",
    "    parser.add_argument(\"--contradiction_threshold\", type=float)\n",
    "    #parser.add_argument('args', nargs=argparse.REMAINDER)  # activate to put keywords in kwargs.\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CONFIG (default values) #########\n",
    "d = dict(\n",
    "    datetime=datetime_now,\n",
    "    ds_name = \"rotten_tomatoes\",\n",
    "    split = 'valid',\n",
    "    attack_name = 'BeamSearchCFEmbeddingAttack',\n",
    "    num_examples = -1,\n",
    "    beam_sz = 1,\n",
    "    max_candidates = 2,\n",
    "    sts_threshold = 0.9,\n",
    "    contradiction_threshold = 0.1\n",
    ")\n",
    "###########################################\n",
    "\n",
    "if not in_jupyter():  # override with any script options\n",
    "    parser = setup_baselines_parser()\n",
    "    newargs = vars(parser.parse_args())\n",
    "    for k,v in newargs.items(): \n",
    "        if v is not None: d[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StsScoreConstraint(Constraint): \n",
    "    def __init__(self, sts_model, sts_threshold): \n",
    "        super().__init__(True)  # need the true here to compare against original (as opposed to previous x') I think\n",
    "        self.sts_threshold = sts_threshold\n",
    "        self.sts_model     = sts_model\n",
    "        \n",
    "    @functools.lru_cache(maxsize=2**14)\n",
    "    def get_embedding(self, text):  return self.sts_model.encode(text)\n",
    "    \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig_embedding = self.get_embedding(current_text.text)\n",
    "        pp_embedding   = self.get_embedding(transformed_text.text)\n",
    "        sts_score = pytorch_cos_sim(orig_embedding, pp_embedding).item()\n",
    "        if sts_score > self.sts_threshold:   return True \n",
    "        else:                                return False\n",
    "        \n",
    "\n",
    "class ContradictionScoreConstraint(Constraint): \n",
    "    def __init__(self, cfg, nli_tokenizer, nli_model, contradiction_threshold): \n",
    "        super().__init__(True) \n",
    "        self.cfg = cfg \n",
    "        self.nli_tokenizer = nli_tokenizer\n",
    "        self.nli_model     = nli_model\n",
    "        self.contradiction_threshold = contradiction_threshold\n",
    "        \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig =     current_text.text\n",
    "        pp   = transformed_text.text\n",
    "        contradiction_score = get_nli_probs(orig, pp, self.cfg, self.nli_tokenizer, self.nli_model).cpu()[0][0].item()\n",
    "        if contradiction_score < self.contradiction_threshold:   return True \n",
    "        else:                                                    return False\n",
    "    \n",
    "\n",
    "class BeamSearchCFEmbeddingAttack(AttackRecipe):\n",
    "    \"\"\"Untarged classification + word embedding swap + [no repeat, no stopword, STS, contradiction score] constraints + beam search\"\"\"\n",
    "    @staticmethod\n",
    "    def build(model_wrapper, cfg,  sts_model, nli_tokenizer, nli_model, beam_sz=2, \n",
    "              max_candidates=5, sts_threshold=0.6, contradiction_threshold=0.8):\n",
    "        goal_function = UntargetedClassification(model_wrapper)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\") # The one used by default in textattack\n",
    "        constraints = [RepeatModification(),\n",
    "                       StopwordModification(stopwords), \n",
    "                       StsScoreConstraint(sts_model, sts_threshold), \n",
    "                       ContradictionScoreConstraint(cfg, nli_tokenizer, nli_model, contradiction_threshold)]\n",
    "        transformation = WordSwapEmbedding(max_candidates=max_candidates)\n",
    "        search_method = BeamSearch(beam_width=beam_sz)\n",
    "        attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "        return attack\n",
    "\n",
    "    \n",
    "class BeamSearchLMAttack(AttackRecipe): \n",
    "    \"\"\"Untarged classification + language model word swap + [no repeat, no stopword, STS, contradiction score] constraints + beam search\"\"\"\n",
    "    @staticmethod\n",
    "    def build(model_wrapper, cfg,  sts_model, nli_tokenizer, nli_model, beam_sz=2, \n",
    "              max_candidates=5, sts_threshold=0.6, contradiction_threshold=0.8):\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\") # The one used by default in textattack\n",
    "        goal_function = UntargetedClassification(model_wrapper)\n",
    "        constraints = [RepeatModification(),\n",
    "                       StopwordModification(stopwords), \n",
    "                       StsScoreConstraint(sts_model, sts_threshold), \n",
    "                       ContradictionScoreConstraint(cfg, nli_tokenizer, nli_model, contradiction_threshold)]\n",
    "        transformation = WordSwapMaskedLM(method='bae', masked_language_model='distilroberta-base', max_candidates=max_candidates)\n",
    "        search_method = BeamSearch(beam_width=beam_sz)\n",
    "        attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "        return attack\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if   d['attack_name'] == 'BeamSearchLMAttack':          attack_recipe = BeamSearchLMAttack\n",
    "elif d['attack_name'] == 'BeamSearchCFEmbeddingAttack': attack_recipe = BeamSearchCFEmbeddingAttack\n",
    "filename = f\"{path_baselines}{datetime_now}_{d['ds_name']}_{d['split']}_{d['attack_name']}_beam_sz={d['beam_sz']}_max_candidates={d['max_candidates']}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d['ds_name'] == \"financial_phrasebank\":\n",
    "    cfg = Config().adjust_config_for_financial_dataset()\n",
    "    dsd = prep_dsd_financial(cfg)\n",
    "elif d['ds_name'] == \"rotten_tomatoes\":      \n",
    "    cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "    dsd = prep_dsd_rotten_tomatoes(cfg)\n",
    "elif d['ds_name'] == \"simple\":      \n",
    "    cfg = Config().adjust_config_for_simple_dataset()\n",
    "    dsd = prep_dsd_simple(cfg)\n",
    "    #dataset = ...\n",
    "dataset = HuggingFaceDataset(dsd[d['split']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    }
   ],
   "source": [
    "vm_tokenizer, vm_model, _,_,_, sts_model, nli_tokenizer, nli_model, cfg = prepare_models(cfg)\n",
    "vm_model_wrapper = HuggingFaceModelWrapper(vm_model, vm_tokenizer)\n",
    "attack = attack_recipe.build(vm_model_wrapper, cfg,  sts_model, nli_tokenizer, nli_model,\n",
    "           beam_sz=d['beam_sz'], max_candidates=d['max_candidates'], sts_threshold=d['sts_threshold'],\n",
    "           contradiction_threshold=d['contradiction_threshold']\n",
    ")\n",
    "attack_args = AttackArgs(num_examples=d['num_examples'], enable_advance_metrics=True,\n",
    "                        log_to_csv=filename, csv_coloring_style='plain', disable_stdout=True)\n",
    "attacker = Attacker(attack, dataset, attack_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config for attack:\n",
      "{'datetime': '2022-04-20_132518', 'ds_name': 'rotten_tomatoes', 'split': 'valid', 'attack_name': 'BeamSearchCFEmbeddingAttack', 'num_examples': 4, 'beam_sz': 1, 'max_candidates': 1, 'sts_threshold': 0.6, 'contradiction_threshold': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(\"Current config for attack:\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path ./baselines/2022-04-20_132518_rotten_tomatoes_valid_BeamSearchCFEmbeddingAttack_beam_sz=1_max_candidates=1.csv\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): BeamSearch(\n",
      "    (beam_width):  1\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  1\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): StsScoreConstraint(\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): ContradictionScoreConstraint(\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): RepeatModification\n",
      "    (3): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 1      |\n",
      "| Number of failed attacks:     | 3      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 75.0%  |\n",
      "| Attack success rate:          | 25.0%  |\n",
      "| Average perturbed word %:     | 11.11% |\n",
      "| Average num. words per input: | 16.0   |\n",
      "| Avg num queries:              | 54.5   |\n",
      "| Average Original Perplexity:  | 55.91  |\n",
      "| Average Attack Perplexity:    | 619.99 |\n",
      "| Average Attack USE Score:     | 0.81   |\n",
      "+-------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "attack_results = attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_result_metrics = {\n",
    "    **AttackSuccessRate().calculate(attack_results), \n",
    "    **WordsPerturbed().calculate(attack_results),\n",
    "    **AttackQueries().calculate(attack_results),\n",
    "    **Perplexity().calculate(attack_results),\n",
    "    **USEMetric().calculate(attack_results)\n",
    "}\n",
    "attack_result_metrics.pop('num_words_changed_until_success')\n",
    "d = merge_dicts(d, attack_result_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example-specific metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_adv_example(df): \n",
    "    from IPython.core.display import display, HTML\n",
    "    pd.options.display.max_colwidth = 480 # increase column width so we can actually read the examples\n",
    "    #display(HTML(df[['original_text', 'perturbed_text']].to_html(escape=False)))\n",
    "    display(df[['original_text', 'perturbed_text']])\n",
    "\n",
    "def add_vm_score_and_label_flip(df, dataset, cfg, vm_tokenizer, vm_model): \n",
    "    truelabels = torch.tensor(dataset._dataset['label'], device =cfg.device)\n",
    "    orig_probs =  get_vm_probs(df['original_text'].tolist(), cfg, vm_tokenizer, vm_model, return_predclass=False)\n",
    "    pp_probs = get_vm_probs(df['perturbed_text'].tolist(), cfg, vm_tokenizer, vm_model, return_predclass=False)\n",
    "    orig_predclass = torch.argmax(orig_probs, axis=1)\n",
    "    pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "    orig_truelabel_probs = torch.gather(orig_probs, 1, truelabels[:,None]).squeeze()\n",
    "    pp_truelabel_probs   = torch.gather(pp_probs, 1,   truelabels[:,None]).squeeze()\n",
    "    pp_predclass_probs   = torch.gather(pp_probs, 1,   pp_predclass[ :,None]).squeeze()\n",
    "    \n",
    "    df['truelabel'] = truelabels.cpu().tolist()\n",
    "    df['orig_predclass'] = orig_predclass.cpu().tolist()\n",
    "    df['pp_predclass'] = pp_predclass.cpu().tolist()\n",
    "    df['orig_truelabel_probs'] = orig_truelabel_probs.cpu().tolist()\n",
    "    df['pp_truelabel_probs'] = pp_truelabel_probs.cpu().tolist()\n",
    "    df['vm_scores'] = (orig_truelabel_probs - pp_truelabel_probs).cpu().tolist()\n",
    "    df['label_flip'] = ((pp_predclass != truelabels) * 1).cpu().tolist()\n",
    "    return df\n",
    "\n",
    "def add_sts_score(df, sts_model, cfg): \n",
    "    orig_embeddings  = sts_model.encode(df['original_text'].tolist(),  convert_to_tensor=True, device=cfg.device)\n",
    "    pp_embeddings    = sts_model.encode(df['perturbed_text'].tolist(), convert_to_tensor=True, device=cfg.device)\n",
    "    df['sts_scores'] = pytorch_cos_sim(orig_embeddings, pp_embeddings).diagonal().cpu().tolist()\n",
    "    return df\n",
    "\n",
    "def add_contradiction_score(df, cfg, nli_tokenizer, nli_model): \n",
    "    contradiction_scores = get_nli_probs(df['original_text'].tolist(), df['perturbed_text'].tolist(), cfg, nli_tokenizer, nli_model)\n",
    "    df['contradiction_scores'] =  contradiction_scores[:,0].cpu().tolist()\n",
    "    return df \n",
    "\n",
    "def get_df_mean_cols(df): \n",
    "    cols = ['label_flip', 'vm_scores', 'sts_scores',\n",
    "            'contradiction_scores', 'sts_threshold_met', 'contradiction_threshold_met']\n",
    "    s = df[cols].mean()\n",
    "    s.index = [f\"{o}_mean\" for o in s.index]\n",
    "    return dict(s)\n",
    "\n",
    "def get_cts_summary_stats(df): \n",
    "    cols = ['vm_scores', 'sts_scores', 'contradiction_scores']\n",
    "    df_summary = df[cols].describe(percentiles=[.1,.25,.5,.75,.9]).loc[['std','10%','25%','50%','75%','90%']]\n",
    "    tmp_d = dict()\n",
    "    for c in cols: \n",
    "        s = df_summary[c]\n",
    "        s.index = [f\"{c}_{o}\" for o in s.index]\n",
    "        tmp_d = merge_dicts(tmp_d, dict(s))\n",
    "    return tmp_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b724b84d0221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#display_adv_example(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_vm_score_and_label_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvm_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result_type != 'Skipped'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_sts_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#filename1 = f\"/data/tproth/travis_attack/baselines/2022-04-21_044443_rotten_tomatoes_valid_BeamSearchLMAttack_beam_sz=2_max_candidates=5.csv\"\n",
    "#filename = filename1\n",
    "df = pd.read_csv(filename)\n",
    "#display_adv_example(df)\n",
    "df = add_vm_score_and_label_flip(df, dataset, cfg, vm_tokenizer, vm_model)\n",
    "df = df.query(\"result_type != 'Skipped'\")\n",
    "df = add_sts_score(df, sts_model, cfg)\n",
    "df = add_contradiction_score(df, cfg, nli_tokenizer, nli_model)\n",
    "\n",
    "df['sts_threshold_met'] = df['sts_scores'] > d['sts_threshold']\n",
    "df['contradiction_threshold_met'] = df['contradiction_scores'] < d['contradiction_threshold']\n",
    "df.to_csv(f\"{filename[:-4]}_processed.csv\", index=False)\n",
    "\n",
    "d = merge_dicts(d, get_df_mean_cols(df))\n",
    "d = merge_dicts(d, get_cts_summary_stats(df))\n",
    "\n",
    "summary_df = pd.Series(d).to_frame().T\n",
    "append_df_to_csv(summary_df, f\"{path_baselines}results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.sample(5)\n",
    "# orig_l = df1['original_text'].tolist()\n",
    "# pp_l = df1['perturbed_text'].tolist()\n",
    "# print(orig_l)\n",
    "# print(pp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for orig, adv in zip(df1['original_text'].tolist(), df1['perturbed_text'].tolist()): \n",
    "#     print(f\"{orig}{adv}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[104][['original_text', 'perturbed_text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename1 = f\"/data/tproth/travis_attack/baselines/2022-04-20_133329_rotten_tomatoes_valid_BeamSearchCFEmbeddingAttack_beam_sz=1_max_candidates=1_processed.csv\"\n",
    "#df = pd.read_csv(filename1)\n",
    "#display_all(df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.read_csv(f\"/data/tproth/travis_attack/baselines/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
