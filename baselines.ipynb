{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, nltk, pandas as pd, torch\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from textattack import Attack, AttackArgs,Attacker\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_recipes import AttackRecipe\n",
    "from textattack.search_methods import BeamSearch\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.transformations import WordSwapEmbedding, WordSwapMaskedLM\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.metrics.attack_metrics.attack_success_rate import AttackSuccessRate\n",
    "from textattack.metrics.attack_metrics.words_perturbed import WordsPerturbed\n",
    "from textattack.metrics.attack_metrics.attack_queries import AttackQueries\n",
    "from textattack.metrics.quality_metrics.perplexity import Perplexity\n",
    "from textattack.metrics.quality_metrics.use import USEMetric\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "\n",
    "from travis_attack.utils import display_all, merge_dicts, append_df_to_csv\n",
    "from travis_attack.data import prep_dsd_rotten_tomatoes,prep_dsd_simple,prep_dsd_financial\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import _prepare_vm_tokenizer_and_model, get_vm_probs, prepare_models, get_nli_probs\n",
    "from fastcore.basics import in_jupyter\n",
    "\n",
    "path_baselines = \"./baselines/\"\n",
    "datetime_now = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook baselines.ipynb to python\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert \\\n",
    "    --TagRemovePreprocessor.enabled=True \\\n",
    "    --TagRemovePreprocessor.remove_cell_tags=\"['hide']\" \\\n",
    "    --TemplateExporter.exclude_markdown=True \\\n",
    "    --to python \"baselines.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchCFEmbeddingAttack(AttackRecipe):\n",
    "    \"\"\"Untarged classification + word embedding swap + [no repeat, no stopword] constraints + beam search\"\"\"\n",
    "    @staticmethod\n",
    "    def build(model_wrapper, beam_sz=2, max_candidates=5):\n",
    "        goal_function = UntargetedClassification(model_wrapper)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\") # The one used by default in textattack\n",
    "        constraints = [RepeatModification(),\n",
    "                       StopwordModification(stopwords)]\n",
    "        transformation = WordSwapEmbedding(max_candidates=max_candidates)\n",
    "        search_method = BeamSearch(beam_width=beam_sz)\n",
    "        attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "        return attack\n",
    "\n",
    "class BeamSearchLMAttack(AttackRecipe): \n",
    "    \"\"\"\"\"\"\n",
    "    @staticmethod\n",
    "    def build(model_wrapper, beam_sz=2, max_candidates=5):\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\") # The one used by default in textattack\n",
    "        goal_function = UntargetedClassification(model_wrapper)\n",
    "        constraints = [RepeatModification(),\n",
    "                       StopwordModification()]\n",
    "        transformation = WordSwapMaskedLM(method='bae', masked_language_model='distilroberta-base', max_candidates=max_candidates)\n",
    "        search_method = BeamSearch(beam_width=beam_sz)\n",
    "        attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "        return attack\n",
    "\n",
    "def setup_baselines_parser(): \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ds_name\")\n",
    "    parser.add_argument(\"--split\")\n",
    "    parser.add_argument(\"--attack_recipe\")\n",
    "    parser.add_argument(\"--num_examples\", type=int)\n",
    "    parser.add_argument(\"--beam_sz\", type=int)\n",
    "    parser.add_argument(\"--max_candidates\", type=int)\n",
    "    parser.add_argument(\"--sts_threshold\", type=float)\n",
    "    parser.add_argument(\"--contradiction_threshold\", type=float)\n",
    "    #parser.add_argument('args', nargs=argparse.REMAINDER)  # activate to put keywords in kwargs.\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CONFIG (default values) #########\n",
    "d = dict(\n",
    "    datetime=datetime_now,\n",
    "    ds_name = \"rotten_tomatoes\",\n",
    "    split = 'valid',\n",
    "    attack_name = 'BeamSearchCFEmbeddingAttack',\n",
    "    num_examples = -1,\n",
    "    beam_sz = 1,\n",
    "    max_candidates = 1,\n",
    "    sts_threshold = 0.6,\n",
    "    contradiction_threshold = 0.8\n",
    ")\n",
    "###########################################\n",
    "\n",
    "if not in_jupyter():  # override with any script options\n",
    "    parser = setup_baselines_parser()\n",
    "    newargs = vars(parser.parse_args())\n",
    "    for k,v in newargs.items(): \n",
    "        if v is not None: d[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if   d['attack_name'] == 'BeamSearchLMAttack':          attack_recipe = BeamSearchLMAttack\n",
    "elif d['attack_name'] == 'BeamSearchCFEmbeddingAttack': attack_recipe = BeamSearchCFEmbeddingAttack\n",
    "filename = f\"{path_baselines}{datetime_now}_{d['ds_name']}_{d['split']}_{d['attack_name']}_beam_sz={d['beam_sz']}_max_candidates={d['max_candidates']}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (/data/tproth/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054bd58e1b2b4d4fbbe9ed592da3d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if d['ds_name'] == \"financial_phrasebank\":\n",
    "    cfg = Config().adjust_config_for_financial_dataset()\n",
    "    dsd = prep_dsd_financial(cfg)\n",
    "elif d['ds_name'] == \"rotten_tomatoes\":      \n",
    "    cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "    dsd = prep_dsd_rotten_tomatoes(cfg)\n",
    "elif d['ds_name'] == \"simple\":      \n",
    "    cfg = Config().adjust_config_for_simple_dataset()\n",
    "    dsd = prep_dsd_simple(cfg)\n",
    "    #dataset = ...\n",
    "dataset = HuggingFaceDataset(dsd[d['split']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    }
   ],
   "source": [
    "vm_tokenizer, vm_model, _,_, sts_model, nli_tokenizer, nli_model, cfg = prepare_models(cfg)\n",
    "vm_model_wrapper = HuggingFaceModelWrapper(vm_model, vm_tokenizer)\n",
    "attack = attack_recipe.build(vm_model_wrapper, d['beam_sz'], d['max_candidates'])\n",
    "attack_args = AttackArgs(num_examples=d['num_examples'], enable_advance_metrics=True,\n",
    "                        log_to_csv=filename, csv_coloring_style='plain', disable_stdout=True)\n",
    "attacker = Attacker(attack, dataset, attack_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current config for attack:\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path ./baselines/2022-04-19_141935_rotten_tomatoes_valid_BeamSearchCFEmbeddingAttack_beam_sz=1_max_candidates=1.csv\n",
      "  0%|          | 0/1066 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): BeamSearch(\n",
      "    (beam_width):  1\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  1\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "    (1): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 295 / 600 / 171 / 1066: 100%|██████████| 1066/1066 [04:14<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7483 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 295    |\n",
      "| Number of failed attacks:     | 600    |\n",
      "| Number of skipped attacks:    | 171    |\n",
      "| Original accuracy:            | 83.96% |\n",
      "| Accuracy under attack:        | 56.29% |\n",
      "| Attack success rate:          | 32.96% |\n",
      "| Average perturbed word %:     | 14.18% |\n",
      "| Average num. words per input: | 18.53  |\n",
      "| Avg num queries:              | 46.01  |\n",
      "| Average Original Perplexity:  | 71.48  |\n",
      "| Average Attack Perplexity:    | 114.77 |\n",
      "| Average Attack USE Score:     | 0.86   |\n",
      "+-------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "attack_results = attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7483 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "attack_result_metrics = {\n",
    "    **AttackSuccessRate().calculate(attack_results), \n",
    "    **WordsPerturbed().calculate(attack_results),\n",
    "    **AttackQueries().calculate(attack_results),\n",
    "    **Perplexity().calculate(attack_results),\n",
    "    **USEMetric().calculate(attack_results)\n",
    "}\n",
    "attack_result_metrics.pop('num_words_changed_until_success')\n",
    "d = merge_dicts(d, attack_result_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example-specific metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_adv_example(df): \n",
    "    from IPython.core.display import display, HTML\n",
    "    pd.options.display.max_colwidth = 480 # increase column width so we can actually read the examples\n",
    "    #display(HTML(df[['original_text', 'perturbed_text']].to_html(escape=False)))\n",
    "    display(df[['original_text', 'perturbed_text']])\n",
    "\n",
    "def add_vm_score_and_label_flip(df, dataset, cfg, vm_tokenizer, vm_model): \n",
    "    truelabels = torch.tensor(dataset._dataset['label'], device =cfg.device)\n",
    "    orig_probs =  get_vm_probs(df['original_text'].tolist(), cfg, vm_tokenizer, vm_model, return_predclass=False)\n",
    "    pp_probs = get_vm_probs(df['perturbed_text'].tolist(), cfg, vm_tokenizer, vm_model, return_predclass=False)\n",
    "    orig_predclass = torch.argmax(orig_probs, axis=1)\n",
    "    pp_predclass = torch.argmax(pp_probs, axis=1)\n",
    "    orig_truelabel_probs = torch.gather(orig_probs, 1, truelabels[:,None]).squeeze()\n",
    "    pp_truelabel_probs   = torch.gather(pp_probs, 1,   truelabels[:,None]).squeeze()\n",
    "    pp_predclass_probs   = torch.gather(pp_probs, 1,   pp_predclass[ :,None]).squeeze()\n",
    "    \n",
    "    df['truelabel'] = truelabels.cpu().tolist()\n",
    "    df['orig_predclass'] = orig_predclass.cpu().tolist()\n",
    "    df['pp_predclass'] = pp_predclass.cpu().tolist()\n",
    "    df['orig_truelabel_probs'] = orig_truelabel_probs.cpu().tolist()\n",
    "    df['pp_truelabel_probs'] = pp_truelabel_probs.cpu().tolist()\n",
    "    df['vm_scores'] = (orig_truelabel_probs - pp_truelabel_probs).cpu().tolist()\n",
    "    df['label_flip'] = ((pp_predclass != truelabels) * 1).cpu().tolist()\n",
    "    return df\n",
    "\n",
    "def add_sts_score(df, sts_model, cfg): \n",
    "    orig_embeddings  = sts_model.encode(df['original_text'].tolist(),  convert_to_tensor=True, device=cfg.device)\n",
    "    pp_embeddings    = sts_model.encode(df['perturbed_text'].tolist(), convert_to_tensor=True, device=cfg.device)\n",
    "    df['sts_scores'] = pytorch_cos_sim(orig_embeddings, pp_embeddings).diagonal().cpu().tolist()\n",
    "    return df\n",
    "\n",
    "def add_contradiction_score(df, cfg, nli_tokenizer, nli_model): \n",
    "    contradiction_scores = get_nli_probs(df['original_text'].tolist(), df['perturbed_text'].tolist(), cfg, nli_tokenizer, nli_model)\n",
    "    df['contradiction_scores'] =  contradiction_scores[:,0].cpu().tolist()\n",
    "    return df \n",
    "\n",
    "def get_df_mean_cols(df): \n",
    "    cols = ['label_flip', 'vm_scores', 'sts_scores',\n",
    "            'contradiction_scores', 'sts_threshold_met', 'contradiction_threshold_met']\n",
    "    s = df[cols].mean()\n",
    "    s.index = [f\"{o}_mean\" for o in s.index]\n",
    "    return dict(s)\n",
    "\n",
    "def get_cts_summary_stats(df): \n",
    "    cols = ['vm_scores', 'sts_scores', 'contradiction_scores']\n",
    "    df_summary = df[cols].describe(percentiles=[.1,.25,.5,.75,.9]).loc[['std','10%','25%','50%','75%','90%']]\n",
    "    tmp_d = dict()\n",
    "    for c in cols: \n",
    "        s = df_summary[c]\n",
    "        s.index = [f\"{c}_{o}\" for o in s.index]\n",
    "        tmp_d = merge_dicts(tmp_d, dict(s))\n",
    "    return tmp_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3377e86f6f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{filename[:-4]}_processed.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_df_mean_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_cts_summary_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tproth/travis_attack/travis_attack/utils.py\u001b[0m in \u001b[0;36mmerge_dicts\u001b[0;34m(d1, d2)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmerge_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m\"\"\"Merge the two dicts and return the result. Check first that there is no key overlap.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdisjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#filename1 = f\"/data/tproth/travis_attack/baselines/rotten_tomatoes_valid_BeamSearchLMAttack_beam_sz=5_max_candidates=25.csv\"\n",
    "#filename = filename1\n",
    "df = pd.read_csv(filename)\n",
    "#display_adv_example(df)\n",
    "df = add_vm_score_and_label_flip(df,dataset, cfg, vm_tokenizer, vm_model)\n",
    "df = df.query(\"result_type != 'Skipped'\")\n",
    "df = add_sts_score(df, sts_model, cfg)\n",
    "df = add_contradiction_score(df, cfg, nli_tokenizer, nli_model)\n",
    "\n",
    "df['sts_threshold_met'] = df['sts_scores'] > d['sts_threshold']\n",
    "df['contradiction_threshold_met'] = df['contradiction_scores'] < d['contradiction_threshold']\n",
    "df.to_csv(f\"{filename[:-4]}_processed.csv\", index=False)\n",
    "\n",
    "d = merge_dicts(d, get_df_mean_cols(df))\n",
    "d = merge_dicts(d, get_cts_summary_stats(df))\n",
    "\n",
    "summary_df = pd.Series(d).to_frame().T\n",
    "append_df_to_csv(summary_df, f\"{path_baselines}results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.sample(5)\n",
    "# orig_l = df1['original_text'].tolist()\n",
    "# pp_l = df1['perturbed_text'].tolist()\n",
    "# print(orig_l)\n",
    "# print(pp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for orig, adv in zip(df1['original_text'].tolist(), df1['perturbed_text'].tolist()): \n",
    "#     print(f\"{orig}{adv}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[104][['original_text', 'perturbed_text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_all(df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
