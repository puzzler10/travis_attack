{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUE sets: model will be trained on eval set, so you shouldn't also test on the eval set. The problem is that the labels are withheld for the test set. \n",
    "Start with SNLI. MultiNLI is a later option too. As is rotten_tomatoes. \n",
    "* Victim model performance on dataset train, valid, test set. (done, written code to measure it)\n",
    "* Create new paraphrased valid + test datasets (done a preliminary version on the valid set) \n",
    "* Measure victim model performance on paraphrased datasets (done. on vanilla valid set is about 87% accuracy. generating 16 paraphrases (i.e. not many) and evaluating performance on all of them, we get ~75% accuracy)\n",
    "* Get document embeddings of original and paraphrased and compare (done)\n",
    "  * https://github.com/UKPLab/sentence-transformers\n",
    "* Write a simple way to measure paraphrase quality (done) \n",
    "* Construct reward function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets, transformers\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pprint import pprint\n",
    "import numpy as np, pandas as pd\n",
    "import scipy\n",
    "from utils import *   # local script \n",
    "import pyarrow\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import seaborn as sns\n",
    "from itertools import repeat\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_cache = './cache/'\n",
    "path_results = \"./results/\"\n",
    "\n",
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "devicenum = torch.cuda.current_device() if device.type == 'cuda' else -1\n",
    "n_wkrs = 4 * torch.cuda.device_count()\n",
    "batch_size = 64\n",
    "pd.set_option(\"display.max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase model (para)\n",
    "para_name = \"tuner007/pegasus_paraphrase\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victim Model (VM)\n",
    "vm_name = \"textattack/distilbert-base-cased-snli\"\n",
    "vm_tokenizer = AutoTokenizer.from_pretrained(vm_name)\n",
    "vm_model = AutoModelForSequenceClassification.from_pretrained(vm_name).to(device)\n",
    "vm_idx2lbl = vm_model.config.id2label\n",
    "vm_lbl2idx = vm_model.config.label2id\n",
    "vm_num_labels = vm_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity model \n",
    "embedding_model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/data/tproth/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bd1e7a7b2b48a28f38c220b66aa692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184825bd9671479d9a53b7478b3e23cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a365214d714602b5bfb115d441cac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"snli\")\n",
    "train,valid,test = dataset['train'],dataset['validation'],dataset['test']\n",
    "\n",
    "label_cname = 'label'\n",
    "remove_minus1_labels = lambda x: x[label_cname] != -1\n",
    "train = train.filter(remove_minus1_labels)\n",
    "valid = valid.filter(remove_minus1_labels)\n",
    "test = test.filter(remove_minus1_labels)\n",
    "\n",
    "# make sure that all datasets have the same number of labels as what the victim model predicts\n",
    "assert train.features[label_cname].num_classes == vm_num_labels\n",
    "assert valid.features[label_cname].num_classes == vm_num_labels\n",
    "assert test.features[ label_cname].num_classes == vm_num_labels\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size, shuffle=True, num_workers=n_wkrs)\n",
    "test_dl = DataLoader( test,  batch_size=batch_size, shuffle=True, num_workers=n_wkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def get_paraphrases(input_text,num_return_sequences,num_beams, num_beam_groups=1,diversity_penalty=0):\n",
    "    batch = para_tokenizer(input_text,truncation=True,padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = para_model.generate(**batch,num_beams=num_beams, num_return_sequences=num_return_sequences, \n",
    "                                   temperature=1.5, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty)\n",
    "    tgt_text = para_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def gen_dataset_paraphrases(x, cname_input, cname_output, n_seed_seqs=32): \n",
    "    \"\"\" x: one row of a dataset. \n",
    "    cname_input: column to generate paraphrases for \n",
    "    cname_output: column name to give output of paraphrases \n",
    "    n_seed_seqs: rough indicator of how many paraphrases to return. \n",
    "            For now, keep at 4,8,16,32,64 etc\"\"\"\n",
    "    # TODO: figure out how to batch this. \n",
    "    if n_seed_seqs % 4 != 0: raise ValueError(\"keep n_seed_seqs divisible by 4 for now\")\n",
    "    n = n_seed_seqs/2\n",
    "    #low diversity (ld) paraphrases \n",
    "    ld_l = get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "                            num_beams=int(n))\n",
    "    #high diversity (hd) paraphrases. We can use num_beam_groups and diversity_penalty as hyperparameters. \n",
    "    hd_l =  get_paraphrases(x[cname_input],num_return_sequences=int(n),\n",
    "                            num_beams=int(n), num_beam_groups=int(n),diversity_penalty=50002.5)\n",
    "    l = ld_l + hd_l \n",
    "    x[cname_output] = l #TODO: change to list(set(l))             \n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate paraphrase dataset\n",
    "n_seed_seqs = 48\n",
    "date = '20210629'\n",
    "fname = path_cache + 'valid_small_'+ date + '_' + str(n_seed_seqs)\n",
    "if os.path.exists(fname):  # simple caching\n",
    "    valid_small = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    valid_small = valid.shard(20, 0, contiguous=True)\n",
    "    valid_small = valid_small.map(lambda x: gen_dataset_paraphrases(x, n_seed_seqs=n_seed_seqs,\n",
    "                        cname_input='hypothesis', cname_output='hypothesis_paraphrases'),\n",
    "                    batched=False)\n",
    "    valid_small.save_to_disk(fname)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a new version of paraphrase dataset by repeating all other fields to be same \n",
    "# length as number of paraphrases. \n",
    "def create_paraphrase_dataset(batch, l_cname): \n",
    "    \"\"\"Repeat the other fields to be the same length as the number of paraphrases.\n",
    "    l_cname: column name that contains the list of paraphrases\"\"\"    \n",
    "    return_d = defaultdict(list) \n",
    "    for o in zip(*batch.values()):\n",
    "        d = dict(zip(batch.keys(), o))\n",
    "        n_paraphrases = len(d[l_cname])\n",
    "        for k,v in d.items(): \n",
    "            return_d[k] += v if k == l_cname else [v for o in range(n_paraphrases)]\n",
    "    return return_d      \n",
    "\n",
    "fname = path_cache + 'valid_small_paraphrases_' + date + '_'+ str(n_seed_seqs)\n",
    "if os.path.exists(fname):     \n",
    "    valid_small_paraphrases = datasets.load_from_disk(fname)\n",
    "else:\n",
    "    # Need to call this with batched=True to work. \n",
    "    valid_small_paraphrases = valid_small.map(lambda x: create_paraphrase_dataset(x,\n",
    "                                                             l_cname='hypothesis_paraphrases'), \n",
    "                                              batched=True)\n",
    "    valid_small_paraphrases.save_to_disk(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Generate results dataframe \n",
    "def get_vm_scores(): \n",
    "    \"\"\"very hacky procedure to generate victim model scores \"\"\"\n",
    "    # Get preds and accuracy on the paraphrase dataset\n",
    "    print(\"Getting victim model scores.\")\n",
    "    some_dl = DataLoader(valid_small_paraphrases, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=n_wkrs, pin_memory=True)\n",
    "    dl = some_dl\n",
    "    metric = load_metric('accuracy')\n",
    "    para_probs_l,orig_probs_l = [], []\n",
    "    assert vm_model.training == False  # checks that model is in eval mode \n",
    "    #monitor = Monitor(2)  # track GPU usage and memory\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dl): \n",
    "            if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "            labels,premise = data['label'].to(device),data[\"premise\"]\n",
    "            paraphrases,orig = data[\"hypothesis_paraphrases\"],data[\"hypothesis\"]\n",
    "\n",
    "            # predictions for original\n",
    "            inputs = vm_tokenizer(premise,orig,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            orig_probs_l.append(probs.cpu())  \n",
    "\n",
    "            # predictions for paraphrases\n",
    "            inputs = vm_tokenizer(premise,paraphrases, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "            outputs = vm_model(**inputs, labels=labels)\n",
    "            probs = outputs.logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            para_probs_l.append(probs.cpu())\n",
    "            metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "    orig_probs_t, para_probs_t = torch.cat(orig_probs_l),torch.cat(para_probs_l)\n",
    "    #monitor.stop()\n",
    "    return para_probs_t, orig_probs_t\n",
    "\n",
    "def generate_sim_scores(): \n",
    "    \"\"\"Function to just loop and generate sim scores for each input\"\"\"\n",
    "    print(\"Getting similarity scores\")\n",
    "    sim_score_l = []\n",
    "    for i, data in enumerate(valid_small): \n",
    "        if i % 50 == 0 : print(i, \"out of\", len(valid_small))\n",
    "        orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "        orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "        cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "        sim_score_l.append(cos_sim)\n",
    "    sim_score_t = torch.cat(sim_score_l)\n",
    "    return sim_score_t\n",
    "\n",
    "fname = path_cache + 'results_df_'+ date + \"_\" + str(n_seed_seqs) + \".csv\"\n",
    "if os.path.exists(fname):\n",
    "    results_df = pd.read_csv(fname)\n",
    "else: \n",
    "    sim_score_t = generate_sim_scores()\n",
    "    para_probs_t, orig_probs_t = get_vm_scores()\n",
    "    vm_para_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],para_probs_t)])\n",
    "    vm_orig_scores = torch.tensor([r[idx] for idx,r in zip(valid_small_paraphrases['label'],orig_probs_t)])\n",
    "    \n",
    "    results_df = pd.DataFrame({'premise': valid_small_paraphrases['premise'],\n",
    "                  'orig': valid_small_paraphrases['hypothesis'],\n",
    "                  'para': valid_small_paraphrases['hypothesis_paraphrases'],\n",
    "                  'sim_score': sim_score_t,\n",
    "                  'label_true': valid_small_paraphrases['label'], \n",
    "                  'label_vm_orig': orig_probs_t.argmax(1),\n",
    "                  'label_vm_para': para_probs_t.argmax(1),\n",
    "                  'vm_orig_truelabel': vm_orig_scores,             \n",
    "                  'vm_para_truelabel': vm_para_scores,\n",
    "                  'vm_truelabel_change': vm_orig_scores - vm_para_scores,\n",
    "                  'vm_orig_class0': orig_probs_t[:,0], \n",
    "                  'vm_orig_class1': orig_probs_t[:,1], \n",
    "                  'vm_orig_class2': orig_probs_t[:,2],  \n",
    "                  'vm_para_class0': para_probs_t[:,0], \n",
    "                  'vm_para_class1': para_probs_t[:,1], \n",
    "                  'vm_para_class2': para_probs_t[:,2]     \n",
    "                  })\n",
    "    results_df['vm_truelabel_change_X_sim_score'] = results_df['vm_truelabel_change'] * results_df['sim_score']\n",
    "    results_df.to_csv(fname, index_label = 'idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation method to detect label flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take each example $Ex$ in the filtered set and generate paraphrases (e.g. 16) of it (or it might work better with a simple token-replacement strategy). Run each through the victim model (might be better with a different model, but still trained on dataset) and record predictions. Then tally up the label predictions (or maybe take average of the probabilities). Each prediction is a vote for the true label. \n",
    "\n",
    "Idea is that if $Ex$ changes ground truth label to class 4, then most of the paraphrases of $Ex$ will be of class 4 too. If $Ex$ is truly adversarial, then most of the paraphrases of $Ex$ are likely to be of the original class (or at least of other classes). So in other words: \n",
    "* if `is_adversarial = 1` then we expect most votes to be for other classes to `label_vm_para`. This means we expect more variance in the voting. If we take model confidence for the class of `label_vm_para` and work out entropy/variance, we expect it to be high. \n",
    "* if `is_adversarial = 0` then we expect most votes to be for the same class as `label_vm_para`. This means we expect less variance in the voting. If we take model confidence for the class of `label_vm_para` and work out entropy/variance, we expect it to be low. \n",
    "\n",
    "Variations \n",
    "\n",
    "* Instead of generating further paraphrases for all label flippers, try the checklist tests on the input. e.g. replace number/proper noun\n",
    "* Try systematic perturbations\n",
    "* Record probability of the true class or the predicted class and put it into a distribution. Calculate entropy of it (STRIP style). The idea is that there is some reliable difference in these probabilities between ground-truth flips and otherwise and that entropy can be used as a rough measurement to distinguish between it. \n",
    "* Can try the above while keeping track of sentence embeddings + attention layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ebc62bd8d2fb84e0\n",
      "Reusing dataset csv (/data/tproth/.cache/huggingface/datasets/csv/default-ebc62bd8d2fb84e0/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# Read in manually labelled data. This is to track results. \n",
    "fname = path_cache + 'results_df_48_20210514_labelled_subset.csv'\n",
    "dset_advlbl = load_dataset('csv', data_files=fname)['train'].train_test_split(test_size=0.25)\n",
    "train_advlbl,test_advlbl = dset_advlbl['train'],dset_advlbl['test']\n",
    "\n",
    "# # as pandas df\n",
    "# df_advlbl = pd.read_csv(fname)\n",
    "# train_advlbl,_,test_advlbl = create_train_valid_test(df_advlbl, frac_train=0.75, frac_valid = 0.001)\n",
    "# # To join with the original. (might be some issues with the idx/row-number col)\n",
    "# # x = pd.merge(results_df, df_advlbl, on =['idx', 'premise','orig', 'para'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paraphrases of paraphrases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp dataset -> gen_paraphrases (returns dataset) -> create_paraphrase_dataset -> get vm labels -> save in data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f951f5c8b7448c87b57ec2ad4dd3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac394a5e073b43af8540a9be5684508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 48\n",
    "cols_to_drop = ['is_adversarial','label_true','label_vm_orig','orig','sim_score']\n",
    "def paraphrase_and_return_dict(x, n_seed_seqs=16): \n",
    "    x['perms'] = get_paraphrases(x['para'], num_return_sequences=n, num_beams=n, \n",
    "                                num_beam_groups=8, diversity_penalty=100000.0)\n",
    "    return x \n",
    "train_advlbl_perms = train_advlbl.map(lambda x: paraphrase_and_return_dict(x, n_seed_seqs=n),\n",
    "                  batched=False, remove_columns = cols_to_drop)\n",
    "train_advlbl_expanded = train_advlbl_perms.map(lambda x: create_paraphrase_dataset(x, l_cname='perms'),\n",
    "                           batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 149\n",
      "50 out of 149\n",
      "100 out of 149\n"
     ]
    }
   ],
   "source": [
    "# Get victim model predictions for each prediction  \n",
    "advlbl_expanded_dl = DataLoader(train_advlbl_expanded, batch_size=batch_size, shuffle=False, \n",
    "                                num_workers=n_wkrs, pin_memory=True)\n",
    "dl = advlbl_expanded_dl\n",
    "probs_l = []\n",
    "assert vm_model.training == False  # checks that model is in eval mode \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dl): \n",
    "        if i % 50 == 0 : print(i, \"out of\", len(dl))\n",
    "        premise,perms = data[\"premise\"],data[\"perms\"]\n",
    "        # predictions for original\n",
    "        inputs = vm_tokenizer(premise,perms,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        outputs = vm_model(**inputs)\n",
    "        probs = outputs.logits.softmax(1)\n",
    "        # preds = probs.argmax(1)\n",
    "        probs_l.append(probs.cpu()) \n",
    "\n",
    "probs_t = torch.cat(probs_l)\n",
    "preds_t = torch.argmax(probs_t,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back to original\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_label', preds_t.tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob0', probs_t[:,0].tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob1', probs_t[:,1].tolist())\n",
    "train_advlbl_expanded = train_advlbl_expanded.add_column('vm_prob2', probs_t[:,2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make into pandas_df \n",
    "advlbl_df = pd.DataFrame(train_advlbl_expanded) \n",
    "advlbl_df.vm_label = advlbl_df.vm_label.astype('category')\n",
    "\n",
    "# Count \"votes\" of each set of permutations \n",
    "votes_df = advlbl_df.groupby(['idx'])['vm_label'].describe()\n",
    "votes_df = votes_df.rename(columns={'count':'votes','unique': \"n_cats_with_votes\",\n",
    "                                     \"top\": 'top_cat', 'freq': 'top_cat_votes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get entropy and variance from each set of permutations, then choose only the values\n",
    "# that correspond to the predicted label of the paraphrase\n",
    "def get_entropy(x, bins=10): \n",
    "    \"\"\"Return shannon entropy of a vector. Used in pandas summary functions\"\"\"\n",
    "    # the bins parameters affects the entropy quite a bit (it introduces zeros)\n",
    "    hist,_ = np.histogram(x, bins=bins)  \n",
    "    hist = hist/sum(hist)  # turn into PMF (not strictly required for scipy entropy, but easier to interpret)\n",
    "    return scipy.stats.entropy(hist)\n",
    "grp = advlbl_df.groupby(['idx'])[['vm_prob0','vm_prob1','vm_prob2']]\n",
    "entropy_df = grp.agg(func = get_entropy)\n",
    "var_df     = grp.agg(func = 'var')\n",
    "entropy_df.columns = [o + \"_entropy\" for o in entropy_df.columns]\n",
    "var_df.columns     = [o + \"_var\"     for o in var_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "label_df =  advlbl_df[['idx','label_vm_para']].drop_duplicates()\n",
    "def choose_col_of_df_from_label_column(df, labeldf, name='entropy'): \n",
    "    \"\"\"Picks columns of df corresponding to the predicted vm label of the paraphrase. \n",
    "    Works only if probs of classes are the first columns of df in order.\"\"\"\n",
    "    df = df.merge(labeldf,left_index=True, right_on='idx')\n",
    "    v = df['label_vm_para'].values\n",
    "    # See https://stackoverflow.com/a/61234228/5381490\n",
    "    df[name+'_label_vm_para'] = np.take_along_axis(df.values, v[:,None] ,axis=1)\n",
    "    return df \n",
    "entropy_df = choose_col_of_df_from_label_column(entropy_df, label_df, name='entropy')\n",
    "var_df     = choose_col_of_df_from_label_column(var_df,     label_df, name='var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Change original labelled set to a pandas data frame and merge it in \n",
    "train_advlbl_df,test_advlbl_df = pd.DataFrame(dset_advlbl['train']),pd.DataFrame(dset_advlbl['test'])\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, votes_df, left_on ='idx', right_index=True)\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, entropy_df[['idx','entropy_label_vm_para']], \n",
    "                           left_on ='idx', right_on='idx')\n",
    "train_advlbl_df = pd.merge(train_advlbl_df, var_df[['idx', 'var_label_vm_para']], \n",
    "                           left_on ='idx', right_on='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_adversarial</th>\n",
       "      <th>-2</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_flip</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>44</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>102</td>\n",
       "      <td>68</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_adversarial  -2  -1    0   1  All\n",
       "label_flip                          \n",
       "False            2  15   91  44  152\n",
       "True             2   9   11  24   46\n",
       "All              4  24  102  68  198"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label flip percentage and measure success\n",
    "train_advlbl_df['label_flip'] = train_advlbl_df['top_cat'] != train_advlbl_df['label_vm_para'] \n",
    "def permutation_success(x,y): \n",
    "    result = None\n",
    "    if   x == 1 and y == True:   result = True\n",
    "    elif x == 0 and y == False:  result = True\n",
    "    elif x == -1 or x == -2:     result = \"To be determined\"\n",
    "    else:                        result = False\n",
    "    return result\n",
    "v1,v2 = train_advlbl_df['is_adversarial'].values, train_advlbl_df['label_flip'].values\n",
    "train_advlbl_df['permutation_success'] = list(map(permutation_success, v1,v2))\n",
    "\n",
    "pd.crosstab(index=train_advlbl_df['label_flip'], \n",
    "                             columns=train_advlbl_df['is_adversarial'],\n",
    "                             margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    152\n",
       "True      46\n",
       "Name: label_flip, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_advlbl_df.label_flip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label_vm_para</th>\n",
       "      <th>para</th>\n",
       "      <th>perms</th>\n",
       "      <th>premise</th>\n",
       "      <th>vm_label</th>\n",
       "      <th>vm_prob0</th>\n",
       "      <th>vm_prob1</th>\n",
       "      <th>vm_prob2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969092</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.012004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman isn't awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928273</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.034710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>The woman is not awake.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963114</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.013559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is asleep.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.997646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7299</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman is not awake.</td>\n",
       "      <td>A woman is sleeping.</td>\n",
       "      <td>A woman is talking on the phone while standing next to a dog.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.998818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9499</th>\n",
       "      <td>19904</td>\n",
       "      <td>1</td>\n",
       "      <td>bricks are used in the Building</td>\n",
       "      <td>a brick building is made of brick</td>\n",
       "      <td>A large group of people are gathered outside of a brick building lit with spotlights.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>0.107462</td>\n",
       "      <td>0.074324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>19904</td>\n",
       "      <td>1</td>\n",
       "      <td>bricks are used in the Building</td>\n",
       "      <td>a brick building is utilized</td>\n",
       "      <td>A large group of people are gathered outside of a brick building lit with spotlights.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.918170</td>\n",
       "      <td>0.077881</td>\n",
       "      <td>0.003949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9501</th>\n",
       "      <td>19904</td>\n",
       "      <td>1</td>\n",
       "      <td>bricks are used in the Building</td>\n",
       "      <td>More than one item can be found inside The Building</td>\n",
       "      <td>A large group of people are gathered outside of a brick building lit with spotlights.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852742</td>\n",
       "      <td>0.051356</td>\n",
       "      <td>0.095902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9502</th>\n",
       "      <td>19904</td>\n",
       "      <td>1</td>\n",
       "      <td>bricks are used in the Building</td>\n",
       "      <td>More than one item can be found insidethe Building</td>\n",
       "      <td>A large group of people are gathered outside of a brick building lit with spotlights.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902995</td>\n",
       "      <td>0.029699</td>\n",
       "      <td>0.067306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>19904</td>\n",
       "      <td>1</td>\n",
       "      <td>bricks are used in the Building</td>\n",
       "      <td>More than one item can be found inside this Building</td>\n",
       "      <td>A large group of people are gathered outside of a brick building lit with spotlights.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.864814</td>\n",
       "      <td>0.041641</td>\n",
       "      <td>0.093545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9504 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx  label_vm_para                             para  \\\n",
       "0      7299              0            A woman is not awake.   \n",
       "1      7299              0            A woman is not awake.   \n",
       "2      7299              0            A woman is not awake.   \n",
       "3      7299              0            A woman is not awake.   \n",
       "4      7299              0            A woman is not awake.   \n",
       "...     ...            ...                              ...   \n",
       "9499  19904              1  bricks are used in the Building   \n",
       "9500  19904              1  bricks are used in the Building   \n",
       "9501  19904              1  bricks are used in the Building   \n",
       "9502  19904              1  bricks are used in the Building   \n",
       "9503  19904              1  bricks are used in the Building   \n",
       "\n",
       "                                                     perms  \\\n",
       "0                                    A woman is not awake.   \n",
       "1                                     A woman isn't awake.   \n",
       "2                                  The woman is not awake.   \n",
       "3                                       A woman is asleep.   \n",
       "4                                     A woman is sleeping.   \n",
       "...                                                    ...   \n",
       "9499                     a brick building is made of brick   \n",
       "9500                          a brick building is utilized   \n",
       "9501   More than one item can be found inside The Building   \n",
       "9502    More than one item can be found insidethe Building   \n",
       "9503  More than one item can be found inside this Building   \n",
       "\n",
       "                                                                                    premise  \\\n",
       "0                             A woman is talking on the phone while standing next to a dog.   \n",
       "1                             A woman is talking on the phone while standing next to a dog.   \n",
       "2                             A woman is talking on the phone while standing next to a dog.   \n",
       "3                             A woman is talking on the phone while standing next to a dog.   \n",
       "4                             A woman is talking on the phone while standing next to a dog.   \n",
       "...                                                                                     ...   \n",
       "9499  A large group of people are gathered outside of a brick building lit with spotlights.   \n",
       "9500  A large group of people are gathered outside of a brick building lit with spotlights.   \n",
       "9501  A large group of people are gathered outside of a brick building lit with spotlights.   \n",
       "9502  A large group of people are gathered outside of a brick building lit with spotlights.   \n",
       "9503  A large group of people are gathered outside of a brick building lit with spotlights.   \n",
       "\n",
       "     vm_label  vm_prob0  vm_prob1  vm_prob2  \n",
       "0           0  0.969092  0.018904  0.012004  \n",
       "1           0  0.928273  0.037017  0.034710  \n",
       "2           0  0.963114  0.023327  0.013559  \n",
       "3           2  0.000436  0.001917  0.997646  \n",
       "4           2  0.000251  0.000932  0.998818  \n",
       "...       ...       ...       ...       ...  \n",
       "9499        0  0.818214  0.107462  0.074324  \n",
       "9500        0  0.918170  0.077881  0.003949  \n",
       "9501        0  0.852742  0.051356  0.095902  \n",
       "9502        0  0.902995  0.029699  0.067306  \n",
       "9503        0  0.864814  0.041641  0.093545  \n",
       "\n",
       "[9504 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advlbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Example with idx **21096**   \n",
       "\n",
       "|    |   idx | premise                            | orig                        | para                      |   sim_score |   label_true |   label_vm_orig |   label_vm_para |   is_adversarial |   votes |   n_cats_with_votes |   top_cat |   top_cat_votes |   entropy_label_vm_para |   var_label_vm_para | label_flip   | permutation_success   |\n",
       "|---:|------:|:-----------------------------------|:----------------------------|:--------------------------|------------:|-------------:|----------------:|----------------:|-----------------:|--------:|--------------------:|----------:|----------------:|------------------------:|--------------------:|:-------------|:----------------------|\n",
       "| 57 | 21096 | a man is swimming inside of a pool | there is a person drowning. | A person is in the water. |    0.641395 |            2 |               2 |               0 |                0 |      48 |                   3 |         0 |              41 |                 1.27239 |           0.0831694 | False        | True                  |   \n",
       "\n",
       "\n",
       "* **Premise**: `a man is swimming inside of a pool`  \n",
       "* **Hypothesis (original)**: `there is a person drowning.` (True label **2**, Victim Model (VM) label **2**)    \n",
       "* **Hypothesis paraphrase**: `A person is in the water.` (VM label **0**)     \n",
       "\n",
       "This example is **unsuccessful**: it flips the true label.    \n",
       "\n",
       "We generate 48 further *permutations* of the hypothesis paraphrase and get VM votes and confidence for \n",
       "each of them. The label of the hypothesis paraphrase was **0**. \n",
       "Here are five of these permutations (randomly chosen):  \n",
       "\n",
       "|      |   idx |   label_vm_para | para                      | perms                                | premise                            |   vm_label |   vm_prob0 |   vm_prob1 |   vm_prob2 |\n",
       "|-----:|------:|----------------:|:--------------------------|:-------------------------------------|:-----------------------------------|-----------:|-----------:|-----------:|-----------:|\n",
       "| 2783 | 21096 |               0 | A person is in the water. | water, person                        | a man is swimming inside of a pool |          1 |   0.351933 |  0.568916  | 0.0791504  |\n",
       "| 2757 | 21096 |               0 | A person is in the water. | People in the water                  | a man is swimming inside of a pool |          0 |   0.919389 |  0.0726834 | 0.00792719 |\n",
       "| 2744 | 21096 |               0 | A person is in the water. | A person is in the water             | a man is swimming inside of a pool |          0 |   0.946072 |  0.0462132 | 0.00771503 |\n",
       "| 2745 | 21096 |               0 | A person is in the water. | There is a person in a body of water | a man is swimming inside of a pool |          0 |   0.984732 |  0.0134389 | 0.00182862 |\n",
       "| 2739 | 21096 |               0 | A person is in the water. | The person is wet.                   | a man is swimming inside of a pool |          0 |   0.917546 |  0.0812952 | 0.00115936 |\n",
       "\n",
       "**Voting strategy results** \n",
       "\n",
       "We get 3 categories with votes. The most voted for category is **label 0** with 41\n",
       "votes. The paraphrase initially had label **0**.\n",
       "\n",
       "This does not flip the predicted label. \n",
       "\n",
       "If the theory is correct we expect the label does not flip for an unadversarial example.\n",
       " The label flip did not occur, so this was **successful**.\n",
       "\n",
       "\n",
       "\n",
       "Now we look at the variance and entropy of the predicted probabilities of each class. \n",
       "We are interested in class **0** as it is the label of the hypothesis paraphrase. \n",
       "\n",
       "*Entropy*  \n",
       "\n",
       "|      |   vm_prob0_entropy |   vm_prob1_entropy |   vm_prob2_entropy |   idx |   label_vm_para |   entropy_label_vm_para |\n",
       "|-----:|-------------------:|-------------------:|-------------------:|------:|----------------:|------------------------:|\n",
       "| 2736 |               1.27 |               1.14 |               0.47 | 21096 |               0 |                    1.27 |\n",
       "\n",
       "*Variance*   \n",
       "\n",
       "|      |   vm_prob0_var |   vm_prob1_var |   vm_prob2_var |   idx |   label_vm_para |   var_label_vm_para |\n",
       "|-----:|---------------:|---------------:|---------------:|------:|----------------:|--------------------:|\n",
       "| 2736 |           0.08 |           0.04 |           0.03 | 21096 |               0 |                0.08 |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Exploring the method via reporting ####\n",
    "\n",
    "## Set up parameters \n",
    "idx = train_advlbl_df.sample()[['idx']].values[0][0] #sample an index randomly from the table\n",
    "main_tbl = train_advlbl_df.query(\"idx==@idx\")\n",
    "def getval(cname): return  main_tbl.loc[:,cname].values[0]\n",
    "prem,hyp,para,sim_score = getval('premise'),getval('orig'),getval('para'),getval('sim_score')  \n",
    "label_true,label_vm_orig,label_vm_para = getval('label_true'),getval('label_vm_orig'),getval('label_vm_para')\n",
    "advlbl = getval('is_adversarial')\n",
    "d_advlbl2str = {\n",
    "    1: \"is a **successful** adversarial example\",\n",
    "    0: \"is **unsuccessful**: it flips the true label\",\n",
    "    -1: \"contains a hypothesis paraphrase that **doesn't make sense** or is nonsensical.\", \n",
    "    -2: \"is **excluded**: the original label might be wrong\"\n",
    "}\n",
    "advstr = d_advlbl2str[advlbl]\n",
    "perm_samples = advlbl_df.query(\"idx==@idx\").sample(5).to_markdown()\n",
    "ncats,top_cat,top_cat_votes = getval('n_cats_with_votes'),getval('top_cat'),getval('top_cat_votes')\n",
    "\n",
    "label_flip               = top_cat != label_vm_para\n",
    "label_flip_to_orig_label = top_cat == label_vm_orig\n",
    "label_flip_to_diff_label = top_cat != label_vm_para and top_cat != label_vm_orig\n",
    "\n",
    "results_msg = \"\"\n",
    "if not label_flip:           results_msg += \"This does not flip the predicted label. \\n\"\n",
    "if label_flip_to_orig_label: results_msg += \"This flips the label to the vm predicted label (\" +\\\n",
    "    str(label_vm_orig) + \") of the original hypothesis. \\n\"\n",
    "if label_flip_to_diff_label: results_msg += \"This flips the predicted label but to a different class to the vm prediction of the original hypothesis.\\n\"\n",
    "\n",
    "results_msg += \"\\n\"\n",
    "if  advlbl == 1:  \n",
    "    results_msg += \"If the theory is correct we expected a label flip for an adversarial example.\\n \"\n",
    "    if label_flip: results_msg +=  \"The label flip occured, so this was **successful**.\\n\"\n",
    "    else:          results_msg +=  \"The label flip did not occur, so this was **unsuccessful**.\\n\"   \n",
    "elif advlbl == 0:  \n",
    "    results_msg += \"If the theory is correct we expect the label does not flip for an unadversarial example.\\n \"\n",
    "    if label_flip: results_msg +=  \"The label flip occured, so this was **unsuccessful**.\\n\"\n",
    "    else:          results_msg +=  \"The label flip did not occur, so this was **successful**.\\n\"  \n",
    "elif advlbl == -1: \n",
    "    results_msg += \"The original paraphrase didn't make sense, so we should figure out how to detect this.\\n \"\n",
    "else: \n",
    "    results_msg += \"The SNLI example was wrong or strange: disregard this example.\\n\"\n",
    "\n",
    "## Insert into template \n",
    "Markdown(f\"\"\"\n",
    "Example with idx **{idx}**   \n",
    "\n",
    "{main_tbl.to_markdown(index=True)}   \n",
    "\n",
    "\n",
    "* **Premise**: `{prem}`  \n",
    "* **Hypothesis (original)**: `{hyp}` (True label **{label_true}**, Victim Model (VM) label **{label_vm_orig}**)    \n",
    "* **Hypothesis paraphrase**: `{para}` (VM label **{label_vm_para}**)     \n",
    "\n",
    "This example {advstr}.    \n",
    "\n",
    "We generate {n} further *permutations* of the hypothesis paraphrase and get VM votes and confidence for \n",
    "each of them. The label of the hypothesis paraphrase was **{label_vm_para}**. \n",
    "Here are five of these permutations (randomly chosen):  \n",
    "\n",
    "{perm_samples}\n",
    "\n",
    "**Voting strategy results** \n",
    "\n",
    "We get {ncats} categories with votes. The most voted for category is **label {top_cat}** with {top_cat_votes}\n",
    "votes. The paraphrase initially had label **{label_vm_para}**.\n",
    "\n",
    "{results_msg}\n",
    "\n",
    "\n",
    "Now we look at the variance and entropy of the predicted probabilities of each class. \n",
    "We are interested in class **{label_vm_para}** as it is the label of the hypothesis paraphrase. \n",
    "\n",
    "*Entropy*  \n",
    "\n",
    "{entropy_df.query(\"idx==@idx\").round(2).to_markdown(index=True)}\n",
    "\n",
    "*Variance*   \n",
    "\n",
    "{var_df.query(\"idx==@idx\").round(2).to_markdown(index=True)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculates performance of victim model on a dataloader\n",
    "\n",
    "# dl = valid_dl\n",
    "# metric = load_metric('accuracy')\n",
    "# for i, data in enumerate(dl): \n",
    "#     if i % 10 == 0 : print(i, \"out of\", len(dl)) \n",
    "#     labels,premise,hypothesis = data['label'].to(device),data[\"premise\"],data[\"hypothesis\"]\n",
    "#     inputs = vm_tokenizer(premise,hypothesis, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "#     inputs.to(device)\n",
    "#     outputs = vm_model(**inputs, labels=labels)\n",
    "#     probs = outputs.logits.softmax(1)\n",
    "#     preds = probs.argmax(1)\n",
    "#     metric.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "# metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score semantic similarity with cross encoders\n",
    "\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "# cross_encoder= CrossEncoder('cross-encoder/quora-distilroberta-base')\n",
    "# i =11\n",
    "# data = valid_small[i]\n",
    "# orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "# orig_rep = [orig for i in range(len(para))]\n",
    "# pairs = list(zip(orig_rep,para))\n",
    "# scores = cross_encoder.predict(pairs)\n",
    "# results_df = pd.DataFrame({'pairs':pairs, 'para': para,'score': cos_sim})\n",
    "# print(orig)\n",
    "# results_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with sentence transformers\n",
    "\n",
    "# valid_small_dl = DataLoader(valid_small, batch_size=4, shuffle=False, \n",
    "#                      num_workers=n_wkrs, pin_memory=True)\n",
    "# sim_score_l = []\n",
    "# for i, data in enumerate(valid_small_dl): \n",
    "#     pass\n",
    "#     orig, para = data['hypothesis'], data['hypothesis_paraphrases']\n",
    "#     orig_emb,para_emb  = embedding_model.encode(orig),embedding_model.encode(para)\n",
    "# #     cos_sim = util.cos_sim(orig_emb,para_emb)[0]\n",
    "# #     results_df = pd.DataFrame({'para': para,'score': cos_sim})\n",
    "# #     print(orig)\n",
    "# #     results_df.sort_values('score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
