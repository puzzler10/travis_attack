{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp baseline_attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools, string, nltk, torch, numpy as np, pandas as pd, transformers\n",
    "from functools import partial\n",
    "import argparse\n",
    "\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "\n",
    "from textattack.constraints import Constraint\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.transformations import WordSwapEmbedding, WordSwapMaskedLM\n",
    "from textattack import Attack, AttackArgs,Attacker\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.loggers import CSVLogger # tracks a dataframe for us.\n",
    "from textattack.attack_recipes import AttackRecipe\n",
    "from textattack.search_methods import BeamSearch, ImprovedGeneticAlgorithm\n",
    "from textattack.constraints import Constraint\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.transformations import CompositeTransformation, WordSwapEmbedding, WordSwapMaskedLM\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "\n",
    "from textattack.transformations.word_insertions import WordInsertionMaskedLM\n",
    "from textattack.transformations.word_merges import  WordMergeMaskedLM\n",
    "\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "\n",
    "from travis_attack.models import _prepare_vm_tokenizer_and_model, get_vm_probs, prepare_models, get_nli_probs, get_cola_probs\n",
    "from travis_attack.utils import display_all, merge_dicts, append_df_to_csv, set_seed\n",
    "from travis_attack.data import prep_dsd_rotten_tomatoes,prep_dsd_simple,prep_dsd_financial, ProcessedDataset\n",
    "\n",
    "from travis_attack.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_baselines_parser(): \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ds_name\")\n",
    "    parser.add_argument(\"--split\")\n",
    "    parser.add_argument(\"--attack_name\")\n",
    "    parser.add_argument(\"--num_examples\", type=int)\n",
    "    parser.add_argument(\"--beam_sz\", type=int)\n",
    "    parser.add_argument(\"--max_candidates\", type=int)\n",
    "    parser.add_argument(\"--sts_threshold\", type=float)\n",
    "    parser.add_argument(\"--contradiction_threshold\", type=float)\n",
    "    #parser.add_argument('args', nargs=argparse.REMAINDER)  # activate to put keywords in kwargs.\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StsScoreConstraint(Constraint): \n",
    "    def __init__(self, sts_model, sts_threshold): \n",
    "        super().__init__(True)  # need the true here to compare against original (as opposed to previous x') I think\n",
    "        self.sts_threshold = sts_threshold\n",
    "        self.sts_model     = sts_model\n",
    "        \n",
    "    @functools.lru_cache(maxsize=2**14)\n",
    "    def get_embedding(self, text):  return self.sts_model.encode(text)\n",
    "    \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig_embedding = self.get_embedding(current_text.text)\n",
    "        pp_embedding   = self.get_embedding(transformed_text.text)\n",
    "        sts_score = pytorch_cos_sim(orig_embedding, pp_embedding).item()\n",
    "        if sts_score > self.sts_threshold:   return True \n",
    "        else:                                return False\n",
    "        \n",
    "\n",
    "class ContradictionScoreConstraint(Constraint): \n",
    "    def __init__(self, cfg, nli_tokenizer, nli_model, contradiction_threshold): \n",
    "        super().__init__(True) \n",
    "        self.cfg = cfg \n",
    "        self.nli_tokenizer = nli_tokenizer\n",
    "        self.nli_model     = nli_model\n",
    "        self.contradiction_threshold = contradiction_threshold\n",
    "        \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig =     current_text.text\n",
    "        pp   = transformed_text.text\n",
    "        contradiction_score = get_nli_probs(orig, pp, self.cfg, self.nli_tokenizer, self.nli_model).cpu()[0][self.cfg.contra_label].item()\n",
    "        if contradiction_score < self.contradiction_threshold:   return True \n",
    "        else:                                                    return False\n",
    "\n",
    "\n",
    "class AcceptabilityScoreConstraint(Constraint): \n",
    "    def __init__(self, cfg, cola_tokenizer, cola_model, acceptability_threshold): \n",
    "        super().__init__(True) \n",
    "        self.cfg = cfg \n",
    "        self.cola_tokenizer = cola_tokenizer\n",
    "        self.cola_model     = cola_model\n",
    "        self.acceptability_threshold = acceptability_threshold\n",
    "        \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        pp = transformed_text.text\n",
    "        acceptability_score = get_cola_probs(pp, self.cfg, self.cola_tokenizer, self.cola_model)[0, self.cfg.cola_positive_label].cpu().item() \n",
    "        if acceptability_score > self.acceptability_threshold:  return True \n",
    "        else:                                                   return False\n",
    "        \n",
    "\n",
    "class PpLetterDiffConstraint(Constraint): \n",
    "    def __init__(self, pp_letter_diff_threshold): \n",
    "        super().__init__(True) \n",
    "        self.pp_letter_diff_threshold = pp_letter_diff_threshold\n",
    "        \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig =     current_text.text\n",
    "        pp   = transformed_text.text\n",
    "        return abs(len(orig) - len(pp)) < self.pp_letter_diff_threshold\n",
    "        \n",
    "\n",
    "class LCPConstraint(Constraint): \n",
    "    def __init__(self, linking_contrast_phrases): \n",
    "        super().__init__(True) \n",
    "        self.linking_contrast_phrases = linking_contrast_phrases\n",
    "        \n",
    "    def _get_linking_contrast_phrase_conditions(self, orig_l, pp_l):\n",
    "        \"\"\"True: ok, False: fail. Logic: it's ok to include a linking contrast phrase if there is \n",
    "        one in the original to start with, but not if there isn't. \n",
    "        Copied from trainer class\"\"\"\n",
    "        def clean_sen_l(sen_l): return [sen.strip(string.punctuation).strip().lower() for sen in sen_l]\n",
    "        def has_linking_contrast_phrase(sen): \n",
    "            return any([sen.startswith(phrase + \" \") or sen.endswith(\" \" + phrase) for phrase in self.linking_contrast_phrases])\n",
    "        orig_l_cleaned,pp_l_cleaned = clean_sen_l(orig_l),clean_sen_l(pp_l)\n",
    "        phrase_present_orig_l = [has_linking_contrast_phrase(sen=orig) for orig in orig_l_cleaned]\n",
    "        phrase_present_pp_l   = [has_linking_contrast_phrase(sen=pp)   for pp   in pp_l_cleaned]\n",
    "        return [True if phrase_present_orig else not phrase_present_pp \n",
    "            for phrase_present_orig, phrase_present_pp in zip(phrase_present_orig_l, phrase_present_pp_l)] \n",
    "    \n",
    "    def _check_constraint(self, transformed_text, current_text):\n",
    "        orig =     current_text.text\n",
    "        pp   = transformed_text.text\n",
    "        condition_met = self._get_linking_contrast_phrase_conditions([orig], [pp])[0]\n",
    "        return condition_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttackRecipes: \n",
    "    def __init__(self, param_d): \n",
    "        store_attr()\n",
    "        if   param_d['ds_name'] == \"financial\":              cfg = Config().adjust_config_for_financial_dataset()\n",
    "        elif param_d['ds_name'] == \"rotten_tomatoes\":        cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "        elif param_d['ds_name'] == \"simple\":                 cfg = Config().adjust_config_for_simple_dataset()\n",
    "        vm_tokenizer,vm_model,pp_tokenizer,_,_,sts_model,nli_tokenizer,nli_model,cola_tokenizer,cola_model,cfg = prepare_models(cfg)\n",
    "        vm_tokenizer, vm_model, pp_tokenizer, _, _, sts_model, nli_tokenizer, nli_model, cola_tokenizer, cola_model, cfg\n",
    "        self.ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model, load_processed_from_file=False)\n",
    "        model_wrapper = HuggingFaceModelWrapper(vm_model, vm_tokenizer)\n",
    "        self.goal_function =  UntargetedClassification(model_wrapper)\n",
    "        self.constraints = [\n",
    "             RepeatModification(),\n",
    "             StopwordModification(nltk.corpus.stopwords.words(\"english\")), \n",
    "             StsScoreConstraint(sts_model, param_d['sts_threshold']), \n",
    "             ContradictionScoreConstraint(cfg, nli_tokenizer,  nli_model,  param_d['contradiction_threshold']), \n",
    "             AcceptabilityScoreConstraint(cfg, cola_tokenizer, cola_model, param_d['acceptability_threshold']), \n",
    "             PpLetterDiffConstraint(param_d['pp_letter_diff_threshold']),\n",
    "             LCPConstraint(linking_contrast_phrases=[o.strip() for o in open(\"./linking_contrast_phrases.txt\").readlines()])\n",
    "        ]\n",
    "        self.masked_lm           = transformers.AutoModelForCausalLM.from_pretrained(\"distilroberta-base\")\n",
    "        self.masked_lm_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "        self.WordSwapLM = partial(WordSwapMaskedLM, method=\"bae\", masked_language_model=self.masked_lm,tokenizer=self.masked_lm_tokenizer)\n",
    "    \n",
    "    class CFEmbeddingWordReplaceBeamSearchAttack(AttackRecipe):\n",
    "        @staticmethod\n",
    "        def build(common_class, beam_sz, max_candidates):\n",
    "            transformation = WordSwapEmbedding(max_candidates=max_candidates)\n",
    "            search_method = BeamSearch(beam_width=beam_sz)\n",
    "            attack = Attack(common_class.goal_function, common_class.constraints, transformation, search_method)\n",
    "            return attack\n",
    "\n",
    "    class LMWordReplaceBeamSearchAttack(AttackRecipe): \n",
    "        @staticmethod\n",
    "        def build(common_class, beam_sz, max_candidates):\n",
    "            transformation = common_class.WordSwapLM(max_candidates=max_candidates)\n",
    "            search_method = BeamSearch(beam_width=beam_sz)\n",
    "            attack = Attack(common_class.goal_function, common_class.constraints, transformation, search_method)\n",
    "            return attack\n",
    "        \n",
    "    class LMWordAddDeleteReplaceBeamSearchAttack(AttackRecipe):\n",
    "        @staticmethod\n",
    "        def build(common_class, beam_sz, max_candidates):\n",
    "            transformation = CompositeTransformation(\n",
    "                [common_class.WordSwapLM(max_candidates=max_candidates),\n",
    "                 WordInsertionMaskedLM(        masked_language_model=common_class.masked_lm,tokenizer=common_class.masked_lm_tokenizer,max_candidates=max_candidates),\n",
    "                 WordMergeMaskedLM(            masked_language_model=common_class.masked_lm,tokenizer=common_class.masked_lm_tokenizer,max_candidates=max_candidates)]\n",
    "            )            \n",
    "            search_method = BeamSearch(beam_width=beam_sz)\n",
    "            attack = Attack(common_class.goal_function, common_class.constraints, transformation, search_method)\n",
    "            return attack\n",
    "\n",
    "    class LMWordReplaceGeneticAlgorithmAttack(AttackRecipe): \n",
    "        @staticmethod\n",
    "        def build(common_class, max_candidates, pop_size, max_iters, max_replace_times_per_index):\n",
    "            transformation = common_class.WordSwapLM(max_candidates=max_candidates)\n",
    "            search_method = ImprovedGeneticAlgorithm(pop_size=pop_size, max_iters=max_iters, \n",
    "                                                     max_replace_times_per_index=max_replace_times_per_index, post_crossover_check=False)\n",
    "            attack = Attack(common_class.goal_function, common_class.constraints, transformation, search_method)\n",
    "            return attack     \n",
    "        \n",
    "        \n",
    "    def get_attack_list(self): \n",
    "        common_class = self \n",
    "        attack_list = [\n",
    "            {\n",
    "                \"attack_num\": 1, \n",
    "                \"attack_code\": \"LM-WR-BS-b2m5\", \n",
    "                \"attack_recipe\": self.LMWordReplaceBeamSearchAttack.build(common_class, beam_sz=2, max_candidates=5)\n",
    "            }, {\n",
    "                \"attack_num\": 2, \n",
    "                \"attack_code\": \"LM-WR-BS-b5m25\", \n",
    "                \"attack_recipe\": self.LMWordReplaceBeamSearchAttack.build(common_class, beam_sz=5, max_candidates=25)\n",
    "            }, {\n",
    "                \"attack_num\": 3, \n",
    "                \"attack_code\": \"LM-WR-BS-b10m50\", \n",
    "                \"attack_recipe\": self.LMWordReplaceBeamSearchAttack.build(common_class, beam_sz=10, max_candidates=50)\n",
    "            }, {\n",
    "                \"attack_num\": 4, \n",
    "                \"attack_code\": \"LM-WADR-BS-b5m25\", \n",
    "                \"attack_recipe\": self.LMWordAddDeleteReplaceBeamSearchAttack.build(common_class, beam_sz=5, max_candidates=25)\n",
    "            }, {\n",
    "                \"attack_num\": 5, \n",
    "                \"attack_code\": \"CF-WR-BS-b5m25\", \n",
    "                \"attack_recipe\": self.CFEmbeddingWordReplaceBeamSearchAttack.build(common_class, beam_sz=5, max_candidates=25)\n",
    "            }, {\n",
    "                \"attack_num\": 6, \n",
    "                \"attack_code\": \"LM-WR-GA-m25p60mi20mr5\", \n",
    "                \"attack_recipe\": self.LMWordReplaceGeneticAlgorithmAttack.build(common_class, max_candidates=25, pop_size=60, max_iters=20, max_replace_times_per_index=5)\n",
    "            }]\n",
    "        return attack_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted 50_baseline_attacks.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted baselines.ipynb.\n",
      "Converted baselines_analysis.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted pp_eval_baselines.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n",
      "Converted statistical_tests.ipynb.\n",
      "Converted test_grammar_options.ipynb.\n",
      "Converted test_pp_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
