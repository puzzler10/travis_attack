# AUTOGENERATED! DO NOT EDIT! File to edit: 10_data.ipynb (unless otherwise specified).

__all__ = ['ProcessedDataset']

# Cell
import torch, random, pandas as pd
from torch.utils.data import DataLoader, RandomSampler
from datasets import load_dataset, load_from_disk, DatasetDict, ClassLabel
from IPython.display import display, HTML
from .trainer import get_vm_probs
from .config import Config

# Cell
class ProcessedDataset:
    """Class that wraps a raw dataset (e.g. from huggingface datasets) and performs preprocessing on it."""
    def __init__(self, cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model):
        self._cfg,self._vm_tokenizer,self._vm_model,self._pp_tokenizer,self._sts_model = cfg,vm_tokenizer,vm_model,pp_tokenizer,sts_model
        if   self._cfg.dataset_name == "simple":          self._prep_dsd_raw_simple()
        elif self._cfg.dataset_name == "rotten_tomatoes": self._prep_dsd_raw_rotten_tomatoes()
        else: raise Exception("cfg.dataset_name must be either 'simple' or 'rotten_tomatoes'")
        self._preprocess_dataset()

    def _prep_dsd_raw_simple(self):
        """Load the simple dataset and package it up in a DatasetDict (dsd)
        with splits for train, valid, test."""
        self.dsd_raw = DatasetDict()
        for s in self._cfg.splits:
            self.dsd_raw[s] = load_dataset('csv', data_files=f"{self._cfg.path_data}simple_dataset_{s}.csv")['train']

    def _prep_dsd_raw_rotten_tomatoes(self):
        """Load the rotten tomatoes dataet and package it up in a DatasetDict (dsd)
        with splits for train, valid, test."""
        self.dsd_raw = load_dataset("rotten_tomatoes")
        self.dsd_raw['valid'] = self.dsd_raw.pop('validation')  # "valid" is easier than "validation"
        # make sure that all datasets have the same number of labels as what the victim model predicts
        for _,ds in self.dsd_raw.items(): assert ds.features[self._cfg.label_cname].num_classes == self._cfg.vm_num_labels

    def _prep_dsd_raw_snli(self):
        ## For snli
        # remove_minus1_labels = lambda x: x[label_cname] != -1
        # ds_train = ds_train.filter(remove_minus1_labels)
        # valid = valid.filter(remove_minus1_labels)
        # test = test.filter(remove_minus1_labels)
        raise NotImplementedError("SNLI not implemented yet.")

    def _preprocess_dataset(self):
        """Add columns, tokenize, transform, prepare dataloaders, and do other preprocessing tasks."""
        ##### TODO: why do you need train_eval? how is it different from train?
        dsd = self.dsd_raw.map(self._add_idx, batched=True, with_indices=True)  # add idx column
        if self._cfg.use_small_ds: dsd = self._prep_small_ds(dsd)  # do after adding idx so it's consistent across runs
        dsd = dsd.map(self._add_vm_orig_score, batched=True)  # add VM score
        if self._cfg.remove_misclassified_examples: dsd = dsd.filter(lambda x: x['orig_vm_predclass'] == x['label'])
        dsd = dsd.map(self._add_sts_orig_embeddings, batched=True)  # add STS score
        dsd = dsd.map(self._tokenize_fn,             batched=True)  # tokenize
        dsd = dsd.map(self._add_n_tokens,            batched=True)  # add n_tokens
        if self._cfg.bucket_by_length: dsd = dsd.sort("n_tokens", reverse=True)  # sort by n_tokens (high to low), useful for cuda memory caching
        self.dld_raw = self._get_dataloaders_dict(dsd, collate_fn=self._collate_fn_raw)  # dict of data loaders that serve raw text
        self.dld_tkn = self._get_dataloaders_dict(dsd, collate_fn=self._collate_fn_tkn)  # dict of data loaders that serve tokenized text
        self.dsd_tkn = dsd

    def _add_idx(self, batch, idx):
        """Add row numbers"""
        batch['idx'] = idx
        return batch

    def _add_n_tokens(self, batch):
        """Add the number of tokens of the orig text """
        batch['n_tokens'] = [len(o) for o in batch['input_ids']]
        return batch

    def _add_sts_orig_embeddings(self, batch):
        """Add the sts embeddings of the orig text"""
        batch['orig_sts_embeddings'] = self._sts_model.encode(batch[self._cfg.orig_cname], batch_size=64, convert_to_tensor=False)
        return batch

    def _add_vm_orig_score(self, batch):
        """Add the vm score of the orig text"""
        labels = torch.tensor(batch[self._cfg.label_cname], device=self._cfg.device)
        orig_probs,orig_predclass = get_vm_probs(batch[self._cfg.orig_cname], self._cfg, self._vm_tokenizer,
                                                 self._vm_model, return_predclass=True)
        batch['orig_truelabel_probs'] = torch.gather(orig_probs,1, labels[:,None]).squeeze().cpu().tolist()
        batch['orig_vm_predclass'] = orig_predclass.cpu().tolist()
        return batch

    def _tokenize_fn(self, batch):
        """Tokenize a batch of orig text using the paraphrase tokenizer."""
        return self._pp_tokenizer(batch[self._cfg.orig_cname], truncation=True, max_length=self._cfg.orig_max_length)

    def _collate_fn_tkn(self, x):
        """Collate function used by the DataLoader that serves tokenized data."""
        d = dict()
        for k in ['idx', 'attention_mask', 'input_ids', 'label', 'orig_truelabel_probs', 'orig_sts_embeddings']:
            d[k] = [o[k] for o in x]
        return self._pp_tokenizer.pad(d, pad_to_multiple_of=self._cfg.orig_padding_multiple, return_tensors="pt")

    def _collate_fn_raw(self, x):
        """Collate function used by the DataLoader that serves raw data."""
        d = dict()
        for k in [cfg.orig_cname, 'idx']: d[k] = [o[k] for o in x]
        return d

    def _get_sampler(self, ds):
        """Returns a RandomSampler. Used so we can keep the same shuffle order across multiple data loaders.
        Used when self._cfg.shuffle_train = True"""
        g = torch.Generator()
        g.manual_seed(seed)
        return RandomSampler(ds, generator=g)

    def _prep_small_ds(self, dsd):
        """Replaces a datasetdict with a smaller shard of itself."""
        for k,v in dsd.items():
            dsd[k] = v.shard(self._cfg.n_shards, 0, contiguous=self._cfg.shard_contiguous)
        return dsd

    def _get_dataloaders_dict(self, dsd, collate_fn):
        """Prepare a dict of dataloaders for train, valid and test"""
        if self._cfg.bucket_by_length and self._cfg.shuffle_train:  raise Exception("Can only do one of bucket by length or shuffle")
        d = dict()
        for split, ds in dsd.items():
            if self._cfg.shuffle_train:
                if split == "train":
                    sampler = self.get_sampler(ds)
                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_train,
                                           sampler=sampler, collate_fn=collate_fn,
                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory)
                else:
                    d[split] =  DataLoader(ds, batch_size=self._cfg.batch_size_eval,
                                           shuffle=False, collate_fn=collate_fn,
                                           num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory)
            if self._cfg.bucket_by_length:
                batch_size = self._cfg.batch_size_train if split == "train" else self._cfg.batch_size_eval
                d[split] =  DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn,
                                       num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory)

        # Add eval dataloader for train
        d['train_eval'] = DataLoader(dsd['train'], batch_size=self._cfg.batch_size_eval, shuffle=False,
                                    collate_fn=collate_fn,
                                     num_workers=self._cfg.n_wkrs, pin_memory=self._cfg.pin_memory)
        return d

    def show_random_elements(self, ds, num_examples=10):
        """Print some elements in a nice format so you can take a look at them.
        Split is one of 'train', 'test', 'valid'.
        Use for a dataset `ds` from the `dataset` package.  """
        assert num_examples <= len(ds), "Can't pick more elements than there are in the dataset."
        picks = []
        for _ in range(num_examples):
            pick = random.randint(0, len(ds)-1)
            while pick in picks:
                pick = random.randint(0, len(ds)-1)
            picks.append(pick)
        df = pd.DataFrame(ds[picks])
        for column, typ in ds.features.items():
            if isinstance(typ, ClassLabel):
                df[column] = df[column].transform(lambda i: typ.names[i])
        display(HTML(df.to_html()))