# AUTOGENERATED! DO NOT EDIT! File to edit: 00_utils.ipynb (unless otherwise specified).

__all__ = ['set_seed', 'set_session_options', 'prepare_logger', 'timecode', 'print_device_info', 'dump_tensors',
           'Monitor', 'show_gpu', 'round_t', 'table2df']

# Cell
import torch, numpy as np, pandas as pd, time, GPUtil, wandb, logging, os, sys
from timeit import default_timer as timer
from threading import Thread

# Cell
def set_seed(seed):
    """Sets all seeds for the session"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)


# Cell
def set_session_options():
    """Sets some useful options for the sesson"""
    os.environ["TOKENIZERS_PARALLELISM"] = "true"  # set to false if not working
    pd.set_option("display.max_colwidth", 400)
    # stop truncation of tables in wandb dashboard
    wandb.Table.MAX_ARTIFACT_ROWS = 1000000
    wandb.Table.MAX_ROWS = 1000000

# Cell
def prepare_logger():
    """Set logging level and config + return logger"""
    logging.basicConfig(format='%(message)s', stream=sys.stdout) # stdout while we are doing stdout to file piping
    logger = logging.getLogger("main_logger")
    logger.setLevel(logging.INFO)
    return logger

# Cell
class timecode:
    """This class is used for timing code"""
    def __enter__(self):
        self.t0 = timer()
        return self

    def __exit__(self, type, value, traceback):
        self.t = timer() - self.t0

# Cell
def print_device_info():
    """
    Prints some statistics around versions and the GPU's available for
    the host machine
    """
    import torch
    import sys
    print("######## Diagnostics and version information ######## ")
    print('__Python VERSION:', sys.version)
    print('__pyTorch VERSION:', torch.__version__)
    print('__CUDA VERSION', )
    from subprocess import call
    # call(["nvcc", "--version"]) does not work
    #! nvcc --version
    print('__CUDNN VERSION:', torch.backends.cudnn.version())
    print('__Number CUDA Devices:', torch.cuda.device_count())
    print('__Devices')
    call(["nvidia-smi", "--format=csv", "--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free"])
    print('Active CUDA Device: GPU', torch.cuda.current_device())
    print ('Available devices ', torch.cuda.device_count())
    print("Device name:", torch.cuda.get_device_name())
    print ('Current cuda device ', torch.cuda.current_device())
    print("#################################################################")


# Cell
def dump_tensors(gpu_only=True):
    """Prints a list of the Tensors being tracked by the garbage collector.
    Useful when running into an out of memory error on the GPU. """
    import gc
    total_size = 0
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj):
                if not gpu_only or obj.is_cuda:
                    print("%s:%s%s %s" % (type(obj).__name__,
                                            " GPU" if obj.is_cuda else "",
                                            " pinned" if obj.is_pinned else "",
                                            pretty_size(obj.size())))
                    total_size += obj.numel()
            elif hasattr(obj, "data") and torch.is_tensor(obj.data):
                if not gpu_only or obj.is_cuda:
                    print("%s â†’ %s:%s%s%s%s %s" % (type(obj).__name__,
                                                    type(obj.data).__name__,
                                                    " GPU" if obj.is_cuda else "",
                                                    " pinned" if obj.data.is_pinned else "",
                                                    " grad" if obj.requires_grad else "",
                                                    " volatile" if obj.volatile else "",
                                                    pretty_size(obj.data.size())))
                    total_size += obj.data.numel()
        except Exception as e:
            pass
    print("Total size:", total_size)


# Cell
class Monitor(Thread):
    """Use this to check that you are using the GPU during your pytorch functions and to track memory usage
    of the GPU's as well."""
    def __init__(self, delay):
        super(Monitor, self).__init__()
        self.stopped = False
        self.delay = delay # Time between calls to GPUtil
        self.start()

    def run(self):
        while not self.stopped:
            GPUtil.showUtilization()
            time.sleep(self.delay)

    def stop(self):
        self.stopped = True


# Cell
def show_gpu(msg):
    """
    ref: https://github.com/huggingface/transformers/issues/1742#issue-518262673
    put in logger.info()
    """
    import subprocess
    def query(field):
        return(subprocess.check_output(
            ['nvidia-smi', f'--query-gpu={field}',
                '--format=csv,nounits,noheader'],
            encoding='utf-8'))
    def to_int(result):
        return int(result.strip().split('\n')[0])

    used = to_int(query('memory.used'))
    total = to_int(query('memory.total'))
    pct = used/total
    return f"{msg} {100*pct:2.1f}% ({used} out of {total})"

# Cell
def round_t(t, dp=2):
    """Return rounded tensors for easy viewing. t is a tensor, dp=decimal places"""
    if t.device.type == "cuda": t=t.cpu()
    return t.detach().numpy().round(dp)

# Cell
def table2df(table):
    """Convert wandb table to pandas dataframe"""
    return pd.DataFrame(data=table.data, columns=table.columns)