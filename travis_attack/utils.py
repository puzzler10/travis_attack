# AUTOGENERATED! DO NOT EDIT! File to edit: 00_utils.ipynb (unless otherwise specified).

__all__ = ['set_seed', 'set_session_options', 'prepare_logger', 'setup_logging', 'timecode', 'print_device_info',
           'dump_tensors', 'Monitor', 'show_gpu', 'round_t', 'merge_dicts', 'display_all', 'unpack_nested_lists_in_df',
           'append_df_to_csv', 'robust_rmtree', 'table2df']

# Cell
import torch, numpy as np, pandas as pd, time, GPUtil, wandb, logging, os, sys, shutil
from timeit import default_timer as timer
from threading import Thread

# Cell
def set_seed(seed):
    """Sets all seeds for the session"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)


# Cell
def set_session_options():
    """Sets some useful options for the sesson"""
    os.environ["TOKENIZERS_PARALLELISM"] = "true"  # set to false if not working
    pd.set_option("display.max_colwidth", 400)
    # stop truncation of tables in wandb dashboard
    wandb.Table.MAX_ARTIFACT_ROWS = 1000000
    wandb.Table.MAX_ROWS = 1000000

# Cell
def prepare_logger():
    """Set logging level and config + return logger"""
    logging.basicConfig(format='%(message)s', stream=sys.stdout) # stdout while we are doing stdout to file piping
    logger = logging.getLogger("main_logger")
    logger.setLevel(logging.INFO)
    return logger

# Cell
def setup_logging(cfg, disable_other_loggers=True):
    """taken from this recipe from the logging cookbook:
    https://docs.python.org/3/howto/logging-cookbook.html#logging-to-multiple-destinations """
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                        datefmt='%m-%d %H:%M',
                        filename=cfg.path_logfile,
                        filemode='w')
    # define a Handler which writes INFO messages or higher to the sys.stderr
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') # set a format which is simpler for console use
    console.setFormatter(formatter)  # tell the handler to use this format
    logging.getLogger('').addHandler(console)  # add the handler to the root logger

    if disable_other_loggers:
        allowed_modules = ["travis_attack", "wandb", "sentence_transformers", "transformers", "datasets" ]
        for log_name, log_obj in logging.Logger.manager.loggerDict.items():
            if not any(mod in log_name for mod in allowed_modules):
                log_obj.disabled = True


# Cell
class timecode:
    """This class is used for timing code"""
    def __enter__(self):
        self.t0 = timer()
        return self

    def __exit__(self, type, value, traceback):
        self.t = timer() - self.t0

# Cell
def print_device_info():
    """
    Prints some statistics around versions and the GPU's available for
    the host machine
    """
    import torch
    import sys
    print("######## Diagnostics and version information ######## ")
    print('__Python VERSION:', sys.version)
    print('__pyTorch VERSION:', torch.__version__)
    print('__CUDA VERSION', )
    from subprocess import call
    # call(["nvcc", "--version"]) does not work
    #! nvcc --version
    print('__CUDNN VERSION:', torch.backends.cudnn.version())
    print('__Number CUDA Devices:', torch.cuda.device_count())
    print('__Devices')
    call(["nvidia-smi", "--format=csv", "--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free"])
    print('Active CUDA Device: GPU', torch.cuda.current_device())
    print ('Available devices ', torch.cuda.device_count())
    print("Device name:", torch.cuda.get_device_name())
    print ('Current cuda device ', torch.cuda.current_device())
    print("#################################################################")


# Cell
def dump_tensors(gpu_only=True):
    """Prints a list of the Tensors being tracked by the garbage collector.
    Useful when running into an out of memory error on the GPU. """
    import gc
    total_size = 0
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj):
                if not gpu_only or obj.is_cuda:
                    print("%s:%s%s %s" % (type(obj).__name__,
                                            " GPU" if obj.is_cuda else "",
                                            " pinned" if obj.is_pinned else "",
                                            pretty_size(obj.size())))
                    total_size += obj.numel()
            elif hasattr(obj, "data") and torch.is_tensor(obj.data):
                if not gpu_only or obj.is_cuda:
                    print("%s â†’ %s:%s%s%s%s %s" % (type(obj).__name__,
                                                    type(obj.data).__name__,
                                                    " GPU" if obj.is_cuda else "",
                                                    " pinned" if obj.data.is_pinned else "",
                                                    " grad" if obj.requires_grad else "",
                                                    " volatile" if obj.volatile else "",
                                                    pretty_size(obj.data.size())))
                    total_size += obj.data.numel()
        except Exception as e:
            pass
    print("Total size:", total_size)


# Cell
class Monitor(Thread):
    """Use this to check that you are using the GPU during your pytorch functions and to track memory usage
    of the GPU's as well."""
    def __init__(self, delay):
        super(Monitor, self).__init__()
        self.stopped = False
        self.delay = delay # Time between calls to GPUtil
        self.start()

    def run(self):
        while not self.stopped:
            GPUtil.showUtilization()
            time.sleep(self.delay)

    def stop(self):
        self.stopped = True


# Cell
def show_gpu(msg):
    """
    ref: https://github.com/huggingface/transformers/issues/1742#issue-518262673
    put in logger.info()
    """
    import subprocess
    def query(field):
        return(subprocess.check_output(
            ['nvidia-smi', f'--query-gpu={field}',
                '--format=csv,nounits,noheader'],
            encoding='utf-8'))
    def to_int(result):
        return int(result.strip().split('\n')[0])

    used = to_int(query('memory.used'))
    total = to_int(query('memory.total'))
    pct = used/total
    return f"{msg} {100*pct:2.1f}% ({used} out of {total})"

# Cell
def round_t(t, dp=2):
    """Return rounded tensors for easy viewing. t is a tensor, dp=decimal places"""
    if t.device.type == "cuda": t=t.cpu()
    return t.detach().numpy().round(dp)

# Cell
def merge_dicts(d1, d2):
    """Merge the two dicts and return the result. Check first that there is no key overlap."""
    assert set(d1.keys()).isdisjoint(d2.keys())
    return {**d1, **d2}

# Cell
def display_all(df):
    with pd.option_context("display.max_rows", 1000):
        with pd.option_context("display.max_columns", 1000):
            with pd.option_context("max_colwidth", 480):
                display(df)

# Cell
def unpack_nested_lists_in_df(df, scalar_cols=[]):
    """Take a df where we have lists stored in the cells and convert it to many rows.
    Put all columns without lists stored in the cells into `scalar_cols`."""
    return df.set_index(scalar_cols).apply(pd.Series.explode).reset_index()

# Cell
def append_df_to_csv(df, path):
    """Checks columns and other stuff before appending"""
    import os
    if not os.path.isfile(path):   df.to_csv(path, mode='a', index=False)  # create with header if not exists
    elif len(df.columns) != len(pd.read_csv(path, nrows=1).columns):
        raise Exception("Columns do not match!! Dataframe has " + str(len(df.columns)) + " columns. CSV file has " + str(len(pd.read_csv(path, nrows=1).columns)) + " columns.")
    elif not (df.columns == pd.read_csv(path, nrows=1).columns).all():
        raise Exception("Columns and column order of dataframe and csv file do not match!!")
    else:
        df.to_csv(path, mode='a', index=False, header=False)

# Cell
def robust_rmtree(path, logger=None, max_retries=6):
    """Robustly tries to delete paths.
    Retries several times (with increasing delays) if an OSError
    occurs.  If the final attempt fails, the Exception is propagated
    to the caller.
    """
    dt = 1
    for i in range(max_retries):
        try:
            shutil.rmtree(path)
            return
        except OSError:
            if logger:
                logger.info('Unable to remove path: %s' % path)
                logger.info('Retrying after %d seconds' % dt)
            time.sleep(dt)
            dt *= 2

    # Final attempt, pass any Exceptions up to caller.
    shutil.rmtree(path)

# Cell
def table2df(table):
    """Convert wandb table to pandas dataframe"""
    return pd.DataFrame(data=table.data, columns=table.columns)