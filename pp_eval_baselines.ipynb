{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from travis_attack.utils import set_seed, set_session_options, setup_logging, setup_parser, display_all\n",
    "from travis_attack.config import Config\n",
    "from travis_attack.models import prepare_models, get_optimizer\n",
    "from travis_attack.data import ProcessedDataset\n",
    "from travis_attack.trainer import Trainer\n",
    "#from travis_attack.insights import (postprocess_df, create_and_log_wandb_postrun_plots, get_training_dfs)\n",
    "from fastcore.basics import in_jupyter\n",
    "\n",
    "import logging \n",
    "logger = logging.getLogger(\"paraphrase_eval\")\n",
    "\n",
    "path_baselines = \"./pp_eval_baselines/\"\n",
    "datetime_now = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pp_eval_parser(): \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_name\")\n",
    "    parser.add_argument(\"--split\")\n",
    "    parser.add_argument(\"--sts_threshold\", type=float)\n",
    "    parser.add_argument(\"--contradiction_threshold\", type=float)\n",
    "    #parser.add_argument('args', nargs=argparse.REMAINDER)  # activate to put keywords in kwargs.\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CONFIG (default values) #########\n",
    "d = dict(\n",
    "    datetime=datetime_now,\n",
    "    dataset_name = \"rotten_tomatoes\",\n",
    "    split = 'valid',\n",
    "    sts_threshold = 0.7,\n",
    "    contradiction_threshold = 0.2\n",
    "#         pp = {\n",
    "#         \"do_sample\": False if self.sampling_strategy == \"greedy\" else True,\n",
    "#         \"min_length\": 4, \n",
    "#         \"max_length\": 48, \n",
    "#         \"temperature\": 0.7,\n",
    "#         \"top_p\": 0.98, \n",
    "#         \"length_penalty\" : 1.,\n",
    "#         \"repetition_penalty\": 1.\n",
    "#    }\n",
    "        \n",
    ")\n",
    "###########################################\n",
    "\n",
    "if not in_jupyter():  # override with any script options\n",
    "    parser = setup_pp_eval_parser()\n",
    "    newargs = vars(parser.parse_args())\n",
    "    for k,v in newargs.items(): \n",
    "        if v is not None: d[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config() \n",
    "for k,v in d.items(): setattr(cfg, k, v)\n",
    "if   cfg.dataset_name == \"rotten_tomatoes\": cfg.adjust_config_for_rotten_tomatoes_dataset()\n",
    "elif cfg.dataset_name == \"financial\":       cfg.adjust_config_for_financial_dataset()\n",
    "elif cfg.dataset_name == \"simple\":          cfg.adjust_config_for_simple_dataset()\n",
    "if cfg.use_small_ds:  cfg = cfg.small_ds()\n",
    "set_seed(cfg.seed)\n",
    "set_session_options()\n",
    "setup_logging(cfg, disable_other_loggers=True)\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, ref_pp_model, sts_model, nli_tokenizer, nli_model, cfg = prepare_models(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "travis_attack.data: INFO     Will load dataset rotten_tomatoes with use_small_ds set to False\n",
      "travis_attack.data: INFO     Will load dataset rotten_tomatoes with use_small_ds set to False\n",
      "travis_attack.data: INFO     Cache file found for processed dataset, so loading that dataset.\n",
      "travis_attack.data: INFO     Cache file found for processed dataset, so loading that dataset.\n"
     ]
    }
   ],
   "source": [
    "optimizer = get_optimizer(cfg, pp_model)\n",
    "ds = ProcessedDataset(cfg, vm_tokenizer, vm_model, pp_tokenizer, sts_model, load_processed_from_file=True)\n",
    "trainer = Trainer(cfg, vm_tokenizer, vm_model, pp_tokenizer, pp_model, ref_pp_model, sts_model, nli_tokenizer, nli_model, optimizer,\n",
    "                  ds, initial_eval=False, use_cpu=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Trainer)\n",
    "def get_ref_model_baseline(self): \n",
    "    \"\"\"Calculate baselines for the ref model.\"\"\"\n",
    "     # Put models in eval mode and do the forward pass \n",
    "    # Current logic: push all batches together into one big list.   \n",
    "    self._reset_batch_dicts()\n",
    "    if self.pp_model.training: self.pp_model.eval()\n",
    "    if self.vm_model.training: self.vm_model.eval()\n",
    "    # The \"train_eval\" dataloader is the same as train but a bigger batch size and explicitly no shuffling\n",
    "    dl_key = \"train_eval\" if split == \"train\" else split\n",
    "    dl_raw = self.ds.dld_raw[dl_key]\n",
    "    dl_tkn = self.ds.dld_tkn[dl_key]\n",
    "    with torch.no_grad(): \n",
    "        for self.batch_num, (data, raw) in enumerate(zip(dl_tkn, dl_raw)):\n",
    "            logger.debug(f\"EVAL: {split} with dl_key {dl_key}\")\n",
    "            logger.debug(f\"Elements in data_d[{split}]: {len(self.data_d[split])}\")\n",
    "            logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loading data: '))\n",
    "            assert data['input_ids'].shape[0] == len(raw['text_with_prefix'])\n",
    "            self._reset_batch_dicts()\n",
    "            assert len(self.batch_d) == len(self.batch_time_d) == len(self.batch_wandb_d) == 0 \n",
    "            for k, v in data.items():\n",
    "                # Eval data isn't loaded on GPU by default unlike train data. This is because train dataloader goes \n",
    "                # through accelerator `prepare` function, but eval dataloaders don't. So here we load the data onto GPU \n",
    "                if data[k].device != self._cfg.device: data[k] = data[k].to(self._cfg.device)\n",
    "            pp_output, pp_l = self._pp_model_forward(data)\n",
    "            _ = self._loss_fn(data, raw, pp_output, pp_l)\n",
    "            self._add_batch_vars_to_batch_d(raw, data, pp_l)\n",
    "            self.data_d[split].append(self.batch_d)\n",
    "            logger.debug(show_gpu(f'EVAL, epoch {self.epoch}, batch {self.batch_num}, GPU memory usage after loss_fn pass: '))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "dataset = ds.dsd_raw[d['split']]\n",
    "dataloader = ds.dld_raw['train_eval'] if d['split'] == 'train' else ds.dld_raw[d['split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.basics import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2abba6658250>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
