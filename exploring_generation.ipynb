{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.distributions import Categorical\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s') \n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "\n",
    "## PEGASUS model\n",
    "pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "pp_tokenizer_pegasus = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_pegasus.generate)\n",
    "pp_model_pegasus.generate_with_grad = MethodType(generate_with_grad, pp_model_pegasus)\n",
    "\n",
    "## BART model\n",
    "pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "pp_tokenizer_bart = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_bart = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_bart.generate)\n",
    "pp_model_bart.generate_with_grad = MethodType(generate_with_grad, pp_model_bart)\n",
    "\n",
    "## Select which one to use as default\n",
    "pp_tokenizer = pp_tokenizer_pegasus\n",
    "pp_model = pp_model_pegasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "code_folding": [
     0,
     20,
     58,
     79
    ]
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(translated.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "#     if np.any(np.isnan(seq_log_prob.detach().cpu()).tolist()): \n",
    "#         warnings.warn(f\"Warning: NAN's detected in pp_logp calclulations.\\n seq_token_log_probs: {seq_token_log_probs}\")\n",
    "    return seq_log_prob\n",
    "\n",
    "def get_tokens_from_token_ids_batch(tokenizer, ids_batch):\n",
    "    l = []\n",
    "    for i in range(ids_batch.shape[0]): \n",
    "        l.append(tokenizer.convert_ids_to_tokens(ids_batch[i,:]))\n",
    "    return l\n",
    "\n",
    "def assert_start_and_end_tokens_are_correct(tokenizer, orig_token_ids, pp_token_ids):\n",
    "    \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "    right special tokens (depends on tokenizer)\"\"\"\n",
    "    def get_start_end_special_token_ids(tokenizer): \n",
    "        \"\"\"The token id's that input/output sequences should start and end with\"\"\"\n",
    "        d = {}\n",
    "        if pp_tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "            d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "            d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "            d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "            d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        elif pp_tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "            d[\"input_start_id\"] =  None\n",
    "            d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "            d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "            d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        else: \n",
    "            raise Exception(\"unrecognised tokenizer\")\n",
    "        return d\n",
    "    start_end_token_d = get_start_end_special_token_ids(pp_tokenizer)\n",
    "    \n",
    "    # Input\n",
    "    if start_end_token_d['input_start_id'] is not None: \n",
    "        assert torch.all(orig_token_ids[:,0] == start_end_token_d['input_start_id'])\n",
    "    # can probs rewrite this to make it nicer but it's fine for now\n",
    "    assert torch.all(torch.logical_or(orig_token_ids[:,-1] == start_end_token_d['input_end_id'][0], \n",
    "                                      orig_token_ids[:,-1] == start_end_token_d['input_end_id'][1]))\n",
    "    \n",
    "    # Output\n",
    "    assert torch.all(pp_token_ids[:,0] == start_end_token_d['output_start_id'])\n",
    "    assert torch.all(torch.logical_or(pp_token_ids[:,-1] == start_end_token_d['output_end_id'][0], \n",
    "                                      pp_token_ids[:,-1] == start_end_token_d['output_end_id'][1]))\n",
    "\n",
    "def check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked): \n",
    "    \"\"\"Check we don't have any postive inf or nan, and that all negative inf values are expected\"\"\"\n",
    "    idx_posinf = torch.nonzero(torch.isposinf(scores_stacked))\n",
    "    idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "    idx_nan    = torch.nonzero(torch.isnan(scores_stacked))\n",
    "    \n",
    "    # Check we don't get any positive inf or nan\n",
    "    assert len(idx_posinf) == 0\n",
    "    assert len(idx_nan)    == 0 \n",
    "    \n",
    "    # We expect to see negative inf for the eos_token when we have not reached min_length. \n",
    "    # But we shouldn't expect it for any other tokens\n",
    "    assert torch.all(idx_neginf[:,2] == pp_tokenizer.eos_token_id)\n",
    "    # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "    # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "    # shouldn't be set to -inf\n",
    "    # Not a 100% test but very likely to identify\n",
    "    assert idx_neginf.shape[0] == (pp_model_params[\"min_length\"] -1) * batch_size  \n",
    "    # Check that no elements after min_length are -inf\n",
    "    assert torch.all(idx_neginf[:,1] < (pp_model_params[\"min_length\"] -1 ))\n",
    "\n",
    "def check_scores_log_softmax_sums_and_shape(scores_log_softmax):\n",
    "    sums = scores_log_softmax.exp().sum(2)\n",
    "    # check that the axes is right\n",
    "    # we want to sum over token probabilities at each generation step, so we \n",
    "    # should end up with a shape [batch_size, generated_length]\n",
    "    assert sums.shape[0] == batch_size  \n",
    "    assert sums.shape[1] == generated_length - 1\n",
    "    # check that they sum to 1 along the generated_length axis\n",
    "    assert torch.allclose(sums, torch.ones(sums.size()), atol = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INPUT #####\n",
    "orig_l = [\n",
    "    \"Hello my name is zfldlfoqd\", \n",
    "    \"The cat is brown and it looks cute!\"\n",
    "]\n",
    "batch_size = len(orig_l)\n",
    "orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "input_length = orig_tokens['input_ids'].size()[1]\n",
    "orig_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, orig_tokens['input_ids'])\n",
    "\n",
    "\n",
    "##### PARAPHRASE #####\n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                      return_dict_in_generate=True,\n",
    "                                      output_scores=True,\n",
    "                                    remove_invalid_values=False)\n",
    "generated_length = pp_output.sequences.shape[1]\n",
    "pp_l             = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "pp_l_list_of_tokens = pp_tokenizer.convert_ids_to_tokens(pp_output.sequences[0,:])\n",
    "pp_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, pp_output.sequences)\n",
    "\n",
    "\n",
    "assert_start_and_end_tokens_are_correct(pp_tokenizer, orig_token_ids=orig_tokens['input_ids'],\n",
    "                                        pp_token_ids= pp_output.sequences)\n",
    "\n",
    "\n",
    "\n",
    "###### SCORES ########\n",
    "scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "# The second argument to stack (i.e. dim) determines which axis the tensors are stacked along. \n",
    "# It determines the axis that becomes generated_length - 1\n",
    "# dim=0 gives shape [generated_length-1, batch_size, vocab_size]\n",
    "# dim=1 gives shape [batch_size, generated_length-1, vocab_size]\n",
    "# dim=2 gives shape [batch_size, vocab_size, generated_length-1]\n",
    "# Our scores_stacked is stacked on dim 1 so it should be second \n",
    "assert scores_stacked.shape == torch.Size([batch_size, (generated_length - 1), pp_tokenizer.vocab_size])\n",
    "check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked)\n",
    "\n",
    "\n",
    "# These scores are logits \n",
    "# see some of the docs on this page https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput\n",
    "# so we got to take softmax over them \n",
    "# but if we take regular softmax then we run into numerical errors\n",
    "# so instead we take log_softmax\n",
    "scores_log_softmax = torch.log_softmax(scores_stacked, 2)\n",
    "check_scores_log_softmax_sums_and_shape(scores_log_softmax)\n",
    "\n",
    "\n",
    "# Entropy and similar stats \n",
    "ent = Categorical(logits = scores_stacked).entropy()\n",
    "assert ent.shape == torch.Size([batch_size, generated_length - 1])\n",
    "scores_softmax = scores_log_softmax.exp()\n",
    "k=3\n",
    "tkn_kmaxprob, tkn_kmaxidx = torch.topk(scores_softmax, k=k, dim=2)\n",
    "# The third dimension indexes top1, top2, top3 etc \n",
    "assert tkn_kmaxprob[:,:,0].shape == torch.Size([batch_size, generated_length - 1])\n",
    "# I'd naively expect True everywhere for tkn_kmaxidx[:,:,0] == pp_output.sequences[:, 1:] but it turns \n",
    "# out this is not the case because padding tokens seem to have prob 0 and eos tokens are outputted \n",
    "# instead by the token generation process and then later replaced by pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_tokens_from_token_ids_batch() missing 1 required positional argument: 'ids_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-320-845a44a85333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_tokens_from_token_ids_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkn_kmaxidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_tokens_from_token_ids_batch() missing 1 required positional argument: 'ids_batch'"
     ]
    }
   ],
   "source": [
    "get_tokens_from_token_ids_batch(tkn_kmaxidx[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 3])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(scores_softmax, k=k, dim=2).values.shape\n",
    "# [batch_size, generated_length-1, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8896e-01, 2.6108e-01, 6.9979e-02],\n",
       "        [9.3538e-01, 4.9477e-03, 1.6081e-03],\n",
       "        [8.3225e-01, 1.7378e-02, 3.8617e-03],\n",
       "        [3.8864e-01, 1.8792e-02, 8.6064e-03],\n",
       "        [3.5333e-01, 3.8666e-02, 1.8665e-02],\n",
       "        [6.3073e-01, 1.9265e-02, 1.1664e-02],\n",
       "        [8.0162e-01, 3.7079e-02, 7.5843e-03],\n",
       "        [8.2441e-01, 4.3555e-03, 2.6471e-03],\n",
       "        [8.6826e-01, 3.0741e-02, 7.2997e-03],\n",
       "        [4.7315e-01, 1.6260e-01, 1.4396e-01],\n",
       "        [9.0402e-01, 3.6067e-04, 8.6157e-05]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_kmaxprob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁My', '▁name', '▁is', '▁Z', 'FL', 'dl', 'fo', 'q', 'd', '.', '</s>'],\n",
       " ['▁The',\n",
       "  '▁cat',\n",
       "  '▁is',\n",
       "  '▁brown',\n",
       "  '.',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_from_token_ids_batch(pp_tokenizer, tkn_kmaxidx[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁I', '▁Name', \"'\", '▁z', 'fl', 'DL', 'FO', '</s>', 'D', '</s>', '.'],\n",
       " ['▁A',\n",
       "  '▁brown',\n",
       "  '▁looks',\n",
       "  '▁cute',\n",
       "  '▁and',\n",
       "  '.',\n",
       "  '▁is',\n",
       "  '▁looks',\n",
       "  '▁is',\n",
       "  '.',\n",
       "  '.']]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_from_token_ids_batch(pp_tokenizer, tkn_kmaxidx[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁Hello', 'name', '▁was', 'Z', 'liff', 'D', '▁FO', '.', '.', '▁and', ','],\n",
       " ['▁There',\n",
       "  '▁animal',\n",
       "  '▁has',\n",
       "  '▁adorable',\n",
       "  '</s>',\n",
       "  '▁',\n",
       "  '.',\n",
       "  '▁is',\n",
       "  '▁looks',\n",
       "  '▁the',\n",
       "  '▁is']]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_from_token_ids_batch(pp_tokenizer, tkn_kmaxidx[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3364, 0.9830, 0.9765, 0.6290, 0.9962, 0.9999, 0.9977, 0.9974, 0.9993,\n",
       "         0.9800, 0.5818, 0.1753],\n",
       "        [0.9161, 0.9887, 0.9813, 0.9776, 0.9055, 0.7203, 0.9604, 0.8809, 0.9890,\n",
       "         0.9279, 0.6757, 0.6300]], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_maxprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2387,   766,    16,   992,   506,  4779, 13491,   139,  1343,   417,\n",
       "             4,     2],\n",
       "        [  133,  4758,    16,  6219,     8,  1326, 11962,   328,     2,     1,\n",
       "             1,     1]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now calculating sequence probabilities\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-76d0035fb1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseq_without_first_tkn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Now calculating sequence probabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseq_token_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_without_first_tkn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mseq_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_token_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sequence probability: {seq_prob}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "tmp = scores_log_softmax[i]\n",
    "seq_without_first_tkn = pp_output.sequences[i, 1:]\n",
    "logger.info(\"Now calculating sequence probabilities\")\n",
    "seq_token_probs = torch.gather(tmp,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "seq_prob = seq_token_probs.prod(-1).item()\n",
    "logger.info(f\"Sequence probability: {seq_prob}\")\n",
    "# Get the 2nd and 3rd most likely tokens at each st\n",
    "topk_ids = torch.topk(tmp,3,dim=2).indices[:,:,1:]\n",
    "topk_tokens_probs = torch.gather(tmp,2,topk_ids).squeeze(-1)\n",
    "toks2 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,0].squeeze())\n",
    "toks3 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,1].squeeze())\n",
    "tok_probs2 = topk_tokens_probs[:,:,0].squeeze()\n",
    "tok_probs3 = topk_tokens_probs[:,:,1].squeeze()\n",
    "\n",
    "logger.info(f\"Probabilities of getting the top 3 tokens at each step:\")\n",
    "tokens = pp_tokenizer.convert_ids_to_tokens(seq_without_first_tkn.squeeze())\n",
    "for (p, t, p2,t2,p3,t3)  in zip(seq_token_probs.squeeze(), tokens, tok_probs2, toks2, tok_probs3, toks3): \n",
    "    logger.info(f\"{t}: {round(p.item(),3)}  {t2}: {round(p2.item(),3)}  {t3}: {round(p3.item(),3)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-14.0859, -17.9504,     -inf,  ..., -18.2348, -18.0437, -16.5071],\n",
       "        [-30.0271, -21.9822,     -inf,  ..., -21.9696, -21.9111, -20.4842],\n",
       "        [-24.9215, -22.6132,     -inf,  ..., -22.1891, -21.9841, -24.1049],\n",
       "        ...,\n",
       "        [-19.5661, -22.5410,  -9.6751,  ..., -21.6367, -21.7456, -22.1351],\n",
       "        [-18.2801, -17.6360,  -2.3842,  ..., -17.3905, -17.2129, -17.8064],\n",
       "        [-17.3701, -17.1515,  -1.7413,  ..., -17.5883, -17.5943, -16.4318]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "\n",
      "Tokenizer has these special tokens:['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "The bos token is <s> and has id 0\n",
      "The eos token is </s> and has id 2\n",
      "The pad token is <pad> and has id 1\n",
      "The unk token is <unk> and has id 3\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Original text: ['Hello my name is zfldlfoqd', 'The cat is brown and it looks cute!']\n",
      "This is tokenised to get a dict with keys input_ids and attention_mask \n",
      "Tokens:\n",
      "The input_ids look like this: tensor([[    0, 31414,   127,   766,    16,   992,   506,  4779, 13491,   139,\n",
      "          1343,   417,     2,     1,     1,     1],\n",
      "        [    0,   133,  4758,    16,  6219,     8,    24,  1326, 11962,   328,\n",
      "             2,     1,     1,     1,     1,     1]])\n",
      "The tokens are: [['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>'], ['<s>', 'The', 'Ġcat', 'Ġis', 'Ġbrown', 'Ġand', 'Ġit', 'Ġlooks', 'Ġcute', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "This has shape torch.Size([2, 16]) or [batch_size, input_length], which also might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\n",
      "The attention_mask looks like this: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "This has shape torch.Size([2, 16]) or [batch_size, input_length]\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Paraphrases: ['My name is zfldlfoqd.', 'The cat is brown and looks cute!']\n",
      "Paraphrases with special tokens: ['</s>My name is zfldlfoqd.</s>', '</s>The cat is brown and looks cute!</s><pad><pad><pad>']\n",
      "List of pp tokens:[['</s>', 'My', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '.', '</s>'], ['</s>', 'The', 'Ġcat', 'Ġis', 'Ġbrown', 'Ġand', 'Ġlooks', 'Ġcute', '!', '</s>', '<pad>', '<pad>', '<pad>']]\n",
      "Paraphrase token sequences: tensor([[    2,  2387,   766,    16,   992,   506,  4779, 13491,   139,  1343,\n",
      "           417,     4,     2],\n",
      "        [    2,   133,  4758,    16,  6219,     8,  1326, 11962,   328,     2,\n",
      "             1,     1,     1]])\n",
      "Shape of pp token sequences:torch.Size([2, 13]) or [batch_size, generated_length]\n",
      "Each pp should start with the bos_token and end with either eos token or pad token.\n",
      "Writing assert to confirm this.\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Scores is a tuple of length 12 which is one less than the generated_length, or the number of tokens in the pp token sequences (this has shape torch.Size([2, 13])\n",
      "Each score is a tensor of shape torch.Size([2, 50265]) or [batch_size, vocab_size]\n",
      "We stack them to get a tensor of shape torch.Size([2, 12, 50265]) or [batch_size, generated_length, vocab_size]\n",
      "Scores are really logits so we have to take softmax to get probabilities.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer.bos_token} and has id {pp_tokenizer.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer.eos_token} and has id {pp_tokenizer.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer.pad_token} and has id {pp_tokenizer.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer.unk_token} and has id {pp_tokenizer.unk_token_id}\")\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Original text: {orig_l}\")\n",
    "logger.info(f\"This is tokenised to get a dict with keys input_ids and attention_mask \")\n",
    "logger.info(f\"Tokens:\")\n",
    "logger.info(f\"The input_ids look like this: {orig_tokens['input_ids']}\")\n",
    "logger.info(f\"The tokens are: {orig_l_tokens_list}\")\n",
    "logger.info(f\"This has shape {orig_tokens['input_ids'].shape} or [batch_size, input_length], which also\\\n",
    " might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\")\n",
    "logger.info(f\"The attention_mask looks like this: {orig_tokens['attention_mask']}\")\n",
    "logger.info(f\"This has shape {orig_tokens['attention_mask'].shape} or [batch_size, input_length]\")\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Paraphrases: {pp_l}\")\n",
    "logger.info(f\"Paraphrases with special tokens: {pp_l_with_tokens}\")\n",
    "logger.info(f\"List of pp tokens:{pp_l_tokens_list}\")\n",
    "logger.info(f\"Paraphrase token sequences: {pp_output.sequences}\")\n",
    "logger.info(f\"Shape of pp token sequences:{pp_output.sequences.shape} or [batch_size, generated_length]\")\n",
    "logger.info(\"Each pp should start with the bos_token and end with either eos token or pad token.\")\n",
    "logger.info(\"Writing assert to confirm this.\")\n",
    "\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Scores is a tuple of length {len(pp_output.scores)} which is one less than the generated_length, or \\\n",
    "the number of tokens in the pp token sequences (this has shape {pp_output.sequences.shape}\")\n",
    "logger.info(f\"Each score is a tensor of shape {pp_output.scores[0].shape} or [batch_size, vocab_size]\")\n",
    "#logger.info(f\"Full shape:{[o.shape for o in pp_output.scores]}\")\n",
    "logger.info(f\"We stack them to get a tensor of shape {scores_stacked.shape} or [batch_size, generated_length, vocab_size]\")\n",
    "logger.info(f\"Scores are really logits so we have to take softmax to get probabilities.\")\n",
    "\n",
    "\n",
    "# seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "# logger.info(\"Now calculating sequence probabilities\")\n",
    "# seq_token_probs = torch.gather(scores_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "# seq_prob = seq_token_probs.prod(-1).item()\n",
    "# logger.info(f\"Sequence probability: {seq_prob}\")\n",
    "\n",
    "# # Get the 2nd and 3rd most likely tokens at each st\n",
    "# topk_ids = torch.topk(scores_softmax,3,dim=2).indices[:,:,1:]\n",
    "# topk_tokens_probs = torch.gather(scores_softmax,2,topk_ids).squeeze(-1)\n",
    "# toks2 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,0].squeeze())\n",
    "# toks3 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,1].squeeze())\n",
    "# tok_probs2 = topk_tokens_probs[:,:,0].squeeze()\n",
    "# tok_probs3 = topk_tokens_probs[:,:,1].squeeze()\n",
    "\n",
    "# logger.info(f\"Probabilities of getting the top 3 tokens at each step:\")\n",
    "# tokens = pp_tokenizer.convert_ids_to_tokens(seq_without_first_tkn.squeeze())\n",
    "# for (p, t, p2,t2,p3,t3)  in zip(seq_token_probs.squeeze(), tokens, tok_probs2, toks2, tok_probs3, toks3): \n",
    "#     logger.info(f\"{t}: {round(p.item(),3)}  {t2}: {round(p2.item(),3)}  {t3}: {round(p3.item(),3)}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_info_on_generated_text():\n",
    "    \"\"\"\n",
    "        Prints a bunch of statistics around the generated text. Useful for debugging purposes.\n",
    "        So far only works for greedy search.\n",
    "    \"\"\"\n",
    "\n",
    "    tgt_text = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=True)\n",
    "    tgt_text_with_tokens = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=False)\n",
    "    logger.info(f\"Generated text: {tgt_text}\")\n",
    "    logger.info(f\"Generated text with special tokens: {tgt_text_with_tokens}\")\n",
    "    logger.info(f\"Shape of translated.sequences:{translated.sequences.shape}\")\n",
    "    logger.info(f\"translated.sequences:{translated.sequences}\")\n",
    "    logger.info(f\"Scores is a tuple of length {len(translated.scores)} \\\n",
    "    and each score is a tensor of shape {translated.scores[0].shape}\")\n",
    "    scores_stacked = torch.stack(translated.scores, 1)\n",
    "    logger.info(f\"Stacking the scores into a tensor of shape {scores_stacked.shape}\")\n",
    "    scores_softmax = torch.softmax(scores_stacked, 2)\n",
    "    logger.info(f\"Now taking softmax. This shouldn't change the shape, but just to check,\\\n",
    "    its shape is {scores_softmax.shape}\")\n",
    "    probsums = scores_softmax.sum(axis=2)\n",
    "    logger.info(f\"These are probabilities now and so they should all sum to 1 (or close to it) in the axis \\\n",
    "    corresponding to each time step. We can check the sums here: {probsums}, but it's a long tensor \\\n",
    "    of shape {probsums.shape} and hard to see, so summing over all these values and removing 1 \\\n",
    "    from each gives {torch.sum(probsums - 1)} \\\n",
    "    which should be close to 0.\")\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    logger.info(\"Now calculating sequence probabilities\")\n",
    "    seq_token_probs = torch.gather(scores_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    seq_prob = seq_token_probs.prod(-1).item()\n",
    "    logger.info(f\"Sequence probability: {seq_prob}\")\n",
    "\n",
    "    # Get the 2nd and 3rd most likely tokens at each st\n",
    "    topk_ids = torch.topk(scores_softmax,3,dim=2).indices[:,:,1:]\n",
    "    topk_tokens_probs = torch.gather(scores_softmax,2,topk_ids).squeeze(-1)\n",
    "    toks2 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,0].squeeze())\n",
    "    toks3 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,1].squeeze())\n",
    "    tok_probs2 = topk_tokens_probs[:,:,0].squeeze()\n",
    "    tok_probs3 = topk_tokens_probs[:,:,1].squeeze()\n",
    "\n",
    "    logger.info(f\"Probabilities of getting the top 3 tokens at each step:\")\n",
    "    tokens = pp_tokenizer.convert_ids_to_tokens(seq_without_first_tkn.squeeze())\n",
    "    for (p, t, p2,t2,p3,t3)  in zip(seq_token_probs.squeeze(), tokens, tok_probs2, toks2, tok_probs3, toks3): \n",
    "        logger.info(f\"{t}: {round(p.item(),3)}  {t2}: {round(p2.item(),3)}  {t3}: {round(p3.item(),3)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  2.1281,  -1.7363,     -inf,  ...,  -2.0207,  -1.8297,  -0.2931],\n",
       "         [-12.2530,  -4.2081,     -inf,  ...,  -4.1955,  -4.1370,  -2.7101],\n",
       "         [ -7.8899,  -5.5816,     -inf,  ...,  -5.1575,  -4.9525,  -7.0733],\n",
       "         ...,\n",
       "         [ -0.6762,  -3.6511,   9.2148,  ...,  -2.7468,  -2.8557,  -3.2452],\n",
       "         [ -5.2623,  -4.6181,  10.6337,  ...,  -4.3726,  -4.1950,  -4.7886],\n",
       "         [ -4.3985,  -4.1799,  11.2302,  ...,  -4.6168,  -4.6227,  -3.4602]],\n",
       "\n",
       "        [[ -3.3283,  -1.4830,     -inf,  ...,  -1.2559,  -1.7851,  -0.3878],\n",
       "         [ -5.4642,  -3.7450,     -inf,  ...,  -3.5129,  -4.9730,  -2.3698],\n",
       "         [ -1.4317,  -4.9811,     -inf,  ...,  -4.1592,  -5.8414,  -4.4424],\n",
       "         ...,\n",
       "         [ -0.0560,  -2.8219,  15.0593,  ...,  -3.0489,  -4.4363,  -2.9208],\n",
       "         [  1.1428,  -3.6452,  12.2761,  ...,  -3.3208,  -3.9210,  -2.6603],\n",
       "         [ -0.7912,  -3.7815,  11.9712,  ...,  -3.3675,  -3.8684,  -3.5179]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the \"eugenesiow/bart-paraphrase\" model and the \"tdopierre/ProtAugment-ParaphraseGenerator\" are BART tokenizers and have type BartTokenizerFast. The implementation is identical to RobertaTokenizerFast according to the docs, which in turn was derived from GPT-2. They use byte-level Byte Pair Encoding.  \n",
    "\n",
    "The \"tuner007/pegasus_paraphrase\" model is a Pegasus tokenizer has type PegasusTokenizerFast. This uses Unigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BART tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'hello', 'Ġthere', '</s>'], ['<s>', 'Ġhello', 'Ġthere', '</s>']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pp_tokenizer_bart(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_bart, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasus tokenizer doesn't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁hello', '▁there', '</s>'], ['▁hello', '▁there', '</s>']]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pp_tokenizer_pegasus(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_pegasus, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizers represent tokens differently.  \n",
    "The BART models use Ġ to indicate start of a word for a token. Its generated tokens look like `['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>']`  \n",
    "The Pegasus model uses \\_ to indicate start of a word for a token. Its generated tokens look like `['▁Hello', '▁my', '▁name', '▁is', '▁z', 'fl', 'dl', 'fo', 'q', 'd', '</s>', '<pad>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer has these special tokens:['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "The bos token is <s> and has id 0\n",
      "The eos token is </s> and has id 2\n",
      "The pad token is <pad> and has id 1\n",
      "The unk token is <unk> and has id 3\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_bart.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_bart.bos_token} and has id {pp_tokenizer_bart.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_bart.eos_token} and has id {pp_tokenizer_bart.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_bart.pad_token} and has id {pp_tokenizer_bart.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_bart.unk_token} and has id {pp_tokenizer_bart.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer has these special tokens:['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "Using bos_token, but it is not set yet.\n",
      "The bos token is None and has id None\n",
      "The eos token is </s> and has id 1\n",
      "The pad token is <pad> and has id 0\n",
      "The unk token is <unk> and has id 105\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_pegasus.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_pegasus.bos_token} and has id {pp_tokenizer_pegasus.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_pegasus.eos_token} and has id {pp_tokenizer_pegasus.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_pegasus.pad_token} and has id {pp_tokenizer_pegasus.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_pegasus.unk_token} and has id {pp_tokenizer_pegasus.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special token usage with input and output sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use this format \n",
    "```\n",
    "single sequence: <s> X </s>\n",
    "pair of sequences: <s> A </s></s> B </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format: \n",
    "```\n",
    "- single sequence: ``X </s>``\n",
    "- pair of sequences: ``A B </s>`` (not intended use)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS token is never used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tokenizers also use different tokens when representing input sequences and generating output sentences. Here is a quick summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bart': {'special_tokens': ['<s>', '</s>', '<unk>', '<pad>', '<mask>'],\n",
       "  'input_start': '<s>',\n",
       "  'input_end': ['<pad>', '</s>'],\n",
       "  'output_start': '</s>',\n",
       "  'output_end': ['<pad>', '</s>']},\n",
       " 'pegasus': {'special_tokens': ['</s>',\n",
       "   '<unk>',\n",
       "   '<pad>',\n",
       "   '<mask_2>',\n",
       "   '<mask_1>',\n",
       "   '<unk_2>',\n",
       "   '<unk_3>',\n",
       "   '<unk_4>',\n",
       "   '<unk_5>',\n",
       "   '<unk_6>',\n",
       "   '<unk_7>',\n",
       "   '<unk_8>',\n",
       "   '<unk_9>',\n",
       "   '<unk_10>',\n",
       "   '<unk_11>',\n",
       "   '<unk_12>',\n",
       "   '<unk_13>',\n",
       "   '<unk_14>',\n",
       "   '<unk_15>',\n",
       "   '<unk_16>',\n",
       "   '<unk_17>',\n",
       "   '<unk_18>',\n",
       "   '<unk_19>',\n",
       "   '<unk_20>',\n",
       "   '<unk_21>',\n",
       "   '<unk_22>',\n",
       "   '<unk_23>',\n",
       "   '<unk_24>',\n",
       "   '<unk_25>',\n",
       "   '<unk_26>',\n",
       "   '<unk_27>',\n",
       "   '<unk_28>',\n",
       "   '<unk_29>',\n",
       "   '<unk_30>',\n",
       "   '<unk_31>',\n",
       "   '<unk_32>',\n",
       "   '<unk_33>',\n",
       "   '<unk_34>',\n",
       "   '<unk_35>',\n",
       "   '<unk_36>',\n",
       "   '<unk_37>',\n",
       "   '<unk_38>',\n",
       "   '<unk_39>',\n",
       "   '<unk_40>',\n",
       "   '<unk_41>',\n",
       "   '<unk_42>',\n",
       "   '<unk_43>',\n",
       "   '<unk_44>',\n",
       "   '<unk_45>',\n",
       "   '<unk_46>',\n",
       "   '<unk_47>',\n",
       "   '<unk_48>',\n",
       "   '<unk_49>',\n",
       "   '<unk_50>',\n",
       "   '<unk_51>',\n",
       "   '<unk_52>',\n",
       "   '<unk_53>',\n",
       "   '<unk_54>',\n",
       "   '<unk_55>',\n",
       "   '<unk_56>',\n",
       "   '<unk_57>',\n",
       "   '<unk_58>',\n",
       "   '<unk_59>',\n",
       "   '<unk_60>',\n",
       "   '<unk_61>',\n",
       "   '<unk_62>',\n",
       "   '<unk_63>',\n",
       "   '<unk_64>',\n",
       "   '<unk_65>',\n",
       "   '<unk_66>',\n",
       "   '<unk_67>',\n",
       "   '<unk_68>',\n",
       "   '<unk_69>',\n",
       "   '<unk_70>',\n",
       "   '<unk_71>',\n",
       "   '<unk_72>',\n",
       "   '<unk_73>',\n",
       "   '<unk_74>',\n",
       "   '<unk_75>',\n",
       "   '<unk_76>',\n",
       "   '<unk_77>',\n",
       "   '<unk_78>',\n",
       "   '<unk_79>',\n",
       "   '<unk_80>',\n",
       "   '<unk_81>',\n",
       "   '<unk_82>',\n",
       "   '<unk_83>',\n",
       "   '<unk_84>',\n",
       "   '<unk_85>',\n",
       "   '<unk_86>',\n",
       "   '<unk_87>',\n",
       "   '<unk_88>',\n",
       "   '<unk_89>',\n",
       "   '<unk_90>',\n",
       "   '<unk_91>',\n",
       "   '<unk_92>',\n",
       "   '<unk_93>',\n",
       "   '<unk_94>',\n",
       "   '<unk_95>',\n",
       "   '<unk_96>',\n",
       "   '<unk_97>',\n",
       "   '<unk_98>',\n",
       "   '<unk_99>',\n",
       "   '<unk_100>',\n",
       "   '<unk_101>',\n",
       "   '<unk_102>'],\n",
       "  'input_start': None,\n",
       "  'input_end': ['<pad>', '</s>'],\n",
       "  'output_start': '<pad>',\n",
       "  'output_end': ['<pad>', '</s>']}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_d = {\n",
    "    \"bart\": {\n",
    "        \"special_tokens\": pp_tokenizer_bart.all_special_tokens, \n",
    "        \"input_start\": pp_tokenizer_bart.bos_token,\n",
    "        \"input_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token], \n",
    "        \"output_start\": pp_tokenizer_bart.eos_token, \n",
    "        \"output_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token]\n",
    "    }, \n",
    "    \"pegasus\": {\n",
    "        \"special_tokens\": pp_tokenizer_pegasus.all_special_tokens,\n",
    "        \"input_start\": None,\n",
    "        \"input_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "        \"output_start\": pp_tokenizer_pegasus.pad_token,  \n",
    "        \"output_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "    }\n",
    "}\n",
    "tokens_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_from_ids(tokenizer, start_id=100, end_id=200):\n",
    "    ids = list(range(start_id,end_id))\n",
    "    print(*list(zip(ids, tokenizer.convert_ids_to_tokens(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having a look at generated tokens makes me suspect that they are indexed in whatever order they are encountered in the source text they are trained on. It seems like a rough frequency of english tokens but there are also tokens that are definitely out of order. \n",
    "\n",
    "The first few are reserved for special tokens, and the other low numbers (e.g. up to 100) are pretty common suffixes and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<s>') (1, '<pad>') (2, '</s>') (3, '<unk>') (4, '.') (5, 'Ġthe') (6, ',') (7, 'Ġto') (8, 'Ġand') (9, 'Ġof') (10, 'Ġa') (11, 'Ġin') (12, '-') (13, 'Ġfor') (14, 'Ġthat') (15, 'Ġon') (16, 'Ġis') (17, 'âĢ') (18, \"'s\") (19, 'Ġwith') (20, 'ĠThe') (21, 'Ġwas') (22, 'Ġ\"') (23, 'Ġat') (24, 'Ġit') (25, 'Ġas') (26, 'Ġsaid') (27, 'Ļ') (28, 'Ġbe') (29, 's') (30, 'Ġby') (31, 'Ġfrom') (32, 'Ġare') (33, 'Ġhave') (34, 'Ġhas') (35, ':') (36, 'Ġ(') (37, 'Ġhe') (38, 'ĠI') (39, 'Ġhis') (40, 'Ġwill') (41, 'Ġan') (42, 'Ġthis') (43, ')') (44, 'ĠâĢ') (45, 'Ġnot') (46, 'Ŀ') (47, 'Ġyou') (48, 'ľ') (49, 'Ġtheir')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at 100 to 200 you can see some words (e.g. Trump at 140, or 2017 at 193) that aren't common enough to be that high. This makes me suspect that words are in encounter order in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'I') (101, 'Ġlike') (102, 'a') (103, 'Ġsome') (104, 'S') (105, 'Ã«') (106, 'Ġthem') (107, 'Ġyears') (108, \"'\") (109, 'Ġdo') (110, 'Ġyour') (111, 'Ġ-') (112, 'Ġ1') (113, '\"') (114, 'Ġif') (115, 'Ġcould') (116, '?') (117, 'Ġno') (118, 'i') (119, 'm') (120, 'Ġget') (121, 'ĠU') (122, 'Ġnow') (123, 'Ġhim') (124, 'Ġback') (125, 'ĠBut') (126, 'ĠâĢĵ') (127, 'Ġmy') (128, \"Ġ'\") (129, 'Ġonly') (130, 'Ġthree') (131, ';') (132, 'Ġ2') (133, 'The') (134, '1') (135, 'Ġpercent') (136, 'Ġagainst') (137, 'Ġbefore') (138, 'Ġcompany') (139, 'o') (140, 'ĠTrump') (141, 'Ġhow') (142, 'Ġbecause') (143, 'Ġany') (144, 'Ġmost') (145, 'Ġbeing') (146, 'Ġmake') (147, 'Ġwhere') (148, 'Ġduring') (149, 'Ġthrough') (150, 'Ġwhile') (151, '000') (152, 'ĠThis') (153, 'Ġmillion') (154, 'ing') (155, 'Ġ3') (156, 'Ġmade') (157, 'Ġwell') (158, 'Ġ10') (159, 'Ġdown') (160, 'Ġoff') (161, 'Ġsays') (162, 'Ġme') (163, 'ĠB') (164, 'Ġgoing') (165, 'Ġteam') (166, 'ĠWe') (167, 'Ġthose') (168, 'Ġgovernment') (169, 'Ġway') (170, 'We') (171, 'Ġmany') (172, 'Ġthen') (173, 'Ġwork') (174, 'Ġtold') (175, 'com') (176, '2') (177, 'Ġgame') (178, 'ĠAnd') (179, 'in') (180, 'year') (181, 'Ġp') (182, 'Ġvery') (183, 'Ġday') (184, 'Ġhome') (185, 'Ġtake') (186, 'Ġweek') (187, 'Ġsince') (188, 'ĠNew') (189, 'Ġmay') (190, 'Ġeven') (191, 'Ġseason') (192, 'Ġsee') (193, 'Ġ2017') (194, 'Ġstate') (195, 'Ġ5') (196, 'ed') (197, 'Ġshould') (198, 'Ġaround') (199, 'Ġ2018')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 100,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokens towards the end are gibberish or mispellings encountered in the input. The fifth last token is something labelled <|endoftext|> and I don't know what that is. Then there is a bunch of tokens like \"madeupword0001\". The last token is the mask token and then token indicies after that return None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50245, 'ĠSetTextColor') (50246, 'Ġfixme') (50247, 'ĠãĤµãĥ¼ãĥĨãĤ£') (50248, 'ĠãĤµãĥ¼ãĥĨãĤ£ãĥ¯ãĥ³') (50249, 'ĠÂłĠÂłĠÂłĠÂłĠÂłĠÂłĠÂłĠÂł') (50250, 'ĠAdinida') (50251, 'ItemTracker') (50252, 'ĠDevOnline') (50253, 'ĠÂłÂł') (50254, '<?') (50255, '*=-') (50256, 'ÃĽÃĽ') (50257, 'ĠEntityItem') (50258, 'EngineDebug') (50259, 'ĠstrutConnector') (50260, '<|endoftext|>') (50261, 'madeupword0000') (50262, 'madeupword0001') (50263, 'madeupword0002') (50264, '<mask>') (50265, None) (50266, None) (50267, None) (50268, None) (50269, None) (50270, None) (50271, None) (50272, None) (50273, None) (50274, None)\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, pp_tokenizer_bart.vocab_size-20, pp_tokenizer_bart.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens make up the first hundred or so. After that there's a token \\<n> that seems like some new line thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<pad>') (1, '</s>') (2, '<mask_1>') (3, '<mask_2>') (4, '<unk_2>') (5, '<unk_3>') (6, '<unk_4>') (7, '<unk_5>') (8, '<unk_6>') (9, '<unk_7>') (10, '<unk_8>') (11, '<unk_9>') (12, '<unk_10>') (13, '<unk_11>') (14, '<unk_12>') (15, '<unk_13>') (16, '<unk_14>') (17, '<unk_15>') (18, '<unk_16>') (19, '<unk_17>') (20, '<unk_18>') (21, '<unk_19>') (22, '<unk_20>') (23, '<unk_21>') (24, '<unk_22>') (25, '<unk_23>') (26, '<unk_24>') (27, '<unk_25>') (28, '<unk_26>') (29, '<unk_27>') (30, '<unk_28>') (31, '<unk_29>') (32, '<unk_30>') (33, '<unk_31>') (34, '<unk_32>') (35, '<unk_33>') (36, '<unk_34>') (37, '<unk_35>') (38, '<unk_36>') (39, '<unk_37>') (40, '<unk_38>') (41, '<unk_39>') (42, '<unk_40>') (43, '<unk_41>') (44, '<unk_42>') (45, '<unk_43>') (46, '<unk_44>') (47, '<unk_45>') (48, '<unk_46>') (49, '<unk_47>') (50, '<unk_48>') (51, '<unk_49>') (52, '<unk_50>') (53, '<unk_51>') (54, '<unk_52>') (55, '<unk_53>') (56, '<unk_54>') (57, '<unk_55>') (58, '<unk_56>') (59, '<unk_57>') (60, '<unk_58>') (61, '<unk_59>') (62, '<unk_60>') (63, '<unk_61>') (64, '<unk_62>') (65, '<unk_63>') (66, '<unk_64>') (67, '<unk_65>') (68, '<unk_66>') (69, '<unk_67>') (70, '<unk_68>') (71, '<unk_69>') (72, '<unk_70>') (73, '<unk_71>') (74, '<unk_72>') (75, '<unk_73>') (76, '<unk_74>') (77, '<unk_75>') (78, '<unk_76>') (79, '<unk_77>') (80, '<unk_78>') (81, '<unk_79>') (82, '<unk_80>') (83, '<unk_81>') (84, '<unk_82>') (85, '<unk_83>') (86, '<unk_84>') (87, '<unk_85>') (88, '<unk_86>') (89, '<unk_87>') (90, '<unk_88>') (91, '<unk_89>') (92, '<unk_90>') (93, '<unk_91>') (94, '<unk_92>') (95, '<unk_93>') (96, '<unk_94>') (97, '<unk_95>') (98, '<unk_96>') (99, '<unk_97>') (100, '<unk_98>') (101, '<unk_99>') (102, '<unk_100>') (103, '<unk_101>') (104, '<unk_102>') (105, '<unk>') (106, '<n>') (107, '.') (108, ',') (109, '▁the') (110, '▁') (111, '▁and') (112, '▁to') (113, '▁of') (114, '▁a') (115, '▁in') (116, 's') (117, '▁is') (118, '▁for') (119, '▁you')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 0,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the BART models I can believe that these tokens are in order of frequency. I can't see anything that is obviously out of place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, '▁easy') (401, '...') (402, '▁‘') (403, '▁show') (404, '▁children') (405, '▁project') (406, '▁care') (407, '▁market') (408, '▁money') (409, '▁Our') (410, '▁book') (411, '▁change') (412, '▁So') (413, '▁To') (414, '▁put') (415, 'y') (416, '▁say') (417, 'You') (418, '▁room') (419, '▁got') (420, 'er') (421, '▁create') (422, '▁course') (423, '▁large') (424, '▁together') (425, '▁food') (426, '▁health') (427, '▁community') (428, '▁open') (429, '▁away') (430, '▁until') (431, '▁program') (432, '▁often') (433, '▁possible') (434, '▁When') (435, '▁again') (436, '▁All') (437, '▁case') (438, '▁page') (439, '▁car') (440, '▁real') (441, '▁With') (442, '▁name') (443, '▁call') (444, '▁include') (445, 'ly') (446, '▁per') (447, '▁why') (448, '▁product') (449, '▁state') (450, '▁post') (451, '▁based') (452, '▁She') (453, '▁second') (454, 'n') (455, '▁event') (456, '▁group') (457, 'i') (458, '▁having') (459, '▁old') (460, '▁become') (461, '▁big') (462, '▁play') (463, '▁What') (464, '▁against') (465, '▁person') (466, '▁along') (467, '▁list') (468, '“') (469, '▁price') (470, 'D') (471, '▁contact') (472, '▁comes') (473, '▁research') (474, '▁thing') (475, '▁U') (476, '▁level') (477, '▁side') (478, '▁less') (479, '▁done') (480, '▁house') (481, '▁public') (482, '▁across') (483, ',”') (484, '▁power') (485, '▁That') (486, '▁development') (487, '▁below') (488, '▁times') (489, '▁access') (490, 'or') (491, '▁point') (492, '▁—') (493, '▁makes') (494, '▁job') (495, '▁means') (496, '.\"') (497, 'to') (498, '▁live') (499, '▁range')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 120,250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing special at the end, just looks like isolated tokens and None values after the tokens finish. It's also worth noting that the Pegasus model has ~96100 tokens which is way more than the ~50270 of the BART models (almost double). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96083, '0:09') (96084, '▁Pietra') (96085, 'webhost') (96086, '8:48') (96087, '▁psychoanalytic') (96088, '1335') (96089, '▁Happies') (96090, '▁Tamale') (96091, '▁Seidel') (96092, '▁Muppet') (96093, '▁Quota') (96094, '▁polyphenol') (96095, 'utyrate') (96096, 'saari') (96097, '▁WASTE') (96098, '▁$6,500') (96099, '.06%') (96100, 'constitutional') (96101, '▁$6.4') (96102, 'ospermum') (96103, None) (96104, None) (96105, None) (96106, None) (96107, None) (96108, None) (96109, None) (96110, None) (96111, None) (96112, None)\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, pp_tokenizer_pegasus.vocab_size-20, pp_tokenizer_pegasus.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When does the model generate padding tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models padding tokens are generated after the EOS token. Additionally for Pegasus generated text starts with the padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do generated paraphrases start with the EOS token? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only the case for BART models. For pegasus models they use a padding token to start generated paraphrases. \n",
    "\n",
    "I don't know what the BOS token isn't used for these things. Pegasus has an open issue [here](https://github.com/huggingface/transformers/issues/12474). \n",
    "\n",
    "Whatever the reason you should just do the default because that is what the preprocessing does and you will get the best results that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does p(PAD) =1 after an eos token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both BART and Pegasus models it appears that probability of outputting a pad token is actually zero at all timesteps. Instead the model outputs the eos token over and over, and there must be some post-processing that takes place that replaces eos token with padding token. \n",
    "For Pegasus it appears it is the same behaviour. \n",
    "Example code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.16 0.9 ]\n",
      " [0.   0.   0.   0.   0.13 0.9  0.81 0.51 0.74 0.71 0.78]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(round_t(scores_softmax[:,:,pp_tokenizer.eos_token_id]))\n",
    "print(round_t(scores_softmax[:,:,pp_tokenizer.pad_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting is that there is probability assigned to other tokens other than eos and pad after a eos token is outputted. Again there must be some kind of postprocessing that takes care of this situation because I haven't really seen it in the wild. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models (e.g. GPT2) don't even have a PAD token. Instead they use the eos token on repeat. See this [issue](https://github.com/huggingface/transformers/issues/8452#issuecomment-739008168). What is confusing is seeing this behaviour with models that have a padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do rows always sum to 1 when looking at token generation scores? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they should. I put in an assert to check this. \n",
    "If you have a nan or an inf then they won't sum to 1. To confirm this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(torch.sum(torch.tensor([1,2,3, torch.nan]))))\n",
    "print(torch.isinf(torch.sum(torch.tensor([1,2,3, torch.inf]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does first row sum to 0? (the one corresponding to the startoff token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is no token scores that correspond to the first token (usually a bos or pad token). The scores are a tuple of length (`generated_length - 1`). So there shouldn't be a \"zero\" row really. \n",
    "I remember seeing something like this at some point so I'll keep an eye out for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are logits containing nan or inf transformed with softmax and log_softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore this through some code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla case  \n",
    "First we look at the case without any nan or inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000, -1.0000,  3.0000,  2.0000])\n",
      "tensor([0.1271, 0.0115, 0.6297, 0.2316])\n",
      "tensor([-2.0625, -4.4625, -0.4625, -1.4625])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax values are interpreted as probabilities, and the log softmax is just the log of the probabilities, done for numerical stability. We can just take exponents to return to probabilities if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1271, 0.0115, 0.6297, 0.2316])\n"
     ]
    }
   ],
   "source": [
    "print(torch.log_softmax(logits,0).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we introduce a positive inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000, -1.0000,  3.0000,  2.0000,     inf])\n",
      "tensor([nan, nan, nan, nan, nan])\n",
      "tensor([nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get nan values in the softmax and log_softmax.  So if you see nans in the softmax, remember that an inf in the scores is one reason why it may happen.  \n",
    "\n",
    "This is interesting because if we just assume inf is a large positive number, we'd expect a softmax with basically a 1 and all zeros, and a log softmax of a 0 and a lot of negatives. We can try it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000e+00, -1.0000e+00,  3.0000e+00,  2.0000e+00,  1.0000e+10])\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([-1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,  0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, 10000000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we get. So this indicates that if we get a positive inf we might be able to mitigate this problem by clipping it to some kind of maximum value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000, -1.0000,  3.0000,  2.0000,    -inf])\n",
      "tensor([0.1271, 0.0115, 0.6297, 0.2316, 0.0000])\n",
      "tensor([-2.0625, -4.4625, -0.4625, -1.4625,    -inf])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative inf behaves a bit differently. The softmax is unaffected and basically just assigns a prob of 0 to the corresponding entry. The log softmax carries the `-inf` through. \n",
    "\n",
    "Again clipping the -inf to a large negative value can mitigate this problem somewhat: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000e+00, -1.0000e+00,  3.0000e+00,  2.0000e+00, -1.0000e+07])\n",
      "tensor([0.1271, 0.0115, 0.6297, 0.2316, 0.0000])\n",
      "tensor([-2.0625e+00, -4.4625e+00, -4.6253e-01, -1.4625e+00, -1.0000e+07])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -10000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4000, -1.0000,  3.0000,  2.0000,     nan])\n",
      "tensor([nan, nan, nan, nan, nan])\n",
      "tensor([nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.nan])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nan in the logits propagates and affects the entire softmax and log_softmax tensors. The network basically gives up and says \"no idea how to deal with this. \n",
    "\n",
    "This seems to be the case with most torch functions; e.g.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n",
      "tensor([ 7., -5., 15., 10., nan])\n",
      "tensor([ 2.8000, -2.0000,  6.0000,  4.0000,     nan])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(logits,0))\n",
    "print(torch.divide(logits,0.2))\n",
    "print(logits + logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you interpret token entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token scores (when stacked) are a tensor of dimensions (batch_size, generated_length - 1, vocab_size). We take softmax to get a tensor of probability distributions across all possible tokens. We can also calculate the entropy of each of these probability distributions. \n",
    "\n",
    "Entropy is a measure of how \"peaky\" or \"flat\" a probability distribution is. It is the expected value of the self-information of an event, which is basically a measure of how \"surprised\" you would be if that event occured. \n",
    "If we have a discrete random variable $X$ with probability distribution $P(x) $, entropy is given by $$H(X) = \\mathbb{E}_{ X\\sim P} [I(x)] = -\\mathbb{E}_{X \\sim P} [\\log P(x)] $$ which is practically calculated by $$H(X) =  -\\sum_{x=-\\infty}^\\infty p(x) \\log(p(x))$$\n",
    "\n",
    "The lowest value of entropy is 0, which is when you have $P(x)=1$ for some event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1921e-07)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorical(probs = torch.tensor([1,0,0,0])).entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High values of entropy occur when the probability distribution is very flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2206)\n",
      "tensor(1.3863)\n"
     ]
    }
   ],
   "source": [
    "print(Categorical(probs = torch.tensor([0.20,0.50,0.10,0.20])).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor([0.25,0.25,0.25,0.25])).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of tokens, we can show two realistic distributions below. The first has two likely tokens, one somewhat likely, and the rest unlikely. The second has many more likely tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9989)\n",
      "tensor(2.4412)\n"
     ]
    }
   ],
   "source": [
    "l1 = [0.001,0.001,0.001,0.001,0.5,0.001,0.4,0.001,0.001,0.001,0.001,0.001,0.09]\n",
    "l2 = [0.1,0.1,0.1,0.1,0.025,0.1,0.1,0.1,0.1,0.1,0.025,0.025,0.025]\n",
    "print(Categorical(probs = torch.tensor(l1)).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor(l2)).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a theoretical maximum for entropy, but for tokens you'll be governed by vocab size. Here we show some practical maximums for some different vocab sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9078)\n",
      "tensor(9.2103)\n",
      "tensor(10.8198)\n",
      "tensor(11.5129)\n"
     ]
    }
   ],
   "source": [
    "v_size = 1000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # v small vocab\n",
    "v_size = 10000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # small vocab\n",
    "v_size = 50000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~BART vocab\n",
    "v_size = 100000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~PEGASUS vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be a bit hard to interpret so maybe you should also look at some other token-level stats, like max_prob, second_max_prob, third_max_prob, mean, variance or other things like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other things to note. First you still get an entropy value if your probability dist sums up to more than 1, so make sure to check this before doing entropy. Secondly if you have nan or inf in the probability values then you will get an error. This is true if you use either `probs` or `logits` in the Categorical function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3322)\n"
     ]
    }
   ],
   "source": [
    "print(Categorical(probs = torch.tensor([0.5,0.25,0.25,0.25])).entropy())       # sums to more than 1, gives result\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.inf])).entropy())  # throws error\n",
    "#print(Categorical(logits = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you get nan and inf introduced into token scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that you get -inf for the first (min_length - 1) steps when you introduce a min_length parameter for the generated sequences for the eos_token_id slot. This is to stop the token from appearing and truncating the sequence. \n",
    "\n",
    "You might also get -inf when setting other parameters to the `generate()` function (e.g. bad_words_ids or something similar).\n",
    "\n",
    "Wrote an assert for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to check \n",
    "\n",
    "\n",
    "* how do you get inf introduced into token scores?\n",
    "* how big is the action space? \n",
    "  * initial estimate: on the order of vocab_size ^ sequence length (so bloody huge)\n",
    "  * but only a very small proportion are valid actions\n",
    "* how many pp has probs over: 1e-5, 1e-4, 1e-3, 1e-2, 1e-1. would be a good plot. x axis epoch, then ether do (a) for individual examples, or (b) as averages across examples\n",
    "* log top X sampled sentences and their probs (maybe probs can be a graph)\n",
    "\n",
    "* how does padding mask affect things?\n",
    "* how does token-type-ids affect things? \n",
    "* which dist do you calculate KL divergence and entropy over? \n",
    "  * is it the token entropy’s at each generation step? \n",
    "  * is it the entropy of the generated paraphrase tokens? \n",
    "* given size of action space is this a good candidate for differential entropy? \n",
    "* when do you hit floating point threshold for token probabilities? When do nans and inf get introduced? \n",
    "* does using fp32 affect token calculations? \n",
    "* how does dropout affect generated probabilities? How does train/eval mode affect generated probs for a sentence? \n",
    "* how does layer-norm affect the probs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "### token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decoder_start_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log each generated word and the next few probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
