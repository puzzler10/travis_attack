{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I explore some properties about the generation process for paraphrase generation. I also answer some questions I had. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.distributions import Categorical\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s') \n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "\n",
    "## PEGASUS model\n",
    "pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "pp_tokenizer_pegasus = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_pegasus.generate)\n",
    "pp_model_pegasus.generate_with_grad = MethodType(generate_with_grad, pp_model_pegasus)\n",
    "\n",
    "## BART model\n",
    "pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "pp_tokenizer_bart = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_bart = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_bart.generate)\n",
    "pp_model_bart.generate_with_grad = MethodType(generate_with_grad, pp_model_bart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     20,
     26,
     43,
     48,
     65,
     82,
     92,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(translated.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "#     if np.any(np.isnan(seq_log_prob.detach().cpu()).tolist()): \n",
    "#         warnings.warn(f\"Warning: NAN's detected in pp_logp calclulations.\\n seq_token_log_probs: {seq_token_log_probs}\")\n",
    "    return seq_log_prob\n",
    "\n",
    "def get_tokens_from_token_ids_batch(tokenizer, ids_batch):\n",
    "    l = []\n",
    "    for i in range(ids_batch.shape[0]): \n",
    "        l.append(tokenizer.convert_ids_to_tokens(ids_batch[i,:]))\n",
    "    return l\n",
    "\n",
    "def get_start_end_special_token_ids(tokenizer): \n",
    "    \"\"\"The token id's that input/output sequences should start and end with\"\"\"\n",
    "    d = {}\n",
    "    if pp_tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "        d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    elif pp_tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "        d[\"input_start_id\"] =  None\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "        d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    else: \n",
    "        raise Exception(\"unrecognised tokenizer\")\n",
    "    return d\n",
    "\n",
    "def check_no_nans_or_infs(x):\n",
    "    assert torch.all(~torch.isnan(x))\n",
    "    assert torch.all(~torch.isneginf(x))\n",
    "    assert torch.all(~torch.isposinf(x))\n",
    "\n",
    "def assert_start_and_end_tokens_are_correct(tokenizer, orig_token_ids, pp_token_ids):\n",
    "    \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "    right special tokens (depends on tokenizer)\"\"\"\n",
    "    start_end_token_d = get_start_end_special_token_ids(pp_tokenizer)\n",
    "    \n",
    "    # Input\n",
    "    if start_end_token_d['input_start_id'] is not None: \n",
    "        assert torch.all(orig_token_ids[:,0] == start_end_token_d['input_start_id'])\n",
    "    # can probs rewrite this to make it nicer but it's fine for now\n",
    "    assert torch.all(torch.logical_or(orig_token_ids[:,-1] == start_end_token_d['input_end_id'][0], \n",
    "                                      orig_token_ids[:,-1] == start_end_token_d['input_end_id'][1]))\n",
    "    \n",
    "    # Output\n",
    "    assert torch.all(pp_token_ids[:,0] == start_end_token_d['output_start_id'])\n",
    "    assert torch.all(torch.logical_or(pp_token_ids[:,-1] == start_end_token_d['output_end_id'][0], \n",
    "                                      pp_token_ids[:,-1] == start_end_token_d['output_end_id'][1]))\n",
    "\n",
    "def check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked): \n",
    "    \"\"\"Check we don't have any postive inf or nan, and that all negative inf values are expected\"\"\"\n",
    "    assert torch.all(~torch.isposinf(scores_stacked))\n",
    "    assert torch.all(~torch.isnan(scores_stacked))\n",
    "\n",
    "    # We expect to see negative inf for the eos_token when we have not reached min_length. \n",
    "    # But we shouldn't expect it for any other tokens\n",
    "    idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "    assert torch.all(idx_neginf[:,2] == pp_tokenizer.eos_token_id)\n",
    "    # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "    # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "    # shouldn't be set to -inf\n",
    "    # Not a 100% test but very likely to identify\n",
    "    assert idx_neginf.shape[0] == (pp_model_params[\"min_length\"] -1) * batch_size  \n",
    "    # Check that no elements after min_length are -inf\n",
    "    assert torch.all(idx_neginf[:,1] < (pp_model_params[\"min_length\"] -1 ))\n",
    "\n",
    "def check_scores_log_softmax_sums_and_shape(scores_log_softmax):\n",
    "    sums = scores_log_softmax.exp().sum(2)\n",
    "    # check that the axes is right\n",
    "    # we want to sum over token probabilities at each generation step, so we \n",
    "    # should end up with a shape [batch_size, generated_length]\n",
    "    assert sums.shape[0] == batch_size  \n",
    "    assert sums.shape[1] == generated_length - 1\n",
    "    # check that they sum to 1 along the generated_length axis\n",
    "    assert torch.allclose(sums, torch.ones(sums.size()), atol = 1e-4)\n",
    "    \n",
    "def check_seq_token_log_prob_values_are_correct(): \n",
    "    \"\"\"Just enumerates and checks values\n",
    "    Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i_ex in range(batch_size):\n",
    "        for i_step in range(generated_length - 1):\n",
    "            i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "            l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "    assert all(l)\n",
    "    \n",
    "def pretty_print_pp_batch_and_next_token_probabilities(): \n",
    "    \"\"\"Goes through each paraphrase and shows at each timestep the next likely tokens. \n",
    "    Only will work for greedy search. \n",
    "    e.g. [\n",
    "    \"<pad> ['▁My, 0.289', '▁I, 0.261', '▁Hello, 0.07'] | Entropy: 4.23 \",\n",
    "     \"<pad> My ['▁name, 0.935', '▁Name, 0.005', 'name, 0.002'] | Entropy: 0.80 \"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    str_d = defaultdict(list)\n",
    "    for i_tkn in range(0, generated_length-1): \n",
    "        ids = pp_output.sequences[:, :(i_tkn+1)]\n",
    "        partial_pp = pp_tokenizer.batch_decode(ids)\n",
    "        kth_ids,kth_probs = tkn_kmaxidx[:, i_tkn, :], tkn_kmaxprob[:, i_tkn, :]\n",
    "        kth_tkns = get_tokens_from_token_ids_batch(pp_tokenizer, kth_ids)\n",
    "\n",
    "        # enumerates examples in batch\n",
    "        z = zip(partial_pp, kth_tkns, kth_probs, ent.detach())\n",
    "        for i_ex, (ex_sen, ex_next_tkns, ex_next_probs, ex_e) in enumerate(z): \n",
    "            # Form nice formatted string mixing together tokens and probabilities\n",
    "            tkn_tuples_l = [(tkn, round_t(prob,3)) for tkn, prob in zip(ex_next_tkns, ex_next_probs)]\n",
    "            tkn_str = ['%s, %s' % t for t in tkn_tuples_l]\n",
    "            # Add to dict of lists and add on entropy term. \n",
    "            str_d[i_ex].append(f\"{ex_sen} {tkn_str} | Entropy: {ex_e[i_tkn]:.2f} \")\n",
    "\n",
    "    for v in str_d.values():  pprint(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy search for paraphrase generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT AND PARAMETERS\n",
    "orig_l = [\n",
    "    \"What colour is the moon? I don't know.\", \n",
    "    \"Far out, if I have to write another sentence...it'll be bad.\"\n",
    "]\n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "\n",
    "## Select which model/tokenizer to use\n",
    "pp_tokenizer = pp_tokenizer_bart\n",
    "pp_model = pp_model_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "############################## TOKENIZER ########################################\n",
      "\n",
      "We are using the eugenesiow/bart-paraphrase tokenizer\n",
      "Tokenizer has these special tokens:['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "The bos token is <s> and has id 0\n",
      "The eos token is </s> and has id 2\n",
      "The pad token is <pad> and has id 1\n",
      "The unk token is <unk> and has id 3\n",
      "\n",
      "############################### INPUT #######################################\n",
      "\n",
      "Original text: [\"What colour is the moon? I don't know.\", \"Far out, if I have to write another sentence...it'll be bad.\"]\n",
      "Batch size is: 2\n",
      "This is tokenised to get a dict with keys dict_keys(['input_ids', 'attention_mask']) which should be input_ids and attention_mask \n",
      "The input_ids look like this: tensor([[    0,  2264,  7705,    16,     5,  6950,   116,    38,   218,    75,\n",
      "           216,     4,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 20680,    66,     6,   114,    38,    33,     7,  3116,   277,\n",
      "          3645,   734,   405,   581,    28,  1099,     4,     2,     1,     1]])\n",
      "The tokens are: [['<s>', 'What', 'Ġcolour', 'Ġis', 'Ġthe', 'Ġmoon', '?', 'ĠI', 'Ġdon', \"'t\", 'Ġknow', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<s>', 'Far', 'Ġout', ',', 'Ġif', 'ĠI', 'Ġhave', 'Ġto', 'Ġwrite', 'Ġanother', 'Ġsentence', '...', 'it', \"'ll\", 'Ġbe', 'Ġbad', '.', '</s>', '<pad>', '<pad>']]\n",
      "This has shape torch.Size([2, 20]) or [batch_size, input_length], which also might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\n",
      "Input length is: 20\n",
      "The attention_mask looks like this: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "This has shape torch.Size([2, 20]) or [batch_size, input_length]\n",
      "\n",
      "#################################### PARAPHRASES ##################################\n",
      "\n",
      "Paraphrases: ['What colour is the moon?', \"If I have to write another sentence, it'll be bad.\"]\n",
      "Output has keys odict_keys(['sequences', 'scores'])\n",
      "Paraphrases with special tokens: ['</s>What colour is the moon?</s><pad><pad><pad><pad><pad><pad><pad>', \"</s>If I have to write another sentence, it'll be bad.</s>\"]\n",
      "List of pp tokens:[['</s>', 'What', 'Ġcolour', 'Ġis', 'Ġthe', 'Ġmoon', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['</s>', 'If', 'ĠI', 'Ġhave', 'Ġto', 'Ġwrite', 'Ġanother', 'Ġsentence', ',', 'Ġit', \"'ll\", 'Ġbe', 'Ġbad', '.', '</s>']]\n",
      "Paraphrase token sequences: tensor([[   2, 2264, 7705,   16,    5, 6950,  116,    2,    1,    1,    1,    1,\n",
      "            1,    1,    1],\n",
      "        [   2, 1106,   38,   33,    7, 3116,  277, 3645,    6,   24,  581,   28,\n",
      "         1099,    4,    2]])\n",
      "Shape of pp token sequences:torch.Size([2, 15]) or [batch_size, generated_length]\n",
      "Generated length: 15\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5f3e818b86cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Our scores_stacked is stacked on dim 1 so it should be second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mscores_stacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerated_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mcheck_scores_for_posinf_nan_and_unexpected_neginf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_stacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d0e1e9413eff>\u001b[0m in \u001b[0;36mcheck_scores_for_posinf_nan_and_unexpected_neginf\u001b[0;34m(scores_stacked)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# But we shouldn't expect it for any other tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0midx_neginf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misneginf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_stacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_neginf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Rough check that all idx before min_length are -inf for all elements in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# We do min_length - 1 because sequences are allowed to have length min_length so that idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### TOKENIZER INFORMATION #####\n",
    "logger.info(\"\\n############################## TOKENIZER ########################################\\n\")\n",
    "logger.info(f\"We are using the {pp_tokenizer.name_or_path} tokenizer\")\n",
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer.bos_token} and has id {pp_tokenizer.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer.eos_token} and has id {pp_tokenizer.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer.pad_token} and has id {pp_tokenizer.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer.unk_token} and has id {pp_tokenizer.unk_token_id}\")\n",
    "\n",
    "\n",
    "#### INPUT #####\n",
    "batch_size = len(orig_l)\n",
    "orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "input_length = orig_tokens['input_ids'].size()[1]\n",
    "orig_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, orig_tokens['input_ids'])\n",
    "\n",
    "logger.info(\"\\n############################### INPUT #######################################\\n\")\n",
    "logger.info(f\"Original text: {orig_l}\")\n",
    "logger.info(f\"Batch size is: {batch_size}\")\n",
    "logger.info(f\"This is tokenised to get a dict with keys {orig_tokens.keys()} which should be input_ids and attention_mask \")\n",
    "logger.info(f\"The input_ids look like this: {orig_tokens['input_ids']}\")\n",
    "logger.info(f\"The tokens are: {orig_l_tokens_list}\")\n",
    "logger.info(f\"This has shape {orig_tokens['input_ids'].shape} or [batch_size, input_length], which also\\\n",
    " might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\")\n",
    "logger.info(f\"Input length is: {input_length}\")\n",
    "logger.info(f\"The attention_mask looks like this: {orig_tokens['attention_mask']}\")\n",
    "logger.info(f\"This has shape {orig_tokens['attention_mask'].shape} or [batch_size, input_length]\")\n",
    "\n",
    "##### PARAPHRASE #####\n",
    "pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                      return_dict_in_generate=True,\n",
    "                                      output_scores=True,\n",
    "                                    remove_invalid_values=False)\n",
    "generated_length = pp_output.sequences.shape[1]\n",
    "pp_l             = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "pp_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, pp_output.sequences)\n",
    "\n",
    "assert_start_and_end_tokens_are_correct(pp_tokenizer, orig_token_ids=orig_tokens['input_ids'],\n",
    "                                        pp_token_ids=pp_output.sequences)\n",
    "\n",
    "logger.info(\"\\n#################################### PARAPHRASES ##################################\\n\")\n",
    "logger.info(f\"Paraphrases: {pp_l}\")\n",
    "logger.info(f\"Output has keys {pp_output.keys()}\")\n",
    "logger.info(f\"Paraphrases with special tokens: {pp_l_with_tokens}\")\n",
    "logger.info(f\"List of pp tokens:{pp_l_tokens_list}\")\n",
    "logger.info(f\"Paraphrase token sequences: {pp_output.sequences}\")\n",
    "logger.info(f\"Shape of pp token sequences:{pp_output.sequences.shape} or [batch_size, generated_length]\")\n",
    "logger.info(f\"Generated length: {generated_length}\")\n",
    "\n",
    "###### SCORES AND PROBABILITIES ########\n",
    "scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "# The second argument to stack (i.e. dim) determines which axis the tensors are stacked along. \n",
    "# It determines the axis that becomes generated_length - 1\n",
    "# dim=0 gives shape [generated_length-1, batch_size, vocab_size]\n",
    "# dim=1 gives shape [batch_size, generated_length-1, vocab_size]\n",
    "# dim=2 gives shape [batch_size, vocab_size, generated_length-1]\n",
    "# Our scores_stacked is stacked on dim 1 so it should be second \n",
    "assert scores_stacked.shape == torch.Size([batch_size, (generated_length - 1), pp_tokenizer.vocab_size])\n",
    "check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked)\n",
    "\n",
    "\n",
    "# These scores are logits \n",
    "# see some of the docs on this page https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput\n",
    "# so we got to take softmax over them \n",
    "# but if we take regular softmax then we run into numerical errors\n",
    "# so instead we take log_softmax\n",
    "scores_log_softmax = torch.log_softmax(scores_stacked, 2)\n",
    "check_scores_log_softmax_sums_and_shape(scores_log_softmax)\n",
    "\n",
    "\n",
    "####### SEQUENCE PROBABILITIES #######\n",
    "# We select the token probability corresponding to each token \n",
    "# However because the scores represent transitions we need to remove the first token from each \n",
    "# sequence to match them up. \n",
    "seq_without_first_tkn = pp_output.sequences[:,1:]\n",
    "assert seq_without_first_tkn.shape == torch.Size([batch_size, generated_length - 1])\n",
    "# Now select prob corresponding to each token\n",
    "seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "assert seq_token_log_probs.shape == seq_without_first_tkn.shape  # probs should be 1-1 with the filtered tkns\n",
    "check_no_nans_or_infs(seq_token_log_probs)\n",
    "# Check that the last token probability corresponds to a possible end token \n",
    "output_end_ids = get_start_end_special_token_ids(pp_tokenizer)['output_end_id']\n",
    "assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "check_seq_token_log_prob_values_are_correct()\n",
    "\n",
    "# The attention mask has 1 everywhere except for where padding tokens occur, where it has 0. \n",
    "# It is used to filter out padding tokens from the sequence probablity because then the sequence \n",
    "# probability will depend on how many padding tokens there are and the probability of generating them, \n",
    "# which (a) we don't want and (b) the probability isn't correct anyway \n",
    "attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "    seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    ")\n",
    "seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "# check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "assert all(seq_without_first_tkn[attention_mask == 0] == pp_tokenizer.pad_token_id)\n",
    "assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "assert torch.all(seq_token_log_probs  > -10)  # we shouldn't be picking extrememly rare tokens\n",
    "seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "assert seq_log_prob.shape == torch.Size([batch_size])\n",
    "check_no_nans_or_infs(seq_log_prob)\n",
    "\n",
    "logger.info(\"\\n##########################  SCORES AND PROBABILITIES ####################################\\n\")\n",
    "logger.info(f\"Scores is a tuple of length {len(pp_output.scores)} which is one less than the generated_length, or \\\n",
    "the number of tokens in the pp token sequences (this has shape {pp_output.sequences.shape}\")\n",
    "logger.info(f\"Each score is a tensor of shape {pp_output.scores[0].shape} or [batch_size, vocab_size]\")\n",
    "#logger.info(f\"Full shape:{[o.shape for o in pp_output.scores]}\")\n",
    "logger.info(f\"We stack them to get a tensor of shape {scores_stacked.shape} or [batch_size, generated_length - 1, vocab_size]\")\n",
    "logger.info(f\"Scores are really logits so we have to take softmax to get probabilities. \")\n",
    "logger.info(\"But if we take regular softmax then we run into numerical errors so we take log softmax\")\n",
    "logger.info(\"We then select the token probability corresponding to each token and sum them to get the log \\\n",
    "probability of the sequence.\")\n",
    "\n",
    "############# ENTROPY AND TOKEN PROBABILITIES ####################\n",
    "ent = Categorical(logits = scores_stacked).entropy()\n",
    "assert ent.shape == torch.Size([batch_size, generated_length - 1])\n",
    "scores_softmax = scores_log_softmax.exp()\n",
    "k=3\n",
    "tkn_kmaxprob, tkn_kmaxidx = torch.topk(scores_softmax, k=k, dim=2)\n",
    "tkn_kmaxprob = tkn_kmaxprob.detach()  # log these \n",
    "# The third dimension indexes top1, top2, top3 etc \n",
    "assert tkn_kmaxprob[:,:,0].shape == torch.Size([batch_size, generated_length - 1])\n",
    "# I'd naively expect True everywhere for tkn_kmaxidx[:,:,0] == pp_output.sequences[:, 1:] but it turns \n",
    "# out this is not the case because padding tokens seem to have prob 0 and eos tokens are outputted \n",
    "# instead by the token generation process and then later replaced by pad\n",
    "\n",
    "#Uncomment to show how paraphrase is formed. \n",
    "\n",
    "\n",
    "logger.info(\"\\n########################## ENTROPY AND TOKEN PROBABILITIES ####################################\\n\")\n",
    "logger.info(f\"Originals: {orig_l}\")\n",
    "pretty_print_pp_batch_and_next_token_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search for paraphrase generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_l = [\n",
    "    \"Look! A small dog. Isn't it cute?\", \n",
    "    \"Far out, if I have to write another sentence...it'll be bad.\"\n",
    "]\n",
    "n_seq = 5\n",
    "pp_model_params = {\n",
    "    \"num_beams\": 5, \n",
    "    \"num_return_sequences\": n_seq, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,  \n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 0,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "batch_size = len(orig_l)\n",
    "logger.info(f\"Input: {orig_l}\")\n",
    "orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "input_length = orig_tokens['input_ids'].shape[1]\n",
    "pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                      return_dict_in_generate=True,\n",
    "                                      output_scores=True,\n",
    "                                    remove_invalid_values=False)\n",
    "logger.info(f\"Input: {orig_l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Output has keys odict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrases:\n",
      "['<pad> There is a small dog.</s><pad><pad><pad><pad><pad><pad><pad>',\n",
      " '<pad> A small dog.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
      " \"<pad> Isn't the dog cute?</s><pad><pad><pad><pad><pad><pad>\",\n",
      " '<pad> A small dog is cute.</s><pad><pad><pad><pad><pad><pad><pad>',\n",
      " \"<pad> Isn't it cute?</s><pad><pad><pad><pad><pad><pad><pad>\",\n",
      " '<pad> It will be bad if I have to write another sentence.</s><pad>',\n",
      " '<pad> I will be bad if I have to write another sentence.</s><pad>',\n",
      " \"<pad> It'll be bad if I have to write another sentence.</s>\",\n",
      " '<pad> If I have to write another sentence, it will be bad.</s>',\n",
      " '<pad> It will be bad if I have to write a new sentence.</s>']\n",
      "logprobs (has grad): tensor([ -73.2909,  -62.3275,  -88.2333,  -83.2530,  -79.5468, -134.9267,\n",
      "        -124.6652, -144.2008, -140.2372, -131.8344], grad_fn=<SumBackward1>)\n",
      "sequences scores (no grad): tensor([-3.4677, -3.5051, -3.6984, -4.4127, -4.5878, -5.1774, -5.8165, -6.0948,\n",
      "        -6.4582, -6.7450])\n",
      "baseline prob (selecting token with 1/vocab_size every input): -172.09763717929377\n",
      "baseline short seq prob: -80.31223068367044\n",
      "baseline high prob seq -68.48130799456172\n",
      "baseline high prob short seq -31.957943730795467\n"
     ]
    }
   ],
   "source": [
    "generated_length = pp_output.sequences.shape[1]\n",
    "assert pp_output.sequences.shape == torch.Size([batch_size * n_seq, generated_length ])\n",
    "pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "print(\"Paraphrases:\")\n",
    "pprint(pp_l_with_tokens)\n",
    "logger.info(f\"Output has keys {pp_output.keys()}\")\n",
    "assert pp_output.sequences_scores.shape == torch.Size([batch_size * n_seq])\n",
    "assert len(pp_output.scores) == generated_length  # different to greedy search: not generated_length - 1 \n",
    "assert pp_output.scores[0].shape == torch.Size([batch_size * n_seq, pp_tokenizer.vocab_size])\n",
    "\n",
    "# scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "# assert scores_stacked.shape == torch.Size([batch_size * n_seq, generated_length, pp_tokenizer.vocab_size])\n",
    "transition_logprobs = pp_model.compute_transition_beam_scores(\n",
    "    pp_output.sequences, pp_output.scores, pp_output.beam_indices, eos_token_id = pp_tokenizer.eos_token_id)\n",
    "pp_logprobs = transition_logprobs.sum(-1)\n",
    "assert pp_logprobs.shape == torch.Size([batch_size * n_seq])\n",
    "print(\"logprobs (has grad):\", pp_logprobs)\n",
    "baseline_seq_prob = np.log(1/pp_tokenizer.vocab_size)* generated_length\n",
    "baseline_short_seq_prob = np.log(1/pp_tokenizer.vocab_size)* np.floor(generated_length /2 )\n",
    "baseline_high_prob_seq =  np.log(1000/pp_tokenizer.vocab_size)* generated_length\n",
    "baseline_high_prob_short_seq =  np.log(1000/pp_tokenizer.vocab_size)* np.floor(generated_length /2 ) \n",
    "print(\"sequences scores (no grad):\", pp_output.sequences_scores)\n",
    "\n",
    "print(\"baseline prob (selecting token with prob 1/vocab_size every input):\",baseline_seq_prob )\n",
    "print(\"baseline short seq prob:\",baseline_short_seq_prob )\n",
    "print(\"baseline high prob seq\", baseline_high_prob_seq)\n",
    "print(\"baseline high prob short seq\", baseline_high_prob_short_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0 step 0 tkn_id 0 score -13.32 transition_logprob -13.32\n",
      "example 0 step 1 tkn_id 353 score -10.63 transition_logprob -10.63\n",
      "example 0 step 2 tkn_id 117 score -11.42 transition_logprob -11.42\n",
      "example 0 step 3 tkn_id 114 score -4.43 transition_logprob -4.43\n",
      "example 0 step 4 tkn_id 360 score -4.81 transition_logprob -4.81\n",
      "example 0 step 5 tkn_id 1296 score -12.34 transition_logprob -10.04\n",
      "example 0 step 6 tkn_id 107 score -7.78 transition_logprob -9.04\n",
      "example 0 step 7 tkn_id 1 score -0.09 transition_logprob -9.59\n",
      "example 0 step 8 tkn_id 0 score -14.78 transition_logprob 0.0\n",
      "example 0 step 9 tkn_id 0 score -14.1 transition_logprob 0.0\n",
      "example 0 step 10 tkn_id 0 score -12.72 transition_logprob 0.0\n",
      "example 0 step 11 tkn_id 0 score -12.96 transition_logprob 0.0\n",
      "example 0 step 12 tkn_id 0 score -13.45 transition_logprob 0.0\n",
      "example 0 step 13 tkn_id 0 score -13.51 transition_logprob 0.0\n",
      "example 0 step 14 tkn_id 0 score -12.31 transition_logprob 0.0\n",
      "example 1 step 0 tkn_id 0 score -13.32 transition_logprob -13.32\n",
      "example 1 step 1 tkn_id 202 score -11.07 transition_logprob -10.58\n",
      "example 1 step 2 tkn_id 360 score -9.04 transition_logprob -13.26\n",
      "example 1 step 3 tkn_id 1296 score -10.31 transition_logprob -8.52\n",
      "example 1 step 4 tkn_id 107 score -11.33 transition_logprob -10.0\n",
      "example 1 step 5 tkn_id 1 score -11.23 transition_logprob -6.64\n",
      "example 1 step 6 tkn_id 0 score -13.82 transition_logprob 0.0\n",
      "example 1 step 7 tkn_id 0 score -14.99 transition_logprob 0.0\n",
      "example 1 step 8 tkn_id 0 score -14.56 transition_logprob 0.0\n",
      "example 1 step 9 tkn_id 0 score -14.94 transition_logprob 0.0\n",
      "example 1 step 10 tkn_id 0 score -12.72 transition_logprob 0.0\n",
      "example 1 step 11 tkn_id 0 score -12.96 transition_logprob 0.0\n",
      "example 1 step 12 tkn_id 0 score -13.45 transition_logprob 0.0\n",
      "example 1 step 13 tkn_id 0 score -13.51 transition_logprob 0.0\n",
      "example 1 step 14 tkn_id 0 score -12.31 transition_logprob 0.0\n",
      "example 2 step 0 tkn_id 0 score -13.32 transition_logprob -13.32\n",
      "example 2 step 1 tkn_id 16099 score -10.26 transition_logprob -11.0\n",
      "example 2 step 2 tkn_id 131 score -11.24 transition_logprob -8.83\n",
      "example 2 step 3 tkn_id 144 score -12.3 transition_logprob -8.97\n",
      "example 2 step 4 tkn_id 109 score -12.31 transition_logprob -6.65\n",
      "example 2 step 5 tkn_id 1296 score -10.04 transition_logprob -10.04\n",
      "example 2 step 6 tkn_id 2860 score -13.31 transition_logprob -9.54\n",
      "example 2 step 7 tkn_id 152 score -11.61 transition_logprob -11.18\n",
      "example 2 step 8 tkn_id 1 score -8.0 transition_logprob -8.7\n",
      "example 2 step 9 tkn_id 0 score -15.21 transition_logprob 0.0\n",
      "example 2 step 10 tkn_id 0 score -12.72 transition_logprob 0.0\n",
      "example 2 step 11 tkn_id 0 score -12.96 transition_logprob 0.0\n",
      "example 2 step 12 tkn_id 0 score -13.45 transition_logprob 0.0\n",
      "example 2 step 13 tkn_id 0 score -13.51 transition_logprob 0.0\n",
      "example 2 step 14 tkn_id 0 score -12.31 transition_logprob 0.0\n",
      "example 3 step 0 tkn_id 0 score -13.32 transition_logprob -13.32\n",
      "example 3 step 1 tkn_id 202 score -11.85 transition_logprob -10.58\n",
      "example 3 step 2 tkn_id 360 score -8.58 transition_logprob -13.26\n",
      "example 3 step 3 tkn_id 1296 score -9.0 transition_logprob -8.52\n",
      "example 3 step 4 tkn_id 117 score -8.76 transition_logprob -8.7\n",
      "example 3 step 5 tkn_id 2860 score -11.39 transition_logprob -10.22\n",
      "example 3 step 6 tkn_id 107 score -9.04 transition_logprob -9.04\n",
      "example 3 step 7 tkn_id 1 score -9.54 transition_logprob -9.59\n",
      "example 3 step 8 tkn_id 0 score -14.64 transition_logprob 0.0\n",
      "example 3 step 9 tkn_id 0 score -14.96 transition_logprob 0.0\n",
      "example 3 step 10 tkn_id 0 score -12.72 transition_logprob 0.0\n",
      "example 3 step 11 tkn_id 0 score -12.96 transition_logprob 0.0\n",
      "example 3 step 12 tkn_id 0 score -13.45 transition_logprob 0.0\n",
      "example 3 step 13 tkn_id 0 score -13.51 transition_logprob 0.0\n",
      "example 3 step 14 tkn_id 0 score -12.31 transition_logprob 0.0\n",
      "example 4 step 0 tkn_id 0 score -13.32 transition_logprob -13.32\n",
      "example 4 step 1 tkn_id 16099 score -12.69 transition_logprob -11.0\n",
      "example 4 step 2 tkn_id 131 score -5.82 transition_logprob -8.83\n",
      "example 4 step 3 tkn_id 144 score -10.12 transition_logprob -8.97\n",
      "example 4 step 4 tkn_id 126 score -8.8 transition_logprob -10.66\n",
      "example 4 step 5 tkn_id 2860 score -11.35 transition_logprob -10.22\n",
      "example 4 step 6 tkn_id 152 score -9.27 transition_logprob -6.95\n",
      "example 4 step 7 tkn_id 1 score -9.28 transition_logprob -9.59\n",
      "example 4 step 8 tkn_id 0 score -14.83 transition_logprob 0.0\n",
      "example 4 step 9 tkn_id 0 score -14.34 transition_logprob 0.0\n",
      "example 4 step 10 tkn_id 0 score -12.72 transition_logprob 0.0\n",
      "example 4 step 11 tkn_id 0 score -12.96 transition_logprob 0.0\n",
      "example 4 step 12 tkn_id 0 score -13.45 transition_logprob 0.0\n",
      "example 4 step 13 tkn_id 0 score -13.51 transition_logprob 0.0\n",
      "example 4 step 14 tkn_id 0 score -12.31 transition_logprob 0.0\n",
      "example 5 step 0 tkn_id 0 score -14.15 transition_logprob -14.15\n",
      "example 5 step 1 tkn_id 168 score -10.47 transition_logprob -10.47\n",
      "example 5 step 2 tkn_id 138 score -7.62 transition_logprob -7.62\n",
      "example 5 step 3 tkn_id 129 score -9.06 transition_logprob -9.06\n",
      "example 5 step 4 tkn_id 1025 score -10.82 transition_logprob -10.82\n",
      "example 5 step 5 tkn_id 175 score -10.33 transition_logprob -10.33\n",
      "example 5 step 6 tkn_id 125 score -8.65 transition_logprob -8.65\n",
      "example 5 step 7 tkn_id 133 score -12.74 transition_logprob -12.74\n",
      "example 5 step 8 tkn_id 112 score -8.92 transition_logprob -9.07\n",
      "example 5 step 9 tkn_id 1094 score -10.04 transition_logprob -14.51\n",
      "example 5 step 10 tkn_id 372 score -8.31 transition_logprob -12.76\n",
      "example 5 step 11 tkn_id 5577 score -11.99 transition_logprob -13.24\n",
      "example 5 step 12 tkn_id 107 score -7.52 transition_logprob -0.15\n",
      "example 5 step 13 tkn_id 1 score -0.11 transition_logprob -1.35\n",
      "example 5 step 14 tkn_id 0 score -13.53 transition_logprob 0.0\n",
      "example 6 step 0 tkn_id 0 score -14.15 transition_logprob -14.15\n",
      "example 6 step 1 tkn_id 125 score -10.55 transition_logprob -0.21\n",
      "example 6 step 2 tkn_id 138 score -7.72 transition_logprob -7.62\n",
      "example 6 step 3 tkn_id 129 score -8.5 transition_logprob -9.06\n",
      "example 6 step 4 tkn_id 1025 score -10.88 transition_logprob -10.82\n",
      "example 6 step 5 tkn_id 175 score -10.35 transition_logprob -10.33\n",
      "example 6 step 6 tkn_id 125 score -9.42 transition_logprob -8.65\n",
      "example 6 step 7 tkn_id 133 score -9.19 transition_logprob -12.74\n",
      "example 6 step 8 tkn_id 112 score -0.11 transition_logprob -9.07\n",
      "example 6 step 9 tkn_id 1094 score -0.37 transition_logprob -14.51\n",
      "example 6 step 10 tkn_id 372 score -0.63 transition_logprob -12.76\n",
      "example 6 step 11 tkn_id 5577 score -13.24 transition_logprob -13.24\n",
      "example 6 step 12 tkn_id 107 score -7.43 transition_logprob -0.15\n",
      "example 6 step 13 tkn_id 1 score -0.11 transition_logprob -1.35\n",
      "example 6 step 14 tkn_id 0 score -14.08 transition_logprob 0.0\n",
      "example 7 step 0 tkn_id 0 score -14.15 transition_logprob -14.15\n",
      "example 7 step 1 tkn_id 168 score -10.83 transition_logprob -10.47\n",
      "example 7 step 2 tkn_id 131 score -6.52 transition_logprob -4.68\n",
      "example 7 step 3 tkn_id 267 score -8.71 transition_logprob -12.42\n",
      "example 7 step 4 tkn_id 129 score -8.21 transition_logprob -8.12\n",
      "example 7 step 5 tkn_id 1025 score -10.59 transition_logprob -9.97\n",
      "example 7 step 6 tkn_id 175 score -10.33 transition_logprob -10.94\n",
      "example 7 step 7 tkn_id 125 score -9.39 transition_logprob -4.12\n",
      "example 7 step 8 tkn_id 133 score -11.83 transition_logprob -11.83\n",
      "example 7 step 9 tkn_id 112 score -9.91 transition_logprob -9.91\n",
      "example 7 step 10 tkn_id 1094 score -11.24 transition_logprob -11.24\n",
      "example 7 step 11 tkn_id 372 score -8.14 transition_logprob -9.31\n",
      "example 7 step 12 tkn_id 5577 score -11.94 transition_logprob -12.68\n",
      "example 7 step 13 tkn_id 107 score -7.55 transition_logprob -7.31\n",
      "example 7 step 14 tkn_id 1 score -6.89 transition_logprob -7.04\n",
      "example 8 step 0 tkn_id 0 score -14.15 transition_logprob -14.15\n",
      "example 8 step 1 tkn_id 240 score -12.38 transition_logprob -13.17\n",
      "example 8 step 2 tkn_id 125 score -10.65 transition_logprob -9.42\n",
      "example 8 step 3 tkn_id 133 score -10.91 transition_logprob -8.22\n",
      "example 8 step 4 tkn_id 112 score -10.03 transition_logprob -8.74\n",
      "example 8 step 5 tkn_id 1094 score -11.13 transition_logprob -9.4\n",
      "example 8 step 6 tkn_id 372 score -5.96 transition_logprob -7.95\n",
      "example 8 step 7 tkn_id 5577 score -12.62 transition_logprob -12.02\n",
      "example 8 step 8 tkn_id 108 score -9.01 transition_logprob -9.05\n",
      "example 8 step 9 tkn_id 126 score -5.53 transition_logprob -7.22\n",
      "example 8 step 10 tkn_id 138 score -11.02 transition_logprob -6.81\n",
      "example 8 step 11 tkn_id 129 score -11.46 transition_logprob -8.29\n",
      "example 8 step 12 tkn_id 1025 score -11.54 transition_logprob -11.54\n",
      "example 8 step 13 tkn_id 107 score -0.11 transition_logprob -7.31\n",
      "example 8 step 14 tkn_id 1 score -7.04 transition_logprob -6.94\n",
      "example 9 step 0 tkn_id 0 score -14.15 transition_logprob -14.15\n",
      "example 9 step 1 tkn_id 168 score -11.11 transition_logprob -10.83\n",
      "example 9 step 2 tkn_id 138 score -6.72 transition_logprob -7.72\n",
      "example 9 step 3 tkn_id 129 score -0.38 transition_logprob -8.5\n",
      "example 9 step 4 tkn_id 1025 score -10.8 transition_logprob -10.88\n",
      "example 9 step 5 tkn_id 175 score -0.13 transition_logprob -10.35\n",
      "example 9 step 6 tkn_id 125 score -9.45 transition_logprob -9.42\n",
      "example 9 step 7 tkn_id 133 score -12.75 transition_logprob -9.19\n",
      "example 9 step 8 tkn_id 112 score -9.69 transition_logprob -8.92\n",
      "example 9 step 9 tkn_id 1094 score -10.92 transition_logprob -10.04\n",
      "example 9 step 10 tkn_id 114 score -8.29 transition_logprob -8.29\n",
      "example 9 step 11 tkn_id 177 score -11.27 transition_logprob -11.27\n",
      "example 9 step 12 tkn_id 5577 score -12.05 transition_logprob -12.05\n",
      "example 9 step 13 tkn_id 107 score -7.31 transition_logprob -0.11\n",
      "example 9 step 14 tkn_id 1 score -0.17 transition_logprob -0.11\n"
     ]
    }
   ],
   "source": [
    "def compare_scores_and_transition_probs(): \n",
    "    # This code indicates that the transition probabilities are not the same as the scores. \n",
    "    # It seems they are the same sometimes but other times they are not. \n",
    "    for ex in range(batch_size * n_seq): \n",
    "        for step in range(generated_length):\n",
    "            tkn_id = pp_output.sequences[ex][step].item()\n",
    "            score = pp_output.scores[step][ex][tkn_id]\n",
    "            prob = transition_probs[ex][step]\n",
    "            print(\"example\", ex, \"step\", step, \"tkn_id\", tkn_id, \n",
    "                  \"score\", round_t(score), \"transition_logprob\", round_t(prob))\n",
    "compare_scores_and_transition_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the \"eugenesiow/bart-paraphrase\" model and the \"tdopierre/ProtAugment-ParaphraseGenerator\" are BART tokenizers and have type BartTokenizerFast. The implementation is identical to RobertaTokenizerFast according to the docs, which in turn was derived from GPT-2. They use byte-level Byte Pair Encoding.  \n",
    "\n",
    "The \"tuner007/pegasus_paraphrase\" model is a Pegasus tokenizer has type PegasusTokenizerFast. This uses Unigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BART tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pp_tokenizer_bart(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_bart, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasus tokenizer doesn't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pp_tokenizer_pegasus(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_pegasus, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizers represent tokens differently.  \n",
    "The BART models use Ġ to indicate start of a word for a token. Its generated tokens look like `['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>']`  \n",
    "The Pegasus model uses \\_ to indicate start of a word for a token. Its generated tokens look like `['▁Hello', '▁my', '▁name', '▁is', '▁z', 'fl', 'dl', 'fo', 'q', 'd', '</s>', '<pad>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_bart.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_bart.bos_token} and has id {pp_tokenizer_bart.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_bart.eos_token} and has id {pp_tokenizer_bart.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_bart.pad_token} and has id {pp_tokenizer_bart.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_bart.unk_token} and has id {pp_tokenizer_bart.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_pegasus.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_pegasus.bos_token} and has id {pp_tokenizer_pegasus.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_pegasus.eos_token} and has id {pp_tokenizer_pegasus.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_pegasus.pad_token} and has id {pp_tokenizer_pegasus.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_pegasus.unk_token} and has id {pp_tokenizer_pegasus.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special token usage with input and output sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use this format \n",
    "```\n",
    "single sequence: <s> X </s>\n",
    "pair of sequences: <s> A </s></s> B </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format: \n",
    "```\n",
    "- single sequence: ``X </s>``\n",
    "- pair of sequences: ``A B </s>`` (not intended use)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS token is never used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tokenizers also use different tokens when representing input sequences and generating output sentences. Here is a quick summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_d = {\n",
    "    \"bart\": {\n",
    "        \"special_tokens\": pp_tokenizer_bart.all_special_tokens, \n",
    "        \"input_start\": pp_tokenizer_bart.bos_token,\n",
    "        \"input_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token], \n",
    "        \"output_start\": pp_tokenizer_bart.eos_token, \n",
    "        \"output_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token]\n",
    "    }, \n",
    "    \"pegasus\": {\n",
    "        \"special_tokens\": pp_tokenizer_pegasus.all_special_tokens,\n",
    "        \"input_start\": None,\n",
    "        \"input_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "        \"output_start\": pp_tokenizer_pegasus.pad_token,  \n",
    "        \"output_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "    }\n",
    "}\n",
    "tokens_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_from_ids(tokenizer, start_id=100, end_id=200):\n",
    "    ids = list(range(start_id,end_id))\n",
    "    print(*list(zip(ids, tokenizer.convert_ids_to_tokens(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having a look at generated tokens makes me suspect that they are indexed in whatever order they are encountered in the source text they are trained on. It seems like a rough frequency of english tokens but there are also tokens that are definitely out of order. \n",
    "\n",
    "The first few are reserved for special tokens, and the other low numbers (e.g. up to 100) are pretty common suffixes and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at 100 to 200 you can see some words (e.g. Trump at 140, or 2017 at 193) that aren't common enough to be that high. This makes me suspect that words are in encounter order in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 100,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokens towards the end are gibberish or mispellings encountered in the input. The fifth last token is something labelled <|endoftext|> and I don't know what that is. Then there is a bunch of tokens like \"madeupword0001\". The last token is the mask token and then token indicies after that return None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, pp_tokenizer_bart.vocab_size-20, pp_tokenizer_bart.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens make up the first hundred or so. After that there's a token \\<n> that seems like some new line thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 0,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the BART models I can believe that these tokens are in order of frequency. I can't see anything that is obviously out of place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 120,250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing special at the end, just looks like isolated tokens and None values after the tokens finish. It's also worth noting that the Pegasus model has ~96100 tokens which is way more than the ~50270 of the BART models (almost double). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, pp_tokenizer_pegasus.vocab_size-20, pp_tokenizer_pegasus.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### When does the model generate padding tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For both models padding tokens are generated after the EOS token. Additionally for Pegasus generated text starts with the padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Why do generated paraphrases start with the EOS token? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is only the case for BART models. For pegasus models they use a padding token to start generated paraphrases. \n",
    "\n",
    "I don't know what the BOS token isn't used for these things. Pegasus has an open issue [here](https://github.com/huggingface/transformers/issues/12474). \n",
    "\n",
    "Whatever the reason you should just do the default because that is what the preprocessing does and you will get the best results that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does p(PAD) =1 after an eos token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For both BART and Pegasus models it appears that probability of outputting a pad token is actually zero at all timesteps. Instead the model outputs the eos token over and over, and there must be some post-processing that takes place that replaces eos token with padding token. \n",
    "For Pegasus it appears it is the same behaviour. \n",
    "Example code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(round_t(scores_softmax[:,:,pp_tokenizer.eos_token_id]))\n",
    "print(round_t(scores_softmax[:,:,pp_tokenizer.pad_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is interesting is that there is probability assigned to other tokens other than eos and pad after a eos token is outputted. Again there must be some kind of postprocessing that takes care of this situation because I haven't really seen it in the wild. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some models (e.g. GPT2) don't even have a PAD token. Instead they use the eos token on repeat. See this [issue](https://github.com/huggingface/transformers/issues/8452#issuecomment-739008168). What is confusing is seeing this behaviour with models that have a padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Do rows always sum to 1 when looking at token generation scores? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yes, they should. I put in an assert to check this. \n",
    "If you have a nan or an inf then they won't sum to 1. To confirm this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.isnan(torch.sum(torch.tensor([1,2,3, torch.nan]))))\n",
    "print(torch.isinf(torch.sum(torch.tensor([1,2,3, torch.inf]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does first row sum to 0? (the one corresponding to the startoff token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So there is no token scores that correspond to the first token (usually a bos or pad token). The scores are a tuple of length (`generated_length - 1`). So there shouldn't be a \"zero\" row really. \n",
    "I remember seeing something like this at some point so I'll keep an eye out for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How are logits containing nan or inf transformed with softmax and log_softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can explore this through some code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Vanilla case  \n",
    "First we look at the case without any nan or inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The softmax values are interpreted as probabilities, and the log softmax is just the log of the probabilities, done for numerical stability. We can just take exponents to return to probabilities if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.log_softmax(logits,0).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Positive inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's see what happens if we introduce a positive inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We get nan values in the softmax and log_softmax.  So if you see nans in the softmax, remember that an inf in the scores is one reason why it may happen.  \n",
    "\n",
    "This is interesting because if we just assume inf is a large positive number, we'd expect a softmax with basically a 1 and all zeros, and a log softmax of a 0 and a lot of negatives. We can try it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, 10000000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basically what we get. So this indicates that if we get a positive inf we might be able to mitigate this problem by clipping it to some kind of maximum value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Negative inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Negative inf behaves a bit differently. The softmax is unaffected and basically just assigns a prob of 0 to the corresponding entry. The log softmax carries the `-inf` through. \n",
    "\n",
    "Again clipping the -inf to a large negative value can mitigate this problem somewhat: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -10000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.nan])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A nan in the logits propagates and affects the entire softmax and log_softmax tensors. The network basically gives up and says \"no idea how to deal with this. \n",
    "\n",
    "This seems to be the case with most torch functions; e.g.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.sum(logits,0))\n",
    "print(torch.divide(logits,0.2))\n",
    "print(logits + logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How do you interpret token entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The token scores (when stacked) are a tensor of dimensions (batch_size, generated_length - 1, vocab_size). We take softmax to get a tensor of probability distributions across all possible tokens. We can also calculate the entropy of each of these probability distributions. \n",
    "\n",
    "Entropy is a measure of how \"peaky\" or \"flat\" a probability distribution is. It is the expected value of the self-information of an event, which is basically a measure of how \"surprised\" you would be if that event occured. \n",
    "If we have a discrete random variable $X$ with probability distribution $P(x) $, entropy is given by $$H(X) = \\mathbb{E}_{ X\\sim P} [I(x)] = -\\mathbb{E}_{X \\sim P} [\\log P(x)] $$ which is practically calculated by $$H(X) =  -\\sum_{x=-\\infty}^\\infty p(x) \\log(p(x))$$\n",
    "\n",
    "The lowest value of entropy is 0, which is when you have $P(x)=1$ for some event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Categorical(probs = torch.tensor([1,0,0,0])).entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "High values of entropy occur when the probability distribution is very flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(Categorical(probs = torch.tensor([0.20,0.50,0.10,0.20])).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor([0.25,0.25,0.25,0.25])).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In terms of tokens, we can show two realistic distributions below. The first has two likely tokens, one somewhat likely, and the rest unlikely. The second has many more likely tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l1 = [0.001,0.001,0.001,0.001,0.5,0.001,0.4,0.001,0.001,0.001,0.001,0.001,0.09]\n",
    "l2 = [0.1,0.1,0.1,0.1,0.025,0.1,0.1,0.1,0.1,0.1,0.025,0.025,0.025]\n",
    "print(Categorical(probs = torch.tensor(l1)).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor(l2)).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There isn't a theoretical maximum for entropy, but for tokens you'll be governed by vocab size. Here we show some practical maximums for some different vocab sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v_size = 1000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # v small vocab\n",
    "v_size = 10000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # small vocab\n",
    "v_size = 50000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~BART vocab\n",
    "v_size = 100000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~PEGASUS vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These can be a bit hard to interpret so maybe you should also look at some other token-level stats, like max_prob, second_max_prob, third_max_prob, mean, variance or other things like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some other things to note. First you still get an entropy value if your probability dist sums up to more than 1, so make sure to check this before doing entropy. Secondly if you have nan or inf in the probability values then you will get an error. This is true if you use either `probs` or `logits` in the Categorical function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(Categorical(probs = torch.tensor([0.5,0.25,0.25,0.25])).entropy())       # sums to more than 1, gives result\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.inf])).entropy())  # throws error\n",
    "#print(Categorical(logits = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean to take average of token entropy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average entropy over tokens will depend on how many padding tokens are added. This in turn depends on how many examples there are in the batch.   \n",
    "The entropy of padding tokens appears to be quite high based on some quick experiments.  \n",
    "It might or might not depend on sentence length.   \n",
    "It could be useful as a broad measure to see if the model is getting more \"peaky\" in selecting tokens at each time step.   \n",
    "It might be useful when tracking a paraphrase over time and seeing its variations?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you calculate KL divergence of? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your parameters to the policy network get updated, you calculate change in the output space and work out KL divergence of the prob dists. \n",
    "\n",
    "This might be hard for your case: \n",
    "* if you consider full sequences as the action space, these have such low probabilities that it's hard to see how they change really. you can't get a dist\n",
    "* you can get a dist of individual tokens but then you have to have the same paraphrase for that to make any sense whatsoever. \n",
    "  * you could extract input_ids for a sequence and feed it into pp_model.generate_beam_transition_probabilities() to get the prob of that specific sentence. then you could maybe use this to keep token probs the same? \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you get nan and inf introduced into token scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is that you get -inf for the first (min_length - 1) steps when you introduce a min_length parameter for the generated sequences for the eos_token_id slot. This is to stop the token from appearing and truncating the sequence. \n",
    "\n",
    "I also seem to get -inf for padding, eos tokens and sometimes just random other tokens as well. There doesn't seem to be much rhyme or reason to it. \n",
    "\n",
    "There is an option to generate to automatically handle these tokens. I haven't been using it though. \n",
    "\n",
    "You might also get -inf when setting other parameters to the `generate()` function (e.g. bad_words_ids or something similar).\n",
    "\n",
    "Nan can come in if you multiple -inf by 0 - this is equal to nan under ieee 754 standard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the action space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each different paraphrase can be considered a different action. This makes the action space a discrete action space rather than continuous. An initial estimate of its size is on the order of `vocab_size ^ generated_length`, but the vast majority of these sequences aren't valid English sentences and have a very low probability of being obtained. In addition the actions available are heavily dependent on the state (i.e. the original paraphrase). It is still a very large space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you log the top X most probable sentences and the probability of obtaining them? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently you get your action using greedy search which picks the most likely token at each point. But this might not give the most likely sequence: you could have a high probability token in two timesteps that requires choosing the second-most-probable token to get there. So you can’t say that the greedy-search paraphrase is the most probable sequence. \n",
    "\n",
    "Beam search will always find outputs with greater than or equal probability to greedy search. However it is not guaranteed to find the most likely output. Yet maybe it can be an estimator of it. \n",
    "\n",
    "\n",
    "The most probable returned sentences will depend heavily on the hyperparameters given to the generate function, such as temperature, diversity_penalty, num_beam_groups, min_length, length_penalty and so on. You can only ever get the most likely tokens for a set of hyperparameters. \n",
    "\n",
    "So what you can do is generate a bunch of output with beam search and then calculate the sequence probabilities of it (or set length_penalty to 0 and use the sequence_scores). Then this is roughly what you want. But there is no guarantee that the greedy search output will be part of the generated sentences. And if it is then it probably won't be the most likely sentence output either. \n",
    "\n",
    "This might make more sense if we start using beam search instead of greedy search.\n",
    "\n",
    "If you can log this you could make a plot tracking how many pp has probs over: 1e-5, 1e-4, 1e-3, 1e-2, 1e-1. would be a good plot. x axis epoch, then ether do (a) for individual examples, or (b) as averages across examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Why can't we use sampling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From what I've read the sampling operation is not differentiable. This means that autograd doesn't carry a gradient through the sampling operation.   \n",
    "I've read that \"RL gets around this\" but don't really understand the details at this point. Maybe with a non-differentiable policy gradient method? \n",
    "\n",
    "Links: [explanations](https://www.google.com/search?q=sampling+non+differentiable&oq=sampling+non+differentiable&aqs=chrome..69i57j69i60.7731j0j4&sourceid=chrome&ie=UTF-8) [hugginface forum post](https://discuss.huggingface.co/t/finetuning-gpt2-with-user-defined-loss/163?page=3)  [alt method](https://leolaugier.wp.imt.fr/2019/09/09/workarounds-non-differentiability/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are beam search scores and sequence_scores calculated? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE: raised a github issue, let's see.   \n",
    "\n",
    "There was a [PR](https://github.com/huggingface/transformers/pull/14654) that was merged in transformers v4.16.0 that seems to fix up the issue of the scores not being correct. Now there is a function  [`compute_transition_beam_scores`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/model#transformers.generation_utils.GenerationMixin.compute_transition_beam_scores) that seems to give you what you want. \n",
    "\n",
    "**Previously:**\n",
    "\n",
    "According to [this post](https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/15?u=tomroth1001) they are calculated like this: \n",
    "\n",
    "**`sequence_scores`**: cumulative log probabilities of the `num_beams` most probable beams. It can be formulated as a recursive formula: sequence_scores[k]_i = sequence_score[k]_{i-1} + log_probs[i-1, :])_topk(2)[k] with sequence_score[k]_{i=start_token} = 0` (i being the time step, k being kth beam). \n",
    " * Then you divide this by length_penalty i think (or 1 + length penalty?), so a lot of people seem to just set length_penalty to zero   \n",
    " \n",
    "**`scores`**: this is where it becomes confusing. At the moment the scores[i, j] are defined as log_probs[i-1, j] + sequence_score[j % vocab_size]_{i-1} whereas j % vocab_size essentially defines the beam index.\n",
    "  * don't really know how to interpret this overly much. \n",
    "  \n",
    "  \n",
    "NOTE: scores for greedy search are not logprobs but rather logits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does layer-norm affect the probs? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure. leaving this for now and lumping in with the dropout question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the size of the action space is this a good candidate for differential entropy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think so - looks like it's continuous spaces only based on a cursory look\n",
    "\n",
    "You might need something more specialised - m[this paper](https://arxiv.org/pdf/1806.00589.pdf) talks about entropy for high dimensional spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do you hit floating point threshold for token probabilities? When do nans and inf get introduced? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You work with log-probabilities exactly so you don't hit this issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does using fp32 affect token calculations? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shouldn't because you are working with log probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does dropout affect generated probabilities? How does train/eval mode affect generated probs for a sentence? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on observations when we put a network into eval mode there is less randomness in the generated probabilities. This makes sense because we have reduced randomness because we remove the stochastic behaviour of the dropout node. \n",
    "\n",
    "We can have a look at the differences for some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probs_for_mode(orig_l, mode): \n",
    "    if mode == \"train\": pp_model.train()\n",
    "    elif mode == \"eval\":  pp_model.eval()\n",
    "    else:   raise Exception(\"shouldn't get here\")\n",
    "    orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "    pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                          return_dict_in_generate=True,\n",
    "                                          output_scores=True,\n",
    "                                        remove_invalid_values=False)\n",
    "    pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "\n",
    "    seq_without_first_tkn = pp_output.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(pp_output.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "\n",
    "    return (pp_l_with_tokens, seq_token_log_probs, seq_log_prob)\n",
    "\n",
    "orig_l = [\n",
    "    \"Hello I am tom\", \n",
    "    \"yes hello.\"\n",
    "]\n",
    "get_token_probs_for_mode(orig_l, mode ='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pad> I am tom.</s>', '<pad> Yes, hello.</s><pad>'],\n",
       " tensor([[-0.4700, -0.5260, -2.3377, -0.4375, -0.5740, -0.0981],\n",
       "         [-1.4130, -0.2326, -0.4826, -0.2805, -0.0873, -0.0000]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-4.4433, -2.4960], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_probs_for_mode(orig_l, mode ='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to make too many inferences from this really. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `attention_mask`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `attention_mask` is used when you have a batch of texts that are different lengths. The shorter one might be padded up to some minimum length with 0’s. The `attention_mask` is a binary vector that tell the model where the padding is. In effect, they can tell the model what to ignore. The 0’s will be padding. \n",
    "\n",
    "It is used to filter out padding tokens from the sequence probablity because then the sequence \n",
    "probability will depend on how many padding tokens there are and the probability of generating them, \n",
    "which (a) we don't want and (b) the probability isn't correct anyway \n",
    "\n",
    "It is returned when you tokenise the input. \n",
    "\n",
    "I also create it at one point with the code for the paraphrase\n",
    "```\n",
    "attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "    seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    ")\n",
    "```\n",
    "so that I can get the right probabilities for the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `token_type_ids`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`token_type_ids`:** Some models/tasks use pairs of sentences concatenated together with a [SEP] token (or something) separating them. Tasks that might need this include textual entailment or duplicate detection. The `token_type_ids` is just a vector of 1’s and 0’s that indicate if a given word is part of the first sentence (a 0) or the second sentence (a 1). \n",
    "\n",
    "Looks something like this: \n",
    "```\n",
    ">>> encoded_dict[\"token_type_ids\"]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "```\n",
    "\n",
    "So you won't need this for sentiment classification but you might for entailment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `encoder_input_ids`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will leave this until I see it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `decoder_input_ids`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`decoder_input_ids`:** Only applies to encoder-decoder models. [One source](https://huggingface.co/transformers/glossary.html#decoder-input-ids) puts this as the input id’s that will be fed into the decoder. [Another source](https://huggingface.co/transformers/model_doc/t5.html#training) puts this as the target sequence (shifted by one place) when doing seq2seq training . Often when training you pass in the `labels` attribute and the model figures out what this should be. See [here](https://huggingface.co/transformers/model_doc/t5.html#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the tokenized input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder_start_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will leave this until I see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "680px",
    "left": "0px",
    "top": "111.125px",
    "width": "211.392px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
