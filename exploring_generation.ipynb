{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s') \n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "\n",
    "## PEGASUS model\n",
    "pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "pp_tokenizer_pegasus = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_pegasus.generate)\n",
    "pp_model_pegasus.generate_with_grad = MethodType(generate_with_grad, pp_model_pegasus)\n",
    "\n",
    "## BART model\n",
    "pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "pp_tokenizer_bart = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_bart = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_bart.generate)\n",
    "pp_model_bart.generate_with_grad = MethodType(generate_with_grad, pp_model_bart)\n",
    "\n",
    "## Select which one to use as default\n",
    "pp_tokenizer = pp_tokenizer_bart\n",
    "pp_model = pp_model_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(translated.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "#     if np.any(np.isnan(seq_log_prob.detach().cpu()).tolist()): \n",
    "#         warnings.warn(f\"Warning: NAN's detected in pp_logp calclulations.\\n seq_token_log_probs: {seq_token_log_probs}\")\n",
    "    return seq_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_tokens_from_token_ids_batch(tokenizer, ids_batch):\n",
    "    l = []\n",
    "    for i in range(ids_batch.shape[0]): \n",
    "        l.append(tokenizer.convert_ids_to_tokens(ids_batch[i,:]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_l = [\n",
    "    \"Hello my name is zfldlfoqd\", \n",
    "    \"The cat is brown and it looks cute!\"\n",
    "]\n",
    "orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "orig_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, orig_tokens['input_ids'])\n",
    "\n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                      return_dict_in_generate=True,\n",
    "                                      output_scores=True,\n",
    "                                    remove_invalid_values=False)\n",
    "pp_l             = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "pp_l_list_of_tokens = pp_tokenizer.convert_ids_to_tokens(pp_output.sequences[0,:])\n",
    "pp_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, pp_output.sequences)\n",
    "\n",
    "scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "# The second argument to stack (i.e. dim) determines which axis the tensors are stacked along. \n",
    "# It determines the axis that becomes generated_length\n",
    "# dim=0 gives shape [generated_length, batch_size, vocab_size]\n",
    "# dim=1 gives shape [batch_size, generated_length, vocab_size]\n",
    "# dim=2 gives shape [batch_size, vocab_size, generated_length]\n",
    "assert torch.stack(pp_output.scores, 1).shape[1] == len(pp_output.scores)\n",
    "\n",
    "# These scores are logits \n",
    "# see some of the docs on this page https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput\n",
    "# so we got to take softmax over them \n",
    "scores_softmax = torch.softmax(scores_stacked, 2)\n",
    "def check_scores_softmax(scores_softmax):\n",
    "    sums = scores_softmax.sum(2)\n",
    "    # check that the axes is right\n",
    "    assert sums.shape[0] == len(orig_l)  # batch_size\n",
    "    assert sums.shape[1] == len(pp_output.scores)  # generated_length\n",
    "    # check that they sum to 1 along the generated_length axis\n",
    "    assert torch.allclose(sums, torch.ones(sums.size()), atol = 1e-2)\n",
    "check_scores_softmax(scores_softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 50265])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "\n",
      "Tokenizer has these special tokens:['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "The bos token is <s> and has id 0\n",
      "The eos token is </s> and has id 2\n",
      "The pad token is <pad> and has id 1\n",
      "The unk token is <unk> and has id 3\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Original text: ['Hello my name is zfldlfoqd', 'The cat is brown and it looks cute!']\n",
      "This is tokenised to get a dict with keys input_ids and attention_mask \n",
      "Tokens:\n",
      "The input_ids look like this: tensor([[    0, 31414,   127,   766,    16,   992,   506,  4779, 13491,   139,\n",
      "          1343,   417,     2,     1,     1,     1],\n",
      "        [    0,   133,  4758,    16,  6219,     8,    24,  1326, 11962,   328,\n",
      "             2,     1,     1,     1,     1,     1]])\n",
      "The tokens are: [['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>'], ['<s>', 'The', 'Ġcat', 'Ġis', 'Ġbrown', 'Ġand', 'Ġit', 'Ġlooks', 'Ġcute', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "This has shape torch.Size([2, 16]) or [batch_size, input_length], which also might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\n",
      "Each sequence should start with the bos_token and end with either eos token or pad token.\n",
      "Writing assert to confirm this.\n",
      "The attention_mask looks like this: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "This has shape torch.Size([2, 16]) or [batch_size, input_length]\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Paraphrases: ['My name is zfldlfoqd.', 'The cat is brown and looks cute!']\n",
      "Paraphrases with special tokens: ['</s>My name is zfldlfoqd.</s>', '</s>The cat is brown and looks cute!</s><pad><pad><pad>']\n",
      "List of pp tokens:[['</s>', 'My', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '.', '</s>'], ['</s>', 'The', 'Ġcat', 'Ġis', 'Ġbrown', 'Ġand', 'Ġlooks', 'Ġcute', '!', '</s>', '<pad>', '<pad>', '<pad>']]\n",
      "Paraphrase token sequences: tensor([[    2,  2387,   766,    16,   992,   506,  4779, 13491,   139,  1343,\n",
      "           417,     4,     2],\n",
      "        [    2,   133,  4758,    16,  6219,     8,  1326, 11962,   328,     2,\n",
      "             1,     1,     1]])\n",
      "Shape of pp token sequences:torch.Size([2, 13]) or [batch_size, generated_length]\n",
      "Each pp should start with the bos_token and end with either eos token or pad token.\n",
      "Writing assert to confirm this.\n",
      "\n",
      "######################################################################\n",
      "\n",
      "Scores is a tuple of length 12 which is one less than the generated_length, or the number of tokens in the pp token sequences (this has shape torch.Size([2, 13])\n",
      "Each score is a tensor of shape torch.Size([2, 50265]) or [batch_size, vocab_size]\n",
      "We stack them to get a tensor of shape torch.Size([2, 12, 50265]) or [batch_size, generated_size, vocab_size]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer.bos_token} and has id {pp_tokenizer.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer.eos_token} and has id {pp_tokenizer.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer.pad_token} and has id {pp_tokenizer.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer.unk_token} and has id {pp_tokenizer.unk_token_id}\")\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Original text: {orig_l}\")\n",
    "logger.info(f\"This is tokenised to get a dict with keys input_ids and attention_mask \")\n",
    "logger.info(f\"Tokens:\")\n",
    "logger.info(f\"The input_ids look like this: {orig_tokens['input_ids']}\")\n",
    "logger.info(f\"The tokens are: {orig_l_tokens_list}\")\n",
    "logger.info(f\"This has shape {orig_tokens['input_ids'].shape} or [batch_size, input_length], which also\\\n",
    " might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\")\n",
    "logger.info(\"Each sequence should start with the bos_token and end with either eos token or pad token.\")\n",
    "logger.info(\"Writing assert to confirm this.\")\n",
    "# Check each tokenized input sequence begins with bos token\n",
    "#assert torch.all(orig_tokens['input_ids'][:,0] == pp_tokenizer.bos_token_id)\n",
    "# Check each tokenized input sequence ends with either pad or eos token. \n",
    "#assert torch.all(torch.logical_or(orig_tokens['input_ids'][:,-1] == pp_tokenizer.eos_token_id, \n",
    "#                                  orig_tokens['input_ids'][:,-1] == pp_tokenizer.pad_token_id))\n",
    "logger.info(f\"The attention_mask looks like this: {orig_tokens['attention_mask']}\")\n",
    "logger.info(f\"This has shape {orig_tokens['attention_mask'].shape} or [batch_size, input_length]\")\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Paraphrases: {pp_l}\")\n",
    "logger.info(f\"Paraphrases with special tokens: {pp_l_with_tokens}\")\n",
    "logger.info(f\"List of pp tokens:{pp_l_tokens_list}\")\n",
    "logger.info(f\"Paraphrase token sequences: {pp_output.sequences}\")\n",
    "logger.info(f\"Shape of pp token sequences:{pp_output.sequences.shape} or [batch_size, generated_length]\")\n",
    "logger.info(\"Each pp should start with the bos_token and end with either eos token or pad token.\")\n",
    "logger.info(\"Writing assert to confirm this.\")\n",
    "# Check each tokenized input sequence begins with bos token\n",
    "#assert torch.all(pp_output.sequences[:,0] == pp_tokenizer.bos_token_id)\n",
    "# Check each tokenized input sequence ends with either pad or eos token. \n",
    "#assert torch.all(torch.logical_or(pp_output.sequences[:,-1] == pp_tokenizer.eos_token_id, \n",
    "#                                  pp_output.sequences[:,-1] == pp_tokenizer.pad_token_id))\n",
    "logger.info(\"\\n######################################################################\\n\")\n",
    "logger.info(f\"Scores is a tuple of length {len(pp_output.scores)} which is one less than the generated_length, or \\\n",
    "the number of tokens in the pp token sequences (this has shape {pp_output.sequences.shape}\")\n",
    "logger.info(f\"Each score is a tensor of shape {pp_output.scores[0].shape} or [batch_size, vocab_size]\")\n",
    "#logger.info(f\"Full shape:{[o.shape for o in pp_output.scores]}\")\n",
    "logger.info(f\"We stack them to get a tensor of shape {scores_stacked.shape} or [batch_size, generated_length, vocab_size]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pp_tokenizer.eos_token_id, pp_tokenizer.pad_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pp_model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "initial_scores = pp_output.scores[0]\n",
    "second_scores = pp_output.scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50265])\n",
      "torch.Size([2, 12, 50265])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(initial_scores.shape)\n",
    "print(scores_stacked.shape)\n",
    "print(len(pp_output.scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_info_on_generated_text():\n",
    "    \"\"\"\n",
    "        Prints a bunch of statistics around the generated text. Useful for debugging purposes.\n",
    "        So far only works for greedy search.\n",
    "    \"\"\"\n",
    "\n",
    "    tgt_text = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=True)\n",
    "    tgt_text_with_tokens = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=False)\n",
    "    logger.info(f\"Generated text: {tgt_text}\")\n",
    "    logger.info(f\"Generated text with special tokens: {tgt_text_with_tokens}\")\n",
    "    logger.info(f\"Shape of translated.sequences:{translated.sequences.shape}\")\n",
    "    logger.info(f\"translated.sequences:{translated.sequences}\")\n",
    "    logger.info(f\"Scores is a tuple of length {len(translated.scores)} \\\n",
    "    and each score is a tensor of shape {translated.scores[0].shape}\")\n",
    "    scores_stacked = torch.stack(translated.scores, 1)\n",
    "    logger.info(f\"Stacking the scores into a tensor of shape {scores_stacked.shape}\")\n",
    "    scores_softmax = torch.softmax(scores_stacked, 2)\n",
    "    logger.info(f\"Now taking softmax. This shouldn't change the shape, but just to check,\\\n",
    "    its shape is {scores_softmax.shape}\")\n",
    "    probsums = scores_softmax.sum(axis=2)\n",
    "    logger.info(f\"These are probabilities now and so they should all sum to 1 (or close to it) in the axis \\\n",
    "    corresponding to each time step. We can check the sums here: {probsums}, but it's a long tensor \\\n",
    "    of shape {probsums.shape} and hard to see, so summing over all these values and removing 1 \\\n",
    "    from each gives {torch.sum(probsums - 1)} \\\n",
    "    which should be close to 0.\")\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    logger.info(\"Now calculating sequence probabilities\")\n",
    "    seq_token_probs = torch.gather(scores_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    seq_prob = seq_token_probs.prod(-1).item()\n",
    "    logger.info(f\"Sequence probability: {seq_prob}\")\n",
    "\n",
    "    # Get the 2nd and 3rd most likely tokens at each st\n",
    "    topk_ids = torch.topk(scores_softmax,3,dim=2).indices[:,:,1:]\n",
    "    topk_tokens_probs = torch.gather(scores_softmax,2,topk_ids).squeeze(-1)\n",
    "    toks2 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,0].squeeze())\n",
    "    toks3 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,1].squeeze())\n",
    "    tok_probs2 = topk_tokens_probs[:,:,0].squeeze()\n",
    "    tok_probs3 = topk_tokens_probs[:,:,1].squeeze()\n",
    "\n",
    "    logger.info(f\"Probabilities of getting the top 3 tokens at each step:\")\n",
    "    tokens = pp_tokenizer.convert_ids_to_tokens(seq_without_first_tkn.squeeze())\n",
    "    for (p, t, p2,t2,p3,t3)  in zip(seq_token_probs.squeeze(), tokens, tok_probs2, toks2, tok_probs3, toks3): \n",
    "        logger.info(f\"{t}: {round(p.item(),3)}  {t2}: {round(p2.item(),3)}  {t3}: {round(p3.item(),3)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  2.1281,  -1.7363,     -inf,  ...,  -2.0207,  -1.8297,  -0.2931],\n",
       "         [-12.2530,  -4.2081,     -inf,  ...,  -4.1955,  -4.1370,  -2.7101],\n",
       "         [ -7.8899,  -5.5816,     -inf,  ...,  -5.1575,  -4.9525,  -7.0733],\n",
       "         ...,\n",
       "         [ -0.6762,  -3.6511,   9.2148,  ...,  -2.7468,  -2.8557,  -3.2452],\n",
       "         [ -5.2623,  -4.6181,  10.6337,  ...,  -4.3726,  -4.1950,  -4.7886],\n",
       "         [ -4.3985,  -4.1799,  11.2302,  ...,  -4.6168,  -4.6227,  -3.4602]],\n",
       "\n",
       "        [[ -3.3283,  -1.4830,     -inf,  ...,  -1.2559,  -1.7851,  -0.3878],\n",
       "         [ -5.4642,  -3.7450,     -inf,  ...,  -3.5129,  -4.9730,  -2.3698],\n",
       "         [ -1.4317,  -4.9811,     -inf,  ...,  -4.1592,  -5.8414,  -4.4424],\n",
       "         ...,\n",
       "         [ -0.0560,  -2.8219,  15.0593,  ...,  -3.0489,  -4.4363,  -2.9208],\n",
       "         [  1.1428,  -3.6452,  12.2761,  ...,  -3.3208,  -3.9210,  -2.6603],\n",
       "         [ -0.7912,  -3.7815,  11.9712,  ...,  -3.3675,  -3.8684,  -3.5179]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenizer differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Both the \"eugenesiow/bart-paraphrase\" model and the \"tdopierre/ProtAugment-ParaphraseGenerator\" are BART tokenizers and have type BartTokenizerFast. The implementation is identical to RobertaTokenizerFast according to the docs, which in turn was derived from GPT-2. They use byte-level Byte Pair Encoding.  \n",
    "\n",
    "The \"tuner007/pegasus_paraphrase\" model is a Pegasus tokenizer has type PegasusTokenizerFast. This uses Unigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenization differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The BART tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'hello', 'Ġthere', '</s>'], ['<s>', 'Ġhello', 'Ġthere', '</s>']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pp_tokenizer_bart(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_bart, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Pegasus tokenizer doesn't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁hello', '▁there', '</s>'], ['▁hello', '▁there', '</s>']]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pp_tokenizer_pegasus(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_pegasus, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Representing tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The tokenizers represent tokens differently.  \n",
    "The BART models use Ġ to indicate start of a word for a token. Its generated tokens look like `['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>']`  \n",
    "The Pegasus model uses \\_ to indicate start of a word for a token. Its generated tokens look like `['▁Hello', '▁my', '▁name', '▁is', '▁z', 'fl', 'dl', 'fo', 'q', 'd', '</s>', '<pad>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer has these special tokens:['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "The bos token is <s> and has id 0\n",
      "The eos token is </s> and has id 2\n",
      "The pad token is <pad> and has id 1\n",
      "The unk token is <unk> and has id 3\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_bart.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_bart.bos_token} and has id {pp_tokenizer_bart.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_bart.eos_token} and has id {pp_tokenizer_bart.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_bart.pad_token} and has id {pp_tokenizer_bart.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_bart.unk_token} and has id {pp_tokenizer_bart.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer has these special tokens:['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "Using bos_token, but it is not set yet.\n",
      "The bos token is None and has id None\n",
      "The eos token is </s> and has id 1\n",
      "The pad token is <pad> and has id 0\n",
      "The unk token is <unk> and has id 105\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_pegasus.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_pegasus.bos_token} and has id {pp_tokenizer_pegasus.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_pegasus.eos_token} and has id {pp_tokenizer_pegasus.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_pegasus.pad_token} and has id {pp_tokenizer_pegasus.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_pegasus.unk_token} and has id {pp_tokenizer_pegasus.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Special token usage with input and output sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "They use this format \n",
    "```\n",
    "single sequence: <s> X </s>\n",
    "pair of sequences: <s> A </s></s> B </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Format: \n",
    "```\n",
    "- single sequence: ``X </s>``\n",
    "- pair of sequences: ``A B </s>`` (not intended use)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "BOS token is never used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Tokenizers also use different tokens when representing input sequences and generating output sentences. Here is a quick summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bart': {'special_tokens': ['<s>', '</s>', '<unk>', '<pad>', '<mask>'],\n",
       "  'input_start': '<s>',\n",
       "  'input_end': ['<pad>', '</s>'],\n",
       "  'output_start': '</s>',\n",
       "  'output_end': ['<pad>', '</s>']},\n",
       " 'pegasus': {'special_tokens': ['</s>',\n",
       "   '<unk>',\n",
       "   '<pad>',\n",
       "   '<mask_2>',\n",
       "   '<mask_1>',\n",
       "   '<unk_2>',\n",
       "   '<unk_3>',\n",
       "   '<unk_4>',\n",
       "   '<unk_5>',\n",
       "   '<unk_6>',\n",
       "   '<unk_7>',\n",
       "   '<unk_8>',\n",
       "   '<unk_9>',\n",
       "   '<unk_10>',\n",
       "   '<unk_11>',\n",
       "   '<unk_12>',\n",
       "   '<unk_13>',\n",
       "   '<unk_14>',\n",
       "   '<unk_15>',\n",
       "   '<unk_16>',\n",
       "   '<unk_17>',\n",
       "   '<unk_18>',\n",
       "   '<unk_19>',\n",
       "   '<unk_20>',\n",
       "   '<unk_21>',\n",
       "   '<unk_22>',\n",
       "   '<unk_23>',\n",
       "   '<unk_24>',\n",
       "   '<unk_25>',\n",
       "   '<unk_26>',\n",
       "   '<unk_27>',\n",
       "   '<unk_28>',\n",
       "   '<unk_29>',\n",
       "   '<unk_30>',\n",
       "   '<unk_31>',\n",
       "   '<unk_32>',\n",
       "   '<unk_33>',\n",
       "   '<unk_34>',\n",
       "   '<unk_35>',\n",
       "   '<unk_36>',\n",
       "   '<unk_37>',\n",
       "   '<unk_38>',\n",
       "   '<unk_39>',\n",
       "   '<unk_40>',\n",
       "   '<unk_41>',\n",
       "   '<unk_42>',\n",
       "   '<unk_43>',\n",
       "   '<unk_44>',\n",
       "   '<unk_45>',\n",
       "   '<unk_46>',\n",
       "   '<unk_47>',\n",
       "   '<unk_48>',\n",
       "   '<unk_49>',\n",
       "   '<unk_50>',\n",
       "   '<unk_51>',\n",
       "   '<unk_52>',\n",
       "   '<unk_53>',\n",
       "   '<unk_54>',\n",
       "   '<unk_55>',\n",
       "   '<unk_56>',\n",
       "   '<unk_57>',\n",
       "   '<unk_58>',\n",
       "   '<unk_59>',\n",
       "   '<unk_60>',\n",
       "   '<unk_61>',\n",
       "   '<unk_62>',\n",
       "   '<unk_63>',\n",
       "   '<unk_64>',\n",
       "   '<unk_65>',\n",
       "   '<unk_66>',\n",
       "   '<unk_67>',\n",
       "   '<unk_68>',\n",
       "   '<unk_69>',\n",
       "   '<unk_70>',\n",
       "   '<unk_71>',\n",
       "   '<unk_72>',\n",
       "   '<unk_73>',\n",
       "   '<unk_74>',\n",
       "   '<unk_75>',\n",
       "   '<unk_76>',\n",
       "   '<unk_77>',\n",
       "   '<unk_78>',\n",
       "   '<unk_79>',\n",
       "   '<unk_80>',\n",
       "   '<unk_81>',\n",
       "   '<unk_82>',\n",
       "   '<unk_83>',\n",
       "   '<unk_84>',\n",
       "   '<unk_85>',\n",
       "   '<unk_86>',\n",
       "   '<unk_87>',\n",
       "   '<unk_88>',\n",
       "   '<unk_89>',\n",
       "   '<unk_90>',\n",
       "   '<unk_91>',\n",
       "   '<unk_92>',\n",
       "   '<unk_93>',\n",
       "   '<unk_94>',\n",
       "   '<unk_95>',\n",
       "   '<unk_96>',\n",
       "   '<unk_97>',\n",
       "   '<unk_98>',\n",
       "   '<unk_99>',\n",
       "   '<unk_100>',\n",
       "   '<unk_101>',\n",
       "   '<unk_102>'],\n",
       "  'input_start': None,\n",
       "  'input_end': ['<pad>', '</s>'],\n",
       "  'output_start': '<pad>',\n",
       "  'output_end': ['<pad>', '</s>']}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_d = {\n",
    "    \"bart\": {\n",
    "        \"special_tokens\": pp_tokenizer_bart.all_special_tokens, \n",
    "        \"input_start\": pp_tokenizer_bart.bos_token,\n",
    "        \"input_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token], \n",
    "        \"output_start\": pp_tokenizer_bart.eos_token, \n",
    "        \"output_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token]\n",
    "    }, \n",
    "    \"pegasus\": {\n",
    "        \"special_tokens\": pp_tokenizer_pegasus.all_special_tokens,\n",
    "        \"input_start\": None,\n",
    "        \"input_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "        \"output_start\": pp_tokenizer_pegasus.pad_token,  \n",
    "        \"output_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "    }\n",
    "}\n",
    "tokens_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Token indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_tokens_from_ids(tokenizer, start_id=100, end_id=200):\n",
    "    ids = list(range(start_id,end_id))\n",
    "    print(*list(zip(ids, tokenizer.convert_ids_to_tokens(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having a look at generated tokens makes me suspect that they are indexed in whatever order they are encountered in the source text they are trained on. It seems like a rough frequency of english tokens but there are also tokens that are definitely out of order. \n",
    "\n",
    "The first few are reserved for special tokens, and the other low numbers (e.g. up to 100) are pretty common suffixes and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<s>') (1, '<pad>') (2, '</s>') (3, '<unk>') (4, '.') (5, 'Ġthe') (6, ',') (7, 'Ġto') (8, 'Ġand') (9, 'Ġof') (10, 'Ġa') (11, 'Ġin') (12, '-') (13, 'Ġfor') (14, 'Ġthat') (15, 'Ġon') (16, 'Ġis') (17, 'âĢ') (18, \"'s\") (19, 'Ġwith') (20, 'ĠThe') (21, 'Ġwas') (22, 'Ġ\"') (23, 'Ġat') (24, 'Ġit') (25, 'Ġas') (26, 'Ġsaid') (27, 'Ļ') (28, 'Ġbe') (29, 's') (30, 'Ġby') (31, 'Ġfrom') (32, 'Ġare') (33, 'Ġhave') (34, 'Ġhas') (35, ':') (36, 'Ġ(') (37, 'Ġhe') (38, 'ĠI') (39, 'Ġhis') (40, 'Ġwill') (41, 'Ġan') (42, 'Ġthis') (43, ')') (44, 'ĠâĢ') (45, 'Ġnot') (46, 'Ŀ') (47, 'Ġyou') (48, 'ľ') (49, 'Ġtheir')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at 100 to 200 you can see some words (e.g. Trump at 140, or 2017 at 193) that aren't common enough to be that high. This makes me suspect that words are in encounter order in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'I') (101, 'Ġlike') (102, 'a') (103, 'Ġsome') (104, 'S') (105, 'Ã«') (106, 'Ġthem') (107, 'Ġyears') (108, \"'\") (109, 'Ġdo') (110, 'Ġyour') (111, 'Ġ-') (112, 'Ġ1') (113, '\"') (114, 'Ġif') (115, 'Ġcould') (116, '?') (117, 'Ġno') (118, 'i') (119, 'm') (120, 'Ġget') (121, 'ĠU') (122, 'Ġnow') (123, 'Ġhim') (124, 'Ġback') (125, 'ĠBut') (126, 'ĠâĢĵ') (127, 'Ġmy') (128, \"Ġ'\") (129, 'Ġonly') (130, 'Ġthree') (131, ';') (132, 'Ġ2') (133, 'The') (134, '1') (135, 'Ġpercent') (136, 'Ġagainst') (137, 'Ġbefore') (138, 'Ġcompany') (139, 'o') (140, 'ĠTrump') (141, 'Ġhow') (142, 'Ġbecause') (143, 'Ġany') (144, 'Ġmost') (145, 'Ġbeing') (146, 'Ġmake') (147, 'Ġwhere') (148, 'Ġduring') (149, 'Ġthrough') (150, 'Ġwhile') (151, '000') (152, 'ĠThis') (153, 'Ġmillion') (154, 'ing') (155, 'Ġ3') (156, 'Ġmade') (157, 'Ġwell') (158, 'Ġ10') (159, 'Ġdown') (160, 'Ġoff') (161, 'Ġsays') (162, 'Ġme') (163, 'ĠB') (164, 'Ġgoing') (165, 'Ġteam') (166, 'ĠWe') (167, 'Ġthose') (168, 'Ġgovernment') (169, 'Ġway') (170, 'We') (171, 'Ġmany') (172, 'Ġthen') (173, 'Ġwork') (174, 'Ġtold') (175, 'com') (176, '2') (177, 'Ġgame') (178, 'ĠAnd') (179, 'in') (180, 'year') (181, 'Ġp') (182, 'Ġvery') (183, 'Ġday') (184, 'Ġhome') (185, 'Ġtake') (186, 'Ġweek') (187, 'Ġsince') (188, 'ĠNew') (189, 'Ġmay') (190, 'Ġeven') (191, 'Ġseason') (192, 'Ġsee') (193, 'Ġ2017') (194, 'Ġstate') (195, 'Ġ5') (196, 'ed') (197, 'Ġshould') (198, 'Ġaround') (199, 'Ġ2018')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 100,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokens towards the end are gibberish or mispellings encountered in the input. The fifth last token is something labelled <|endoftext|> and I don't know what that is. Then there is a bunch of tokens like \"madeupword0001\". The last token is the mask token and then token indicies after that return None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50245, 'ĠSetTextColor') (50246, 'Ġfixme') (50247, 'ĠãĤµãĥ¼ãĥĨãĤ£') (50248, 'ĠãĤµãĥ¼ãĥĨãĤ£ãĥ¯ãĥ³') (50249, 'ĠÂłĠÂłĠÂłĠÂłĠÂłĠÂłĠÂłĠÂł') (50250, 'ĠAdinida') (50251, 'ItemTracker') (50252, 'ĠDevOnline') (50253, 'ĠÂłÂł') (50254, '<?') (50255, '*=-') (50256, 'ÃĽÃĽ') (50257, 'ĠEntityItem') (50258, 'EngineDebug') (50259, 'ĠstrutConnector') (50260, '<|endoftext|>') (50261, 'madeupword0000') (50262, 'madeupword0001') (50263, 'madeupword0002') (50264, '<mask>') (50265, None) (50266, None) (50267, None) (50268, None) (50269, None) (50270, None) (50271, None) (50272, None) (50273, None) (50274, None)\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, pp_tokenizer_bart.vocab_size-20, pp_tokenizer_bart.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Special tokens make up the first hundred or so. After that there's a token \\<n> that seems like some new line thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<pad>') (1, '</s>') (2, '<mask_1>') (3, '<mask_2>') (4, '<unk_2>') (5, '<unk_3>') (6, '<unk_4>') (7, '<unk_5>') (8, '<unk_6>') (9, '<unk_7>') (10, '<unk_8>') (11, '<unk_9>') (12, '<unk_10>') (13, '<unk_11>') (14, '<unk_12>') (15, '<unk_13>') (16, '<unk_14>') (17, '<unk_15>') (18, '<unk_16>') (19, '<unk_17>') (20, '<unk_18>') (21, '<unk_19>') (22, '<unk_20>') (23, '<unk_21>') (24, '<unk_22>') (25, '<unk_23>') (26, '<unk_24>') (27, '<unk_25>') (28, '<unk_26>') (29, '<unk_27>') (30, '<unk_28>') (31, '<unk_29>') (32, '<unk_30>') (33, '<unk_31>') (34, '<unk_32>') (35, '<unk_33>') (36, '<unk_34>') (37, '<unk_35>') (38, '<unk_36>') (39, '<unk_37>') (40, '<unk_38>') (41, '<unk_39>') (42, '<unk_40>') (43, '<unk_41>') (44, '<unk_42>') (45, '<unk_43>') (46, '<unk_44>') (47, '<unk_45>') (48, '<unk_46>') (49, '<unk_47>') (50, '<unk_48>') (51, '<unk_49>') (52, '<unk_50>') (53, '<unk_51>') (54, '<unk_52>') (55, '<unk_53>') (56, '<unk_54>') (57, '<unk_55>') (58, '<unk_56>') (59, '<unk_57>') (60, '<unk_58>') (61, '<unk_59>') (62, '<unk_60>') (63, '<unk_61>') (64, '<unk_62>') (65, '<unk_63>') (66, '<unk_64>') (67, '<unk_65>') (68, '<unk_66>') (69, '<unk_67>') (70, '<unk_68>') (71, '<unk_69>') (72, '<unk_70>') (73, '<unk_71>') (74, '<unk_72>') (75, '<unk_73>') (76, '<unk_74>') (77, '<unk_75>') (78, '<unk_76>') (79, '<unk_77>') (80, '<unk_78>') (81, '<unk_79>') (82, '<unk_80>') (83, '<unk_81>') (84, '<unk_82>') (85, '<unk_83>') (86, '<unk_84>') (87, '<unk_85>') (88, '<unk_86>') (89, '<unk_87>') (90, '<unk_88>') (91, '<unk_89>') (92, '<unk_90>') (93, '<unk_91>') (94, '<unk_92>') (95, '<unk_93>') (96, '<unk_94>') (97, '<unk_95>') (98, '<unk_96>') (99, '<unk_97>') (100, '<unk_98>') (101, '<unk_99>') (102, '<unk_100>') (103, '<unk_101>') (104, '<unk_102>') (105, '<unk>') (106, '<n>') (107, '.') (108, ',') (109, '▁the') (110, '▁') (111, '▁and') (112, '▁to') (113, '▁of') (114, '▁a') (115, '▁in') (116, 's') (117, '▁is') (118, '▁for') (119, '▁you')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 0,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike the BART models I can believe that these tokens are in order of frequency. I can't see anything that is obviously out of place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, '▁easy') (401, '...') (402, '▁‘') (403, '▁show') (404, '▁children') (405, '▁project') (406, '▁care') (407, '▁market') (408, '▁money') (409, '▁Our') (410, '▁book') (411, '▁change') (412, '▁So') (413, '▁To') (414, '▁put') (415, 'y') (416, '▁say') (417, 'You') (418, '▁room') (419, '▁got') (420, 'er') (421, '▁create') (422, '▁course') (423, '▁large') (424, '▁together') (425, '▁food') (426, '▁health') (427, '▁community') (428, '▁open') (429, '▁away') (430, '▁until') (431, '▁program') (432, '▁often') (433, '▁possible') (434, '▁When') (435, '▁again') (436, '▁All') (437, '▁case') (438, '▁page') (439, '▁car') (440, '▁real') (441, '▁With') (442, '▁name') (443, '▁call') (444, '▁include') (445, 'ly') (446, '▁per') (447, '▁why') (448, '▁product') (449, '▁state') (450, '▁post') (451, '▁based') (452, '▁She') (453, '▁second') (454, 'n') (455, '▁event') (456, '▁group') (457, 'i') (458, '▁having') (459, '▁old') (460, '▁become') (461, '▁big') (462, '▁play') (463, '▁What') (464, '▁against') (465, '▁person') (466, '▁along') (467, '▁list') (468, '“') (469, '▁price') (470, 'D') (471, '▁contact') (472, '▁comes') (473, '▁research') (474, '▁thing') (475, '▁U') (476, '▁level') (477, '▁side') (478, '▁less') (479, '▁done') (480, '▁house') (481, '▁public') (482, '▁across') (483, ',”') (484, '▁power') (485, '▁That') (486, '▁development') (487, '▁below') (488, '▁times') (489, '▁access') (490, 'or') (491, '▁point') (492, '▁—') (493, '▁makes') (494, '▁job') (495, '▁means') (496, '.\"') (497, 'to') (498, '▁live') (499, '▁range')\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 120,250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's nothing special at the end, just looks like isolated tokens and None values after the tokens finish. It's also worth noting that the Pegasus model has ~96100 tokens which is way more than the ~50270 of the BART models (almost double). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96083, '0:09') (96084, '▁Pietra') (96085, 'webhost') (96086, '8:48') (96087, '▁psychoanalytic') (96088, '1335') (96089, '▁Happies') (96090, '▁Tamale') (96091, '▁Seidel') (96092, '▁Muppet') (96093, '▁Quota') (96094, '▁polyphenol') (96095, 'utyrate') (96096, 'saari') (96097, '▁WASTE') (96098, '▁$6,500') (96099, '.06%') (96100, 'constitutional') (96101, '▁$6.4') (96102, 'ospermum') (96103, None) (96104, None) (96105, None) (96106, None) (96107, None) (96108, None) (96109, None) (96110, None) (96111, None) (96112, None)\n"
     ]
    }
   ],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, pp_tokenizer_pegasus.vocab_size-20, pp_tokenizer_pegasus.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When does the model generate padding tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models padding tokens are generated after the EOS token. Additionally for Pegasus generated text starts with the padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do generated paraphrases start with the EOS token? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only the case for BART models. For pegasus models they use a padding token to start generated paraphrases. \n",
    "\n",
    "I don't know what the BOS token isn't used for these things. Pegasus has an open issue [here](https://github.com/huggingface/transformers/issues/12474). \n",
    "\n",
    "Whatever the reason you should just do the default because that is what the preprocessing does and you will get the best results that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does p(PAD) =1 after an eos token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't find anything about this online easily, so you should definitely put in an assert to check that this is the case. \n",
    "\n",
    "Some models (e.g. GPT2) don't even have a PAD token. Instead they use the eos token on repeat. See this [issue](https://github.com/huggingface/transformers/issues/8452#issuecomment-739008168). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do rows always sum to 1 when looking at token generation scores? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to check \n",
    "\n",
    "\n",
    "* Does first row sum to 0? (the one corresponding to start of sentence token)\n",
    "*  how do you nan introduced into token scores?\n",
    "* how do you get inf introduced into token scores?\n",
    "* how big is the action space? \n",
    "  * initial estimate: on the order of vocab_size ^ sequence length (so bloody huge)\n",
    "  * but only a very small proportion are valid actions\n",
    "* how many pp has probs over: 1e-5, 1e-4, 1e-3, 1e-2, 1e-1. would be a good plot. x axis epoch, then ether do (a) for individual examples, or (b) as averages across examples\n",
    "* log top X sampled sentences and their probs (maybe probs can be a graph)\n",
    "\n",
    "* how does padding mask affect things?\n",
    "* how does token-type-ids affect things? \n",
    "* which dist do you calculate KL divergence and entropy over? \n",
    "  * is it the token entropy’s at each generation step? \n",
    "  * is it the entropy of the generated paraphrase tokens? \n",
    "* given size of action space is this a good candidate for differential entropy? \n",
    "* when do you hit floating point threshold for token probabilities? When do nans and inf get introduced? \n",
    "* does using fp32 affect token calculations? \n",
    "* how does dropout affect generated probabilities? How does train/eval mode affect generated probs for a sentence? \n",
    "* how does layer-norm affect the probs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "### token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decoder_start_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log each generated word and the next few probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "346.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
