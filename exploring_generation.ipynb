{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"  # set to false if not working\n",
    "\n",
    "# Core imports \n",
    "import torch, numpy as np, pandas as pd, gc,sys, logging, warnings\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.distributions import Categorical\n",
    "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, \n",
    "                          AutoTokenizer, AdamW, SchedulerType, get_scheduler)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from collections import defaultdict\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from cachetools import cached, LRUCache\n",
    "from types import MethodType\n",
    "from timeit import default_timer as timer\n",
    "import utils; from utils import *   # local script \n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import copy \n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from undecorated import undecorated\n",
    "\n",
    "\n",
    "# Dev imports (not needed for final script)\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "from GPUtil import showUtilization\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s') \n",
    "logger = logging.getLogger(\"main_logger\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for the pp_model \n",
    "# 1. tuner007/pegasus_paraphrase\n",
    "# 2. tdopierre/ProtAugment-ParaphraseGenerator\n",
    "# 3. eugenesiow/bart-paraphrase\n",
    "\n",
    "## PEGASUS model\n",
    "pp_name = \"tuner007/pegasus_paraphrase\"\n",
    "pp_tokenizer_pegasus = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_pegasus.generate)\n",
    "pp_model_pegasus.generate_with_grad = MethodType(generate_with_grad, pp_model_pegasus)\n",
    "\n",
    "## BART model\n",
    "pp_name = \"eugenesiow/bart-paraphrase\"\n",
    "pp_tokenizer_bart = AutoTokenizer.from_pretrained(pp_name)\n",
    "pp_model_bart = AutoModelForSeq2SeqLM.from_pretrained(pp_name, local_files_only=True)\n",
    "generate_with_grad = undecorated(pp_model_bart.generate)\n",
    "pp_model_bart.generate_with_grad = MethodType(generate_with_grad, pp_model_bart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     20,
     26,
     43,
     48,
     65,
     82,
     92
    ]
   },
   "outputs": [],
   "source": [
    "def get_pp_logp(translated): \n",
    "    \"\"\"log(p(pp|orig)) basically.\n",
    "    works for greedy search, will need tweaking for other types probably\"\"\"\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "        seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    "    )\n",
    "    scores_log_softmax = torch.stack(translated.scores, 1).log_softmax(2)\n",
    "    seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    del scores_log_softmax\n",
    "    # account for nan values by setting them to 0 (maybe a bit of a hack)\n",
    "    # will also handle inf and -inf values too by default\n",
    "    seq_token_log_probs = torch.nan_to_num(seq_token_log_probs)\n",
    "    # account for the padding tokens at the end \n",
    "    seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "    seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "#     if np.any(np.isnan(seq_log_prob.detach().cpu()).tolist()): \n",
    "#         warnings.warn(f\"Warning: NAN's detected in pp_logp calclulations.\\n seq_token_log_probs: {seq_token_log_probs}\")\n",
    "    return seq_log_prob\n",
    "\n",
    "def get_tokens_from_token_ids_batch(tokenizer, ids_batch):\n",
    "    l = []\n",
    "    for i in range(ids_batch.shape[0]): \n",
    "        l.append(tokenizer.convert_ids_to_tokens(ids_batch[i,:]))\n",
    "    return l\n",
    "\n",
    "def get_start_end_special_token_ids(tokenizer): \n",
    "    \"\"\"The token id's that input/output sequences should start and end with\"\"\"\n",
    "    d = {}\n",
    "    if pp_tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "        d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "        d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    elif pp_tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "        d[\"input_start_id\"] =  None\n",
    "        d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "        d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "        d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "    else: \n",
    "        raise Exception(\"unrecognised tokenizer\")\n",
    "    return d\n",
    "\n",
    "def check_no_nans_or_infs(x):\n",
    "    assert torch.all(~torch.isnan(x))\n",
    "    assert torch.all(~torch.isneginf(x))\n",
    "    assert torch.all(~torch.isposinf(x))\n",
    "\n",
    "def assert_start_and_end_tokens_are_correct(tokenizer, orig_token_ids, pp_token_ids):\n",
    "    \"\"\"Make sure input sequences (orig) and output sequences (pp) start and end with the \n",
    "    right special tokens (depends on tokenizer)\"\"\"\n",
    "    start_end_token_d = get_start_end_special_token_ids(pp_tokenizer)\n",
    "    \n",
    "    # Input\n",
    "    if start_end_token_d['input_start_id'] is not None: \n",
    "        assert torch.all(orig_token_ids[:,0] == start_end_token_d['input_start_id'])\n",
    "    # can probs rewrite this to make it nicer but it's fine for now\n",
    "    assert torch.all(torch.logical_or(orig_token_ids[:,-1] == start_end_token_d['input_end_id'][0], \n",
    "                                      orig_token_ids[:,-1] == start_end_token_d['input_end_id'][1]))\n",
    "    \n",
    "    # Output\n",
    "    assert torch.all(pp_token_ids[:,0] == start_end_token_d['output_start_id'])\n",
    "    assert torch.all(torch.logical_or(pp_token_ids[:,-1] == start_end_token_d['output_end_id'][0], \n",
    "                                      pp_token_ids[:,-1] == start_end_token_d['output_end_id'][1]))\n",
    "\n",
    "def check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked): \n",
    "    \"\"\"Check we don't have any postive inf or nan, and that all negative inf values are expected\"\"\"\n",
    "    assert torch.all(~torch.isposinf(scores_stacked))\n",
    "    assert torch.all(~torch.isnan(scores_stacked))\n",
    "\n",
    "    # We expect to see negative inf for the eos_token when we have not reached min_length. \n",
    "    # But we shouldn't expect it for any other tokens\n",
    "    idx_neginf = torch.nonzero(torch.isneginf(scores_stacked))\n",
    "    assert torch.all(idx_neginf[:,2] == pp_tokenizer.eos_token_id)\n",
    "    # Rough check that all idx before min_length are -inf for all elements in batch\n",
    "    # We do min_length - 1 because sequences are allowed to have length min_length so that idx \n",
    "    # shouldn't be set to -inf\n",
    "    # Not a 100% test but very likely to identify\n",
    "    assert idx_neginf.shape[0] == (pp_model_params[\"min_length\"] -1) * batch_size  \n",
    "    # Check that no elements after min_length are -inf\n",
    "    assert torch.all(idx_neginf[:,1] < (pp_model_params[\"min_length\"] -1 ))\n",
    "\n",
    "def check_scores_log_softmax_sums_and_shape(scores_log_softmax):\n",
    "    sums = scores_log_softmax.exp().sum(2)\n",
    "    # check that the axes is right\n",
    "    # we want to sum over token probabilities at each generation step, so we \n",
    "    # should end up with a shape [batch_size, generated_length]\n",
    "    assert sums.shape[0] == batch_size  \n",
    "    assert sums.shape[1] == generated_length - 1\n",
    "    # check that they sum to 1 along the generated_length axis\n",
    "    assert torch.allclose(sums, torch.ones(sums.size()), atol = 1e-4)\n",
    "    \n",
    "def check_seq_token_log_prob_values_are_correct(): \n",
    "    \"\"\"Just enumerates and checks values\n",
    "    Quite slow for large batches so run as a test rather than an assert in every batch. \n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i_ex in range(batch_size):\n",
    "        for i_step in range(generated_length - 1):\n",
    "            i_tkn = seq_without_first_tkn[i_ex][i_step].item()\n",
    "            l.append(scores_log_softmax[i_ex,i_step, i_tkn] == seq_token_log_probs[i_ex,i_step])\n",
    "    assert all(l)\n",
    "    \n",
    "def pretty_print_pp_batch_and_next_token_probabilities(): \n",
    "    \"\"\"Goes through each paraphrase and shows at each timestep the next likely tokens. \n",
    "    Only will work for greedy search. \n",
    "    e.g. [\n",
    "    \"<pad> ['▁My, 0.289', '▁I, 0.261', '▁Hello, 0.07'] | Entropy: 4.23 \",\n",
    "     \"<pad> My ['▁name, 0.935', '▁Name, 0.005', 'name, 0.002'] | Entropy: 0.80 \"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    str_d = defaultdict(list)\n",
    "    for i_tkn in range(0, generated_length-1): \n",
    "        ids = pp_output.sequences[:, :(i_tkn+1)]\n",
    "        partial_pp = pp_tokenizer.batch_decode(ids)\n",
    "        kth_ids,kth_probs = tkn_kmaxidx[:, i_tkn, :], tkn_kmaxprob[:, i_tkn, :]\n",
    "        kth_tkns = get_tokens_from_token_ids_batch(pp_tokenizer, kth_ids)\n",
    "\n",
    "        # enumerates examples in batch\n",
    "        z = zip(partial_pp, kth_tkns, kth_probs, ent.detach())\n",
    "        for i_ex, (ex_sen, ex_next_tkns, ex_next_probs, ex_e) in enumerate(z): \n",
    "            # Form nice formatted string mixing together tokens and probabilities\n",
    "            tkn_tuples_l = [(tkn, round_t(prob,3)) for tkn, prob in zip(ex_next_tkns, ex_next_probs)]\n",
    "            tkn_str = ['%s, %s' % t for t in tkn_tuples_l]\n",
    "            # Add to dict of lists and add on entropy term. \n",
    "            str_d[i_ex].append(f\"{ex_sen} {tkn_str} | Entropy: {ex_e[i_tkn]:.2f} \")\n",
    "\n",
    "    for v in str_d.values():  pprint(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring paraphrase generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT AND PARAMETERS\n",
    "orig_l = [\n",
    "    \"Look! A small dog. Isn't it cute?\", \n",
    "    \"Far out, if I have to write another sentence...it'll be bad.\"\n",
    "]\n",
    "pp_model_params = {\n",
    "    \"num_beams\": 1, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"num_beam_groups\": 1, \n",
    "    \"diversity_penalty\": 0.,   # must be a float\n",
    "    \"temperature\": 1.5,\n",
    "    \"length_penalty\" : 1,\n",
    "    \"min_length\" : 5\n",
    "}\n",
    "\n",
    "## Select which model/tokenizer to use\n",
    "pp_tokenizer = pp_tokenizer_pegasus\n",
    "pp_model = pp_model_pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "############################## TOKENIZER ########################################\n",
      "\n",
      "We are using the tuner007/pegasus_paraphrase tokenizer\n",
      "Tokenizer has these special tokens:['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "Using bos_token, but it is not set yet.\n",
      "The bos token is None and has id None\n",
      "The eos token is </s> and has id 1\n",
      "The pad token is <pad> and has id 0\n",
      "The unk token is <unk> and has id 105\n",
      "\n",
      "############################### INPUT #######################################\n",
      "\n",
      "Original text: [\"Look! A small dog. Isn't it cute?\", \"Far out, if I have to write another sentence...it'll be bad.\"]\n",
      "Batch size is: 2\n",
      "This is tokenised to get a dict with keys dict_keys(['input_ids', 'attention_mask']) which should be input_ids and attention_mask \n",
      "The input_ids look like this: tensor([[ 3842,   147,   202,   360,  1296,   107, 16099,   131,   144,   126,\n",
      "          2860,   152,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 6825,   165,   108,   175,   125,   133,   112,  1094,   372,  5577,\n",
      "           401,  1418,   131,   267,   129,  1025,   107,     1,     0,     0]])\n",
      "The tokens are: [['▁Look', '!', '▁A', '▁small', '▁dog', '.', '▁Isn', \"'\", 't', '▁it', '▁cute', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['▁Far', '▁out', ',', '▁if', '▁I', '▁have', '▁to', '▁write', '▁another', '▁sentence', '...', 'it', \"'\", 'll', '▁be', '▁bad', '.', '</s>', '<pad>', '<pad>']]\n",
      "This has shape torch.Size([2, 20]) or [batch_size, input_length], which also might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\n",
      "Input length is: 20\n",
      "The attention_mask looks like this: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "This has shape torch.Size([2, 20]) or [batch_size, input_length]\n",
      "\n",
      "#################################### PARAPHRASES ##################################\n",
      "\n",
      "Paraphrases: [\"Isn't it cute that a small dog is staring?\", 'If I have to write another sentence, it will be bad.']\n",
      "Output has keys odict_keys(['sequences', 'scores'])\n",
      "Paraphrases with special tokens: [\"<pad> Isn't it cute that a small dog is staring?</s><pad>\", '<pad> If I have to write another sentence, it will be bad.</s>']\n",
      "List of pp tokens:[['<pad>', '▁Isn', \"'\", 't', '▁it', '▁cute', '▁that', '▁a', '▁small', '▁dog', '▁is', '▁staring', '?', '</s>', '<pad>'], ['<pad>', '▁If', '▁I', '▁have', '▁to', '▁write', '▁another', '▁sentence', ',', '▁it', '▁will', '▁be', '▁bad', '.', '</s>']]\n",
      "Paraphrase token sequences: tensor([[    0, 16099,   131,   144,   126,  2860,   120,   114,   360,  1296,\n",
      "           117, 14505,   152,     1,     0],\n",
      "        [    0,   240,   125,   133,   112,  1094,   372,  5577,   108,   126,\n",
      "           138,   129,  1025,   107,     1]])\n",
      "Shape of pp token sequences:torch.Size([2, 15]) or [batch_size, generated_length]\n",
      "Generated length: 15\n",
      "\n",
      "##########################  SCORES AND PROBABILITIES ####################################\n",
      "\n",
      "Scores is a tuple of length 14 which is one less than the generated_length, or the number of tokens in the pp token sequences (this has shape torch.Size([2, 15])\n",
      "Each score is a tensor of shape torch.Size([2, 96103]) or [batch_size, vocab_size]\n",
      "We stack them to get a tensor of shape torch.Size([2, 14, 96103]) or [batch_size, generated_length - 1, vocab_size]\n",
      "Scores are really logits so we have to take softmax to get probabilities. \n",
      "But if we take regular softmax then we run into numerical errors so we take log softmax\n",
      "We then select the token probability corresponding to each token and sum them to get the log probability of the sequence.\n",
      "\n",
      "########################## ENTROPY AND TOKEN PROBABILITIES ####################################\n",
      "\n",
      "Originals: [\"Look! A small dog. Isn't it cute?\", \"Far out, if I have to write another sentence...it'll be bad.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad> ['▁Isn, 0.245', '▁A, 0.176', '▁There, 0.091'] | Entropy: 4.71 \",\n",
      " '<pad> Isn [\"\\', 0.881\", \\'▁the, 0.002\\', \\'▁it, 0.001\\'] | Entropy: 1.66 ',\n",
      " \"<pad> Isn' ['t, 0.933', 'ta, 0.001', 'T, 0.001'] | Entropy: 0.98 \",\n",
      " \"<pad> Isn't ['▁it, 0.293', '▁the, 0.29', '▁this, 0.155'] | Entropy: 3.44 \",\n",
      " \"<pad> Isn't it ['▁cute, 0.577', '▁adorable, 0.109', '▁a, 0.022'] | Entropy: \"\n",
      " '2.87 ',\n",
      " \"<pad> Isn't it cute ['▁that, 0.375', '?, 0.357', ',, 0.024'] | Entropy: \"\n",
      " '3.09 ',\n",
      " \"<pad> Isn't it cute that ['▁a, 0.405', '▁the, 0.181', '▁there, 0.111'] | \"\n",
      " 'Entropy: 3.32 ',\n",
      " \"<pad> Isn't it cute that a ['▁small, 0.735', '▁dog, 0.149', '▁little, \"\n",
      " \"0.032'] | Entropy: 1.55 \",\n",
      " \"<pad> Isn't it cute that a small ['▁dog, 0.911', '▁animal, 0.018', '▁canine, \"\n",
      " \"0.003'] | Entropy: 1.05 \",\n",
      " \"<pad> Isn't it cute that a small dog ['▁is, 0.729', '?, 0.039', '▁looks, \"\n",
      " \"0.034'] | Entropy: 2.41 \",\n",
      " \"<pad> Isn't it cute that a small dog is ['▁staring, 0.078', '▁looking, \"\n",
      " \"0.073', '▁there, 0.069'] | Entropy: 5.46 \",\n",
      " \"<pad> Isn't it cute that a small dog is staring ['?, 0.57', '▁at, 0.227', \"\n",
      " \"'!, 0.022'] | Entropy: 2.39 \",\n",
      " \"<pad> Isn't it cute that a small dog is staring? ['</s>, 0.911', '., 0.0', \"\n",
      " \"'▁The, 0.0'] | Entropy: 1.31 \",\n",
      " \"<pad> Isn't it cute that a small dog is staring?</s> ['</s>, 0.878', '., \"\n",
      " \"0.001', '▁and, 0.0'] | Entropy: 1.75 \"]\n",
      "[\"<pad> ['▁If, 0.265', '▁I, 0.253', '▁It, 0.252'] | Entropy: 3.11 \",\n",
      " \"<pad> If ['▁I, 0.809', '▁it, 0.011', '▁there, 0.006'] | Entropy: 2.24 \",\n",
      " \"<pad> If I ['▁have, 0.746', '▁need, 0.043', '▁must, 0.018'] | Entropy: 2.06 \",\n",
      " \"<pad> If I have ['▁to, 0.9', '▁another, 0.012', '▁a, 0.011'] | Entropy: \"\n",
      " '1.20 ',\n",
      " \"<pad> If I have to ['▁write, 0.699', '▁do, 0.028', '▁make, 0.019'] | \"\n",
      " 'Entropy: 2.82 ',\n",
      " \"<pad> If I have to write ['▁another, 0.573', '▁a, 0.219', '▁again, 0.032'] | \"\n",
      " 'Entropy: 2.36 ',\n",
      " \"<pad> If I have to write another ['▁sentence, 0.628', '▁one, 0.044', \"\n",
      " \"'▁thing, 0.039'] | Entropy: 3.46 \",\n",
      " \"<pad> If I have to write another sentence [',, 0.409', '▁it, 0.207', '..., \"\n",
      " \"0.201'] | Entropy: 2.78 \",\n",
      " \"<pad> If I have to write another sentence, ['▁it, 0.755', '▁I, 0.062', \"\n",
      " \"'▁that, 0.059'] | Entropy: 1.80 \",\n",
      " '<pad> If I have to write another sentence, it [\\'▁will, 0.588\\', \"\\', 0.24\", '\n",
      " \"'▁would, 0.027'] | Entropy: 2.27 \",\n",
      " \"<pad> If I have to write another sentence, it will ['▁be, 0.834', '▁not, \"\n",
      " \"0.01', '▁make, 0.004'] | Entropy: 1.95 \",\n",
      " \"<pad> If I have to write another sentence, it will be ['▁bad, 0.371', \"\n",
      " \"'▁terrible, 0.178', '▁very, 0.069'] | Entropy: 3.40 \",\n",
      " \"<pad> If I have to write another sentence, it will be bad ['., 0.86', '..., \"\n",
      " \"0.01', '</s>, 0.01'] | Entropy: 1.68 \",\n",
      " \"<pad> If I have to write another sentence, it will be bad. ['</s>, 0.895', \"\n",
      " \"'., 0.0', '▁, 0.0'] | Entropy: 1.53 \"]\n"
     ]
    }
   ],
   "source": [
    "#### TOKENIZER INFORMATION #####\n",
    "logger.info(\"\\n############################## TOKENIZER ########################################\\n\")\n",
    "logger.info(f\"We are using the {pp_tokenizer.name_or_path} tokenizer\")\n",
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer.bos_token} and has id {pp_tokenizer.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer.eos_token} and has id {pp_tokenizer.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer.pad_token} and has id {pp_tokenizer.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer.unk_token} and has id {pp_tokenizer.unk_token_id}\")\n",
    "\n",
    "\n",
    "#### INPUT #####\n",
    "batch_size = len(orig_l)\n",
    "orig_tokens = pp_tokenizer(orig_l, return_tensors='pt', padding=True, pad_to_multiple_of=4)\n",
    "input_length = orig_tokens['input_ids'].size()[1]\n",
    "orig_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, orig_tokens['input_ids'])\n",
    "\n",
    "logger.info(\"\\n############################### INPUT #######################################\\n\")\n",
    "logger.info(f\"Original text: {orig_l}\")\n",
    "logger.info(f\"Batch size is: {batch_size}\")\n",
    "logger.info(f\"This is tokenised to get a dict with keys {orig_tokens.keys()} which should be input_ids and attention_mask \")\n",
    "logger.info(f\"The input_ids look like this: {orig_tokens['input_ids']}\")\n",
    "logger.info(f\"The tokens are: {orig_l_tokens_list}\")\n",
    "logger.info(f\"This has shape {orig_tokens['input_ids'].shape} or [batch_size, input_length], which also\\\n",
    " might be padded to hit a padding multiple (so input_length is not just the longest example length in the batch).\")\n",
    "logger.info(f\"Input length is: {input_length}\")\n",
    "logger.info(f\"The attention_mask looks like this: {orig_tokens['attention_mask']}\")\n",
    "logger.info(f\"This has shape {orig_tokens['attention_mask'].shape} or [batch_size, input_length]\")\n",
    "\n",
    "##### PARAPHRASE #####\n",
    "pp_output = pp_model.generate_with_grad(**orig_tokens, **pp_model_params, do_sample=False, \n",
    "                                      return_dict_in_generate=True,\n",
    "                                      output_scores=True,\n",
    "                                    remove_invalid_values=False)\n",
    "generated_length = pp_output.sequences.shape[1]\n",
    "pp_l             = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=True)\n",
    "pp_l_with_tokens = pp_tokenizer.batch_decode(pp_output.sequences, skip_special_tokens=False)\n",
    "pp_l_tokens_list = get_tokens_from_token_ids_batch(pp_tokenizer, pp_output.sequences)\n",
    "\n",
    "assert_start_and_end_tokens_are_correct(pp_tokenizer, orig_token_ids=orig_tokens['input_ids'],\n",
    "                                        pp_token_ids=pp_output.sequences)\n",
    "\n",
    "logger.info(\"\\n#################################### PARAPHRASES ##################################\\n\")\n",
    "logger.info(f\"Paraphrases: {pp_l}\")\n",
    "logger.info(f\"Output has keys {pp_output.keys()}\")\n",
    "logger.info(f\"Paraphrases with special tokens: {pp_l_with_tokens}\")\n",
    "logger.info(f\"List of pp tokens:{pp_l_tokens_list}\")\n",
    "logger.info(f\"Paraphrase token sequences: {pp_output.sequences}\")\n",
    "logger.info(f\"Shape of pp token sequences:{pp_output.sequences.shape} or [batch_size, generated_length]\")\n",
    "logger.info(f\"Generated length: {generated_length}\")\n",
    "\n",
    "###### SCORES AND PROBABILITIES ########\n",
    "scores_stacked = torch.stack(pp_output.scores, 1)\n",
    "# The second argument to stack (i.e. dim) determines which axis the tensors are stacked along. \n",
    "# It determines the axis that becomes generated_length - 1\n",
    "# dim=0 gives shape [generated_length-1, batch_size, vocab_size]\n",
    "# dim=1 gives shape [batch_size, generated_length-1, vocab_size]\n",
    "# dim=2 gives shape [batch_size, vocab_size, generated_length-1]\n",
    "# Our scores_stacked is stacked on dim 1 so it should be second \n",
    "assert scores_stacked.shape == torch.Size([batch_size, (generated_length - 1), pp_tokenizer.vocab_size])\n",
    "check_scores_for_posinf_nan_and_unexpected_neginf(scores_stacked)\n",
    "\n",
    "\n",
    "# These scores are logits \n",
    "# see some of the docs on this page https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput\n",
    "# so we got to take softmax over them \n",
    "# but if we take regular softmax then we run into numerical errors\n",
    "# so instead we take log_softmax\n",
    "scores_log_softmax = torch.log_softmax(scores_stacked, 2)\n",
    "check_scores_log_softmax_sums_and_shape(scores_log_softmax)\n",
    "\n",
    "\n",
    "####### SEQUENCE PROBABILITIES #######\n",
    "# We select the token probability corresponding to each token \n",
    "# However because the scores represent transitions we need to remove the first token from each \n",
    "# sequence to match them up. \n",
    "seq_without_first_tkn = pp_output.sequences[:,1:]\n",
    "assert seq_without_first_tkn.shape == torch.Size([batch_size, generated_length - 1])\n",
    "# Now select prob corresponding to each token\n",
    "seq_token_log_probs = torch.gather(scores_log_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "assert seq_token_log_probs.shape == seq_without_first_tkn.shape  # probs should be 1-1 with the filtered tkns\n",
    "check_no_nans_or_infs(seq_token_log_probs)\n",
    "# Check that the last token probability corresponds to a possible end token \n",
    "output_end_ids = get_start_end_special_token_ids(pp_tokenizer)['output_end_id']\n",
    "assert all([o in scores_log_softmax[:, -1, output_end_ids] for o in seq_token_log_probs[:,-1]])\n",
    "check_seq_token_log_prob_values_are_correct()\n",
    "\n",
    "# The attention mask has 1 everywhere except for where padding tokens occur, where it has 0. \n",
    "# It is used to filter out padding tokens from the sequence probablity because then the sequence \n",
    "# probability will depend on how many padding tokens there are and the probability of generating them, \n",
    "# which (a) we don't want and (b) the probability isn't correct anyway \n",
    "attention_mask = pp_model._prepare_attention_mask_for_generation(\n",
    "    seq_without_first_tkn, pp_tokenizer.pad_token_id, pp_tokenizer.eos_token_id\n",
    ")\n",
    "seq_token_log_probs = seq_token_log_probs * attention_mask\n",
    "# check attention mask only has 0 for padding tokens and not eos tokens or anything else\n",
    "assert all(seq_without_first_tkn[attention_mask == 0] == pp_tokenizer.pad_token_id)\n",
    "assert seq_token_log_probs.shape == attention_mask.shape == seq_token_log_probs.shape\n",
    "assert torch.all(seq_token_log_probs  > -10)  # we shouldn't be picking extrememly rare tokens\n",
    "seq_log_prob = seq_token_log_probs.sum(-1)\n",
    "assert seq_log_prob.shape == torch.Size([batch_size])\n",
    "check_no_nans_or_infs(seq_log_prob)\n",
    "\n",
    "logger.info(\"\\n##########################  SCORES AND PROBABILITIES ####################################\\n\")\n",
    "logger.info(f\"Scores is a tuple of length {len(pp_output.scores)} which is one less than the generated_length, or \\\n",
    "the number of tokens in the pp token sequences (this has shape {pp_output.sequences.shape}\")\n",
    "logger.info(f\"Each score is a tensor of shape {pp_output.scores[0].shape} or [batch_size, vocab_size]\")\n",
    "#logger.info(f\"Full shape:{[o.shape for o in pp_output.scores]}\")\n",
    "logger.info(f\"We stack them to get a tensor of shape {scores_stacked.shape} or [batch_size, generated_length - 1, vocab_size]\")\n",
    "logger.info(f\"Scores are really logits so we have to take softmax to get probabilities. \")\n",
    "logger.info(\"But if we take regular softmax then we run into numerical errors so we take log softmax\")\n",
    "logger.info(\"We then select the token probability corresponding to each token and sum them to get the log \\\n",
    "probability of the sequence.\")\n",
    "\n",
    "############# ENTROPY AND TOKEN PROBABILITIES ####################\n",
    "ent = Categorical(logits = scores_stacked).entropy()\n",
    "assert ent.shape == torch.Size([batch_size, generated_length - 1])\n",
    "scores_softmax = scores_log_softmax.exp()\n",
    "k=3\n",
    "tkn_kmaxprob, tkn_kmaxidx = torch.topk(scores_softmax, k=k, dim=2)\n",
    "tkn_kmaxprob = tkn_kmaxprob.detach()  # log these \n",
    "# The third dimension indexes top1, top2, top3 etc \n",
    "assert tkn_kmaxprob[:,:,0].shape == torch.Size([batch_size, generated_length - 1])\n",
    "# I'd naively expect True everywhere for tkn_kmaxidx[:,:,0] == pp_output.sequences[:, 1:] but it turns \n",
    "# out this is not the case because padding tokens seem to have prob 0 and eos tokens are outputted \n",
    "# instead by the token generation process and then later replaced by pad\n",
    "\n",
    "#Uncomment to show how paraphrase is formed. \n",
    "\n",
    "\n",
    "logger.info(\"\\n########################## ENTROPY AND TOKEN PROBABILITIES ####################################\\n\")\n",
    "logger.info(f\"Originals: {orig_l}\")\n",
    "pretty_print_pp_batch_and_next_token_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenizer differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Both the \"eugenesiow/bart-paraphrase\" model and the \"tdopierre/ProtAugment-ParaphraseGenerator\" are BART tokenizers and have type BartTokenizerFast. The implementation is identical to RobertaTokenizerFast according to the docs, which in turn was derived from GPT-2. They use byte-level Byte Pair Encoding.  \n",
    "\n",
    "The \"tuner007/pegasus_paraphrase\" model is a Pegasus tokenizer has type PegasusTokenizerFast. This uses Unigram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenization differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The BART tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens = pp_tokenizer_bart(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_bart, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Pegasus tokenizer doesn't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens = pp_tokenizer_pegasus(['hello there',' hello there'], return_tensors='pt')\n",
    "get_tokens_from_token_ids_batch(pp_tokenizer_pegasus, tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Representing tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The tokenizers represent tokens differently.  \n",
    "The BART models use Ġ to indicate start of a word for a token. Its generated tokens look like `['<s>', 'Hello', 'Ġmy', 'Ġname', 'Ġis', 'Ġz', 'f', 'ld', 'lf', 'o', 'q', 'd', '</s>', '<pad>', '<pad>', '<pad>']`  \n",
    "The Pegasus model uses \\_ to indicate start of a word for a token. Its generated tokens look like `['▁Hello', '▁my', '▁name', '▁is', '▁z', 'fl', 'dl', 'fo', 'q', 'd', '</s>', '<pad>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_bart.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_bart.bos_token} and has id {pp_tokenizer_bart.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_bart.eos_token} and has id {pp_tokenizer_bart.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_bart.pad_token} and has id {pp_tokenizer_bart.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_bart.unk_token} and has id {pp_tokenizer_bart.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Tokenizer has these special tokens:{pp_tokenizer_pegasus.all_special_tokens}\")\n",
    "logger.info(f\"The bos token is {pp_tokenizer_pegasus.bos_token} and has id {pp_tokenizer_pegasus.bos_token_id}\")\n",
    "logger.info(f\"The eos token is {pp_tokenizer_pegasus.eos_token} and has id {pp_tokenizer_pegasus.eos_token_id}\")\n",
    "logger.info(f\"The pad token is {pp_tokenizer_pegasus.pad_token} and has id {pp_tokenizer_pegasus.pad_token_id}\")\n",
    "logger.info(f\"The unk token is {pp_tokenizer_pegasus.unk_token} and has id {pp_tokenizer_pegasus.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Special token usage with input and output sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "They use this format \n",
    "```\n",
    "single sequence: <s> X </s>\n",
    "pair of sequences: <s> A </s></s> B </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Format: \n",
    "```\n",
    "- single sequence: ``X </s>``\n",
    "- pair of sequences: ``A B </s>`` (not intended use)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "BOS token is never used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Differences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Tokenizers also use different tokens when representing input sequences and generating output sentences. Here is a quick summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens_d = {\n",
    "    \"bart\": {\n",
    "        \"special_tokens\": pp_tokenizer_bart.all_special_tokens, \n",
    "        \"input_start\": pp_tokenizer_bart.bos_token,\n",
    "        \"input_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token], \n",
    "        \"output_start\": pp_tokenizer_bart.eos_token, \n",
    "        \"output_end\": [pp_tokenizer_bart.pad_token, pp_tokenizer_bart.eos_token]\n",
    "    }, \n",
    "    \"pegasus\": {\n",
    "        \"special_tokens\": pp_tokenizer_pegasus.all_special_tokens,\n",
    "        \"input_start\": None,\n",
    "        \"input_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "        \"output_start\": pp_tokenizer_pegasus.pad_token,  \n",
    "        \"output_end\": [pp_tokenizer_pegasus.pad_token, pp_tokenizer_pegasus.eos_token], \n",
    "    }\n",
    "}\n",
    "tokens_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Token indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_tokens_from_ids(tokenizer, start_id=100, end_id=200):\n",
    "    ids = list(range(start_id,end_id))\n",
    "    print(*list(zip(ids, tokenizer.convert_ids_to_tokens(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### BART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having a look at generated tokens makes me suspect that they are indexed in whatever order they are encountered in the source text they are trained on. It seems like a rough frequency of english tokens but there are also tokens that are definitely out of order. \n",
    "\n",
    "The first few are reserved for special tokens, and the other low numbers (e.g. up to 100) are pretty common suffixes and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at 100 to 200 you can see some words (e.g. Trump at 140, or 2017 at 193) that aren't common enough to be that high. This makes me suspect that words are in encounter order in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, 100,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokens towards the end are gibberish or mispellings encountered in the input. The fifth last token is something labelled <|endoftext|> and I don't know what that is. Then there is a bunch of tokens like \"madeupword0001\". The last token is the mask token and then token indicies after that return None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_bart, pp_tokenizer_bart.vocab_size-20, pp_tokenizer_bart.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Special tokens make up the first hundred or so. After that there's a token \\<n> that seems like some new line thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 0,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike the BART models I can believe that these tokens are in order of frequency. I can't see anything that is obviously out of place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, 120,250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's nothing special at the end, just looks like isolated tokens and None values after the tokens finish. It's also worth noting that the Pegasus model has ~96100 tokens which is way more than the ~50270 of the BART models (almost double). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_tokens_from_ids(pp_tokenizer_pegasus, pp_tokenizer_pegasus.vocab_size-20, pp_tokenizer_pegasus.vocab_size+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### When does the model generate padding tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For both models padding tokens are generated after the EOS token. Additionally for Pegasus generated text starts with the padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Why do generated paraphrases start with the EOS token? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is only the case for BART models. For pegasus models they use a padding token to start generated paraphrases. \n",
    "\n",
    "I don't know what the BOS token isn't used for these things. Pegasus has an open issue [here](https://github.com/huggingface/transformers/issues/12474). \n",
    "\n",
    "Whatever the reason you should just do the default because that is what the preprocessing does and you will get the best results that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Does p(PAD) =1 after an eos token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For both BART and Pegasus models it appears that probability of outputting a pad token is actually zero at all timesteps. Instead the model outputs the eos token over and over, and there must be some post-processing that takes place that replaces eos token with padding token. \n",
    "For Pegasus it appears it is the same behaviour. \n",
    "Example code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(round_t(scores_softmax[:,:,pp_tokenizer.eos_token_id]))\n",
    "print(round_t(scores_softmax[:,:,pp_tokenizer.pad_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is interesting is that there is probability assigned to other tokens other than eos and pad after a eos token is outputted. Again there must be some kind of postprocessing that takes care of this situation because I haven't really seen it in the wild. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some models (e.g. GPT2) don't even have a PAD token. Instead they use the eos token on repeat. See this [issue](https://github.com/huggingface/transformers/issues/8452#issuecomment-739008168). What is confusing is seeing this behaviour with models that have a padding token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Do rows always sum to 1 when looking at token generation scores? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yes, they should. I put in an assert to check this. \n",
    "If you have a nan or an inf then they won't sum to 1. To confirm this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.isnan(torch.sum(torch.tensor([1,2,3, torch.nan]))))\n",
    "print(torch.isinf(torch.sum(torch.tensor([1,2,3, torch.inf]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Does first row sum to 0? (the one corresponding to the startoff token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So there is no token scores that correspond to the first token (usually a bos or pad token). The scores are a tuple of length (`generated_length - 1`). So there shouldn't be a \"zero\" row really. \n",
    "I remember seeing something like this at some point so I'll keep an eye out for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How are logits containing nan or inf transformed with softmax and log_softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can explore this through some code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Vanilla case  \n",
    "First we look at the case without any nan or inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The softmax values are interpreted as probabilities, and the log softmax is just the log of the probabilities, done for numerical stability. We can just take exponents to return to probabilities if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.log_softmax(logits,0).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Positive inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's see what happens if we introduce a positive inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We get nan values in the softmax and log_softmax.  So if you see nans in the softmax, remember that an inf in the scores is one reason why it may happen.  \n",
    "\n",
    "This is interesting because if we just assume inf is a large positive number, we'd expect a softmax with basically a 1 and all zeros, and a log softmax of a 0 and a lot of negatives. We can try it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, 10000000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Basically what we get. So this indicates that if we get a positive inf we might be able to mitigate this problem by clipping it to some kind of maximum value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Negative inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -torch.inf])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Negative inf behaves a bit differently. The softmax is unaffected and basically just assigns a prob of 0 to the corresponding entry. The log softmax carries the `-inf` through. \n",
    "\n",
    "Again clipping the -inf to a large negative value can mitigate this problem somewhat: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, -10000000])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.4, -1, 3, 2, torch.nan])\n",
    "print(logits)\n",
    "print(torch.softmax(logits,0))\n",
    "print(torch.log_softmax(logits,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A nan in the logits propagates and affects the entire softmax and log_softmax tensors. The network basically gives up and says \"no idea how to deal with this. \n",
    "\n",
    "This seems to be the case with most torch functions; e.g.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(torch.sum(logits,0))\n",
    "print(torch.divide(logits,0.2))\n",
    "print(logits + logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How do you interpret token entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The token scores (when stacked) are a tensor of dimensions (batch_size, generated_length - 1, vocab_size). We take softmax to get a tensor of probability distributions across all possible tokens. We can also calculate the entropy of each of these probability distributions. \n",
    "\n",
    "Entropy is a measure of how \"peaky\" or \"flat\" a probability distribution is. It is the expected value of the self-information of an event, which is basically a measure of how \"surprised\" you would be if that event occured. \n",
    "If we have a discrete random variable $X$ with probability distribution $P(x) $, entropy is given by $$H(X) = \\mathbb{E}_{ X\\sim P} [I(x)] = -\\mathbb{E}_{X \\sim P} [\\log P(x)] $$ which is practically calculated by $$H(X) =  -\\sum_{x=-\\infty}^\\infty p(x) \\log(p(x))$$\n",
    "\n",
    "The lowest value of entropy is 0, which is when you have $P(x)=1$ for some event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Categorical(probs = torch.tensor([1,0,0,0])).entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "High values of entropy occur when the probability distribution is very flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(Categorical(probs = torch.tensor([0.20,0.50,0.10,0.20])).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor([0.25,0.25,0.25,0.25])).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In terms of tokens, we can show two realistic distributions below. The first has two likely tokens, one somewhat likely, and the rest unlikely. The second has many more likely tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l1 = [0.001,0.001,0.001,0.001,0.5,0.001,0.4,0.001,0.001,0.001,0.001,0.001,0.09]\n",
    "l2 = [0.1,0.1,0.1,0.1,0.025,0.1,0.1,0.1,0.1,0.1,0.025,0.025,0.025]\n",
    "print(Categorical(probs = torch.tensor(l1)).entropy())  # spikier, lower entropy\n",
    "print(Categorical(probs = torch.tensor(l2)).entropy())  # flatter, higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There isn't a theoretical maximum for entropy, but for tokens you'll be governed by vocab size. Here we show some practical maximums for some different vocab sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v_size = 1000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # v small vocab\n",
    "v_size = 10000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # small vocab\n",
    "v_size = 50000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~BART vocab\n",
    "v_size = 100000\n",
    "print(Categorical(probs = torch.tensor([1/v_size for i in range(v_size)])).entropy())  # ~PEGASUS vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These can be a bit hard to interpret so maybe you should also look at some other token-level stats, like max_prob, second_max_prob, third_max_prob, mean, variance or other things like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some other things to note. First you still get an entropy value if your probability dist sums up to more than 1, so make sure to check this before doing entropy. Secondly if you have nan or inf in the probability values then you will get an error. This is true if you use either `probs` or `logits` in the Categorical function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(Categorical(probs = torch.tensor([0.5,0.25,0.25,0.25])).entropy())       # sums to more than 1, gives result\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error\n",
    "#print(Categorical(probs = torch.tensor([0.5,0.25,0.25,torch.inf])).entropy())  # throws error\n",
    "#print(Categorical(logits = torch.tensor([0.5,0.25,0.25,torch.nan])).entropy())  # throws error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### What does it mean to take average of token entropy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Average entropy over tokens will depend on how many padding tokens are added. This in turn depends on how many examples there are in the batch.   \n",
    "The entropy of padding tokens appears to be quite high based on some quick experiments.  \n",
    "It might or might not depend on sentence length.   \n",
    "It could be useful as a broad measure to see if the model is getting more \"peaky\" in selecting tokens at each time step.   \n",
    "It might be useful when tracking a paraphrase over time and seeing its variations?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How do you get nan and inf introduced into token scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "My understanding is that you get -inf for the first (min_length - 1) steps when you introduce a min_length parameter for the generated sequences for the eos_token_id slot. This is to stop the token from appearing and truncating the sequence. \n",
    "\n",
    "You might also get -inf when setting other parameters to the `generate()` function (e.g. bad_words_ids or something similar).\n",
    "\n",
    "Wrote an assert for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Things to check \n",
    "\n",
    "* how big is the action space? \n",
    "  * initial estimate: on the order of vocab_size ^ sequence length (so bloody huge)\n",
    "  * but only a very small proportion are valid actions\n",
    "* how many pp has probs over: 1e-5, 1e-4, 1e-3, 1e-2, 1e-1. would be a good plot. x axis epoch, then ether do (a) for individual examples, or (b) as averages across examples\n",
    "* log top X sampled sentences and their probs (maybe probs can be a graph)\n",
    "\n",
    "* how does padding mask affect things?\n",
    "* how does token-type-ids affect things? \n",
    "* which dist do you calculate KL divergence and entropy over? \n",
    "  * is it the token entropy’s at each generation step? \n",
    "  * is it the entropy of the generated paraphrase tokens? \n",
    "* given size of action space is this a good candidate for differential entropy? \n",
    "* when do you hit floating point threshold for token probabilities? When do nans and inf get introduced? \n",
    "* does using fp32 affect token calculations? \n",
    "* how does dropout affect generated probabilities? How does train/eval mode affect generated probs for a sentence? \n",
    "* how does layer-norm affect the probs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decoder_start_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
