{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from types import MethodType\n",
    "from undecorated import undecorated\n",
    "from travis_attack.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _prepare_pp_tokenizer_and_model(cfg): \n",
    "    \"\"\"As well as preparing the pp model and tokenizer this function also adds a new method `generate_with_grad` to \n",
    "    the pp model so that we can backprop when generating.\"\"\"\n",
    "    pp_tokenizer = AutoTokenizer.from_pretrained(cfg.pp_name)\n",
    "    # takes about 3GB memory space up on the GPU\n",
    "    # change the `local_files_only` argument if changing the model name \n",
    "    pp_model = AutoModelForSeq2SeqLM.from_pretrained(cfg.pp_name, local_files_only=True)\n",
    "    generate_with_grad = undecorated(pp_model.generate)      # remove the @no_grad decorator from generate\n",
    "    pp_model.generate_with_grad = MethodType(generate_with_grad, pp_model) \n",
    "    return pp_tokenizer, pp_model \n",
    "    \n",
    "def _prepare_vm_tokenizer_and_model(cfg): \n",
    "    vm_tokenizer = AutoTokenizer.from_pretrained(cfg.vm_name)\n",
    "    #TODO: do I need the .to(device) for the vm model when using accelerate?\n",
    "    # change the `local_files_only` argument if changing the model name \n",
    "    vm_model = AutoModelForSequenceClassification.from_pretrained(cfg.vm_name, local_files_only=True).to(cfg.device)\n",
    "    return vm_tokenizer, vm_model \n",
    "    \n",
    "def _pad_model_token_embeddings(cfg, pp_model, vm_model, sts_model): \n",
    "    \"\"\"Resize first/embedding layer of all models to be a multiple of cfg.embedding_padding_multiple. \n",
    "    Good for tensor core efficiency when using fp16.\n",
    "    Makes changes to models in-place.\"\"\"\n",
    "    def pad_token_embeddings_to_multiple_of_n(model, n):\n",
    "        def get_new_vocab_size(model): return int((np.floor(model.config.vocab_size / n) + 1) * n)\n",
    "        model.resize_token_embeddings(get_new_vocab_size(model))\n",
    "    pad_token_embeddings_to_multiple_of_n(pp_model, cfg.embedding_padding_multiple)\n",
    "    pad_token_embeddings_to_multiple_of_n(vm_model, cfg.embedding_padding_multiple)\n",
    "    # sts_model is from SentenceTransformers so needs a bit of unwrapping to access the base huggingface model \n",
    "    pad_token_embeddings_to_multiple_of_n(sts_model._first_module().auto_model, cfg.embedding_padding_multiple) \n",
    "\n",
    "def _update_config(cfg, vm_model, pp_model): \n",
    "    cfg.vm_num_labels = vm_model.num_labels\n",
    "    cfg.vocab_size = pp_model.get_input_embeddings().num_embeddings   # unlike pp_tokenizer.vocab_size this includes the padding \n",
    "    return cfg\n",
    "    \n",
    "def prepare_models(cfg): \n",
    "    \"\"\"Load tokenizers and models for vm, pp, sts. \n",
    "    Pad the first embedding layer if specified in the config.  \n",
    "    Do layer freezing if specified (TODO). \n",
    "    Update config with some model-specific variables. \n",
    "    \"\"\"\n",
    "    vm_tokenizer, vm_model = _prepare_vm_tokenizer_and_model(cfg)\n",
    "    pp_tokenizer, pp_model = _prepare_pp_tokenizer_and_model(cfg)\n",
    "    sts_model = SentenceTransformer(cfg.sts_name)\n",
    "    if cfg.pad_token_embeddings:  _pad_model_token_embeddings(cfg, pp_model, vm_model, sts_model)\n",
    "    cfg = _update_config(cfg, vm_model, pp_model)\n",
    "    return vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Externally we only need the `prepare_models(cfg)` function. This gives all the models and tokenizers needed for the other sections and also updates + returns the config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, sts_model, cfg = prepare_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 30_logging.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
