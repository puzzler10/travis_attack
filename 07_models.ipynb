{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np, torch\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from types import MethodType\n",
    "from undecorated import undecorated\n",
    "from travis_attack.config import Config\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"travis_attack.models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "from travis_attack.utils import round_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _prepare_pp_tokenizer_and_model(cfg): \n",
    "    \"\"\"As well as preparing the pp model and tokenizer this function also adds a new method `generate_with_grad` to \n",
    "    the pp model so that we can backprop when generating.\"\"\"\n",
    "    # PEGASUS takes about 3GB memory space up on the GPU\n",
    "    # change the `local_files_only` argument if changing the model name \n",
    "    pp_model = _load_pp_model(cfg)\n",
    "    pp_model.train()\n",
    "    pp_model_freeze_layers(cfg, pp_model)  # dictated by cfg.unfreeze_last_n_layers; set to \"all\" to do no freezing\n",
    "    generate_with_grad = undecorated(pp_model.generate)      # removes the @no_grad decorator from generate so we can backprop\n",
    "    pp_model.generate_with_grad = MethodType(generate_with_grad, pp_model) \n",
    "    pp_tokenizer = AutoTokenizer.from_pretrained(cfg.pp_name)\n",
    "    return pp_tokenizer, pp_model\n",
    "\n",
    "def _load_pp_model(cfg): \n",
    "    if cfg.using_t5():  pp_model = AutoModelForSeq2SeqLM.from_pretrained(cfg.pp_name, local_files_only=True).to(cfg.device)\n",
    "    else:               pp_model = AutoModelForSeq2SeqLM.from_pretrained(cfg.pp_name, local_files_only=True, max_position_embeddings = cfg.orig_max_length + 10).to(cfg.device)\n",
    "    return pp_model\n",
    "    \n",
    "def _prepare_vm_tokenizer_and_model(cfg): \n",
    "    vm_tokenizer = AutoTokenizer.from_pretrained(cfg.vm_name)\n",
    "    vm_model = AutoModelForSequenceClassification.from_pretrained(cfg.vm_name, local_files_only=True).to(cfg.device)\n",
    "    vm_model.eval()\n",
    "    return vm_tokenizer, vm_model \n",
    "\n",
    "def _prepare_sts_model(cfg): \n",
    "    sts_model = SentenceTransformer(cfg.sts_name).to(cfg.device)\n",
    "    return sts_model\n",
    "\n",
    "def _prepare_nli_tokenizer_and_model(cfg): \n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(cfg.nli_name)\n",
    "    nli_model = AutoModelForSequenceClassification.from_pretrained(cfg.nli_name, local_files_only=True).to(cfg.device)\n",
    "    nli_model.eval()\n",
    "    return nli_tokenizer, nli_model\n",
    "\n",
    "def _prepare_cola_tokenizer_and_model(cfg): \n",
    "    cola_tokenizer = AutoTokenizer.from_pretrained(cfg.cola_name)\n",
    "    cola_model = AutoModelForSequenceClassification.from_pretrained(cfg.cola_name, local_files_only=True).to(cfg.device)\n",
    "    cola_model.eval()\n",
    "    return cola_tokenizer, cola_model\n",
    "    \n",
    "# def _pad_model_token_embeddings(cfg, pp_model, vm_model, sts_model): \n",
    "#     \"\"\"Resize first/embedding layer of all models to be a multiple of cfg.embedding_padding_multiple. \n",
    "#     Good for tensor core efficiency when using fp16 (which we aren't...).\n",
    "#     Makes changes to models in-place.\"\"\"\n",
    "#     def pad_token_embeddings_to_multiple_of_n(model, n):\n",
    "#         def get_new_vocab_size(model): return int((np.floor(model.config.vocab_size / n) + 1) * n)\n",
    "#         model.resize_token_embeddings(get_new_vocab_size(model))\n",
    "#     pad_token_embeddings_to_multiple_of_n(pp_model, cfg.embedding_padding_multiple)\n",
    "#     pad_token_embeddings_to_multiple_of_n(vm_model, cfg.embedding_padding_multiple)\n",
    "#     # sts_model is from SentenceTransformers so needs a bit of unwrapping to access the base huggingface model \n",
    "#     pad_token_embeddings_to_multiple_of_n(sts_model._first_module().auto_model, cfg.embedding_padding_multiple) \n",
    "\n",
    "def _update_config(cfg, vm_model, pp_model): \n",
    "    cfg.vm_num_labels = vm_model.num_labels\n",
    "    cfg.vocab_size = pp_model.get_input_embeddings().num_embeddings   # unlike pp_tokenizer.vocab_size this includes the padding \n",
    "    if   cfg.nli_name  == \"microsoft/deberta-base-mnli\"     :   cfg.contra_label = 0 \n",
    "    elif cfg.nli_name  == \"howey/electra-small-mnli\"        :   cfg.contra_label = 2 \n",
    "    if   cfg.cola_name == \"textattack/albert-base-v2-CoLA\"  :   cfg.cola_positive_label = 1 \n",
    "    return cfg\n",
    "\n",
    "def prepare_models(cfg): \n",
    "    \"\"\"Load tokenizers and models for vm, pp, sts. \n",
    "    Pad the first embedding layer if specified in the config.  \n",
    "    Update config with some model-specific variables. \n",
    "    \"\"\"\n",
    "    vm_tokenizer, vm_model =  _prepare_vm_tokenizer_and_model(cfg)\n",
    "    pp_tokenizer, pp_model =  _prepare_pp_tokenizer_and_model(cfg)\n",
    "    ref_pp_model = _load_pp_model(cfg).eval()\n",
    "    sts_model = _prepare_sts_model(cfg)\n",
    "    nli_tokenizer, nli_model   = _prepare_nli_tokenizer_and_model(cfg)\n",
    "    cola_tokenizer, cola_model = _prepare_cola_tokenizer_and_model(cfg)\n",
    " #   if cfg.pad_token_embeddings:  _pad_model_token_embeddings(cfg, pp_model, vm_model, sts_model)\n",
    "    cfg = _update_config(cfg, vm_model, pp_model)\n",
    "    return vm_tokenizer, vm_model, pp_tokenizer, pp_model, ref_pp_model, sts_model, nli_tokenizer, nli_model, cola_tokenizer, cola_model, cfg\n",
    "\n",
    "def _get_layers_to_unfreeze(cfg): \n",
    "    \"\"\"Return a list that determines which layers should be kept unfrozen\"\"\"\n",
    "    if cfg.pp_name   == \"tuner007/pegasus_paraphrase\":               \n",
    "        unfreeze_layer_list,last_layer_num = ['decoder.layer_norm'],          15\n",
    "    elif cfg.pp_name == \"tdopierre/ProtAugment-ParaphraseGenerator\": \n",
    "        unfreeze_layer_list,last_layer_num = ['decoder.layernorm_embedding'],  5\n",
    "    elif cfg.pp_name == \"eugenesiow/bart-paraphrase\":                \n",
    "        unfreeze_layer_list,last_layer_num = ['decoder.layernorm_embedding'], 11\n",
    "    for i in range(last_layer_num, last_layer_num-cfg.unfreeze_last_n_layers, -1): \n",
    "        unfreeze_layer_list.append(f'decoder.layers.{i}')\n",
    "    # self.lm_head is tied (the same parameter as) to self.encoder.embed_tokens and self.decoder.embed_tokens.\n",
    "    # and this is given by shared.weight\n",
    "    # From here: https://github.com/huggingface/transformers/issues/10479#issuecomment-788964822\n",
    "    unfreeze_layer_list.append('shared.weight')\n",
    "    return unfreeze_layer_list\n",
    "\n",
    "def pp_model_freeze_layers(cfg, pp_model): \n",
    "    \"\"\"Freeze all layers of pp_model except the last few decoder layers (determined by cfg.unfreeze_last_n_layers), \n",
    "    the final layer_norm layer, and the linear head (which is tied to the input embeddings). \"\"\"\n",
    "    if cfg.unfreeze_last_n_layers == \"all\":\n",
    "        for i, (name, param) in enumerate(pp_model.named_parameters()): param.requires_grad = True\n",
    "    else: \n",
    "        unfreeze_layer_list = _get_layers_to_unfreeze(cfg)\n",
    "        for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): \n",
    "            if np.any([o in name for o in unfreeze_layer_list]):   param.requires_grad = True\n",
    "            else:                                                  param.requires_grad = False\n",
    "    return pp_model\n",
    "\n",
    "\n",
    "def save_pp_model(pp_model, optimizer, path): \n",
    "    \"\"\"Save training state (for both pp_model and optimiser) as a checkpoint at a given epoch. \"\"\"\n",
    "    torch.save({'pp_model_state_dict': pp_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()}, path)\n",
    "    \n",
    "def resume_pp_model(pp_model, optimizer, path): \n",
    "    \"\"\"Replace the training state with a saved checkpoint.. Reinitialises both pp_model and optimiser state. \"\"\"\n",
    "    state = torch.load(path)\n",
    "    pp_model.load_state_dict( state['pp_model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    return pp_model, optimizer\n",
    "\n",
    "def get_vm_probs(text, cfg, vm_tokenizer, vm_model, return_predclass=False): \n",
    "    \"\"\"Get victim model predictions for a batch of text. \n",
    "    Used in data cleaning and by the reward_fn to get vm_score.\"\"\"\n",
    "    if vm_model.training: vm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        tkns = vm_tokenizer(text, truncation=True, padding=True, pad_to_multiple_of=cfg.orig_padding_multiple,\n",
    "                            return_tensors=\"pt\").to(cfg.device)\n",
    "        logits = vm_model(**tkns).logits\n",
    "        probs = torch.softmax(logits,1)\n",
    "        if return_predclass:    return probs, torch.argmax(probs,1)\n",
    "        else:                   return probs\n",
    "\n",
    "def get_optimizer(cfg, pp_model):  return torch.optim.AdamW(pp_model.parameters(), lr=cfg.lr)\n",
    "\n",
    "# def get_start_end_special_token_ids(tokenizer): \n",
    "#     \"\"\"Return a dict indicating the token id's that input/output sequences should start and end with.\"\"\"\n",
    "#     d = {}\n",
    "#     if tokenizer.name_or_path in ['eugenesiow/bart-paraphrase', 'tdopierre/ProtAugment-ParaphraseGenerator']: \n",
    "#         d[\"input_start_id\"] =  tokenizer.bos_token_id\n",
    "#         d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "#         d[\"output_start_id\"] =  tokenizer.eos_token_id \n",
    "#         d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "#     elif tokenizer.name_or_path == \"tuner007/pegasus_paraphrase\":\n",
    "#         d[\"input_start_id\"] =  None\n",
    "#         d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "#         d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "#         d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "#     elif tokenizer.name_or_path in [\"prithivida/parrot_paraphraser_on_T5\", \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"]:\n",
    "#         d[\"input_start_id\"] =  None\n",
    "#         d[\"input_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id] \n",
    "#         d[\"output_start_id\"] =  tokenizer.pad_token_id\n",
    "#         d[\"output_end_id\"] =  [tokenizer.pad_token_id, tokenizer.eos_token_id]\n",
    "#     else: \n",
    "#         raise Exception(\"unrecognised tokenizer\")\n",
    "#     return d\n",
    "\n",
    "def get_nli_probs(orig_l, pp_l, cfg, nli_tokenizer, nli_model): \n",
    "    inputs = nli_tokenizer(orig_l, pp_l, return_tensors=\"pt\", padding=True, truncation=True).to(cfg.device)\n",
    "    with torch.no_grad():\n",
    "        logits = nli_model(**inputs).logits\n",
    "        probs = logits.softmax(1)\n",
    "    return probs \n",
    "\n",
    "def get_cola_probs(pp_l, cfg, cola_tokenizer, cola_model): \n",
    "    inputs = cola_tokenizer(pp_l, return_tensors=\"pt\", padding=True, truncation=True).to(cfg.device)\n",
    "    with torch.no_grad():\n",
    "        logits = cola_model(**inputs).logits\n",
    "        probs = logits.softmax(1)\n",
    "    return probs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function is `prepare_models(cfg)`. This gives all the models and tokenizers needed for the other sections and also updates + returns the config. It takes care of embedding layer resizing and layer freezing too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "vm_tokenizer, vm_model, pp_tokenizer, pp_model, ref_pp_model, sts_model, \\\n",
    "    nli_tokenizer, nli_model, cola_tokenizer, cola_model, cfg = prepare_models(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_t(probs[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving model checkpoints  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed you can save and reload models from a checkpoint with `save_pp_model()` and `resume_pp_model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do layer freezing on the pp model use the `pp_model_freeze_layers()` function. This will freeze all layers except:  \n",
    "\n",
    "* the last `cfg.unfreeze_last_n_layers` layers of the decoder.\n",
    "* the final layernorm layer \n",
    "* the LM head (which is tied to the input embeddings) which is given by `shared.weight`. \n",
    "\n",
    "At the moment I'm not certain on if I should be unfreezing the linear head and the layernorm layers or leaving them frozen. I am erring on the side of unfreezing and making them trainable).  \n",
    "\n",
    "To freeze all layers except the last few, set the number of layers to unfreeze by assigning cfg.unfreeze_last_n_layers to an int. To unfreeze the whole model, set `cfg.unfreeze_last_n_layers = \"all\"` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "cfg.pp_name  = \"tuner007/pegasus_paraphrase\"\n",
    "cfg.unfreeze_last_n_layers = 3\n",
    "_, pp_model = _prepare_pp_tokenizer_and_model(cfg)\n",
    "pp_model = pp_model_freeze_layers(cfg, pp_model)\n",
    "print(\"Number of decoder layers to unfreeze:\", cfg.unfreeze_last_n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which layers are frozen and unfrozen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test too that we leave the LM head unfrozen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):     assert param.requires_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with no layer freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.unfreeze_last_n_layers = \"all\"\n",
    "pp_model = pp_model_freeze_layers(cfg, pp_model)\n",
    "for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): assert param.requires_grad\n",
    "for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):    assert param.requires_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also test this works with the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.unfreeze_last_n_layers = 2\n",
    "# for name in [\"tdopierre/ProtAugment-ParaphraseGenerator\", \"eugenesiow/bart-paraphrase\"]: \n",
    "#     cfg = Config()\n",
    "#     cfg.pp_name  = name\n",
    "#     _, pp_model = _prepare_pp_tokenizer_and_model(cfg)\n",
    "#     pp_model = pp_model_freeze_layers(cfg, pp_model)\n",
    "#     print(\"Number of decoder layers to unfreeze:\", cfg.unfreeze_last_n_layers)\n",
    "#     for i, (name, param) in enumerate(pp_model.base_model.named_parameters()): print(i, name, param.requires_grad)\n",
    "#     for i, (name, param) in enumerate(pp_model.lm_head.named_parameters()):     assert param.requires_grad \n",
    "#     print(\"\\n#################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NLI models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "cfg.nli_name = \"microsoft/deberta-base-mnli\"\n",
    "nli_tokenizer, nli_model = _prepare_nli_tokenizer_and_model(cfg)\n",
    "cfg = Config()\n",
    "cfg.nli_name = \"howey/electra-small-mnli\"\n",
    "nli_tokenizer_small, nli_model_small = _prepare_nli_tokenizer_and_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.  ]\n",
      " [0.   0.02 0.98]\n",
      " [0.17 0.76 0.07]\n",
      " [1.   0.   0.  ]\n",
      " [0.   0.   1.  ]]\n",
      "[[0.   0.01 0.99]\n",
      " [0.28 0.04 0.68]\n",
      " [0.28 0.69 0.03]\n",
      " [0.15 0.02 0.83]\n",
      " [0.97 0.01 0.02]]\n"
     ]
    }
   ],
   "source": [
    "orig_l = [\"[stephen] earnhart's film is more about the optimism of a group of people who are struggling to give themselves a better lot in life than the ones they currently have .\",    \"the irwins' scenes are fascinating ; the movie as a whole is cheap junk and an insult to their death-defying efforts .\", 'the story , touching though it is , does not quite have enough emotional resonance or variety of incident to sustain a feature , and even at 85 minutes it feels a bit long .',      'starts out strongly before quickly losing its focus , point and purpose in a mess of mixed messages , over-blown drama and bruce willis with a scar .', 'a guilty pleasure at best , and not worth seeing unless you want to laugh at it .']\n",
    "pp_l =   [\"[stephen] earnhart's rant is more about the frustration of a group of people who are struggling to give themselves a better lot in life than the ones they currently have .\", \"the irwins' scenes are fascinating ; the movie as a whole is cheap fun and an insult to their death-defying efforts .\",  'the story , touching though it is , does not simply have enough emotional resonance or variety of incident to sustain a feature , and yet at 85 minutes it feels a lifetime long .', 'starts out strongly before quickly establishing its focus , point and purpose in a tale of mixed messages , fiery drama and bruce willis with a scar .', 'a guilty pleasure at best , and not worth seeing but you want to laugh at it .']\n",
    "\n",
    "probs = get_nli_probs(orig_l, pp_l, cfg, nli_tokenizer, nli_model)\n",
    "probs_small = get_nli_probs(orig_l, pp_l, cfg, nli_tokenizer_small, nli_model_small)\n",
    "print(round_t(probs))\n",
    "print(round_t(probs_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.  ]\n",
      " [1.   0.   0.  ]\n",
      " [1.   0.   0.  ]\n",
      " [0.98 0.02 0.01]]\n",
      "[[0.   0.01 0.99]\n",
      " [0.   0.   0.99]\n",
      " [0.   0.01 0.99]\n",
      " [0.01 0.03 0.96]]\n"
     ]
    }
   ],
   "source": [
    "orig_l = [\"I like this movie\", \"This banana is nice\", \"The play was frustrating to watch.\", \"The wall is red\"]\n",
    "pp_l = [\"I do not like this movie\", \"This banana is nasty\", \"The play was not that frustrating to watch.\", \"The pavement is red\" ]\n",
    "probs = get_nli_probs(orig_l, pp_l, cfg, nli_tokenizer, nli_model)\n",
    "probs_small = get_nli_probs(orig_l, pp_l, cfg, nli_tokenizer_small, nli_model_small)\n",
    "print(round_t(probs))\n",
    "print(round_t(probs_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 02_tests.ipynb.\n",
      "Converted 03_config.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 20_trainer.ipynb.\n",
      "Converted 25_insights.ipynb.\n",
      "Converted 50_baseline_attacks.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted baselines.ipynb.\n",
      "Converted baselines_analysis.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted pp_eval_baselines.ipynb.\n",
      "Converted run.ipynb.\n",
      "Converted show_examples.ipynb.\n",
      "Converted statistical_tests.ipynb.\n",
      "Converted test_grammar_options.ipynb.\n",
      "Converted test_pp_model.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
