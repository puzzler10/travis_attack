{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pprint import pprint\n",
    "import torch\n",
    "# \"coderpotter/T5-for-Adversarial-Paraphrasing\"\n",
    "# ok, few artifacts in the data, seemed not terrible, 850MB\n",
    "\n",
    "#\"shrishail/t5_paraphrase_msrp_paws\"\n",
    "# 250MB, seemed a bit weaker\n",
    "\n",
    "# \"ramsrigouthamg/t5_sentence_paraphraser\"\n",
    "# not bad, 850MB, seemed pretty reasonable for most things. \n",
    "\n",
    "# \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# you know what, i think this is one of the better ones I've seen.  850MB \n",
    "\n",
    "# \"ceshine/t5-paraphrase-quora-paws\"\n",
    "# honestly not bad but its not as good as the parrot paraphraser. \n",
    "\n",
    "# \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "# This model is huge - 2.75 GB. \n",
    "# But it has the best paraphrases, for sure. \n",
    "\n",
    "# Conclusion\n",
    "# For testing, seeing what's good  - go with \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# For when you finally do it, go with \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_l = [\n",
    " \"this 72-minute film does have some exciting scenes , but it's a tad slow .\",\n",
    " 'a very average science fiction film .',\n",
    " \"it doesn't matter that the film is less than 90 minutes . it still feels like a prison stretch .\",\n",
    " 'hardly a masterpiece , but it introduces viewers to a good charitable enterprise and some interesting real people .',\n",
    " 'the good girl is a film in which the talent is undeniable but the results are underwhelming .',\n",
    " 'the stories here suffer from the chosen format .',\n",
    " 'lame sweet home leaves no southern stereotype unturned .',\n",
    " \"funny , sexy , devastating and incurably romantic .\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "prefix = \"paraphrase: \"\n",
    "text = [prefix + sen + \" </s>\" for sen in orig_l]\n",
    "encoding = tokenizer(orig_l, padding=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_eval_seq = 48\n",
    "model_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "   # num_beams = n_eval_seq ,\n",
    "    min_length=0,\n",
    "    max_length=48, \n",
    "    num_return_sequences=n_eval_seq,\n",
    "    temperature=1.15,\n",
    "    top_p =0.95 , \n",
    "#     num_beam_groups = n_eval_seq,\n",
    "#     diversity_penalty=.,\n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "pp_l = tokenizer.batch_decode(model_output.sequences, skip_special_tokens=True)\n",
    "pp_l_nested = [pp_l[i:i+n_eval_seq] for i in range(0, len(pp_l), n_eval_seq)]  # put paraphrases in nested lists \n",
    "\n",
    "# pprint(orig_l, width = 200)\n",
    "# print()\n",
    "# pprint(pp_l, width=200)\n",
    "\n",
    "for orig, pp_l in zip(orig_l, pp_l_nested): \n",
    "    print(orig)\n",
    "    pprint(pp_l, width=200)\n",
    "    print(len(set(pp_l)))\n",
    "    print(len(set(pp_l)) / n_eval_seq)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "# outputs = []\n",
    "# for output in model_output:\n",
    "#     generated_sent = tokenizer.decode(\n",
    "#         output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "#     )\n",
    "#     if (\n",
    "#         generated_sent.lower() != sentence.lower()\n",
    "#         and generated_sent not in outputs\n",
    "#     ):\n",
    "#         outputs.append(generated_sent)\n",
    "# return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_paraphrases(sentence, prefix=\"paraphrase: \", n_predictions=5, top_k=120, max_length=256):\n",
    "        text = prefix + sentence + \" </s>\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\n",
    "            \"attention_mask\"\n",
    "        ].to(device)\n",
    "\n",
    "        model_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            top_p=0.98,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=n_predictions,\n",
    "        )\n",
    "\n",
    "        outputs = []\n",
    "        for output in model_output:\n",
    "            generated_sent = tokenizer.decode(\n",
    "                output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            if (\n",
    "                generated_sent.lower() != sentence.lower()\n",
    "                and generated_sent not in outputs\n",
    "            ):\n",
    "                outputs.append(generated_sent)\n",
    "        return outputs\n",
    "\n",
    "paraphrases = get_paraphrases(\"The actors pull out all the stops in nearly every scene , but to diminishing effect . the characters never change.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The actors pull out all the stops in almost every scene but with diminishing effect the characters never change.',\n",
       " 'In nearly every scene the actors put all the stops but in an unrelenting manner the characters never change.',\n",
       " \"The actors make all the stops in nearly every scene but to diminishing effect. the characters never change. ''\",\n",
       " 'The actors pull out all the stops in almost every scene but the characters never change.',\n",
       " \"The actors pull out nearly everything in almost every scene. but to a diminishing effect the characters never change. ''\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from travis_attack.models import get_vm_probs, _prepare_vm_tokenizer_and_model\n",
    "from travis_attack.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config().adjust_config_for_rotten_tomatoes_dataset()\n",
    "vm_tokenizer, vm_model = _prepare_vm_tokenizer_and_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'textattack/distilbert-base-uncased-rotten-tomatoes'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.vm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2675, 0.7325],\n",
       "        [0.0937, 0.9063]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"the film is quiet, threatening and unforgettable.\",\n",
    "        \"The film is quiet threatening and unforgettable is successful.\"\n",
    "       ]\n",
    "get_vm_probs(text, cfg, vm_tokenizer, vm_model, return_predclass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
