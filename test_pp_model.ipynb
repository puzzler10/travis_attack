{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# \"coderpotter/T5-for-Adversarial-Paraphrasing\"\n",
    "# ok, few artifacts in the data, seemed not terrible, 850MB\n",
    "\n",
    "#\"shrishail/t5_paraphrase_msrp_paws\"\n",
    "# 250MB, seemed a bit weaker\n",
    "\n",
    "# \"ramsrigouthamg/t5_sentence_paraphraser\"\n",
    "# not bad, 850MB, seemed pretty reasonable for most things. \n",
    "\n",
    "# \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# you know what, i think this is one of the better ones I've seen.  850MB \n",
    "\n",
    "# \"ceshine/t5-paraphrase-quora-paws\"\n",
    "# honestly not bad but its not as good as the parrot paraphraser. \n",
    "\n",
    "# \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "# This model is huge - 2.75 GB. \n",
    "# But it has the best paraphrases, for sure. \n",
    "\n",
    "# Conclusion\n",
    "# For testing, seeing what's good  - go with \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# For when you finally do it, go with \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/generation_beam_search.py:196: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the actors pull out all the stops in almost',\n",
       " 'actors pull out all the stops in almost every',\n",
       " 'in almost every scene the actors pull out all',\n",
       " 'actor snares pulls out',\n",
       " 'actors pull out the stops in almost every scene',\n",
       " 'acting is a constant occurrence.',\n",
       " 'The characters never change.',\n",
       " 'the actors in almost every scene do everything they',\n",
       " 'The book explores the seemingly irreconc',\n",
       " 'It explores the seemingly irreconcil',\n",
       " 'In a compassionate way explores the seemingly',\n",
       " 'This compassionately explores the seemingly irrecon',\n",
       " 'As an artist he compassionately explores',\n",
       " 'Judith Grisham explores the',\n",
       " 'A compassionately explored examines the seemingly irre',\n",
       " 'compassionately examines the seemingly irreconc',\n",
       " 'apart from anything else, this is one',\n",
       " 'this is one of the best-sus',\n",
       " \"it's one of the best-s\",\n",
       " 'i think this is one of the best',\n",
       " \"and it's one of the best-\",\n",
       " 'This is the best-sustained idea',\n",
       " 'one of the most thought-provoking ideas ',\n",
       " \"aside from everything else it's one of\",\n",
       " 'My name is Tom and I like driving cars',\n",
       " 'Tom and I like driving cars. My name',\n",
       " \"I'm a Tom and I like\",\n",
       " \"My name's Tom and I like driving\",\n",
       " 'Hello my name is Tom and I love to',\n",
       " 'my name is Tom and I love to drive',\n",
       " 'Hi I am Tom. My name is Tom',\n",
       " 'my Name is Thomas I like driving cars.',\n",
       " 'I like this apple.',\n",
       " 'I like this apple. I like it.',\n",
       " \"It's a good apple. I\",\n",
       " 'What apple I like this apple.',\n",
       " 'What apple I like this apple. I like',\n",
       " 'i like this apple.',\n",
       " 'I like the apple apple. I like the',\n",
       " 'i like this apple. i love']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_l = [\n",
    "    \"the actors pull out all the stops in nearly every scene , but to diminishing effect . the characters never change .\",\n",
    "    \"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\n",
    "    \"apart from anything else , this is one of the best-sustained ideas i have ever seen on the screen .\",\n",
    "    \"my name is Tom and I like driving cars\", \n",
    "    \"I like this apple\", \n",
    "]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "prefix = \"paraphrase: \"\n",
    "text = [prefix + sen + \" </s>\" for sen in orig_l]\n",
    "encoding = tokenizer(orig_l, padding=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# outputs = model.generate(input_ids=input_ids, attention_mask=attention_masks, num_beams = 8, num_return_sequences=3, max_length=256,\n",
    "#                          do_sample=True, top_p=120, temperature=1, return_dict_in_generate=True,\n",
    "#                                                      output_scores=True)\n",
    "\n",
    "model_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_masks,\n",
    "    do_sample=False,\n",
    "    num_beams=8,\n",
    "    diversity_penalty=3., \n",
    "    num_beam_groups=8,\n",
    "    max_length=10,\n",
    "    min_length=5,\n",
    "    num_return_sequences=8,\n",
    ")\n",
    "tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "\n",
    "# outputs = []\n",
    "# for output in model_output:\n",
    "#     generated_sent = tokenizer.decode(\n",
    "#         output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "#     )\n",
    "#     if (\n",
    "#         generated_sent.lower() != sentence.lower()\n",
    "#         and generated_sent not in outputs\n",
    "#     ):\n",
    "#         outputs.append(generated_sent)\n",
    "# return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_paraphrases(sentence, prefix=\"paraphrase: \", n_predictions=5, top_k=120, max_length=256):\n",
    "        text = prefix + sentence + \" </s>\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\n",
    "            \"attention_mask\"\n",
    "        ].to(device)\n",
    "\n",
    "        model_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            top_p=0.98,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=n_predictions,\n",
    "        )\n",
    "\n",
    "        outputs = []\n",
    "        for output in model_output:\n",
    "            generated_sent = tokenizer.decode(\n",
    "                output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            if (\n",
    "                generated_sent.lower() != sentence.lower()\n",
    "                and generated_sent not in outputs\n",
    "            ):\n",
    "                outputs.append(generated_sent)\n",
    "        return outputs\n",
    "\n",
    "paraphrases = get_paraphrases(\"The actors pull out all the stops in nearly every scene , but to diminishing effect . the characters never change.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The actors pull out the stops in nearly every scene. but to an diminishing effect. the characters never change.',\n",
       " 'In almost every scene the actors pull out all the stops but to an diminishing effect the characters never change.',\n",
       " \"the actors almost make everything work in almost every scene but to diminishing effect. the characters never change. ''\",\n",
       " 'The actors pull out all the stops in almost every scene but to diminishing effect. the characters never change.',\n",
       " 'The actors pull out all the stops in almost every scene, but with diminishing effect. characters never change.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
