{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# \"coderpotter/T5-for-Adversarial-Paraphrasing\"\n",
    "# ok, few artifacts in the data, seemed not terrible, 850MB\n",
    "\n",
    "#\"shrishail/t5_paraphrase_msrp_paws\"\n",
    "# 250MB, seemed a bit weaker\n",
    "\n",
    "# \"ramsrigouthamg/t5_sentence_paraphraser\"\n",
    "# not bad, 850MB, seemed pretty reasonable for most things. \n",
    "\n",
    "# \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# you know what, i think this is one of the better ones I've seen.  850MB \n",
    "\n",
    "# \"ceshine/t5-paraphrase-quora-paws\"\n",
    "# honestly not bad but its not as good as the parrot paraphraser. \n",
    "\n",
    "# \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "# This model is huge - 2.75 GB. \n",
    "# But it has the best paraphrases, for sure. \n",
    "\n",
    "# Conclusion\n",
    "# For testing, seeing what's good  - go with \"prithivida/parrot_paraphraser_on_T5\"\n",
    "# For when you finally do it, go with \"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the actors pull out almost every scene - for a diminishing effect the characters never change.',\n",
       " 'actors pull out all the stops in nearly every scene, but to diminishing effect the characters never change.',\n",
       " 'the actors in nearly every scene pull out everything but to diminishing effect the characters never change from their own.',\n",
       " 'actors pulled out all stops in nearly every scene, but to diminishing effect. characters never changed.',\n",
       " 'In compassionately exploring the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children he writ large.',\n",
       " 'A compassionately explored approach explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children.',\n",
       " 'compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children.',\n",
       " 'compassionately examines the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children.',\n",
       " 'this is one of the best-planned ideas i have ever seen.',\n",
       " \"it is the best idea i've seen on screen for awhile now.\",\n",
       " 'from anything else, this is one of the most coherent ideas i have ever seen on the screen.',\n",
       " 'this is one of the most unique and stable ideas i have ever seen on the screen.',\n",
       " \"I'm Tom and I like driving cars.\",\n",
       " 'My name is Tom and I love driving cars because I drive them at 60mph.',\n",
       " 'My name is Tom and I like driving cars.',\n",
       " 'My name is Tom and I like driving cars. :. My name is MC and I like to drive any automobile.',\n",
       " 'I like this apple.',\n",
       " 'I like this apple.',\n",
       " 'I like this apple apple.',\n",
       " 'I like this apple pie.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_l = [\n",
    "    \"the actors pull out all the stops in nearly every scene , but to diminishing effect . the characters never change .\",\n",
    "    \"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\n",
    "    \"apart from anything else , this is one of the best-sustained ideas i have ever seen on the screen .\",\n",
    "    \"my name is Tom and I like driving cars\", \n",
    "    \"I like this apple\", \n",
    "]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "prefix = \"paraphrase: \"\n",
    "text = [prefix + sen + \" </s>\" for sen in orig_l]\n",
    "encoding = tokenizer(orig_l, padding=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# outputs = model.generate(input_ids=input_ids, attention_mask=attention_masks, num_beams = 8, num_return_sequences=3, max_length=256,\n",
    "#                          do_sample=True, top_p=120, temperature=1, return_dict_in_generate=True,\n",
    "#                                                      output_scores=True)\n",
    "\n",
    "model_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_masks,\n",
    "    do_sample=True,\n",
    "    max_length=60,\n",
    "    min_length=5,\n",
    "    top_k=120,\n",
    "    top_p=0.98,\n",
    " #   early_stopping=True,\n",
    "    num_return_sequences=4,\n",
    ")\n",
    "tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "\n",
    "# outputs = []\n",
    "# for output in model_output:\n",
    "#     generated_sent = tokenizer.decode(\n",
    "#         output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "#     )\n",
    "#     if (\n",
    "#         generated_sent.lower() != sentence.lower()\n",
    "#         and generated_sent not in outputs\n",
    "#     ):\n",
    "#         outputs.append(generated_sent)\n",
    "# return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     8, 10485,  3197,    91,   966,   334,  3112,     3,    18,\n",
       "            21,     3,     9, 26999,    53,  1504,     8,  2850,   470,   483,\n",
       "             3,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0, 10485,  3197,    91,    66,     8, 10796,    16,  2111,   334,\n",
       "          3112,     3,     6,    68,    12, 26999,    53,  1504,     8,  2850,\n",
       "           470,   483,     3,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     8, 10485,    16,  2111,   334,  3112,  3197,    91,   762,\n",
       "            68,    12, 26999,    53,  1504,     8,  2850,   470,   483,    45,\n",
       "            70,   293,     3,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0, 10485,  6756,    91,    66, 10796,    16,  2111,   334,  3112,\n",
       "             3,     6,    68,    12, 26999,    53,  1504,     3,     5,  2850,\n",
       "           470,  2130,     3,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    86, 21801,   120,  6990,     8, 13045, 19598,  1018,    75,\n",
       "           173,   179,  1419,   344, 11252,     3, 15294,    23,   152,  1362,\n",
       "            11,    70,   259,  5517,    26, 16998,    11,   110, 12032,   502,\n",
       "             3,    88,     3,   210,    52,   155,   508,     3,     5,     1],\n",
       "        [    0,    71, 21801,   120, 15883,  1295,  2075,     7,     8, 13045,\n",
       "         19598,  1018,    75,   173,   179,  1419,   344, 11252,     3, 15294,\n",
       "            23,   152,  1362,    11,    70,   259,  5517,    26, 16998,    11,\n",
       "           110, 12032,   502,     3,     5,     1,     0,     0,     0,     0],\n",
       "        [    0, 21801,   120,  2075,     7,     8, 13045, 19598,  1018,    75,\n",
       "           173,   179,  1419,   344, 11252,     3, 15294,    23,   152,  1362,\n",
       "            11,    70,   259,  5517,    26, 16998,    11,   110, 12032,   502,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0, 21801,   120,  5443,     7,     8, 13045, 19598,  1018,    75,\n",
       "           173,   179,  1419,   344, 11252,     3, 15294,    23,   152,  1362,\n",
       "            11,    70,   259,  5517,    26, 16998,    11,   110, 12032,   502,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    48,    19,    80,    13,     8,   200,    18, 28379,   912,\n",
       "             3,    23,    43,   664,   894,     5,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    34,    19,     8,   200,   800,     3,    23,    31,   162,\n",
       "           894,    30,  1641,    21, 22546,   230,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    45,   959,  1307,     3,     6,    48,    19,    80,    13,\n",
       "             8,   167, 28911,   912,     3,    23,    43,   664,   894,    30,\n",
       "             8,  1641,     3,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    48,    19,    80,    13,     8,   167,   775,    11,  5711,\n",
       "           912,     3,    23,    43,   664,   894,    30,     8,  1641,     5,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    27,    31,    51,  3059,    11,    27,   114,  2191,  2948,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   499,   564,    19,  3059,    11,    27,   333,  2191,  2948,\n",
       "           250,    27,  1262,   135,    44,  1640,  7656,     5,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   499,   564,    19,  3059,    11,    27,   114,  2191,  2948,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   499,   564,    19,  3059,    11,    27,   114,  2191,  2948,\n",
       "             5,     3,    10,     5,   499,   564,    19,     3,  3698,    11,\n",
       "            27,   114,    12,  1262,   136,  8395,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    27,   114,    48,  8947,     5,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    27,   114,    48,  8947,     5,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    27,   114,    48,  8947,  8947,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,    27,   114,    48,  8947,  6253,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tproth/Programs/miniconda/envs/nlp_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_paraphrases(sentence, prefix=\"paraphrase: \", n_predictions=5, top_k=120, max_length=256):\n",
    "        text = prefix + sentence + \" </s>\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\n",
    "            \"attention_mask\"\n",
    "        ].to(device)\n",
    "\n",
    "        model_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            top_p=0.98,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=n_predictions,\n",
    "        )\n",
    "\n",
    "        outputs = []\n",
    "        for output in model_output:\n",
    "            generated_sent = tokenizer.decode(\n",
    "                output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            if (\n",
    "                generated_sent.lower() != sentence.lower()\n",
    "                and generated_sent not in outputs\n",
    "            ):\n",
    "                outputs.append(generated_sent)\n",
    "        return outputs\n",
    "\n",
    "paraphrases = get_paraphrases(\"The actors pull out all the stops in nearly every scene , but to diminishing effect . the characters never change.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The actors pull out everything in almost every scene but to an infinite effect the characters never change.',\n",
       " 'In nearly every scene the actors pull out all the stops but to diminishing effects their characters never change.',\n",
       " 'The actors pull out everything in nearly every scene but with diminishing effect. the characters never change.',\n",
       " 'The actors pull out all stops in nearly every scene but to diminishing effect. the characters never change.',\n",
       " 'The actors pull out all the stops in nearly every scene but to a declining effect. The characters never change.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
