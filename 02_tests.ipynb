{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains common code used to test different things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def check_no_nans_or_infs(x):\n",
    "    assert torch.all(~torch.isnan(x))\n",
    "    assert torch.all(~torch.isneginf(x))\n",
    "    assert torch.all(~torch.isposinf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "def check_parameters_update(dl): \n",
    "    \"\"\"\n",
    "    This checks which parameters are being updated. \n",
    "    We run one forward pass+backward pass (updating the parameters once) \n",
    "    and look at which ones change. \n",
    "    \"\"\"\n",
    "    # Check which parameters should be updated\n",
    "    params_with_grad = [o for o in pp_model.named_parameters() if o[1].requires_grad]\n",
    "    print(\"---- Parameters with 'requires_grad' and their sizes ------\")\n",
    "    for (name, p) in params_with_grad:  print(name, p.size())\n",
    "        \n",
    "    ## Take a step and see which weights update\n",
    "    params_all = [o for o in pp_model.named_parameters()]  # this is updated by a training step    \n",
    "    params_all_initial = [(name, p.clone()) for (name, p) in params_all]  # Initial values\n",
    "        \n",
    "    # take a step    \n",
    "    loss, reward, pp_logp = training_step(data)\n",
    "    \n",
    "    print(\"\\n---- Matrix norm of parameter update for one step ------\\n\")\n",
    "    for (_,old_p), (name, new_p) in zip(params_all_initial, params_all): \n",
    "        print (name, torch.norm(new_p - old_p).item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_info_on_generated_text():\n",
    "    \"\"\"\n",
    "        Prints a bunch of statistics around the generated text. Useful for debugging purposes.\n",
    "        So far only works for greedy search.\n",
    "        OUTDATED OUTDATED\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n######################################################################\\n\")\n",
    "    logger.info(f\"Original text: {text}\")\n",
    "    tgt_text = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=True)\n",
    "    tgt_text_with_tokens = pp_tokenizer.batch_decode(translated.sequences, skip_special_tokens=False)\n",
    "    logger.info(f\"Generated text: {tgt_text}\")\n",
    "    logger.info(f\"Generated text with special tokens: {tgt_text_with_tokens}\")\n",
    "    logger.info(f\"Shape of translated.sequences:{translated.sequences.shape}\")\n",
    "    logger.info(f\"translated.sequences:{translated.sequences}\")\n",
    "    logger.info(f\"Scores is a tuple of length {len(translated.scores)} \\\n",
    "    and each score is a tensor of shape {translated.scores[0].shape}\")\n",
    "    scores_stacked = torch.stack(translated.scores, 1)\n",
    "    logger.info(f\"Stacking the scores into a tensor of shape {scores_stacked.shape}\")\n",
    "    scores_softmax = torch.softmax(scores_stacked, 2)\n",
    "    logger.info(f\"Now taking softmax. This shouldn't change the shape, but just to check,\\\n",
    "    its shape is {scores_softmax.shape}\")\n",
    "    probsums = scores_softmax.sum(axis=2)\n",
    "    logger.info(f\"These are probabilities now and so they should all sum to 1 (or close to it) in the axis \\\n",
    "    corresponding to each time step. We can check the sums here: {probsums}, but it's a long tensor \\\n",
    "    of shape {probsums.shape} and hard to see, so summing over all these values and removing 1 \\\n",
    "    from each gives {torch.sum(probsums - 1)} \\\n",
    "    which should be close to 0.\")\n",
    "    seq_without_first_tkn = translated.sequences[:, 1:]\n",
    "    logger.info(\"Now calculating sequence probabilities\")\n",
    "    seq_token_probs = torch.gather(scores_softmax,2,seq_without_first_tkn[:,:,None]).squeeze(-1)\n",
    "    seq_prob = seq_token_probs.prod(-1).item()\n",
    "    logger.info(f\"Sequence probability: {seq_prob}\")\n",
    "\n",
    "    # Get the 2nd and 3rd most likely tokens at each st\n",
    "    topk_ids = torch.topk(scores_softmax,3,dim=2).indices[:,:,1:]\n",
    "    topk_tokens_probs = torch.gather(scores_softmax,2,topk_ids).squeeze(-1)\n",
    "    toks2 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,0].squeeze())\n",
    "    toks3 = pp_tokenizer.convert_ids_to_tokens(topk_ids[:,:,1].squeeze())\n",
    "    tok_probs2 = topk_tokens_probs[:,:,0].squeeze()\n",
    "    tok_probs3 = topk_tokens_probs[:,:,1].squeeze()\n",
    "\n",
    "    logger.info(f\"Probabilities of getting the top 3 tokens at each step:\")\n",
    "    tokens = pp_tokenizer.convert_ids_to_tokens(seq_without_first_tkn.squeeze())\n",
    "    for (p, t, p2,t2,p3,t3)  in zip(seq_token_probs.squeeze(), tokens, tok_probs2, toks2, tok_probs3, toks3): \n",
    "        logger.info(f\"{t}: {round(p.item(),3)}  {t2}: {round(p2.item(),3)}  {t3}: {round(p3.item(),3)}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
